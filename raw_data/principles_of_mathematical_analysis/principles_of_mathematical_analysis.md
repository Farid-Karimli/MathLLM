INTERNATIONAL SERIES IN PURE AND APPLIED MATHEMATICS

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-001.jpg?height=51&width=236&top_left_y=465&top_left_x=1015)

PRINCIPLES OF

MATHIEMATICAL ANALYSIS

PRINCIPLES OF

MATHEMATICAL ANALYSIS

# INTERNATIONAL SERIES IN PURE AND APPLIED MATHEMATICS 

William Ted Martin, E. H. Spanier, G. Springer and<br>P. J. Davis. Consulting Editors

AHLFORS: Complex Analysis<br>BucK: Advanced Calculus<br>BUSACKER AND SAATY: Finite Graphs and Networks<br>CHENEY: Introduction to Approximation Theory<br>CHESTER: Techniques in Partial Differential Equations<br>CODDINGTON AND LEVINSON: Theory of Ordinary Differential Equations<br>CONTE AND DE Boor: Elementary Numerical Analysis: An Algorithmic Approach<br>DenNemeYer: Introduction to Partial Differential Equations and Boundary Value<br>Problems<br>Dettman: Mathematical Methods in Physics and Engineering<br>Golomb AND Shanks: Elements of Ordinary Differential Equations<br>GREENSPAN: Introduction to Partial Differential Equations<br>Hamming: Numerical Methods for Scientists and Engineers<br>HiLDEBRAND: Introduction to Numerical Analysis<br>HouseHOLDER: The Numerical Treatment of a Single Nonlinear Equation<br>Kalman, FalB, AND ArBib: Topics in Mathematical Systems Theory<br>LAss: Vector and Tensor Analysis<br>McCARTY: Topology: An Introduction with Applications to Topological Groups<br>MoNK: Introduction to Set Theory<br>MOORE: Elements of Linear Algebra and Matrix Theory<br>Mostow AND SAMPSON: Linear Algebra<br>MOURSUND AND DuRIs: Elementary Theory and Application of Numerical Analysis<br>Pearl: Matrix Theory and Finite Mathematics<br>Pipes and Harvill: Applied Mathematics for Engineers and Physicists<br>RALSTON: A First Course in Numerical Analysis<br>RITGER AND Rose: Differential Equations with Applications<br>RiTT: Fourier Series<br>RuDIN: Principles of Mathematical Analysis<br>SHAPIRO: Introduction to Abstract Algebra<br>Simmons: Differential Equations with Applications and Historical Notes<br>Simmons: Introduction to Topology and Modern Analysis<br>SNEDDON: Elements of Partial Differential Equations<br>STRUBLE: Nonlinear Differential Equations

McGraw-Hill, Inc.

New York St. Louis San Francisco Auckland Bogot√°

Caracas Lisbon London Madrid Mexico City Milan

Montreal New Delhi San Juan Singapore

Sydney Tokyo Toronto

WALTER RUDIN

Professor of Mathematics

University of Wisconsin-Madison

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-004.jpg?height=461&width=870&top_left_y=2060&top_left_x=974)

THIRD EDITION

This book was set in Times New Roman.

The editors were A. Anthony Arthur and Shelly Levine Langman;

the production supervisor was Leroy A. Young.

R. R. Donnelley \& Sons Company was printer and binder.

This book is printed on acid-free paper.

## Library of Congress Cataloging in Publication Data

Rudin, Walter, date

Principles of mathematical analysis.

(International series in pure and applied mathematics)

Bibliography:

Includes index.

1. Mathematical analysis. I. Title.

QA300.R8 1976

ISBN 0-07-054235-X

$515 \quad 75-17903$

## PRINCIPLES OF MATHEMATICAL ANALYSIS

Copyright (C) 1964, 1976 by McGraw-Hill, Inc. All rights reserved. Copyright 1953 by McGraw-Hill, Inc. All rights reserved.

Printed in the United States of America. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of the publisher.

## CONTENTS

Preface $\quad$ ix

Chapter 1 The Real and Complex Number Systems 1

Introduction 1

Ordered Sets 3

Fields $\quad 5$

The Real Field $\quad 8$

The Extended Real Number System 11

The Complex Field $\quad 12$

Euclidean Spaces $\quad 16$

Appendix $\quad 17$

$\begin{array}{ll}\text { Exercises } & 21\end{array}$

Chapter 2 Basic Topology 24

Finite, Countable, and Uncountable Sets 24

Metric Spaces 30

Compact Sets $\quad 36$

Perfect Sets 41

Connected Sets $\quad 42$

Exercises 43

Chapter 3 Numerical Sequences and Series 47

Convergent Sequences $\quad 47$

Subsequences $\quad 51$

Cauchy Sequences $\quad 52$

Upper and Lower Limits $\quad 55$

Some Special Sequences $\quad 57$

Series $\quad 58$

Series of Nonnegative Terms $\quad 61$

The Number $e \quad 63$

The Root and Ratio Tests $\quad 65$

Power Series $\quad 69$

Summation by Parts $\quad 70$

Absolute Convergence $\quad 71$

Addition and Multiplication of Series $\quad 72$

Rearrangements $\quad 75$

$\begin{array}{ll}\text { Exercises } & 78\end{array}$

$\begin{array}{lll}\text { Chapter } 4 & \text { Continuity } & 83\end{array}$

Limits of Functions 83

Continuous Functions $\quad 85$

Continuity and Compactness $\quad 89$

Continuity and Connectedness $\quad 93$

Discontinuities 94

Monotonic Functions 95

Infinite Limits and Limits at Infinity $\quad 97$

Exercises 98

Chapter 5 Differentiation 103

The Derivative of a Real Function 103

Mean Value Theorems $\quad 107$

The Continuity of Derivatives $\quad 108$

L'Hospital's Rule 109

Derivatives of Higher Order $\quad 110$

Taylor's Theorem $\quad 110$

Differentiation of Vector-valued Functions 111

Exercises $\quad 114$

Chapter 6 The Riemann-Stieltjes Integral $\quad 120$

Definition and Existence of the Integral $\quad 120$

Properties of the Integral $\quad 128$

Integration and Differentiation 133

Integration of Vector-valued Functions 135

Rectifiable Curves $\quad 136$

Exercises 138

Chapter 7 Sequences and Series of Functions. 143

Discussion of Main Problem 143

Uniform Convergence $\quad 147$

Uniform Convergence and Continuity 149

Uniform Convergence and Integration 151

Uniform Convergence and Differentiation 152

Equicontinuous Families of Functions 154

The Stone-Weierstrass Theorem 159

Exercises $\quad 165$

Chapter 8 Some Special Functions 172

Power Series $\quad 172$

The Exponential and Logarithmic Functions 178

The Trigonometric Functions $\quad 182$

The Algebraic Completeness of the Complex Field 184

Fourier Series 185

The Gamma Function $\quad 192$

Exercises 196

Chapter 9 Functions of Several Variables 204

Linear Transformations $\quad 204$

Differentiation 211

The Contraction Principle $\quad 220$

The Inverse Function Theorem 221

The Implicit Function Theorem 223

The Rank Theorem $\quad 228$

Determinants 231

Derivatives of Higher Order 235

Differentiation of Integrals $\quad 236$

Exercises 239

Chapter 10 Integration of Differential Forms 245

Integration 245
Primitive Mappings ..... 248
Partitions of Unity ..... 251
Change of Variables ..... 252
Differential Forms ..... 253
Simplexes and Chains ..... 266
Stokes' Theorem ..... 273
Closed Forms and Exact Forms ..... 275
Vector Analysis ..... 280
Exercises ..... 288
Chapter 11 The Lebesgue Theory ..... 300
Set Functions ..... 300
Construction of the Lebesgue Measure ..... 302
Measure Spaces ..... 310
Measurable Functions ..... 310
Simple Functions ..... 313
Integration ..... 314
Comparison with the Riemann Integral ..... 322
Integration of Complex Functions ..... 325
Functions of Class $\mathscr{L}^{2}$ ..... 325
Exercises ..... 332
Bibliography ..... 335
List of Special Symbols ..... 337
Index ..... 339

This book is intended to serve as a text for the course in analysis that is usually taken by advanced undergraduates or by first-year students who study mathematics.

The present edition covers essentially the same topics as the second one, with some additions, a few minor omissions, and considerable rearrangement. I hope that these changes will make the material more accessible amd more attractive to the students who take such a course.

Experience has convinced me that it is pedagogically unsound (though logically correct) to start off with the construction of the real numbers from the rational ones. At the beginning, most students simply fail to appreciate the need for doing this. Accordingly, the real number system is introduced as an ordered field with the least-upper-bound property, and a few interesting applications of this property are quickly made. However, Dedekind's construction is not omitted. It is now in an Appendix to Chapter 1, where it may be studied and enjoyed whenever the time seems ripe.

The material on functions of several variables is almost completely rewritten, with many details filled in, and with more examples and more motivation. The proof of the inverse function theorem-the key item in Chapter 9-is
simplified by means of the fixed point theorem about contraction mappings. Differential forms are discussed in much greater detail. Several applications of Stokes' theorem are included.

As regards other changes, the chapter on the Riemann-Stieltjes integral has been trimmed a bit, a short do-it-yourself section on the gamma function has been added to Chapter 8 , and there is a large number of new exercises, most of them with fairly detailed hints.

I have also included several references to articles appearing in the American Mathematical Monthly and in Mathematics Magazine, in the hope that students will develop the habit of looking into the journal literature. Most of these references were kindly supplied by R. B. Burckel.

Over the years, many people, students as well as teachers, have sent me corrections, criticisms, and other comments concerning the previous editions of this book. I have appreciated these, and I take this opportunity to express my sincere thanks to all who have written me.

WALTER RUDIN

## THE REAL AND COMPLEX NUMBER SYSTEMS

## INTRODUCTION

A satisfactory discussion of the main concepts of analysis (such as convergence, continuity, differentiation, and integration) must be based on an accurately defined number concept. We shall not, however, enter into any discussion of the axioms that govern the arithmetic of the integers, but assume familiarity with the rational numbers (i.e., the numbers of the form $m / n$, where $m$ and $n$ are integers and $n \neq 0$ ).

The rational number system is inadequate for many purposes, both as a field and as an ordered set. (These terms will be defined in Secs. 1.6 and 1.12.) For instance, there is no rational $p$ such that $p^{2}=2$. (We shall prove this presently.) This leads to the introduction of so-called "irrational numbers" which are often written as infinite decimal expansions and are considered to be "approximated" by the corresponding finite decimals. Thus the sequence

$$
1,1.4,1.41,1.414,1.4142, \ldots
$$

"tends to $\sqrt{2}$." But unless the irrational number $\sqrt{2}$ has been clearly defined, the question must arise: Just what is it that this sequence "tends to"?

This sort of question can be answered as soon as the so-called "real number system" is constructed.

### 1.1 Example We now show that the equation

$$
p^{2}=2
$$

is not satisfied by any rational $p$. If there were such a $p$, we could write $p=m / n$ where $m$ and $n$ are integers that are not both even. Let us assume this is done. Then (1) implies

$$
m^{2}=2 n^{2}
$$

This shows that $m^{2}$ is even. Hence $m$ is even (if $m$ were odd, $m^{2}$ would be odd), and so $\mathrm{m}^{2}$ is divisible by 4 . It follows that the right side of (2) is divisible by 4 , so that $n^{2}$ is even, which implies that $n$ is even.

The assumption that (1) holds thus leads to the conclusion that both $m$ and $n$ are even, contrary to our choice of $m$ and $n$. Hence (1) is impossible for rational $p$.

We now examine this situation a little more closely. Let $A$ be the set of all positive rationals $p$ such that $p^{2}<2$ and let $B$ consist of all positive rationals $p$ such that $p^{2}>2$. We shall show that $A$ contains no largest number and $B$ contains no smallest.

More explicitly, for every $p$ in $A$ we can find a rational $q$ in $A$ such that $p<q$, and for every $p$ in $B$ we can find a rational $q$ in $B$ such that $q<p$.

To do this, we associate with each rational $p>0$ the number

$$
q=p-\frac{p^{2}-2}{p+2}=\frac{2 p+2}{p+2}
$$

Then

$$
q^{2}-2=\frac{2\left(p^{2}-2\right)}{(p+2)^{2}}
$$

If $p$ is in $A$ then $p^{2}-2<0$, (3) shows that $q>p$, and (4) shows that $q^{2}<2$. Thus $q$ is in $A$.

If $p$ is in $B$ then $p^{2}-2>0$, (3) shows that $0<q<p$, and (4) shows that $q^{2}>2$. Thus $q$ is in $B$.

1.2 Remark The purpose of the above discussion has been to show that the rational number system has certain gaps, in spite of the fact that between any two rationals there is another: If $r<s$ then $r<(r+s) / 2<s$. The real number system fills these gaps. This is the principal reason for the fundamental role which it plays in analysis.

In order to elucidate its structure, as well as that of the complex numbers, we start with a brief discussion of the general concepts of ordered set and field.

Here is some of the standard set-theoretic terminology that will be used throughout this book.

1.3 Definitions If $A$ is any set (whose elements may be numbers or any other objects), we write $x \in A$ to indicate that $x$ is a member (or an element) of $A$.

If $x$ is not a member of $A$, we write: $x \notin A$.

The set which contains no element will be called the empty set. If a set has at least one element, it is called nonempty.

If $A$ and $B$ are sets, and if every element of $A$ is an element of $B$, we say that $A$ is a subset of $B$, and write $A \subset B$, or $B \supset A$. If, in addition, there is an element of $B$ which is not in $A$, then $A$ is said to be a proper subset of $B$. Note that $A \subset A$ for every set $A$.

If $A \subset B$ and $B \subset A$, we write $A=B$. Otherwise $A \neq B$.

1.4 Definition Throughout Chap. 1, the set of all rational numbers will be denoted by $Q$.

## ORDERED SETS

1.5 Definition Let $S$ be a set. An order on $S$ is a relation, denoted by <, with the following two properties:

(i) If $x \in S$ and $y \in S$ then one and only one of the statements

is true.

$$
x<y, \quad x=y, \quad y<x
$$

(ii) If $x, y, z \in S$, if $x<y$ and $y<z$, then $x<z$.

The statement " $x<y$ " may be read as " $x$ is less than $y$ " or " $x$ is smaller than $y$ " or " $x$ precedes $y$ ".

It is often convenient to write $y>x$ in place of $x<y$.

The notation $x \leq y$ indicates that $x<y$ or $x=y$, without specifying which of these two is to hold. In other words, $x \leq y$ is the negation of $x>y$.

1.6 Definition An ordered set is a set $S$ in which an order is defined.

For example, $Q$ is an ordered set if $r<s$ is defined to mean that $s-r$ is a positive rational number.

1.7 Definition Suppose $S$ is an ordered set, and $E \subset S$. If there exists a $\beta \in S$ such that $x \leq \beta$ for every $x \in E$, we say that $E$ is bounded above, and call $\beta$ an upper bound of $E$.

Lower bounds are defined in the same way (with $\geq$ in place of $\leq$ ).

1.8 Definition Suppose $S$ is an ordered set, $E \subset S$, and $E$ is bounded above. Suppose there exists an $\alpha \in S$ with the following properties:

(i) $\alpha$ is an upper bound of $E$.

(ii) If $\gamma<\alpha$ then $\gamma$ is not an upper bound of $E$.

Then $\alpha$ is called the least upper bound of $E$ [that there is at most one such $\alpha$ is clear from (ii)] or the supremum of $E$, and we write

$$
\alpha=\sup E .
$$

The greatest lower bound, or infimum, of a set $E$ which is bounded below is defined in the same manner: The statement

$$
\alpha=\inf E
$$

means that $\alpha$ is a lower bound of $E$ and that no $\beta$ with $\beta>\alpha$ is a lower bound of $E$.

### 1.9 Examples

(a) Consider the sets $A$ and $B$ of Example 1.1 as subsets of the ordered set $Q$. The set $A$ is bounded above. In fact, the upper bounds of $A$ are exactly the members of $B$. Since $B$ contains no smallest member, $A$ has no least upper bound in $Q$.

Similarly, $B$ is bounded below: The set of all lower bounds of $B$ consists of $A$ and of all $r \in Q$ with $r \leq 0$. Since $A$ has no lasgest member, $B$ has no greatest lower bound in $Q$.

(b) If $\alpha=\sup E$ exists, then $\alpha$ may or may not be a member of $E$. For instance, let $E_{1}$ be the set of all $r \in Q$ with $r<0$. Let $E_{2}$ be the set of all $r \in Q$ with $r \leq 0$. Then

$$
\sup E_{1}=\sup E_{2}=0
$$

and $0 \notin E_{1}, 0 \in \mathrm{E}_{2}$.

(c) Let $E$ consist of all numbers $1 / n$, where $n=1,2,3, \ldots$ Then $\sup E=1$, which is in $E$, and inf $E=0$, which is not in $E$.

1.10 Definition An ordered set $S$ is said to have the least-upper-bound property if the following is true:

If $E \subset \mathrm{S}, E$ is not empty, and $E$ is bounded above, then $\sup E$ exists in $S$.

Example 1.9 $(a)$ shows that $Q$ does not have the least-upper-bound property.

We shall now show that there is a close relation between greatest lower bounds and least upper bounds, and that every ordered set with the least-upperbound property also has the greatest-lower-bound property.

1.11 Theorem Suppose $S$ is an ordered set with the least-upper-bound property, $B \subset S, B$ is not empty, and $B$ is bounded below. Let $L$ be the set of all lower bounds of $B$. Then

exists in $S$, and $\alpha=\inf B$.

$$
\alpha=\sup L
$$

In particular, inf $B$ exists in $S$.

Proof Since $B$ is bounded below, $L$ is not empty. Since $L$ consists of exactly those $y \in S$ which satisfy the inequality $y \leq x$ for every $x \in B$, we see that every $x \in B$ is an upper bound of $L$. Thus $L$ is bounded above. Our hypothesis about $S$ implies therefore that $L$ has a supremum in $S$; call it $\alpha$.

If $\gamma<\alpha$ then (see Definition 1.8) $\gamma$ is not an upper bound of $L$, hence $\gamma \notin B$. It follows that $\alpha \leq x$ for every $x \in B$. Thus $\alpha \in L$.

If $\alpha<\beta$ then $\beta \notin L$, since $\alpha$ is an upper bound of $L$.

We have shown that $\alpha \in L$ but $\beta \notin L$ if $\beta>\alpha$. In other words, $\alpha$ is a lower bound of $B$, but $\beta$ is not if $\beta>\alpha$. This means that $\alpha=\inf B$.

## FIELDS

1.12 Definition A field is a set $F$ with two operations, called addition and multiplication, which satisfy the following so-called "field axioms" (A), (M), and (D):

## (A) Axioms for addition

(A1) If $x \in F$ and $y \in F$, then their sum $x+y$ is in $F$.

(A2) Addition is commutative: $x+y=y+x$ for all $x, y \in F$.

(A3) Addition is associative: $(x+y)+z=x+(y+z)$ for all $x, y, z \in F$.

(A4) $F$ contains an element 0 such that $0+x=x$ for every $x \in F$.

(A5) To every $x \in F$ corresponds an element $-x \in F$ such that

$$
x+(-x)=0
$$

(M) Axioms for multiplication

(M1) If $x \in F$ and $y \in F$, then their product $x y$ is in $F$.

(M2) Multiplication is commutative: $x y=y x$ for all $x, y \in F$.

(M3) Multiplication is associative: $(x y) z=x(y z)$ for all $x, y, z \in F$.

(M4) $F$ contains an element $1 \neq 0$ such that $1 x=x$ for every $x \in F$.

(M5) If $x \in F$ and $x \neq 0$ then there exists an element $1 / x \in F$ such that

$$
x \cdot(1 / x)=1
$$

## (D) The distributive law

$$
x(y+z)=x y+x z
$$

holds for all $x, y, z \in F$.

### 1.13 Remarks

(a) One usually writes (in any field)

$$
x-y, \frac{x}{y}, x+y+z, x y z, x^{2}, x^{3}, 2 x, 3 x, \ldots
$$

in place of

$$
x+(-y), x \cdot\left(\frac{1}{y}\right),(x+y)+z,(x y) z, x x, x x x, x+x, x+x+x, \ldots
$$

(b) The field axioms clearly hold in $Q$, the set of all rational numbers, if addition and multiplication have their customary meaning. Thus $Q$ is a field.

(c) Although it is not our purpose to study fields (or any other algebraic structures) in detail, it is worthwhile to prove that some familiar properties of $Q$ are consequences of the field axioms; once we do this, we will not need to do it again for the real numbers and for the complex numbers.

1.14 Proposition The axioms for addition imply the following statements.
(a) If $x+y=x+z$ then $y=z$.
(b) If $x+y=x$ then $y=0$.
(c) If $x+y=0$ then $y=-x$.
(d) $-(-x)=x$.

Statement $(a)$ is a cancellation law. Note that $(b)$ asserts the uniqueness of the element whose existence is assumed in (A4), and that $(c)$ does the same for (A5).

Proof If $x+y=x+z$, the axioms (A) give

$$
\begin{aligned}
y=0+y & =(-x+x)+y=-x+(x+y) \\
& =-x+(x+z)=(-x+x)+z=0+z=z
\end{aligned}
$$

This proves $(a)$. Take $z=0$ in $(a)$ to obtain $(b)$. Take $z=-x$ in $(a)$ to obtain $(c)$.

Since $-x+x=0,(c)$ (with $-x$ in place of $x$ ) gives $(d)$.

1.15 Proposition The axioms for multiplication imply the following statements.

(a) If $x \neq 0$ and $x y=x z$ then $y=z$.

(b) If $x \neq 0$ and $x y=x$ then $y=1$.

(c) If $x \neq 0$ and $x y=1$ then $y=1 / x$.

(d) If $x \neq 0$ then $1 /(1 / x)=x$.

The proof is so similar to that of Proposition 1.14 that we omit it.

1.16 Proposition The field axioms imply the following statements, for any $x, y$, $z \in F$.

(a) $0 x=0$.

(b) If $x \neq 0$ and $y \neq 0$ then $x y \neq 0$.

(c) $(-x) y=-(x y)=x(-y)$.

(d) $(-x)(-y)=x y$.

Proof $0 x+0 x=(0+0) x=0 x$. Hence $1.14(b)$ implies that $0 x=0$, and (a) holds.

Next, assume $x \neq 0, y \neq 0$, but $x y=0$. Then (a) gives

$$
1=\left(\frac{1}{y}\right)\left(\frac{1}{x}\right) x y=\left(\frac{1}{y}\right)\left(\frac{1}{x}\right) 0=0
$$

a contradiction. Thus $(b)$ holds.

The first equality in $(c)$ comes from

$$
(-x) y+x y=(-x+x) y=0 y=0
$$

combined with $1.14(c)$; the other half of $(c)$ is proved in the same way. Finally,

$$
(-x)(-y)=-[x(-y)]=-[-(x y)]=x y
$$

by $(c)$ and $1.14(d)$.

1.17 Definition An ordered field is a field $F$ which is also an ordered set, such that

(i) $x+y<x+z$ if $x, y, z \in F$ and $y<z$,

(ii) $x y>0$ if $x \in F, y \in F, x>0$, and $y>0$.

If $x>0$, we call $x$ positive; if $x<0, x$ is negative.

For example, $Q$ is an ordered field.

All the familiar rules for working with inequalities apply in every ordered field: Multiplication by positive [negative] quantities preserves [reverses] inequalities, no square is negative, etc. The following proposition lists some of these.

1.18 Proposition The following statements are true in every ordered field.

(a) If $x>0$ then $-x<0$, and vice versa.

(b) If $x>0$ and $y<z$ then $x y<x z$.

(c) If $x<0$ and $y<z$ then $x y>x z$.

(d) If $x \neq 0$ then $x^{2}>0$. In particular, $1>0$.

(e) If $0<x<y$ then $0<1 / y<1 / x$.

## Proof

(a) If $x>0$ then $0=-x+x>-x+0$, so that $-x<0$. If $x<0$ then $0=-x+x<-x+0$, so that $-x>0$. This proves $(a)$.

(b) Since $z>y$, we have $z-y>y-y=0$, hence $x(z-y)>0$, and therefore

$$
x z=x(z-y)+x y>0+x y=x y .
$$

(c) By $(a),(b)$, and Proposition 1.16(c),

$$
-[x(z-y)]=(-x)(z-y)>0
$$

so that $x(z-y)<0$, hence $x z<x y$.

(d) If $x>0$, part (ii) of Definition 1.17 gives $x^{2}>0$. If $x<0$, then $-x>0$, hence $(-x)^{2}>0$. But $x^{2}=(-x)^{2}$, by Proposition $1.16(d)$. Since $1=1^{2}, 1>0$.

(e) If $y>0$ and $v \leq 0$, then $y v \leq 0$. But $y \cdot(1 / y)=1>0$. Hence $1 / y>0$. Likewise, $1 / x>0$. If we multiply both sides of the inequality $x<y$ by the positive quantity $(1 / x)(1 / y)$, we obtain $1 / y<1 / x$.

## THE REAL FIELD

We now state the existence theorem which is the core of this chapter.

1.19 Theorem There exists an ordered field $R$ which has the least-upper-bound property.

Moreover, $R$ contains $Q$ as a subfield.

The second statement means that $Q \subset R$ and that the operations of addition and multiplication in $R$, when applied to members of $Q$, coincide with the usual operations on rational numbers; also, the positive rational numbers are positive elements of $R$.

The members of $R$ are called real numbers.

The proof of Theorem 1.19 is rather long and a bit tedious and is therefore presented in an Appendix to Chap. 1. The proof actually constructs $R$ from $Q$.

The next theorem could be extracted from this construction with very little extra effort. However, we prefer to derive it from Theorem 1.19 since this provides a good illustration of what one can do with the least-upper-bound property.

### 1.20 Theorem

(a) If $x \in R, y \in R$, and $x>0$, then there is a positive integer $n$ such that

$$
n x>y .
$$

(b) If $x \in R, y \in R$, and $x<y$, then there exists a $p \in Q$ such that $x<p<y$.

Part $(a)$ is usually referred to as the archimedean property of $R$. Part (b) may be stated by saying that $Q$ is dense in $R$ : Between any two real numbers there is a rational one.

## Proof

(a) Let $A$ be the set of all $n x$, where $n$ runs through the positive integers. If (a) were false, then $y$ would be an upper bound of $A$. But then $A$ has a least upper bound in $R$. Put $\alpha=\sup A$. Since $x>0, \alpha-x<\alpha$, and $\alpha-x$ is not an upper bound of $A$. Hence $\alpha-x<m x$ for some positive integer $m$. But then $\alpha<(m+1) x \in A$, which is impossible, since $\alpha$ is an upper bound of $A$.

(b) Since $x<y$, we have $y-x>0$, and (a) furnishes a positive integer $n$ such that

$$
n(y-x)>1 \text {. }
$$

Apply (a) again, to obtain positive integers $m_{1}$ and $m_{2}$ such that $m_{1}>n x$, $m_{2}>-n x$. Then

$$
-m_{2}<n x<m_{1} \text {. }
$$

Hence there is an integer $m$ (with $-m_{2} \leq m \leq m_{1}$ ) such that

$$
m-1 \leq n x<m .
$$

If we combine these inequalities, we obtain

$$
n x<m \leq 1+n x<n y .
$$

Since $n>0$, it follows that

$$
x<\frac{m}{n}<y \text {. }
$$

This proves $(b)$, with $p=m / n$.

We shall now prove the existence of $n$th roots of positive reals. This proof will show how the difficulty pointed out in the Introduction (irrationality of $\sqrt{2}$ ) can be handled in $R$.

1.21 Theorem For every real $x>0$ and every integer $n>0$ there is one and only one positive real $y$ such that $y^{n}=x$.

This number $y$ is written $\sqrt[n]{x}$ or $x^{1 / n}$.

Proof That there is at most one such $y$ is clear, since $0<y_{1}<y_{2}$ implies $y_{1}^{n}<y_{2}^{n}$.

Let $E$ be the set consisting of all positive real numbers $t$ such that $t^{n}<x$.

If $t=x /(1+x)$ then $0 \leq t<1$. Hence $t^{n} \leq t<x$. Thus $t \in E$, and $E$ is not empty.

If $t>1+x$ then $t^{n} \geq t>x$, so that $t \notin E$. Thus $1+x$ is an upper bound of $E$.

Hence Theorem 1.19 implies the existence of

$$
y=\sup E .
$$

To prove that $y^{n}=x$ we will show that each of the inequalities $y^{n}<x$ and $y^{n}>x$ leads to a contradiction.

The identity $b^{n}-a^{n}=(b-a)\left(b^{n-1}+b^{n-2} a+\cdots+a^{n-1}\right)$ yields the inequality

$$
b^{n}-a^{n}<(b-a) n b^{n-1}
$$

when $0<a<b$.

Assume $y^{n}<x$. Choose $h$ so that $0<h<1$ and

$$
h<\frac{x-y^{n}}{n(y+1)^{n-1}}
$$

Put $a=y, b=y+h$. Then

$$
(y+h)^{n}-y^{n}<h n(y+h)^{n-1}<h n(y+1)^{n-1}<x-y^{n} .
$$

Thus $(y+h)^{n}<x$, and $y+h \in E$. Since $y+h>y$, this contradicts the fact that $y$ is an upper bound of $E$.

Assume $y^{n}>x$. Put

$$
k=\frac{y^{n}-x}{n y^{n-1}}
$$

Then $0<k<y$. If $t \geq y-k$, we conclude that

$$
y^{n}-t^{n} \leq y^{n}-(y-k)^{n}<k n y^{n-1}=y^{n}-x
$$

Thus $t^{n}>x$, and $t \notin E$. It follows that $y-k$ is an upper bound of $E$.

But $y-k<y$, which contradicts the fact that $y$ is the least upper bound of $E$.

Hence $y^{n}=x$, and the proof is complete.

Corollary If $a$ and $b$ are positive real numbers and $n$ is a positive integer, then

$$
(a b)^{1 / n}=a^{1 / n} b^{1 / n} .
$$

Proof Put $\alpha=a^{1 / n}, \beta=b^{1 / n}$. Then

$$
a b=\alpha^{n} \beta^{n}=(\alpha \beta)^{n},
$$

since multiplication is commutative. [Axiom (M2) in Definition 1.12.] The uniqueness assertion of Theorem 1.21 shows therefore that

$$
(a b)^{1 / n}=\alpha \beta=a^{1 / n} b^{1 / n} .
$$

1.22 Decimals We conclude this section by pointing out the relation between real numbers and decimals.

Let $x>0$ be real. Let $n_{0}$ be the largest integer such that $n_{0} \leq x$. (Note that the existence of $n_{0}$ depends on the archimedean property of $R$.) Having chosen $n_{0}, n_{1}, \ldots, n_{k-1}$, let $n_{k}$ be the largest integer such that

$$
n_{0}+\frac{n_{1}}{10}+\cdots+\frac{n_{k}}{10^{k}} \leq x .
$$

Let $E$ be the set of these numbers

$$
n_{0}+\frac{n_{1}}{10}+\cdots+\frac{n_{k}}{10^{k}} \quad(k=0,1,2, \ldots)
$$

Then $x=\sup E$. The decimal expansion of $x$ is

$$
n_{0} \cdot n_{1} n_{2} n_{3} \cdots \text {. }
$$

Conversely, for any infinite decimal (6) the set $E$ of numbers (5) is bounded above, and (6) is the decimal expansion of $\sup E$.

Since we shall never use decimals, we do not enter into a detailed discussion.

## THE EXTENDED REAL NUMBER SYSTEM

1.23 Definition The extended real number system consists of the real field $R$ and two symbols, $+\infty$ and $-\infty$. We preserve the original order in $R$, and define

for every $x \in R$.

$$
-\infty<x<+\infty
$$

It is then clear that $+\infty$ is an upper bound of every subset of the extended real number system, and that every nonempty subset has a least upper bound. If, for example, $E$ is a nonempty set of real numbers which is not bounded above in $R$, then $\sup E=+\infty$ in the extended real number system.

Exactly the same remarks apply to lower bounds.

The extended real number system does not form a field, but it is customary to make the following conventions:

(a) If $x$ is real then

$$
x+\infty=+\infty, \quad x-\infty=-\infty, \quad \frac{x}{+\infty}=\frac{x}{-\infty}=0
$$

(b) If $x>0$ then $x \cdot(+\infty)=+\infty, x \cdot(-\infty)=-\infty$.

(c) If $x<0$ then $x \cdot(+\infty)=-\infty, x \cdot(-\infty)=+\infty$.

When it is desired to make the distinction between real numbers on the one hand and the symbols $+\infty$ and $-\infty$ on the other quite explicit, the former are called finite.

## THE COMPLEX FIELD

1.24 Definition A complex number is an ordered pair $(a, b)$ of real numbers. "Ordered" means that $(a, b)$ and $(b, a)$ are regarded as distinct if $a \neq b$.

Let $x=(a, b), y=(c, d)$ be two complex numbers. We write $x=y$ if and only if $a=c$ and $b=d$. (Note that this definition is not entirely superfluous; think of equality of rational numbers, represented as quotients of integers.) We define

$$
\begin{aligned}
x+y & =(a+c, b+d) \\
x y & =(a c-b d, a d+b c) .
\end{aligned}
$$

1.25 Theorem These definitions of addition and multiplication turn the set of all complex numbers into a field, with $(0,0)$ and $(1,0)$ in the role of 0 and 1 .

Proof We simply verify the field axioms, as listed in Definition 1.12. (Of course, we use the field structure of $R$.)

Let $x=(a, b), y=(c, d), z=(e, f)$.
(A1) is clear.
(A2) $x+y=(a+c, b+d)=(c+a, d+b)=y+x$.

(A3)

$$
\begin{aligned}
(x+y)+z & =(a+c, b+d)+(e, f) \\
& =(a+c+e, b+d+f) \\
& =(a, b)+(c+e, d+f)=x+(y+z)
\end{aligned}
$$

(A4) $x+0=(a, b)+(0,0)=(a, b)=x$.

(A5) Put $-x=(-a,-b)$. Then $x+(-x)=(0,0)=0$.

(M1) is clear.

(M2) $x y=(a c-b d, a d+b c)=(c a-d b, d a+c b)=y x$.

(M3) $(x y) z=(a c-b d, a d+b c)(e, f)$

$=(a c e-b d e-a d f-b c f, a c f-b d f+a d e+b c e)$

$=(a, b)(c e-d f, c f+d e)=x(y z)$.

(M4) $1 x=(1,0)(a, b)=(a, b)=x$.

(M5) If $x \neq 0$ then $(a, b) \neq(0,0)$, which means that at least one of the real numbers $a, b$ is different from 0 . Hence $a^{2}+b^{2}>0$, by Proposition $1.18(d)$, and we can define

$$
\frac{1}{x}=\left(\frac{a}{a^{2}+b^{2}}, \frac{-b}{a^{2}+b^{2}}\right)
$$

Then

$$
x \cdot \frac{1}{x}=(a, b)\left(\frac{a}{a^{2}+b^{2}}, \frac{-b}{a^{2}+b^{2}}\right)=(1,0)=1 .
$$

$$
\begin{aligned}
x(y+z) & =(a, b)(c+e, d+f) \\
& =(a c+a e-b d-b f, a d+a f+b c+b e) \\
& =(a c-b d, a d+b c)+(a e-b f, a f+b e) \\
& =x y+x z .
\end{aligned}
$$

1.26 Theorem For any real numbers $a$ and $b$ we have

$$
(a, 0)+(b, 0)=(a+b, 0), \quad(a, 0)(b, 0)=(a b, 0)
$$

The proof is trivial.

Theorem 1.26 shows that the complex numbers of the form $(a, 0)$ have the same arithmetic properties as the corresponding real numbers $a$. We can therefore identify $(a, 0)$ with $a$. This identification gives us the real field as a subfield of the complex field.

The reader may have noticed that we have defined the complex numbers without any reference to the mysterious square root of -1 . We now show that the notation $(a, b)$ is equivalent to the more customary $a+b i$.

### 1.27 Definition $i=(0,1)$.

1.28 Theorem $i^{2}=-1$.

Proof $i^{2}=(0,1)(0,1)=(-1,0)=-1$.

1.29 Theorem If $a$ and $b$ are real, then $(a, b)=a+b i$.

Proof

$$
\begin{aligned}
a+b i & =(a, 0)+(b, 0)(0,1) \\
& =(a, 0)+(0, b)=(a, b) .
\end{aligned}
$$

1.30 Definition If $a, b$ are real and $z=a+b i$, then the complex number $\bar{z}=a-b i$ is called the conjugate of $z$. The numbers $a$ and $b$ are the real part and the imaginary part of $z$, respectively.

We shall occasionally write

$$
a=\operatorname{Re}(z), \quad b=\operatorname{Im}(z)
$$

1.31 Theorem If $z$ and $w$ are complex, then

(a) $\overline{z+w}=\bar{z}+\bar{w}$

(b) $\overline{z w}=\bar{z} \cdot \bar{w}$,

(c) $z+\bar{z}=2 \operatorname{Re}(z), z-\bar{z}=2 i \operatorname{Im}(z)$,

(d) $z \bar{z}$ is real and positive (except when $z=0$ ).

Proof $(a),(b)$, and $(c)$ are quite trivial. To prove $(d)$, write $z=a+b i$, and note that $z \bar{z}=a^{2}+b^{2}$.

1.32 Definition If $z$ is a complex number, its absolute value $|z|$ is the nonnegative square root of $z \bar{z}$; that is, $|z|=(z \bar{z})^{1 / 2}$.

The existence (and uniqueness) of $|z|$ follows from Theorem 1.21 and part $(d)$ of Theorem 1.31.

Note that when $x$ is real, then $\bar{x}=x$, hence $|x|=\sqrt{x^{2}}$. Thus $|x|=x$ if $x \geq 0,|x|=-x$ if $x<0$.

1.33 Theorem Let $z$ and $w$ be complex numbers. Then

(a) $|z|>0$ unless $z=0,|0|=0$,

(b) $|\bar{z}|=|z|$,

(c) $|z w|=|z||w|$,

(d) $|\operatorname{Re} z| \leq|z|$

(e) $|z+w| \leq|z|+|w|$.

Proof $(a)$ and $(b)$ are trivial. Put $z=a+b i, w=c+d i$, with $a, b, c, d$ real. Then

$$
|z w|^{2}=(a c-b d)^{2}+(a d+b c)^{2}=\left(a^{2}+b^{2}\right)\left(c^{2}+d^{2}\right)=|z|^{2}|w|^{2}
$$

or $|z w|^{2}=(|z||w|)^{2}$. Now $(c)$ follows from the uniqueness assertion of Theorem 1.21.

To prove $(d)$, note that $a^{2} \leq a^{2}+b^{2}$, hence

$$
|a|=\sqrt{a^{2}} \leq \sqrt{a^{2}+b^{2}} \text {. }
$$

To prove (e), note that $\bar{z} w$ is the conjugate of $z \bar{w}$, so that $z \bar{w}+\bar{z} w=$ $2 \operatorname{Re}(z \bar{w})$. Hence

$$
\begin{aligned}
|z+w|^{2} & =(z+w)(\bar{z}+\bar{w})=z \bar{z}+z \bar{w}+\bar{z} w+w \bar{w} \\
& =|z|^{2}+2 \operatorname{Re}(z \bar{w})+|w|^{2} \\
& \leq|z|^{2}+2|z \bar{w}|+|w|^{2} \\
& =|z|^{2}+2|z||w|+|w|^{2}=(|z|+|w|)^{2} .
\end{aligned}
$$

Now $(e)$ follows by taking square roots.

1.34 Notation If $x_{1}, \ldots, x_{n}$ are complex numbers, we write

$$
x_{1}+x_{2}+\cdots+x_{n}=\sum_{j=1}^{n} x_{j}
$$

We conclude this section with an important inequality, usually known as the Schwarz inequality.

1.35 Theorem If $a_{1}, \ldots, a_{n}$ and $b_{1}, \ldots, b_{n}$ are complex numbers, then

$$
\left|\sum_{j=1}^{n} a_{j} b_{j}\right|^{2} \leq \sum_{j=1}^{n}\left|a_{j}\right|^{2} \sum_{j=1}^{n}\left|b_{j}\right|^{2}
$$

Proof Put $A=\Sigma\left|a_{j}\right|^{2}, B=\Sigma\left|b_{j}\right|^{2}, C=\Sigma a_{j} b_{j}$ (in all sums in this proof, $j$ runs over the values $1, \ldots, n)$. If $B=0$, then $b_{1}=\cdots=b_{n}=0$, and the conclusion is trivial. Assume therefore that $B>0$. By Theorem 1.31 we have

$$
\begin{aligned}
\sum\left|B a_{j}-C b_{j}\right|^{2} & =\sum\left(B a_{j}-C b_{j}\right)\left(B \bar{a}_{j}-\bar{C} b_{j}\right) \\
& =B^{2} \sum\left|a_{j}\right|^{2}-B \bar{C} \sum a_{j} \bar{b}_{j}-B C \sum \bar{a}_{j} b_{j}+|C|^{2} \sum\left|b_{j}\right|^{2} \\
& =B^{2} A-B|C|^{2} \\
& =B\left(A B-|C|^{2}\right) .
\end{aligned}
$$

Since each term in the first sum is nonnegative, we see that

$$
B\left(A B-|C|^{2}\right) \geq 0
$$

Since $B>0$, it follows that $A B-|C|^{2} \geq 0$. This is the desired inequality.

## EUCLIDEAN SPACES

1.36 Definitions For each positive integer $k$, let $R^{k}$ be the set of all ordered $k$-tuples

$$
\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{k}\right)
$$

where $x_{1}, \ldots, x_{k}$ are real numbers, called the coordinates of $\mathbf{x}$. The elements of $R^{k}$ are called points, or vectors, especially when $k>1$. We shall denote vectors by boldfaced letters. If $\mathbf{y}=\left(y_{1}, \ldots, y_{k}\right)$ and if $\alpha$ is a real number, put

$$
\begin{aligned}
\mathbf{x}+\mathbf{y} & =\left(x_{1}+y_{1}, \ldots, x_{k}+y_{k}\right) \\
\alpha \mathbf{x} & =\left(\alpha x_{1}, \ldots, \alpha x_{k}\right)
\end{aligned}
$$

so that $\mathbf{x}+\mathbf{y} \in R^{k}$ and $\alpha \mathbf{x} \in R^{k}$. This defines addition of vectors, as well as multiplication of a vector by a real number (a scalar). These two operations satisfy the commutative, associative, and distributive laws (the proof is trivial, in view of the analogous laws for the real numbers) and make $R^{k}$ into a vector space over the real field. The zero element of $R^{k}$ (sometimes called the origin or the null vector) is the point 0 , all of whose coordinates are 0 .

We also define the so-called "inner product" (or scalar product) of $\mathbf{x}$ and y by

and the norm of $\mathbf{x}$ by

$$
\mathbf{x} \cdot \mathbf{y}=\sum_{i=1}^{k} x_{i} y_{i}
$$

$$
|\mathbf{x}|=(\mathbf{x} \cdot \mathbf{x})^{1 / 2}=\left(\sum_{1}^{k} x_{i}^{2}\right)^{1 / 2}
$$

The structure now defined (the vector space $R^{k}$ with the above inner product and norm) is called euclidean $k$-space.

1.37 Theorem Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then

(a) $|\mathbf{x}| \geq 0$

(b) $|\mathbf{x}|=0$ if and only if $\mathbf{x}=\mathbf{0}$;

(c) $|\alpha \mathbf{x}|=|\alpha||\mathbf{x}|$;

(d) $|\mathbf{x} \cdot \mathbf{y}| \leq|\mathbf{x}||\mathbf{y}|$

(e) $|\mathbf{x}+\mathbf{y}| \leq|\mathbf{x}|+|\mathbf{y}|$;

(f) $|\mathbf{x}-\mathbf{z}| \leq|\mathbf{x}-\mathbf{y}|+|\mathbf{y}-\mathbf{z}|$.

Proof $(a),(b)$, and $(c)$ are obvious, and $(d)$ is an immediate consequence of the Schwarz inequality. By $(d)$ we have

$$
\begin{aligned}
|x+y|^{2} & =(x+y) \cdot(x+y) \\
& =x \cdot x+2 x \cdot y+y \cdot y \\
& \leq|x|^{2}+2|x||y|+|y|^{2} \\
& =(|\mathbf{x}|+|\mathbf{y}|)^{2}
\end{aligned}
$$

so that $(e)$ is proved. Finally, $(f)$ follows from $(e)$ if we replace $\mathbf{x}$ by $\mathbf{x}-\mathbf{y}$ and $\mathbf{y}$ by $\mathbf{y}-\mathbf{z}$.

1.38 Remarks Theorem $1.37(a),(b)$, and $(f)$ will allow us (see Chap. 2) to regard $R^{k}$ as a metric space.

$R^{1}$ (the set of all real numbers) is usually called the line, or the real line. Likewise, $R^{2}$ is called the plane, or the complex plane (compare Definitions 1.24 and 1.36). In these two cases the norm is just the absolute value of the corresponding real or complex number.

## APPENDIX

Theorem 1.19 will be proved in this appendix by constructing $R$ from $Q$. We shall divide the construction into several steps.

Step 1 The members of $R$ will be certain subsets of $Q$, called cuts. A cut is, by definition, any set $\alpha \subset Q$ with the following three properties.

(I) $\alpha$ is not empty, and $\alpha \neq Q$.

(II) If $p \in \alpha, q \in Q$, and $q<p$, then $q \in \alpha$.

(III) If $p \in \alpha$, then $p<r$ for some $r \in \alpha$.

The letters $p, q, r, \ldots$ will always denote rational numbers, and $\alpha, \beta, \gamma, \ldots$ will denote cuts.

Note that (III) simply says that $\alpha$ has no largest member; (II) implies two facts which will be used freely:

If $p \in \alpha$ and $q \notin \alpha$ then $p<q$.

If $r \notin \alpha$ and $r<s$ then $s \notin \alpha$.

Step 2 Define " $\alpha<\beta$ " to mean: $\alpha$ is a proper subset of $\beta$.

Let us check that this meets the requirements of Definition 1.5.

If $\alpha<\beta$ and $\beta<\gamma$ it is clear that $\alpha<\gamma$. (A proper subset of a proper subset is a proper subset.) It is also clear that at most one of the three relations

$$
\alpha<\beta, \quad \alpha=\beta, \quad \beta<\alpha
$$

can hold for any pair $\alpha, \beta$. To show that at least one holds, assume that the first two fail. Then $\alpha$ is not a subset of $\beta$. Hence there is a $p \in \alpha$ with $p \notin \beta$. If $q \in \beta$, it follows that $q<p$ (since $p \notin \beta$ ), hence $q \in \alpha$, by (II). Thus $\beta \subset \alpha$. Since $\beta \neq \alpha$, we conclude: $\beta<\alpha$.

Thus $R$ is now an ordered set.

Step 3 The ordered set $R$ has the least-upper-bound property.

To prove this, let $A$ be a nonempty subset of $R$, and assume that $\beta \in R$ is an upper bound of $A$. Define $\gamma$ to be the union of all $\alpha \in A$. In other words, $p \in \gamma$ if and only if $p \in \alpha$ for some $\alpha \in A$. We shall prove that $\gamma \in R$ and that $\gamma=\sup A$.

Since $A$ is not empty, there exists an $\alpha_{0} \in A$. This $\alpha_{0}$ is not empty. Since $\alpha_{0} \subset \gamma, \gamma$ is not empty. Next, $\gamma \subset \beta$ (since $\alpha \subset \beta$ for every $\alpha \in A$ ), and therefore $\gamma \neq Q$. Thus $\gamma$ satisfies property (I). To prove (II) and (III), pick $p \in \gamma$. Then $p \in \alpha_{1}$ for some $\alpha_{1} \in A$. If $q<p$, then $q \in \alpha_{1}$, hence $q \in \gamma$; this proves (II). If $r \in \alpha_{1}$ is so chosen that $r>p$, we see that $r \in \gamma\left(\right.$ since $\left.\alpha_{1} \subset \gamma\right)$, and therefore $\gamma$ satisfies (III).

Thus $\gamma \in R$.

It is clear that $\alpha \leq \gamma$ for every $\alpha \in A$.

Suppose $\delta<\gamma$. Then there is an $s \in \gamma$ and that $s \notin \delta$. Since $s \in \gamma, s \in \alpha$ for some $\alpha \in A$. Hence $\delta<\alpha$, and $\delta$ is not an upper bound of $A$.

This gives the desired result: $\gamma=\sup A$.

Step 4 If $\alpha \in R$ and $\beta \in R$ we define $\alpha+\beta$ to be the set of all sums $r+s$, where $r \in \alpha$ and $s \in \beta$.

We define $0^{*}$ to be the set of all negative rational numbers. It is clear that $0^{*}$ is a cut. We verify that the axioms for addition (see Definition 1.12) hold in $R$, with $0^{*}$ playing the role of 0 .

(A1) We have to show that $\alpha+\beta$ is a cut. It is clear that $\alpha+\beta$ is a nonempty subset of $Q$. Take $r^{\prime} \notin \alpha, s^{\prime} \notin \beta$. Then $r^{\prime}+s^{\prime}>r+s$ for all choices of $r \in \alpha, s \in \beta$. Thus $r^{\prime}+s^{\prime} \notin \alpha+\beta$. It follows that $\alpha+\beta$ has property (I).

Pick $p \in \alpha+\beta$. Then $p=r+s$, with $r \in \alpha, s \in \beta$. If $q<p$, then $q-s<r$, so $q-s \in \alpha$, and $q=(q-s)+s \in \alpha+\beta$. Thus (II) holds. Choose $t \in \alpha$ so that $t>r$. Then $p<t+s$ and $t+s \in \alpha+\beta$. Thus (III) holds.

(A2) $\alpha+\beta$ is the set of all $r+s$, with $r \in \alpha, s \in \beta$. By the same definition, $\beta+\alpha$ is the set of all $s+r$. Since $r+s=s+r$ for all $r \in Q, s \in Q$, we have $\alpha+\beta=\beta+\alpha$.

(A3) As above, this follows from the associative law in $Q$.

(A4) If $r \in \alpha$ and $s \in 0^{*}$, then $r+s<r$, hence $r+s \in \alpha$. Thus $\alpha+0^{*} \subset \alpha$. To obtain the opposite inclusion, pick $p \in \alpha$, and pick $r \in \alpha, r>p$. Then
$p-r \in 0^{*}$, and $p=r+(p-r) \in \alpha+0^{*}$. Thus $\alpha \subset \alpha+0^{*}$. We conclude that $\alpha+0^{*}=\alpha$.

(A5) Fix $\alpha \in R$. Let $\beta$ be the set of all $p$ with the following property:

There exists $r>0$ such that $-p-r \notin \alpha$.

In other words, some rational number smaller than $-p$ fails to be in $\alpha$.

We show that $\beta \in R$ and that $\alpha+\beta=0^{*}$.

If $s \notin \alpha$ and $p=-s-1$, then $-p-1 \notin \alpha$, hence $p \in \beta$. So $\beta$ is not empty. If $q \in \alpha$, then $-q \notin \beta$. So $\beta \neq Q$. Hence $\beta$ satisfies (I).

Pick $p \in \beta$, and pick $r>0$, so that $-p-r \notin \alpha$. If $q<p$, then $-q-r>-p-r$, hence $-q-r \notin \alpha$. Thus $q \in \beta$, and (II) holds. Put $t=p+(r / 2)$. Then $t>p$, and $-t-(r / 2)=-p-r \notin \alpha$, so that $t \in \beta$. Hence $\beta$ satisfies (III).

We have proved that $\beta \in R$.

If $r \in \alpha$ and $s \in \beta$, then $-s \notin \alpha$, hence $r<-s, r+s<0$. Thus $\alpha+\beta \subset 0^{*}$.

To prove the opposite inclusion, pick $v \in 0^{*}$, put $w=-v / 2$. Then $w>0$, and there is an integer $n$ such that $n w \in \alpha$ but $(n+1) w \notin \alpha$. (Note that this depends on the fact that $Q$ has the archimedean property!) Put $p=-(n+2) w$. Then $p \in \beta$, since $-p-w \notin \alpha$, and

Thus $0^{*} \subset \alpha+\beta$.

$$
v=n w+p \in \alpha+\beta .
$$

We conclude that $\alpha+\beta=0^{*}$.

This $\beta$ will of course be denoted by $-\alpha$.

Step 5 Having proved that the addition defined in Step 4 satisfies Axioms (A) of Definition 1.12, it follows that Proposition 1.14 is valid in $R$, and we can prove one of the requirements of Definition 1.17:

If $\alpha, \beta, \gamma \in R$ and $\beta<\gamma$, then $\alpha+\beta<\alpha+\gamma$.

Indeed, it is obvious from the definition of + in $R$ that $\alpha+\beta \subset \alpha+\gamma$; if we had $\alpha+\beta=\alpha+\gamma$, the cancellation law (Proposition 1.14) would imply $\beta=\gamma$.

It also follows that $\alpha>0^{*}$ if and only if $-\alpha<0^{*}$.

Step 6 Multiplication is a little more bothersome than addition in the present context, since products of negative rationals are positive. For this reason we confine ourselves first to $R^{+}$, the set of all $\alpha \in R$ with $\alpha>0^{*}$.

If $\alpha \in R^{+}$and $\beta \in R^{+}$, we define $\alpha \beta$ to be the set of all $p$ such that $p \leq r s$ for some choice of $r \in \alpha, s \in \beta, r>0, s>0$.

We define $1^{*}$ to be the set of all $q<1$.

Then the axioms $(M)$ and $(D)$ of Definition 1.12 hold, with $R^{+}$in place of $F$, and with $1^{*}$ in the role of 1 . them.

The proofs are so similar to the ones given in detail in Step 4 that we omit

Note, in particular, that the second requirement of Definition 1.17 holds: If $\alpha>0^{*}$ and $\beta>0^{*}$ then $\alpha \beta>0 *$.

Step 7 We complete the definition of multiplication by setting $\alpha 0^{*}=0^{*} \alpha=0^{*}$, and by setting

$$
\alpha \beta= \begin{cases}(-\alpha)(-\beta) & \text { if } \alpha<0^{*}, \beta<0^{*}, \\ -[(-\alpha) \beta] & \text { if } \alpha<0^{*}, \beta>0^{*}, \\ -[\alpha \cdot(-\beta)] & \text { if } \alpha>0^{*}, \beta<0^{*} .\end{cases}
$$

The products on the right were defined in Step 6.

Having proved (in Step 6) that the axioms (M) hold in $R^{+}$, it is now perfectly simple to prove them in $R$, by repeated application of the identity $\gamma=-(-\gamma)$ which is part of Proposition 1.14. (See Step 5.)

The proof of the distributive law

$$
\alpha(\beta+\gamma)=\alpha \beta+\alpha \gamma
$$

breaks into cases. For instance, suppose $\alpha>0^{*}, \beta<0^{*}, \beta+\gamma>0^{*}$. Then $\gamma=(\beta+\gamma)+(-\beta)$, and (since we already know that the distributive law holds in $\left.R^{+}\right)$

$$
\alpha \gamma=\alpha(\beta+\gamma)+\alpha \cdot(-\beta) .
$$

But $\alpha \cdot(-\beta)=-(\alpha \beta)$. Thus

$$
\alpha \beta+\alpha \gamma=\alpha(\beta+\gamma) .
$$

The other cases are handled in the same way.

We have now completed the proof that $R$ is an ordered field with the leastupper-bound property.

Step 8 We associate with each $r \in Q$ the set $r^{*}$ which consists of all $p \in Q$ such that $p<r$. It is clear that each $r^{*}$ is a cut; that is, $r^{*} \in R$. These cuts satisfy the following relations:

(a) $r^{*}+s^{*}=(r+s)^{*}$,

(b) $r^{*} s^{*}=(r s)^{*}$,

(c) $r^{*}<s^{*}$ if and only if $r<s$.

To prove $(a)$, choose $p \in r^{*}+s^{*}$. Then $p=u+v$, where $u<r, v<s$. Hence $p<r+s$, which says that $p \in(r+s)^{*}$.

Conversely, suppose $p \in(r+s)^{*}$. Then $p<r+s$. Choose $t$ so that $2 t=r+s-p$, put

$$
r^{\prime}=r-t, s^{\prime}=s-t \text {. }
$$

Then $r^{\prime} \in r^{*}, s^{\prime} \in s^{*}$, and $p=r^{\prime}+s^{\prime}$, so that $p \in r^{*}+s^{*}$.

This proves $(a)$. The proof of $(b)$ is similar.

If $r<s$ then $r \in s^{*}$, but $r \notin r^{*}$; hence $r^{*}<s^{*}$.

If $r^{*}<s^{*}$, then there is a $p \in s^{*}$ such that $p \notin r^{*}$. Hence $r \leq p<s$, so that $r<s$.

This proves $(c)$.

Step 9 We saw in Step 8 that the replacement of the rational numbers $r$ by the corresponding "rational cuts" $r^{*} \in R$ preserves sums, products, and order. This fact may be expressed by saying that the ordered field $Q$ is isomorphic to the ordered field $Q^{*}$ whose elements are the rational cuts. Of course, $r^{*}$ is by no means the same as $r$, but the properties we are concerned with (arithmetic and order) are the same in the two fields.

It is this identification of $Q$ with $Q^{*}$ which allows us to regard $Q$ as a subfield of $R$.

The second part of Theorem 1.19 is to be understood in terms of this identification. Note that the same phenomenon occurs when the real numbers are regarded as a subfield of the complex field, and it also occurs at a much more elementary level, when the integers are identified with a certain subset of $Q$.

It is a fact, which we will not prove here, that any two ordered fields with the least-upper-bound property are isomorphic. The first part of Theorem 1.19 therefore characterizes the real field $R$ completely.

The books by Landau and Thurston cited in the Bibliography are entirely devoted to number systems. Chapter 1 of Knopp's book contains a more leisurely description of how $R$ can be obtained from $Q$. Another construction, in which each real number is defined to be an equivalence class of Cauchy sequences of rational numbers (see Chap. 3), is carried out in Sec. 5 of the book by Hewitt and Stromberg.

The cuts in $Q$ which we used here were invented by Dedekind. The construction of $R$ from $Q$ by means of Cauchy sequences is due to Cantor. Both Cantor and Dedekind published their constructions in 1872.

## EXERCISES

Unless the contrary is explicitly stated, all numbers that are mentioned in these exercises are understood to be real.

1. If $r$ is rational $(r \neq 0)$ and $x$ is irrational, prove that $r+x$ and $r x$ are irrational.
2. Prove that there is no rational number whose square is 12 .
3. Prove Proposition 1.15.
4. Let $E$ be a nonempty subset of an ordered set; suppose $\alpha$ is a lower bound of $E$ and $\beta$ is an upper bound of $E$. Prove that $\alpha \leq \beta$.
5. Let $A$ be a nonempty set of real numbers which is bounded below. Let $-A$ be the set of all numbers $-x$, where $x \in A$. Prove that

$$
\inf A=-\sup (-A)
$$

6. Fix $b>1$.

(a) If $m, n, p, q$ are integers, $n>0, q>0$, and $r=m / n=p / q$, prove that

$$
\left(b^{m}\right)^{1 / n}=\left(b^{p}\right)^{1 / q} .
$$

Hence it makes sense to define $b^{r}=\left(b^{m}\right)^{1 / n}$.

(b) Prove that $b^{r+s}=b^{r} b^{s}$ if $r$ and $s$ are rational.

(c) If $x$ is real, define $B(x)$ to be the set of all numbers $b^{t}$, where $t$ is rational and $t \leq x$. Prove that

$$
b^{r}=\sup B(r)
$$

when $r$ is rational. Hence it makes sense to define

$$
b^{x}=\sup B(x)
$$

for every real $x$.

(d) Prove that $b^{x+y}=b^{x} b^{y}$ for all real $x$ and $y$.

7. Fix $b>1, y>0$, and prove that there is a unique real $x$ such that $b^{x}=y$, by completing the following outline. (This $x$ is called the logarithm of $y$ to the base $b$.)

(a) For any positive integer $n, b^{n}-1 \geq n(b-1)$.

(b) Hence $b-1 \geq n\left(b^{1 / n}-1\right)$.

(c) If $t>1$ and $n>(b-1) /(t-1)$, then $b^{1 / n}<t$.

(d) If $w$ is such that $b^{w}<y$, then $b^{w+(1 / n)}<y$ for sufficiently large $n$; to see this, apply part (c) with $t=y \cdot b^{-w}$.

(e) If $b^{w}>y$, then $b^{w-(1 / n)}>y$ for sufficiently large $n$.

(f) Let $A$ be the set of all $w$ such that $b^{w}<y$, and show that $x=\sup A$ satisfies $b^{x}=y$.

(g) Prove that this $x$ is unique.

8. Prove that no order can be defined in the complex field that turns it into an ordered field. Hint: -1 is a square.
9. Suppose $z=a+b i, w=c+d i$. Define $z<w$ if $a<c$, and also if $a=c$ but $b<d$. Prove that this turns the set of all complex numbers into an ordered set. (This type of order relation is called a dictionary order, or lexicographic order, for obvious reasons.) Does this ordered set have the least-upper-bound property?
10. Suppose $z=a+b i, w=u+i v$, and

$$
a=\left(\frac{|w|+u}{2}\right)^{1 / 2}, \quad b=\left(\frac{|w|-u}{2}\right)^{1 / 2} .
$$

Prove that $z^{2}=w$ if $v \geq 0$ and that $(\bar{z})^{2}=w$ if $v \leq 0$. Conclude that every complex number (with one exception!) has two complex square roots.

11. If $z$ is a complex number, prove that there exists an $r \geq 0$ and a complex number $w$ with $|w|=1$ such that $z=r w$. Are $w$ and $r$ always uniquely determined by $z$ ?
12. If $z_{1}, \ldots, z_{n}$ are complex, prove that

$$
\left|z_{1}+z_{2}+\cdots+z_{n}\right| \leq\left|z_{1}\right|+\left|z_{2}\right|+\cdots+\left|z_{n}\right| .
$$

13. If $x, y$ are complex, prove that

$$
\| x|-| y|| \leq|x-y|
$$

14. If $z$ is a complex number such that $|z|=1$, that is, such that $z \bar{z}=1$, compute

$$
|1+z|^{2}+|1-z|^{2}
$$

15. Under what conditions does equality hold in the Schwarz inequality?
16. Suppose $k \geq 3, \mathbf{x}, \mathbf{y} \in R^{k},|\mathbf{x}-\mathbf{y}|=d>0$, and $r>0$. Prove:

(a) If $2 r>d$, there are infinitely many $\mathbf{z} \in R^{k}$ such that

$$
|\mathbf{z}-\mathbf{x}|=|\mathbf{z}-\mathbf{y}|=r
$$

(b) If $2 r=d$, there is exactly one such $\mathbf{z}$.

(c) If $2 r<d$, there is no such $z$.

How must these statements be modified if $k$ is 2 or 1 ?

17. Prove that

$$
|x+y|^{2}+|x-y|^{2}=2|x|^{2}+2|y|^{2}
$$

if $\mathbf{x} \in \boldsymbol{R}^{k}$ and $\mathbf{y} \in \boldsymbol{R}^{k}$. Interpret this geometrically, as a statement about parallelograms.

18. If $k \geq 2$ and $\mathbf{x} \in R^{k}$, prove that there exists $y \in R^{k}$ such that $\mathbf{y} \neq 0$ but $\mathbf{x} \cdot \mathbf{y}=0$. Is this also true if $k=1$ ?
19. Suppose $\mathbf{a} \in R^{k}, \mathrm{~b} \in R^{k}$. Find $\mathrm{c} \in R^{k}$ and $r>0$ such that

$$
|x-a|=2|x-b|
$$

if and only if $|\mathbf{x}-\mathbf{c}|=r$.

(Solution: $3 c=4 b-a, 3 r=2|b-a|$.)

20. With reference to the Appendix, suppose that property (III) were omitted from the definition of a cut. Keep the same definitions of order and addition. Show that the resulting ordered set has the least-upper-bound property, that addition satisfies axioms (A1) to (A4) (with a slightly different zero-element!) but that (A5) fails.

## 2

## BASIC TOPOLOGY

## FINITE, COUNTABLE, AND UNCOUNTABLE SETS

We begin this section with a definition of the function concept.

2.1 Definition Consider two sets $A$ and $B$, whose elements may be any objects whatsoever, and suppose that with each element $x$ of $A$ there is associated, in some manner, an element of $B$, which we denote by $f(x)$. Then $f$ is said to be a function from $A$ to $B$ (or a mapping of $A$ into $B$ ). The set $A$ is called the domain of $f$ (we also say $f$ is defined on $A$ ), and the elements $f(x)$ are called the values of $f$. The set of all values of $f$ is called the range of $f$.

2.2 Definition Let $A$ and $B$ be two sets and let $f$ be a mapping of $A$ into $B$. If $E \subset A, f(E)$ is defined to be the set of all elements $f(x)$, for $x \in E$. We call $f(E)$ the image of $E$ under $f$. In this notation, $f(A)$ is the range of $f$. It is clear that $f(A) \subset B$. If $f(A)=B$, we say that $f$ maps $A$ onto $B$. (Note that, according to this usage, onto is more specific than into.)

If $E \subset B, f^{-1}(E)$ denotes the set of all $x \in A$ such that $f(x) \in E$. We call $f^{-1}(E)$ the inverse image of $E$ under $f$. If $y \in B, f^{-1}(y)$ is the set of all $x \in A$
such that $f(x)=y$. If, for each $y \in B, f^{-1}(y)$ consists of at most one element of $A$, then $f$ is said to be a 1-1 (one-to-one) mapping of $A$ into $B$. This may also be expressed as follows: $f$ is a 1-1 mapping of $A$ into $B$ provided that $f\left(x_{1}\right) \neq f\left(x_{2}\right)$ whenever $x_{1} \neq x_{2}, x_{1} \in A, x_{2} \in A$.

(The notation $x_{1} \neq x_{2}$ means that $x_{1}$ and $x_{2}$ are distinct elements; otherwise we write $x_{1}=x_{2}$.)

2.3 Definition If there exists a 1-1 mapping of $A$ onto $B$, we say that $A$ and $B$ can be put in 1-1 correspondence, or that $A$ and $B$ have the same cardinal number, or, briefly, that $A$ and $B$ are equivalent, and we write $A \sim B$. This relation clearly has the following properties:

It is reflexive: $A \sim A$.

It is symmetric: If $A \sim B$, then $B \sim A$.

It is transitive: If $A \sim B$ and $B \sim C$, then $A \sim C$.

Any relation with these three properties is called an equivalence relation.

2.4 Definition For any positive integer $n$, let $J_{n}$ be the set whose elements are the integers $1,2, \ldots, n$; let $J$ be the set consisting of all positive integers. For any set $A$, we say:

(a) $A$ is finite if $A \sim J_{n}$ for some $n$ (the empty set is also considered to be finite).

(b) $A$ is infinite if $A$ is not finite.

(c) $A$ is countable if $A \sim J$.

(d) $A$ is uncountable if $A$ is neither finite nor countable.

(e) $A$ is at most countable if $A$ is finite or countable.

Countable sets are sometimes called enumerable, or denumerable.

For two finite sets $A$ and $B$, we evidently have $A \sim B$ if and only if $A$ and $B$ contain the same number of elements. For infinite sets, however, the idea of "having the same number of elements" becomes quite vague, whereas the notion of 1-1 correspondence retains its clarity.

2.5 Example Let $A$ be the set of all integers. Then $A$ is countable. For, consider the following arrangement of the sets $A$ and $J$ :

$A: \quad 0,1,-1,2,-2,3,-3, \ldots$

$J: \quad 1,2,3,4,5,6,7, \ldots$

We can, in this example, even give an explicit formula for a function $f$ from $J$ to $A$ which sets up a 1-1 correspondence:

$$
f(n)= \begin{cases}\frac{n}{2} & (n \text { even }) \\ -\frac{n-1}{2} & (n \text { odd })\end{cases}
$$

2.6 Remark A finite set cannot be equivalent to one of its proper subsets. That this is, however, possible for infinite sets, is shown by Example 2.5, in which $J$ is a proper subset of $A$.

In fact, we could replace Definition $2.4(b)$ by the statement: $A$ is infinite if $A$ is equivalent to one of its proper subsets.

2.7 Definition By a sequence, we mean a function $f$ defined on the set $J$ of all positive integers. If $f(n)=x_{n}$, for $n \in J$, it is customary to denote the sequence $f$ by the symbol $\left\{x_{n}\right\}$, or sometimes by $x_{1}, x_{2}, x_{3}, \ldots$. The values of $f$, that is, the elements $x_{n}$, are called the terms of the sequence. If $A$ is a set and if $x_{n} \in A$ for all $n \in J$, then $\left\{x_{n}\right\}$ is said to be a sequence in $A$, or a sequence of elements of $A$.

Note that the terms $x_{1}, x_{2}, x_{3}, \ldots$ of a sequence need not be distinct.

Since every countable set is the range of a 1-1 function defined on $J$, we may regard every countable set as the range of a sequence of distinct terms. Speaking more loosely, we may say that the elements of any countable set can be "arranged in a sequence."

Sometimes it is convenient to replace $J$ in this definition by the set of all nonnegative integers, i.e., to start with 0 rather than with 1 .

2.8 Theorem Every infinite subset of a countable set $A$ is countable.

Proof Suppose $E \subset A$, and $E$ is infinite. Arrange the elements $x$ of $A$ in a sequence $\left\{x_{n}\right\}$ of distinct elements. Construct a sequence $\left\{n_{k}\right\}$ as follows:

Let $n_{1}$ be the smallest positive integer such that $x_{n_{1}} \in E$. Having chosen $n_{1}, \ldots, n_{k-1}(k=2,3,4, \ldots)$, let $n_{k}$ be the smallest integer greater than $n_{k-1}$ such that $x_{n_{k}} \in E$.

Putting $f(k)=x_{n_{k}}(k=1,2,3, \ldots)$, we obtain a 1-1 correspondence between $E$ and $J$.

The theorem shows that, roughly speaking, countable sets represent the "smallest" infinity: No uncountable set can be a subset of a countable set.

2.9 Definition Let $A$ and $\Omega$ be sets, and suppose that with each element $\alpha$ of $A$ there is associated a subset of $\Omega$ which we denote by $E_{\alpha}$.

The set whose elements are the sets $E_{\alpha}$ will be denoted by $\left\{E_{\alpha}\right\}$. Instead of speaking of sets of sets, we shall sometimes speak of a collection of sets, or a family of sets.

The union of the sets $E_{\alpha}$ is defined to be the set $S$ such that $x \in S$ if and only if $x \in E_{\alpha}$ for at least one $\alpha \in A$. We use the notation

$$
S=\bigcup_{\alpha \in A} E_{\alpha} .
$$

If $A$ consists of the integers $1,2, \ldots, n$, one usually writes

$$
S=\bigcup_{m=1}^{n} E_{m}
$$

or

$$
S=E_{1} \cup E_{2} \cup \cdots \cup E_{n} .
$$

If $A$ is the set of all positive integers, the usual notation is

$$
S=\bigcup_{m=1}^{\infty} E_{m}
$$

The symbol $\infty$ in (4) merely indicates that the union of a countable collection of sets is taken, and should not be confused with the symbols $+\infty,-\infty$, introduced in Definition 1.23.

The intersection of the sets $E_{\alpha}$ is defined to be the set $P$ such that $x \in P$ if and only if $x \in E_{\alpha}$ for every $\alpha \in A$. We use the notation

$$
P=\bigcap_{\alpha \in A} E_{\alpha},
$$

or

$$
P=\bigcap_{m=1}^{n} E_{m}=E_{1} \cap E_{2} \cap \cdots \cap E_{n}
$$

or

$$
P=\bigcap_{m=1}^{\infty} E_{m}
$$

as for unions. If $A \cap B$ is not empty, we say that $A$ and $B$ intersect; otherwise they are disjoint.

### 2.10 Examples

(a) Suppose $E_{1}$ consists of $1,2,3$ and $E_{2}$ consists of 2,3,4. Then $E_{1} \cup E_{2}$ consists of $1,2,3,4$, whereas $E_{1} \cap E_{2}$ consists of 2,3 .
(b) Let $A$ be the set of real numbers $x$ such that $0<x \leq 1$. For every $x \in A$, let $E_{x}$ be the set of real numbers $y$ such that $0<y<x$. Then

$$
\begin{aligned}
& E_{x} \subset E_{z} \text { if and only if } 0<x \leq z \leq 1 ; \\
& \qquad \bigcup_{x \in A} E_{x}=E_{1} ; \\
& \bigcap_{x \in A} E_{x} \text { is empty; }
\end{aligned}
$$

(i) and (ii) are clear. To prove (iii), we note that for every $y>0, y \notin E_{x}$ if $x<y$. Hence $y \notin \bigcap_{x \in A} E_{x}$.

2.11 Remarks Many properties of unions and intersections are quite similar to those of sums and products; in fact, the words sum and product were sometimes used in this connection, and the symbols $\Sigma$ and $\Pi$ were written in place of $\bigcup$ and $\cap$.

The commutative and associative laws are trivial:

$$
\begin{aligned}
A \cup B & =B \cup A ; & A \cap B & =B \cap A . \\
(A \cup B) \cup C & =A \cup(B \cup C) ; & (A \cap B) \cap C & =A \cap(B \cap C) .
\end{aligned}
$$

Thus the omission of parentheses in (3) and (6) is justified.

The distributive law also holds:

$$
A \cap(B \cup C)=(A \cap B) \cup(A \cap C)
$$

To prove this, let the left and right members of (10) be denoted by $E$ and $F$, respectively.

Suppose $x \in E$. Then $x \in A$ and $x \in B \cup C$, that is, $x \in B$ or $x \in C$ (possibly both). Hence $x \in A \cap B$ or $x \in A \cap C$, so that $x \in F$. Thus $E \subset F$.

Next, suppose $x \in F$. Then $x \in A \cap B$ or $x \in A \cap C$. That is, $x \in A$, and $x \in B \cup C$. Hence $x \in A \cap(B \cup C)$, so that $F \subset E$.

It follows that $E=F$.

We list a few more relations which are easily verified:

$$
\begin{aligned}
& A \subset A \cup B, \\
& A \cap B \subset A .
\end{aligned}
$$

If 0 denotes the empty set, then

$$
A \cup 0=A, \quad A \cap 0=0 .
$$

If $A \subset B$, then

$$
A \cup B=B, \quad A \cap B=A
$$

2.12 Theorem Let $\left\{E_{n}\right\}, n=1,2,3, \ldots$, be a sequence of countable sets, and put

Then $S$ is countable.

$$
S=\bigcup_{n=1}^{\infty} E_{n}
$$

Proof Let every set $E_{n}$ be arranged in a sequence $\left\{x_{n k}\right\}, k=1,2,3, \ldots$, and consider the infinite array

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-040.jpg?height=230&width=684&top_left_y=671&top_left_x=772)

in which the elements of $E_{n}$ form the $n$th row. The array contains all elements of $S$. As indicated by the arrows, these elements can be arranged in a sequence

$$
x_{11} ; x_{21}, x_{12} ; x_{31}, x_{22}, x_{13} ; x_{41}, x_{32}, x_{23}, x_{14} ; \ldots
$$

If any two of the sets $E_{n}$ have elements in common, these will appear more than once in (17). Hence there is a subset $T$ of the set of all positive integers such that $S \sim T$, which shows that $S$ is at most countable (Theorem 2.8). Since $E_{1} \subset S$, and $E_{1}$ is infinite, $S$ is infinite, and thus countable.

Corollary Suppose $A$ is at most countable, and, for every $\alpha \in A, B_{\alpha}$ is at most countable. Put

Then $T$ is at most countable.

$$
T=\bigcup_{\alpha \in A} B_{\alpha} .
$$

For $T$ is equivalent to a subset of (15).

2.13 Theorem Let $A$ be a countable set, and let $B_{n}$ be the set of all $n$-tuples $\left(a_{1}, \ldots, a_{n}\right)$, where $a_{k} \in A(k=1, \ldots, n)$, and the elements $a_{1}, \ldots, a_{n}$ need not be distinct. Then $B_{n}$ is countable.

Proof That $B_{1}$ is countable is evident, since $B_{1}=A$. Suppose $B_{n-1}$ is countable $(n=2,3,4, \ldots)$. The elements of $B_{n}$ are of the form

$$
(b, a) \quad\left(b \in B_{n-1}, a \in A\right) .
$$

For every fixed $b$, the set of pairs $(b, a)$ is equivalent to $A$, and hence countable. Thus $B_{n}$ is the union of a countable set of countable sets. By Theorem 2.12, $B_{n}$ is countable.

The theorem follows by induction.

Corollary The set of all rational numbers is countable.

Proof We apply Theorem 2.13, with $n=2$, noting that every rational $r$ is of the form $b / a$, where $a$ and $b$ are integers. The set of pairs $(a, b)$, and therefore the set of fractions $b / a$, is countable. cise 2).

In fact, even the set of all algebraic numbers is countable (see Exer-

That not all infinite sets are, however, countable, is shown by the next theorem.

2.14 Theorem Let $A$ be the set of all sequences whose elements are the digits 0 and 1 . This set $A$ is uncountable.

The elements of $A$ are sequences like $1,0,0,1,0,1,1,1, \ldots$

Proof Let $E$ be a countable subset of $A$, and let $E$ consist of the sequences $s_{1}, s_{2}, s_{3}, \ldots$ We construct a sequence $s$ as follows. If the $n$th digit in $s_{n}$ is 1 , we let the $n$th digit of $s$ be 0 , and vice versa. Then the sequence $s$ differs from every member of $E$ in at least one place; hence $s \notin E$. But clearly $s \in A$, so that $E$ is a proper subset of $A$.

We have shown that every countable subset of $A$ is a proper subset of $A$. It follows that $A$ is uncountable (for otherwise $A$ would be a proper subset of $A$, which is absurd).

The idea of the above proof was first used by Cantor, and is called Cantor's diagonal process; for, if the sequences $s_{1}, s_{2}, s_{3}, \ldots$ are placed in an array like (16), it is the elements on the diagonal which are involved in the construction of the new sequence.

Readers who are familiar with the binary representation of the real numbers (base 2 instead of 10) will notice that Theorem 2.14 implies that the set of all real numbers is uncountable. We shall give a second proof of this fact in Theorem 2.43.

## METRIC SPACES

2.15 Definition A set $X$, whose elements we shall call points, is said to be a metric space if with any two points $p$ and $q$ of $X$ there is associated a real number $d(p, q)$, called the distance from $p$ to $q$, such that

(a) $d(p, q)>0$ if $p \neq q ; d(p, p)=0$;

(b) $d(p, q)=d(q, p)$;

(c) $d(p, q) \leq d(p, r)+d(r, q)$, for any $r \in X$. a metric.

Any function with these three properties is called a distance function, or

2.16 Examples The most important examples of metric spaces, from our standpoint, are the euclidean spaces $R^{k}$, especially $R^{1}$ (the real line) and $R^{2}$ (the complex plane); the distance in $R^{k}$ is defined by

$$
d(\mathbf{x}, \mathbf{y})=|\mathbf{x}-\mathbf{y}| \quad\left(\mathbf{x}, \mathbf{y} \in R^{k}\right)
$$

By Theorem 1.37, the conditions of Definition 2.15 are satisfied by (19).

It is important to observe that every subset $Y$ of a metric space $X$ is a metric space in its own right, with the same distance function. For it is clear that if conditions $(a)$ to $(c)$ of Definition 2.15 hold for $p, q, r \in X$, they also hold if we restrict $p, q, r$ to lie in $Y$.

Thus every subset of a euclidean space is a metric space. Other examples are the spaces $\mathscr{C}(K)$ and $\mathscr{L}^{2}(\mu)$, which are discussed in Chaps. 7 and 11 , respectively.

2.17 Definition By the segment $(a, b)$ we mean the set of all real numbers $x$ such that $a<x<b$.

By the interval $[a, b]$ we mean the set of all real numbers $x$ such that $a \leq x \leq b$.

Occasionally we shall also encounter "half-open intervals" $[a, b)$ and $(a, b]$; the first consists of all $x$ such that $a \leq x<b$, the second of all $x$ such that $a<x \leq b$.

If $a_{i}<b_{i}$ for $i=1, \ldots, k$, the set of all points $\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right)$ in $R^{k}$ whose coordinates satisfy the inequalities $a_{i} \leq x_{i} \leq b_{i}(1 \leq i \leq k)$ is called a $k$-cell. Thus a 1-cell is an interval, a 2-cell is a rectangle, etc.

If $\mathbf{x} \in R^{k}$ and $r>0$, the open (or closed) ball $B$ with center at $\mathbf{x}$ and radius $r$ is defined to be the set of all $\mathbf{y} \in R^{k}$ such that $|\mathbf{y}-\mathbf{x}|<r$ (or $|\mathbf{y}-\mathbf{x}| \leq r$ ).

We call a set $E \subset R^{k}$ convex if

$$
\lambda \mathbf{x}+(1-\lambda) \mathbf{y} \in E
$$

whenever $\mathbf{x} \in E, \mathbf{y} \in E$, and $0<\lambda<1$.

For example, balls are convex. For if $|\mathbf{y}-\mathbf{x}|<r,|\mathbf{z}-\mathbf{x}|<r$, and $0<\lambda<1$, we have

$$
\begin{aligned}
|\lambda \mathbf{y}+(1-\lambda) \mathbf{z}-\mathbf{x}| & =|\lambda(\mathbf{y}-\mathbf{x})+(1-\lambda)(\mathbf{z}-\mathbf{x})| \\
& \leq \lambda|\mathbf{y}-\mathbf{x}|+(1-\lambda)|\mathbf{z}-\mathbf{x}|<\lambda r+(1-\lambda) r \\
& =r .
\end{aligned}
$$

The same proof applies to closed balls. It is also easy to see that $k$-cells are convex.

2.18 Definition Let $X$ be a metric space. All points and sets mentioned below are understood to be elements and subsets of $X$.

(a) A neighborhood of $p$ is a set $N_{r}(p)$ consisting of all $q$ such that $d(p, q)<r$, for some $r>0$. The number $r$ is called the radius of $N_{r}(p)$.

(b) A point $p$ is a limit point of the set $E$ if every neighborhood of $p$ contains a point $q \neq p$ such that $q \in E$.

(c) If $p \in E$ and $p$ is not a limit point of $E$, then $p$ is called an isolated point of $E$.

(d) $E$ is closed if every limit point of $E$ is a point of $E$.

(e) A point $p$ is an interior point of $E$ if there is a neighborhood $N$ of $p$ such that $N \subset E$.

(f) $E$ is open if every point of $E$ is an interior point of $E$.

(g) The complement of $E$ (denoted by $E^{c}$ ) is the set of all points $p \in X$ such that $p \notin E$.

(h) $E$ is perfect if $E$ is closed and if every point of $E$ is a limit point of $E$.

(i) $E$ is bounded if there is a real number $M$ and a point $q \in X$ such that $d(p, q)<M$ for all $p \in E$.

(j) $E$ is dense in $X$ if every point of $X$ is a limit point of $E$, or a point of $E$ (or both).

Let us note that in $R^{1}$ neighborhoods are segments, whereas in $R^{2}$ neighborhoods are interiors of circles.

2.19 Theorem Every neighborhood is an open set.

Proof Consider a neighborhood $E=N_{r}(p)$, and let $q$ be any point of $E$. Then there is a positive real number $h$ such that

$$
d(p, q)=r-h \text {. }
$$

For all points $s$ such that $d(q, s)<h$, we have then

$$
d(p, s) \leq d(p, q)+d(q, s)<r-h+h=r
$$

so that $s \in E$. Thus $q$ is an interior point of $E$.

2.20 Theorem If $p$ is a limit point of a set $E$, then every neighborhood of $p$ contains infinitely many points of $E$.

Proof Suppose there is a neighborhood $N$ of $p$ which contains only a finite number of points of $E$. Let $q_{1}, \ldots, q_{n}$ be those points of $N \cap E$, which are distinct from $p$, and put

$$
r=\min _{1 \leq m \leq n} d\left(p, q_{m}\right)
$$

[we use this notation to denote the smallest of the numbers $d\left(p, q_{1}\right), \ldots$, $d\left(p, q_{n}\right)$ ]. The minimum of a finite set of positive numbers is clearly positive, so that $r>0$.

The neighborhood $N_{r}(p)$ contains no point $q$ of $E$ such that $q \neq p$, so that $p$ is not a limit point of $E$. This contradiction establishes the theorem.

Corollary A finite point set has no limit points.

2.21 Examples Let us consider the following subsets of $R^{2}$ :

(a) The set of all complex $z$ such that $|z|<1$.

(b) The set of all complex $z$ such that $|z| \leq 1$.

(c) A nonempty finite set.

(d) The set of all integers.

(e) The set consisting of the numbers $1 / n(n=1,2,3, \ldots)$. Let us note that this set $E$ has a limit point (namely, $z=0$ ) but that no point of $E$ is a limit point of $E$; we wish to stress the difference between having a limit point and containing one.

$(f)$ The set of all complex numbers (that is, $R^{2}$ ).

(g) The segment $(a, b)$.

Let us note that $(d),(e),(g)$ can be regarded also as subsets of $R^{1}$. Some properties of these sets are tabulated below:

|  | Closed | Open | Perfect | Bounded |
| :--- | :---: | :---: | :---: | :---: |
| $(a)$ | No | Yes | No | Yes |
| $(b)$ | Yes | No | Yes | Yes |
| $(c)$ | Yes | No | No | Yes |
| $(d)$ | Yes | No | No | No |
| $(e)$ | No | No | No | Yes |
| $(f)$ | Yes | Yes | Yes | No |
| $(g)$ | No |  | No | Yes |

In $(g)$, we left the second entry blank. The reason is that the segment $(a, b)$ is not open if we regard it as a subset of $\boldsymbol{R}^{2}$, but it is an open subset of $\boldsymbol{R}^{\mathbf{1}}$.

2.22 Theorem Let $\left\{E_{\alpha}\right\}$ be a (finite or infinite) collection of sets $E_{a}$. Then

$$
\left(\bigcup_{a} E_{a}\right)^{c}=\bigcap_{a}\left(E_{a}^{c}\right) .
$$

Proof Let $A$ and $B$ be the left and right members of (20). If $x \in A$, then $x \notin \bigcup_{a} E_{a}$, hence $x \notin E_{a}$ for any $\alpha$, hence $x \in E_{a}^{c}$ for every $\alpha$, so that $x \in \cap E_{a}^{c}$. Thus $A \subset B$.

Conversely, if $x \in B$, then $x \in E_{\alpha}^{c}$ for every $\alpha$, hence $x \notin E_{\alpha}$ for any $\alpha$, hence $x \notin \bigcup_{a} E_{a}$, so that $x \in\left(\bigcup_{\alpha} E_{\alpha}\right)^{c}$. Thus $B \subset A$.

It follows that $A=B$.

2.23 Theorem A set $E$ is open if and only if its complement is closed.

Proof First, suppose $E^{c}$ is closed. Choose $x \in E$. Then $x \notin E^{c}$, and $x$ is not a limit point of $E^{c}$. Hence there exists a neighborhood $N$ of $x$ such that $E^{c} \cap N$ is empty, that is, $N \subset E$. Thus $x$ is an interior point of $E$, and $E$ is open.

Next, suppose $E$ is open. Let $x$ be a limit point of $E^{c}$. Then every neighborhood of $x$ contains a point of $E^{c}$, so that $x$ is not an interior point of $E$. Since $E$ is open, this means that $x \in E^{c}$. It follows that $E^{c}$ is closed.

Corollary $A$ set $F$ is closed if and only if its complement is open.

### 2.24 Theorem

(a) For any collection $\left\{G_{a}\right\}$ of open sets, $\cup_{a} G_{a}$ is open.

(b) For any collection $\left\{F_{a}\right\}$ of closed sets, $\bigcap_{a} F_{a}$ is closed.

(c) For any finite collection $G_{1}, \ldots, G_{n}$ of open sets, $\bigcap_{i=1}^{n} G_{i}$ is open.

(d) For any finite collection $F_{1}, \ldots, F_{n}$ of closed sets, $\bigcup_{i=1}^{n} F_{i}$ is closed.

Proof Put $G=\bigcup_{a} G_{a}$. If $x \in G$, then $x \in G_{a}$ for some $\alpha$. Since $x$ is an interior point of $G_{a}, x$ is also an interior point of $G$, and $G$ is open. This proves $(a)$.

By Theorem 2.22,

$$
\left(\bigcap_{a} F_{a}\right)^{c}=\bigcup_{a}\left(F_{a}^{c}\right)
$$

and $F_{\alpha}^{c}$ is open, by Theorem 2.23. Hence (a) implies that (21) is open so that $\bigcap_{a} F_{a}$ is closed.

Next, put $H=\bigcap_{i=1}^{n} G_{l}$. For any $x \in H$, there exist neighborhoods $N_{l}$ of $x$, with radii $r_{l}$, such that $N_{l} \subset G_{l}(i=1, \ldots, n)$. Put

$$
r=\min \left(r_{1}, \ldots, r_{n}\right)
$$

and let $N$ be the neighborhood of $x$ of radius $r$. Then $N \subset G_{i}$ for $i=1$, $\ldots, n$, so that $N \subset H$, and $H$ is open.

By taking complements, $(d)$ follows from $(c)$ :

$$
\left(\bigcup_{l=1}^{n} F_{l}\right)^{c}=\bigcap_{i=1}^{n}\left(F_{i}^{c}\right) .
$$

2.25 Examples In parts $(c)$ and $(d)$ of the preceding theorem, the finiteness of the collections is essential. For let $G_{n}$ be the segment $\left(-\frac{1}{n}, \frac{1}{n}\right)(n=1,2,3, \ldots)$. Then $G_{n}$ is an open subset of $R^{1}$. Put $G=\bigcap_{n=1}^{\infty} G_{n}$. Then $G$ consists of a single point (namely, $x=0$ ) and is therefore not an open subset of $R^{1}$.

Thus the intersection of an infinite collection of open sets need not be open. Similarly, the union of an infinite collection of closed sets need not be closed.

2.26 Definition If $X$ is a metric space, if $E \subset X$, and if $E^{\prime}$ denotes the set of all limit points of $E$ in $X$, then the closure of $E$ is the set $E=E \cup E^{\prime}$.

2.27 Theorem If $X$ is a metric space and $E \subset X$, then

(a) $E$ is closed,

(b) $E=E$ if and only if $E$ is closed,

(c) $E \subset F$ for every closed set $F \subset X$ such that $E \subset F$.

By $(a)$ and $(c), E$ is the smallest closed subset of $X$ that contains $E$.

## Proof

(a) If $p \in X$ and $p \notin E$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. The complement of $E$ is therefore open. Hence $E$ is closed.

(b) If $E=E,(a)$ implies that $E$ is closed. If $E$ is closed, then $E^{\prime} \subset E$ [by Definitions 2.18(d) and 2.26], hence $E=E$.

(c) If $F$ is closed and $F \supset E$, then $F \supset F^{\prime}$, hence $F \supset E^{\prime}$. Thus $F \supset E$.

2.28 Theorem Let $E$ be a nonempty set of real numbers which is bounded above. Let $y=\sup E$. Then $y \in E$. Hence $y \in E$ if $E$ is closed.

Compare this with the examples in Sec. 1.9.

Proof If $y \in E$ then $y \in E$. Assume $y \notin E$. For every $h>0$ there exists then a point $x \in E$ such that $y-h<x<y$, for otherwise $y-h$ would be an upper bound of $E$. Thus $y$ is a limit point of $E$. Hence $y \in E$.

2.29 Remark Suppose $E \subset Y \subset X$, where $X$ is a metric space. To say that $E$ is an open subset of $X$ means that to each point $p \in E$ there is associated a positive number $r$ such that the conditions $d(p, q)<r, q \in X$ imply that $q \in E$. But we have already observed (Sec. 2.16) that $Y$ is also a metric space, so that our definitions may equally well be made within $Y$. To be quite explicit, let us say that $E$ is open relative to $Y$ if to each $p \in E$ there is associated an $r>0$ such that $q \in E$ whenever $d(p, q)<r$ and $q \in Y$. Example $2.21(g)$ showed that a set
may be open relative to $Y$ without being an open subset of $X$. However, there is a simple relation between these concepts, which we now state.

2.30 Theorem Suppose $Y \subset X . A$ subset $E$ of $Y$ is open relative to $Y$ if and only if $E=Y \cap G$ for some open subset $G$ of $X$.

Proof Suppose $E$ is open relative to $Y$. To each $p \in E$ there is a positive number $r_{p}$ such that the conditions $d(p, q)<r_{p}, q \in Y$ imply that $q \in E$. Let $V_{p}$ be the set of all $q \in X$ such that $d(p, q)<r_{p}$, and define

$$
G=\bigcup_{p \in E} V_{p} .
$$

Then $G$ is an open subset of $X$, by Theorems 2.19 and 2.24.

Since $p \in V_{p}$ for all $p \in E$, it is clear that $E \subset G \cap Y$.

By our choice of $V_{p}$, we have $V_{p} \cap Y \subset E$ for every $p \in E$, so that

$G \cap Y \subset E$. Thus $E=G \cap Y$, and one half of the theorem is proved.

Conversely, if $G$ is open in $X$ and $E=G \cap Y$, every $p \in E$ has a

neighborhood $V_{p} \subset G$. Then $V_{p} \cap Y \subset E$, so that $E$ is open relative to $Y$.

## COMPACT SETS

2.31 Definition By an open cover of a set $E$ in a metric space $X$ we mean a collection $\left\{G_{\alpha}\right\}$ of open subsets of $X$ such that $E \subset \bigcup_{\alpha} G_{\alpha}$.

2.32 Definition A subset $K$ of a metric space $X$ is said to be compact if every open cover of $K$ contains a finite subcover.

More explicitly, the requirement is that if $\left\{G_{\alpha}\right\}$ is an open cover of $K$, then there are finitely many indices $\alpha_{1}, \ldots, \alpha_{n}$ such that

$$
K \subset G_{\alpha_{1}} \cup \cdots \cup G_{\alpha_{n}} .
$$

The notion of compactness is of great importance in analysis, especially in connection with continuity (Chap. 4).

It is clear that every finite set is compact. The existence of a large class of infinite compact sets in $R^{k}$ will follow from Theorem 2.41 .

We observed earlier (in Sec. 2.29) that if $E \subset Y \subset X$, then $E$ may be open relative to $Y$ without being open relative to $X$. The property of being open thus depends on the space in which $E$ is embedded. The same is true of the property of being closed.

Compactness, however, behaves better, as we shall now see. To formulate the next theorem, let us say, temporarily, that $K$ is compact relative to $X$ if the requirements of Definition 2.32 are met.

2.33 Theorem Suppose $K \subset Y \subset X$. Then $K$ is compact relative to $X$ if and only if $K$ is compact relative to $Y$.

By virtue of this theorem we are able, in many situations, to regard compact sets as metric spaces in their own right, without paying any attention to any embedding space. In particular, although it makes little sense to talk of open spaces, or of closed spaces (every metric space $X$ is an open subset of itself, and is a closed subset of itself), it does make sense to talk of compact metric spaces.

Proof Suppose $K$ is compact relative to $X$, and let $\left\{V_{\alpha}\right\}$ be a collection of sets, open relative to $Y$, such that $K \subset \bigcup_{\alpha} V_{\alpha}$. By theorem 2.30, there are sets $G_{\alpha}$, open relative to $X$, such that $V_{\alpha}=\mathrm{Y} \cap G_{\alpha}$, for all $\alpha$; and since $K$ is compact relative to $X$, we have

$$
K \subset G_{\alpha_{1}} \cup \cdots \cup G_{\alpha_{n}}
$$

for some choice of finitely many indices $\alpha_{1}, \ldots, \alpha_{n}$. Since $K \subset Y$, (22) implies

$$
K \subset V_{\alpha_{1}} \cup \cdots \cup V_{\alpha_{n}} .
$$

This proves that $K$ is compact relative to $Y$.

Conversely, suppose $K$ is compact relative to $Y$, let $\left\{G_{\alpha}\right\}$ be a collection of open subsets of $X$ which covers $K$, and put $V_{\alpha}=Y \cap G_{\alpha}$. Then (23) will hold for some choice of $\alpha_{1}, \ldots, \alpha_{n}$; and since $V_{\alpha} \subset G_{\alpha}$, (23) implies (22).

This completes the proof.

2.34 Theorem Compact subsets of metric spaces are closed.

Proof Let $K$ be a compact subset of a metric space $X$. We shall prove that the complement of $K$ is an open subset of $X$.

Suppose $p \in X, p \notin K$. If $q \in K$, let $V_{q}$ and $W_{q}$ be neighborhoods of $p$ and $q$, respectively, of radius less than $\frac{1}{2} d(p, q)$ [see Definition 2.18(a)]. Since $K$ is compact, there are finitely many points $q_{1}, \ldots, q_{n}$ in $K$ such that

$$
K \subset W_{q_{1}} \cup \cdots \cup W_{q_{n}}=W .
$$

If $V=V_{q_{1}} \cap \cdots \cap V_{q_{n}}$, then $V$ is a neighborhood of $p$ which does not intersect $W$. Hence $V \subset K^{c}$, so that $p$ is an interior point of $K^{c}$. The theorem follows.

2.35 Theorem Closed subsets of compact sets are compact.

Proof Suppose $F \subset K \subset X, F$ is closed (relative to $X$ ), and $K$ is compact. Let $\left\{V_{\alpha}\right\}$ be an open cover of $F$. If $F^{c}$ is adjoined to $\left\{V_{\alpha}\right\}$, we obtain an
open cover $\Omega$ of $K$. Since $K$ is compact, there is a finite subcollection $\Phi$ of $\Omega$ which covers $K$, and hence $F$. If $F^{c}$ is a member of $\Phi$, we may remove it from $\Phi$ and still retain an open cover of $F$. We have thus shown that a finite subcollection of $\left\{V_{\alpha}\right\}$ covers $F$.

Corollary If $F$ is closed and $K$ is compact, then $F \cap K$ is compact.

Proof Theorems $2.24(b)$ and 2.34 show that $F \cap K$ is closed; since $F \cap K \subset K$, Theorem 2.35 shows that $F \cap K$ is compact.

2.36 Theorem If $\left\{K_{\alpha}\right\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\left\{K_{\alpha}\right\}$ is nonempty, then $\cap K_{\alpha}$ is nonempty.

Proof Fix a member $K_{1}$ of $\left\{K_{a}\right\}$ and put $G_{a}=K_{\alpha}^{c}$. Assume that no point of $K_{1}$ belongs to every $K_{a}$. Then the sets $G_{a}$ form an open cover of $K_{1}$; and since $K_{1}$ is compact, there are finitely many indices $\alpha_{1}, \ldots, \alpha_{n}$ such that $K_{1} \subset G_{a_{1}} \cup \cdots \cup G_{a_{n}}$. But this means that

$$
K_{1} \cap K_{a_{1}} \cap \cdots \cap K_{\alpha_{n}}
$$

is empty, in contradiction to our hypothesis.

Corollary If $\left\{K_{n}\right\}$ is a sequence of nonempty compact sets such that $K_{n} \supset K_{n+1}$ $(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} K_{n}$ is not empty.

2.37 Theorem If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K$.

Proof If no point of $K$ were a limit point of $E$, then each $q \in K$ would have a neighborhood $V_{q}$ which contains at most one point of $E$ (namely, $q$, if $q \in E$ ). It is clear that no finite subcollection of $\left\{V_{q}\right\}$ can cover $E$; and the same is true of $K$, since $E \subset K$. This contradicts the compactness of $K$.

2.38 Theorem If $\left\{I_{n}\right\}$ is a sequence of intervals in $R^{1}$, such that $I_{n} \supset I_{n+1}$ $(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} I_{n}$ is not empty.

Proof If $I_{n}=\left[a_{n}, b_{n}\right]$, let $E$ be the set of all $a_{n}$. Then $E$ is nonempty and bounded above (by $b_{1}$ ). Let $x$ be the sup of $E$. If $m$ and $n$ are positive integers, then

$$
a_{n} \leq a_{m+n} \leq b_{m+n} \leq b_{m},
$$

so that $x \leq b_{m}$ for each $m$. Since it is obvious that $a_{m} \leq x$, we see that $x \in I_{m}$ for $m=1,2,3, \ldots$

2.39 Theorem Let $k$ be a positive integer. If $\left\{I_{n}\right\}$ is a sequence of $k$-cells such that $I_{n} \supset I_{n+1}(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} I_{n}$ is not empty.

Proof Let $I_{n}$ consist of all points $\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right)$ such that

$$
a_{n, j} \leq x_{j} \leq b_{n, j} \quad(1 \leq j \leq k ; n=1,2,3, \ldots)
$$

and put $I_{n, j}=\left[a_{n, j}, b_{n, j}\right]$. For each $j$, the sequence $\left\{I_{n, j}\right\}$ satisfies the hypotheses of Theorem 2.38. Hence there are real numbers $x_{j}^{*}(1 \leq j \leq k)$ such that

$$
a_{n, j} \leq x_{j}^{*} \leq b_{n, j} \quad(1 \leq j \leq k ; n=1,2,3, \ldots)
$$

Setting $\mathbf{x}^{*}=\left(x_{1}^{*}, \ldots, x_{k}^{*}\right)$, we see that $\mathbf{x}^{*} \in I_{n}$ for $n=1,2,3, \ldots$ The theorem follows.

2.40 Theorem Every $k$-cell is compact.

Proof Let $I$ be a $k$-cell, consisting of all points $\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right)$ such that $a_{j} \leq x_{j} \leq b_{j}(1 \leq j \leq k)$. Put

$$
\delta=\left\{\sum_{i}^{k}\left(b_{j}-a_{j}\right)^{2}\right\}^{1 / 2}
$$

Then $|\mathbf{x}-\mathbf{y}| \leq \delta$, if $\mathbf{x} \in I, \mathbf{y} \in I$.

Suppose, to get a contradiction, that there exists an open cover $\left\{G_{\alpha}\right\}$ of $I$ which contains no finite subcover of $I$. Put $c_{j}=\left(a_{j}+b_{j}\right) / 2$. The intervals $\left[a_{j}, c_{j}\right]$ and $\left[c_{j}, b_{j}\right]$ then determine $2^{k} k$-cells $Q_{i}$ whose union is $I$. At least one of these sets $Q_{i}$, call it $I_{1}$, cannot be covered by any finite subcollection of $\left\{G_{\alpha}\right\}$ (otherwise $I$ could be so covered). We next subdivide $I_{1}$ and continue the process. We obtain a sequence $\left\{I_{n}\right\}$ with the following properties:

(a) $I \supset I_{1} \supset I_{2} \supset I_{3} \supset \cdots$;

(b) $I_{n}$ is not covered by any finite subcollection of $\left\{G_{\alpha}\right\}$;

(c) if $\mathbf{x} \in I_{n}$ and $\mathbf{y} \in I_{n}$, then $|\mathbf{x}-\mathbf{y}| \leq 2^{-n} \delta$.

By $(a)$ and Theorem 2.39, there is a point $\mathbf{x}^{*}$ which lies in every $I_{n}$. For some $\alpha, \mathbf{x}^{*} \in G_{\alpha}$. Since $G_{\alpha}$ is open, there exists $r>0$ such that $\left|\mathbf{y}-\mathbf{x}^{*}\right|<r$ implies that $\mathbf{y} \in G_{\alpha}$. If $n$ is so large that $2^{-n} \delta<r$ (there is such an $n$, for otherwise $2^{n} \leq \delta / r$ for all positive integers $n$, which is absurd since $R$ is archimedean), then (c) implies that $I_{n} \subset G_{\alpha}$, which contradicts $(b)$.

This completes the proof.

The equivalence of $(a)$ and $(b)$ in the next theorem is known as the HeineBorel theorem.

2.41 Theorem If a set $E$ in $R^{k}$ has one of the following three properties, then it has the other two:

(a) $E$ is closed and bounded.

(b) $E$ is compact.

(c) Every infinite subset of $E$ has a limit point in $E$.

Proof If (a) holds, then $E \subset I$ for some $k$-cell $I$, and $(b)$ follows from Theorems 2.40 and 2.35. Theorem 2.37 shows that $(b)$ implies $(c)$. It remains to be shown that $(c)$ implies $(a)$.

If $E$ is not bounded, then $E$ contains points $\mathbf{x}_{n}$ with

$$
\left|\mathbf{x}_{n}\right|>n \quad(n=1,2,3, \ldots)
$$

The set $S$ consisting of these points $\mathbf{x}_{n}$ is infinite and clearly has no limit point in $R^{k}$, hence has none in $E$. Thus $(c)$ implies that $E$ is bounded.

If $E$ is not closed, then there is a point $\mathbf{x}_{0} \in R^{k}$ which is a limit point of $E$ but not a point of $E$. For $n=1,2,3, \ldots$, there are points $\mathbf{x}_{n} \in E$ such that $\left|\mathbf{x}_{n}-\mathbf{x}_{0}\right|<1 / n$. Let $S$ be the set of these points $\mathbf{x}_{n}$. Then $S$ is infinite (otherwise $\left|\mathbf{x}_{n}-\mathbf{x}_{0}\right|$ would have a constant positive value, for infinitely many $n$ ), $S$ has $\mathbf{x}_{0}$ as a limit point, and $S$ has no other limit point in $R^{k}$. For if $\mathbf{y} \in R^{k}, \mathbf{y} \neq \mathbf{x}_{0}$, then

$$
\begin{aligned}
\left|\mathbf{x}_{n}-\mathbf{y}\right| & \geq\left|\mathbf{x}_{0}-\mathbf{y}\right|-\left|\mathbf{x}_{n}-\mathbf{x}_{0}\right| \\
& \geq\left|\mathbf{x}_{0}-\mathbf{y}\right|-\frac{1}{n} \geq \frac{1}{2}\left|\mathbf{x}_{0}-\mathbf{y}\right|
\end{aligned}
$$

for all but finitely many $n$; this shows that $\mathbf{y}$ is not a limit point of $S$ (Theorem 2.20).

Thus $S$ has no limit point in $E$; hence $E$ must be closed if $(c)$ holds.

We should remark, at this point, that $(b)$ and $(c)$ are equivalent in any metric space (Exercise 26) but that $(a)$ does not, in general, imply $(b)$ and $(c)$. Examples are furnished by Exercise 16 and by the space $\mathscr{L}^{2}$, which is discussed in Chap. 11.

2.42 Theorem (Weierstrass) Every bounded infinite subset of $R^{k}$ has a limit point in $R^{k}$.

Proof Being bounded, the set $E$ in question is a subset of a $k$-cell $I \subset R^{k}$. By Theorem 2.40, $I$ is compact, and so $E$ has a limit point in $I$, by Theorem 2.37.

## PERFECT SETS

2.43 Theorem Let $P$ be a nonempty perfect set in $R^{k}$. Then $P$ is uncountable.

Proof Since $P$ has limit points, $P$ must be infinite. Suppose $P$ is countable, and denote the points of $P$ by $\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3}, \ldots$ We shall construct a sequence $\left\{V_{n}\right\}$ of neighborhoods, as follows.

Let $V_{1}$ be any neighborhood of $\mathbf{x}_{1}$. If $V_{1}$ consists of all $\mathbf{y} \in R^{k}$ such that $\left|\mathbf{y}-\mathbf{x}_{1}\right|<r$, the closure $\bar{V}_{1}$ of $V_{1}$ is the set of all $\mathbf{y} \in R^{k}$ such that $\left|\mathbf{y}-\mathbf{x}_{1}\right| \leq \boldsymbol{r}$.

Suppose $V_{n}$ has been constructed, so that $V_{n} \cap P$ is not empty. Since every point of $P$ is a limit point of $P$, there is a neighborhood $V_{n+1}$ such that (i) $\bar{V}_{n+1} \subset V_{n}$, (ii) $\mathbf{x}_{n} \notin \bar{V}_{n+1}$, (iii) $V_{n+1} \cap P$ is not empty. By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed.

Put $K_{n}=\bar{V}_{n} \cap P$. Since $\bar{V}_{n}$ is closed and bounded, $\bar{V}_{n}$ is compact. Since $\mathbf{x}_{n} \notin K_{n+1}$, no point of $P$ lies in $\bigcap_{1}^{\infty} K_{n}$. Since $K_{n} \subset P$, this implies that $\bigcap_{1}^{\infty} K_{n}$ is empty. But each $K_{n}$ is nonempty, by (iii), and $K_{n} \supset K_{n+1}$, by (i); this contradicts the Corollary to Theorem 2.36.

Corollary Every interval $[a, b](a<b)$ is uncountable. In particular, the set of all real numbers is uncountable.

2.44 The Cantor set The set which we are now going to construct shows that there exist perfect sets in $R^{1}$ which contain no segment.

Let $E_{0}$ be the interval $[0,1]$. Remove the segment $\left(\frac{1}{3}, \frac{2}{3}\right)$, and let $E_{1}$ be the union of the intervals

$$
\left[0, \frac{1}{3}\right]\left[\frac{2}{3}, 1\right]
$$

Remove the middle thirds of these intervals, and let $E_{2}$ be the union of the intervals

$$
\left[0, \frac{1}{9}\right],\left[\frac{2}{9}, \frac{3}{9}\right],\left[\frac{6}{9}, \frac{7}{9}\right],\left[\frac{8}{9}, 1\right] \text {. }
$$

Continuing in this way, we obtain a sequence of compact sets $E_{n}$, such that

(a) $E_{1} \supset E_{2} \supset E_{3} \supset \cdots$;

(b) $E_{n}$ is the union of $2^{n}$ intervals, each of length $3^{-n}$.

The set

$$
P=\bigcap_{n=1}^{\infty} E_{n}
$$

is called the Cantor set. $P$ is clearly compact, and Theorem 2.36 shows that $P$ is not empty.

No segment of the form

$$
\left(\frac{3 k+1}{3^{m}}, \frac{3 k+2}{3^{m}}\right)
$$

where $k$ and $m$ are positive integers, has a point in common with $P$. Since every segment $(\alpha, \beta)$ contains a segment of the form (24), if

$$
3^{-m}<\frac{\beta-\alpha}{6}
$$

$P$ contains no segment.

To show that $P$ is perfect, it is enough to show that $P$ contains no isolated point. Let $x \in P$, and let $S$ be any segment containing $x$. Let $I_{n}$ be that interval of $E_{n}$ which contains $x$. Choose $n$ large enough, so that $I_{n} \subset S$. Let $x_{n}$ be an endpoint of $I_{n}$, such that $x_{n} \neq x$.

It follows from the construction of $P$ that $x_{n} \in P$. Hence $x$ is a limit point of $P$, and $P$ is perfect.

One of the most interesting properties of the Cantor set is that it provides us with an example of an uncountable set of measure zero (the concept of measure will be discussed in Chap. 11).

## CONNECTED SETS

2.45 Definition Two subsets $A$ and $B$ of a metric space $X$ are said to be separated if both $A \cap \bar{B}$ and $\bar{A} \cap B$ are empty, i.e., if no point of $A$ lies in the closure of $B$ and no point of $B$ lies in the closure of $A$.

$A$ set $E \subset X$ is said to be connected if $E$ is not a union of two nonempty separated sets.

2.46 Remark Separated sets are of course disjoint, but disjoint sets need not be separated. For example, the interval $[0,1]$ and the segment $(1,2)$ are not separated, since 1 is a limit point of $(1,2)$. However, the segments $(0,1)$ and $(1,2)$ are separated.

The connected subsets of the line have a particularly simple structure:

2.47 Theorem $A$ subset $E$ of the real line $R^{1}$ is connected if and only if it has the following property: If $x \in E, y \in E$, and $x<z<y$, then $z_{1} \in E$.

Proof If there exist $x \in E, y \in E$, and some $z \in(x, y)$ such that $z \notin E$, then $E=A_{z} \cup B_{z}$ where

$$
A_{z}=E \cap(-\infty, z), \quad B_{z}=E \cap(z, \infty) .
$$

Since $x \in A_{z}$ and $y \in B_{z}, A$ and $B$ are nonempty. Since $A_{z} \subset(-\infty, z)$ and $B_{z} \subset(z, \infty)$, they are separated. Hence $E$ is not connected.

To prove the converse, suppose $E$ is not connected. Then there are nonempty separated sets $A$ and $B$ such that $A \cup B=E$. Pick $x \in A, y \in B$, and assume (without loss of generality) that $x<y$. Define

$$
z=\sup (A \cap[x, y]) .
$$

By Theorem 2.28, $z \in \bar{A}$; hence $z \notin B$. In particular, $x \leq z<y$.

If $z \notin A$, it follows that $x<z<y$ and $z \notin E$.

If $z \in A$, then $z \notin \bar{B}$, hence there exists $z_{1}$ such that $z<z_{1}<y$ and $z_{1} \notin B$. Then $x<z_{1}<y$ and $z_{1} \notin E$.

## EXERCISES

1. Prove that the empty set is a subset of every set.
2. A complex number $z$ is said to be algebraic if there are integers $a_{0}, \ldots, a_{n}$, not all zero, such that

$$
a_{0} z^{n}+a_{1} z^{n-1}+\cdots+a_{n-1} z+a_{n}=0 .
$$

Prove that the set of all algebraic numbers is countable. Hint: For every positive integer $N$ there are only finitely many equations with

$$
n+\left|a_{0}\right|+\left|a_{1}\right|+\cdots+\left|a_{n}\right|=N \text {. }
$$

3. Prove that there exist real numbers which are not algebraic.
4. Is the set of all irrational real numbers countable?
5. Construct a bounded set of real numbers with exactly three limit points.
6. Let $E^{\prime}$ be the set of all limit points of a set $E$. Prove that $E^{\prime}$ is closed. Prove that $E$ and $E$ have the same limit points. (Recall that $E=E \cup E^{\prime}$.) Do $E$ and $E^{\prime}$ always have the same limit points?
7. Let $A_{1}, A_{2}, A_{3}, \ldots$ be subsets of a metric space.

(a) If $B_{n}=\bigcup_{i=1}^{n} A_{i}$, prove that $\bar{B}_{n}=\bigcup_{i=1}^{n} \bar{A}_{i}$, for $n=1,2,3, \ldots$

(b) If $B=\bigcup_{i=1}^{\infty} A_{i}$, prove that $\bar{B} \supset \bigcup_{i=1}^{\infty} \bar{A}_{i}$.

Show, by an example, that this inclusion can be proper.

8. Is every point of every open set $E \subset R^{2}$ a limit point of $E$ ? Answer the same question for closed sets in $R^{2}$.
9. Let $E^{\circ}$ denote the set of all interior points of a set $E$. [See Definition 2.18(e); $E^{\circ}$ is called the interior of $E$.]

(a) Prove that $E^{\circ}$ is always open.

(b) Prove that $E$ is open if and only if $E^{\circ}=E$.

(c) If $G \subset E$ and $G$ is open, prove that $G \subset E^{\circ}$.

(d) Prove that the complement of $E^{\circ}$ is the closure of the complement of $E$.

$(e)$ Do $E$ and $E$ always have the same interiors?

$(f)$ Do $E$ and $E^{\circ}$ always have the same closures?

10. Let $X$ be an infinite set. For $p \in X$ and $q \in X$, define

$$
d(p, q)= \begin{cases}1 & (\text { if } p \neq q) \\ 0 & (\text { if } p=q)\end{cases}
$$

Prove that this is a metric. Which subsets of the resulting metric space are open? Which are closed? Which are compact?

11. For $x \in R^{1}$ and $y \in R^{1}$, define

$$
\begin{aligned}
d_{1}(x, y) & =(x-y)^{2}, \\
d_{2}(x, y) & =\sqrt{|x-y|}, \\
d_{3}(x, y) & =\left|x^{2}-y^{2}\right|, \\
d_{4}(x, y) & =|x-2 y|, \\
d_{5}(x, y) & =\frac{|x-y|}{1+|x-y|} .
\end{aligned}
$$

Determine, for each of these, whether it is a metric or not.

12. Let $K \subset R^{1}$ consist of 0 and the numbers $1 / n$, for $n=1,2,3, \ldots$ Prove that $K$ is compact directly from the definition (without using the Heine-Borel theorem).
13. Construct a compact set of real numbers whose limit points form a countable set.
14. Give an example of an open cover of the segment $(0,1)$ which has no finite subcover.
15. Show that Theorem 2.36 and its Corollary become false (in $R^{1}$, for example) if the word "compact" is replaced by "closed" or by "bounded."
16. Regard $Q$, the set of all rational numbers, as a metric space, with $d(p, q)=|p-q|$. Let $E$ be the set of all $p \in Q$ such that $2<p^{2}<3$. Show that $E$ is closed and bounded in $Q$, but that $E$ is not compact. Is $E$ open in $Q$ ?
17. Let $E$ be the set of all $x \in[0,1]$ whose decimal expansion contains only the digits 4 and 7. Is $E$ countable? Is $E$ dense in [0,1]? Is $E$ compact? Is $E$ perfect?
18. Is there a nonempty perfect set in $R^{1}$ which contains no rational number?
19. (a) If $A$ and $B$ are disjoint closed sets in some metric space $X$, prove that they are separated.

(b) Prove the same for disjoint open sets.

(c) Fix $p \in X, \delta>0$, define $A$ to be the set of all $q \in X$ for which $d(p, q)<\delta$, define $B$ similarly, with $>$ in place of $<$. Prove that $A$ and $B$ are separated.

(d) Prove that every connected metric space with at least two points is uncountable. Hint: Use (c).

20. Are closures and interiors of connected sets always connected? (Look at subsets of $R^{2}$.)
21. Let $A$ and $B$ be separated subsets of some $R^{k}$, suppose $\mathrm{a} \in A, \mathrm{~b} \in B$, and define

$$
\mathbf{p}(t)=(1-t) \mathbf{a}+t \mathbf{b}
$$

for $t \in R^{1}$. Put $A_{0}=\mathrm{p}^{-1}(A), B_{0}=\mathrm{p}^{-1}(B)$. [Thus $t \in A_{0}$ if and only if $\mathrm{p}(t) \in A$.]
(a) Prove that $A_{0}$ and $B_{0}$ are separated subsets of $R^{1}$.

(b) Prove that there exists $t_{0} \in(0,1)$ such that $\mathrm{p}\left(t_{0}\right) \notin A \cup B$.

(c) Prove that every convex subset of $R^{k}$ is connected.

22. A metric space is called separable if it contains a countable dense subset. Show that $R^{k}$ is separable. Hint: Consider the set of points which have only rational coordinates.
23. A collection $\left\{V_{\alpha}\right\}$ of open subsets of $X$ is said to be a base for $X$ if the following is true: For every $x \in X$ and every open set $G \subset X$ such that $x \in G$, we have $x \in V_{\alpha} \subset G$ for some $\alpha$. In other words, every open set in $X$ is the union of a subcollection of $\left\{V_{\alpha}\right\}$.

Prove that every separable metric space has a countable base. Hint: Take all neighborhoods with rational radius and center in some countable dense subset of $X$.

24. Let $X$ be a metric space in which every infinite subset has a limit point. Prove that $X$ is separable. Hint: Fix $\delta>0$, and pick $x_{1} \in X$. Having chosen $x_{1}, \ldots, x_{j} \in X$, choose $x_{\jmath+1} \in X$, if possible, so that $d\left(x_{l}, x_{j+1}\right) \geq \delta$ for $i=1, \ldots, j$. Show that this process must stop after a finite number of steps, and that $X$ can therefore be covered by finitely many neighborhoods of radius $\delta$. Take $\delta=1 / n(n=1,2,3, \ldots)$, and consider the centers of the corresponding neighborhoods.
25. Prove that every compact metric space $K$ has a countable base, and that $K$ is therefore separable. Hint: For every positive integer $n$, there are finitely many neighborhoods of radius $1 / n$ whose union covers $K$.
26. Let $X$ be a metric space in which every infinite subset has a limit point. Prove that $X$ is compact. Hint: By Exercises 23 and $24, X$ has a countable base. It follows that every open cover of $X$ has a countable subcover $\left\{G_{n}\right\}, n=1,2,3, \ldots$. If no finite subcollection of $\left\{G_{n}\right\}$ covers $X$, then the complement $F_{n}$ of $G_{1} \cup \cdots \cup G_{n}$ is nonempty for each $n$, but $\cap F_{n}$ is empty. If $E$ is a set which contains a point from each $F_{n}$, consider a limit point of $E$, and obtain a contradiction.
27. Define a point $p$ in a metric space $X$ to be a condensation point of a set $E \subset X$ if every neighborhood of $p$ contains uncountably many points of $E$.

Suppose $E \subset R^{k}, E$ is uncountable, and let $P$ be the set of all condensation points of $E$. Prove that $P$ is perfect and that at most countably many points of $E$ are not in $P$. In other words, show that $P^{c} \cap E$ is at most countable. Hint: Let $\left\{V_{n}\right\}$ be a countable base of $R^{k}$, let $W$ be the union of those $V_{n}$ for which $E \cap V_{n}$ is at most countable, and show that $P=W^{c}$.

28. Prove that every closed set in a separable metric space is the union of a (possibly empty) perfect set and a set which is at most countable. (Corollary: Every countable closed set in $R^{k}$ has isolated points.) Hint: Use Exercise 27.
29. Prove that every open set in $R^{1}$ is the union of an at most countable collection of disjoint segments. Hint: Use Exercise 22.
30. Imitate the proof of Theorem 2.43 to obtain the following result:

If $R^{k}=\bigcup_{1}^{\infty} F_{n}$, where each $F_{n}$ is a closed subset of $R^{k}$, then at least one $F_{n}$ has a nonempty interior.

Equivalent statement: If $G_{n}$ is a dense open subset of $R^{k}$, for $n=1,2,3, \ldots$, then $\bigcap_{1}^{\infty} G_{n}$ is not empty (in fact, it is dense in $R^{k}$ ).

(This is a special case of Baire's theorem; see Exercise 22, Chap. 3, for the general case.)

## NUMERICAL SEQUENCES AND SERIES

As the title indicates, this chapter will deal primarily with sequences and series of complex numbers. The basic facts about convergence, however, are just as easily explained in a more general setting. The first three sections will therefore be concerned with sequences in euclidean spaces, or even in metric spaces.

## CONVERGENT SEQUENCES

3.1 Definition A sequence $\left\{p_{n}\right\}$ in a metric space $X$ is said to converge if there is a point $p \in X$ with the following property: For every $\varepsilon>0$ there is an integer $N$ such that $n \geq N$ implies that $d\left(p_{n}, p\right)<\varepsilon$. (Here $d$ denotes the distance in $X$.)

In this case we also say that $\left\{p_{n}\right\}$ converges to $p$, or that $p$ is the limit of $\left\{p_{n}\right\}$ [see Theorem 3.2(b)], and we write $p_{n} \rightarrow p$, or

$$
\lim _{n \rightarrow \infty} p_{n}=p .
$$

If $\left\{p_{n}\right\}$ does not converge, it is said to diverge.

It might be well to point out that our definition of "convergent sequence" depends not only on $\left\{p_{n}\right\}$ but also on $X$; for instance, the sequence $\{1 / n\}$ converges in $R^{1}$ (to 0 ), but fails to converge in the set of all positive real numbers [with $d(x, y)=|x-y|$ ]. In cases of possible ambiguity, we can be more precise and specify "convergent in $X$ " rather than "convergent."

We recall that the set of all points $p_{n}(n=1,2,3, \ldots)$ is the range of $\left\{p_{n}\right\}$. The range of a sequence may be a finite set, or it may be infinite. The sequence $\left\{p_{n}\right\}$ is said to be bounded if its range is bounded.

As examples, consider the following sequences of complex numbers (that is, $X=R^{2}$ ):

(a) If $s_{n}=1 / n$, then $\lim _{n \rightarrow \infty} s_{n}=0$; the range is infinite, and the sequence is bounded.

(b) If $s_{n}=n^{2}$, the sequence $\left\{s_{n}\right\}$ is unbounded, is divergent, and has infinite range.

(c) If $s_{n}=1+\left[(-1)^{n} / n\right]$, the sequence $\left\{s_{n}\right\}$ converges to 1 , is bounded, and has infinite range.

(d) If $s_{n}=i^{n}$, the sequence $\left\{s_{n}\right\}$ is divergent, is bounded, and has finite range.

(e) If $s_{n}=1(n=1,2,3, \ldots)$, then $\left\{s_{n}\right\}$ converges to 1 , is bounded, and has finite range.

We now summarize some important properties of convergent sequences in metric spaces.

3.2 Theorem Let $\left\{p_{n}\right\}$ be a sequence in a metric space $X$.

(a) $\left\{p_{n}\right\}$ converges to $p \in X$ if and only if every neighborhood of $p$ contains $p_{n}$ for all but finitely many $n$.

(b) If $p \in X, p^{\prime} \in X$, and if $\left\{p_{n}\right\}$ converges to $p$ and to $p^{\prime}$, then $p^{\prime}=p$.

(c) If $\left\{p_{n}\right\}$ converges, then $\left\{p_{n}\right\}$ is bounded.

(d) If $E \subset X$ and if $p$ is a limit point of $E$, then there is a sequence $\left\{p_{n}\right\}$ in $E$ such that $p=\lim _{n \rightarrow \infty} p_{n}$.

Proof (a) Suppose $p_{n} \rightarrow p$ and let $V$ be a neighborhood of $p$. For some $\varepsilon>0$, the conditions $d(q, p)<\varepsilon, q \in X$ imply $q \in V$. Corresponding to this $\varepsilon$, there exists $N$ such that $n \geq N$ implies $d\left(p_{n}, p\right)<\varepsilon$. Thus $n \geq N$ implies $p_{n} \in V$.

Conversely, suppose every neighborhood of $p$ contains all but finitely many of the $p_{n}$. Fix $\varepsilon>0$, and let $V$ be the set of all $q \in X$ such that $d(p, q)<\varepsilon$. By assumption, there exists $N$ (corresponding to this $V$ ) such that $p_{n} \in V$ if $n \geq N$. Thus $d\left(p_{n}, p\right)<\varepsilon$ if $n \geq N$; hence $p_{n} \rightarrow p$.
(b) Let $\varepsilon>0$ be given. There exist integers $N, N^{\prime}$ such that

$$
\begin{aligned}
& n \geq N \quad \text { implies } \quad d\left(p_{n}, p\right)<\frac{\varepsilon}{2}, \\
& n \geq N^{\prime} \text { implies } d\left(p_{n}, p^{\prime}\right)<\frac{\varepsilon}{2} .
\end{aligned}
$$

Hence if $n \geq \max \left(N, N^{\prime}\right)$, we have

$$
d\left(p, p^{\prime}\right) \leq d\left(p, p_{n}\right)+d\left(p_{n}, p^{\prime}\right)<\varepsilon .
$$

Since $\varepsilon$ was arbitrary, we conclude that $d\left(p, p^{\prime}\right)=0$.

(c) Suppose $p_{n} \rightarrow p$. There is an integer $N$ such that $n>N$ implies $d\left(p_{n}, p\right)<1$. Put

$$
r=\max \left\{1, d\left(p_{1}, p\right), \ldots, d\left(p_{N}, p\right)\right\}
$$

Then $d\left(p_{n}, p\right) \leq r$ for $n=1,2,3, \ldots$.

(d) For each positive integer $n$, there is a point $p_{n} \in E$ such that $d\left(p_{n}, p\right)<1 / n$. Given $\varepsilon>0$, choose $N$ so that $N \varepsilon>1$. If $n>N$, it follows that $d\left(p_{n}, p\right)<\varepsilon$. Hence $p_{n} \rightarrow p$.

This completes the proof.

For sequences in $R^{k}$ we can study the relation between convergence, on the one hand, and the algebraic operations on the other. We first consider sequences of complex numbers.

3.3 Theorem Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then

(a) $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$

(b) $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;

(c) $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$

(d) $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.

Proof

(a) Given $\varepsilon>0$, there exist integers $N_{1}, N_{2}$ such that

$$
\begin{array}{lll}
n \geq N_{1} \text { implies } & \left|s_{n}-s\right|<\frac{\varepsilon}{2}, \\
n \geq N_{2} \text { implies } & \left|t_{n}-t\right|<\frac{\varepsilon}{2} .
\end{array}
$$

If $N=\max \left(N_{1}, N_{2}\right)$, then $n \geq N$ implies

$$
\left|\left(s_{n}+t_{n}\right)-(s+t)\right| \leq\left|s_{n}-s\right|+\left|t_{n}-t\right|<\varepsilon .
$$

This proves $(a)$. The proof of $(b)$ is trivial.

(c) We use the identity

$$
s_{n} t_{n}-s t=\left(s_{n}-s\right)\left(t_{n}-t\right)+s\left(t_{n}-t\right)+t\left(s_{n}-s\right)
$$

Given $\varepsilon>0$, there are integers $N_{1}, N_{2}$ such that

$$
\begin{array}{lll}
n \geq N_{1} \quad \text { implies } & \left|s_{n}-s\right|<\sqrt{\varepsilon} \\
n \geq N_{2} & \text { implies } & \left|t_{n}-t\right|<\sqrt{\varepsilon}
\end{array}
$$

If we take $N=\max \left(N_{1}, N_{2}\right), n \geq N$ implies

$$
\left|\left(s_{n}-s\right)\left(t_{n}-t\right)\right|<\varepsilon,
$$

so that

$$
\lim _{n \rightarrow \infty}\left(s_{n}-s\right)\left(t_{n}-t\right)=0
$$

We now apply $(a)$ and $(b)$ to (1), and conclude that

$$
\lim _{n \rightarrow \infty}\left(s_{n} t_{n}-s t\right)=0
$$

(d) Choosing $m$ such that $\left|s_{n}-s\right|<\frac{1}{2}|s|$ if $n \geq m$, we see that

$$
\left|s_{n}\right|>\frac{1}{2}|s| \quad(n \geq m) \text {. }
$$

Given $\varepsilon>0$, there is an integer $N>m$ such that $n \geq N$ implies

$$
\left|s_{n}-s\right|<\frac{1}{2}|s|^{2} \varepsilon \text {. }
$$

Hence, for $n \geq N$,

$$
\left|\frac{1}{s_{n}}-\frac{1}{s}\right|=\left|\frac{s_{n}-s}{s_{n} s}\right|<\frac{2}{|s|^{2}}\left|s_{n}-s\right|<\varepsilon
$$

### 3.4 Theorem

(a) Suppose $\mathbf{x}_{n} \in R^{k}(n=1,2,3, \ldots)$ and

$$
\mathbf{x}_{n}=\left(\alpha_{1, n}, \ldots, \alpha_{k, n}\right)
$$

Then $\left\{\mathbf{x}_{n}\right\}$ converges to $\mathbf{x}=\left(\alpha_{1}, \ldots, \alpha_{k}\right)$ if and only if

$$
\lim _{n \rightarrow \infty} \alpha_{f, n}=\alpha_{j} \quad(1 \leq j \leq k) .
$$

(b) Suppose $\left\{\mathbf{x}_{n}\right\},\left\{\mathbf{y}_{n}\right\}$ are sequences in $R^{k},\left\{\beta_{n}\right\}$ is a sequence of real numbers, and $\mathbf{x}_{n} \rightarrow \mathbf{x}, \mathbf{y}_{n} \rightarrow \mathbf{y}, \beta_{n} \rightarrow \beta$. Then

$$
\lim _{n \rightarrow \infty}\left(\mathbf{x}_{n}+\mathbf{y}_{n}\right)=\mathbf{x}+\mathbf{y}, \quad \lim _{n \rightarrow \infty} \mathbf{x}_{n} \cdot \mathbf{y}_{n}=\mathbf{x} \cdot \mathbf{y}, \quad \lim _{n \rightarrow \infty} \beta_{n} \mathbf{x}_{n}=\beta \mathbf{x} .
$$

## Proof

(a) If $\mathbf{x}_{n} \rightarrow \mathbf{x}$, the inequalities

$$
\left|\alpha_{j, n}-\alpha_{j}\right| \leq\left|\mathbf{x}_{n}-\mathbf{x}\right|,
$$

which follow immediately from the definition of the norm in $R^{k}$, show that (2) holds.

Conversely, if (2) holds, then to each $\varepsilon>0$ there corresponds an integer $N$ such that $n \geq N$ implies

$$
\left|\alpha_{j, n}-\alpha_{j}\right|<\frac{\varepsilon}{\sqrt{k}} \quad(1 \leq j \leq k)
$$

Hence $n \geq N$ implies

$$
\left|\mathbf{x}_{n}-\mathbf{x}\right|=\left\{\sum_{j=1}^{k}\left|\alpha_{j, n}-\alpha_{j}\right|^{2}\right\}^{1 / 2}<\varepsilon,
$$

so that $\mathbf{x}_{n} \rightarrow \mathbf{x}$. This proves $(a)$.

Part $(b)$ follows from $(a)$ and Theorem 3.3.

## SUBSEQUENCES

3.5 Definition Given a sequence $\left\{p_{n}\right\}$, consider a sequence $\left\{n_{k}\right\}$ of positive integers, such that $n_{1}<n_{2}<n_{3}<\cdots$. Then the sequence $\left\{p_{n i t}\right\}$ is called a subsequence of $\left\{p_{n}\right\}$. If $\left\{p_{n}\right\}$ converges, its limit is called a subsequential limit of $\left\{p_{n}\right\}$.

It is clear that $\left\{p_{n}\right\}$ converges to $p$ if and only if every subsequence of $\left\{p_{n}\right\}$ converges to $p$. We leave the details of the proof to the reader.

### 3.6 Theorem

(a) If $\left\{p_{n}\right\}$ is a sequence in a compact metric space $X$, then some subsequence of $\left\{p_{n}\right\}$ converges to a point of $X$.

(b) Every bounded sequence in $R^{k}$ contains a convergent subsequence.

## Proof

(a) Let $E$ be the range of $\left\{p_{n}\right\}$. If $E$ is finite then there is a $p \in E$ and a sequence $\left\{n_{i}\right\}$ with $n_{1}<n_{2}<n_{3}<\cdots$, such that

$$
p_{n_{1}}=p_{n_{2}}=\cdots=p .
$$

The subsequence $\left\{p_{n t}\right\}$ so obtained converges evidently to $p$.

If $E$ is infinite, Theorem 2.37 shows that $E$ has a limit point $p \in X$. Choose $n_{1}$ so that $d\left(p, p_{n_{1}}\right)<1$. Having chosen $n_{1}, \ldots, n_{i-1}$, we see from Theorem 2.20 that there is an integer $n_{i}>n_{i-1}$ such that $d\left(p, p_{n_{i}}\right)<1 / i$. Then $\left\{p_{n}\right\}$ converges to $p$.

(b) This follows from $(a)$, since Theorem 2.41 implies that every bounded subset of $R^{k}$ lies in a compact subset of $R^{k}$.

3.7 Theorem The subsequential limits of a sequence $\left\{p_{n}\right\}$ in a metric space $X$ form a closed subset of $X$.

Proof Let $E^{*}$ be the set of all subsequential limits of $\left\{p_{n}\right\}$ and let $q$ be a limit point of $E^{*}$. We have to show that $q \in E^{*}$.

Choose $n_{1}$ so that $p_{n_{1}} \neq q$. (If no such $n_{1}$ exists, then $E^{*}$ has only one point, and there is nothing to prove.) Put $\delta=d\left(q, p_{n_{1}}\right)$. Suppose $n_{1}, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E^{*}$, there is an $x \in E^{*}$ with $d(x, q)<2^{-1} \delta$. Since $x \in E^{*}$, there is an $n_{i}>n_{i-1}$ such that $d\left(x, p_{n_{1}}\right)<2^{-1} \delta$. Thus

$$
d\left(q, p_{n_{1}}\right) \leq 2^{1-i} \delta
$$

for $i=1,2,3, \ldots$. This says that $\left\{p_{n}\right\}$ converges to $q$. Hence $q \in E^{*}$.

## CAUCHY SEQUENCES

3.8 Definition A sequence $\left\{p_{n}\right\}$ in a metric space $X$ is said to be a Cauchy sequence if for every $\varepsilon>0$ there is an integer $N$ such that $d\left(p_{n}, p_{m}\right)<\varepsilon$ if $n \geq N$ and $m \geq N$.

In our discussion of Cauchy sequences, as well as in other situations which will arise later, the following geometric concept will be useful.

3.9 Definition Let $E$ be a nonempty subset of a metric space $X$, and let $S$ be the set of all real numbers of the form $d(p, q)$, with $p \in E$ and $q \in E$. The sup of $S$ is called the diameter of $E$.

If $\left\{p_{n}\right\}$ is a sequence in $X$ and if $E_{N}$ consists of the points $p_{N}, p_{N+1}, p_{N+2}, \ldots$, it is clear from the two preceding definitions that $\left\{p_{n}\right\}$ is a Cauchy sequence if and only if

$$
\lim _{N \rightarrow \infty} \operatorname{diam} E_{N}=0 \text {. }
$$

### 3.10 Theorem

(a) If $\bar{E}$ is the closure of a set $E$ in a metric space $X$, then $\operatorname{diam} \bar{E}=\operatorname{diam} E$.

(b) If $K_{n}$ is a sequence of compact sets in $X$ such that $K_{n} \supset K_{n+1}$ $(n=1,2,3, \ldots)$ and if

$$
\lim _{n \rightarrow \infty} \operatorname{diam} K_{n}=0 \text {, }
$$

then $\bigcap_{1}^{\infty} K_{n}$ consists of exactly one point.

## Proof

(a) Since $E \subset \bar{E}$, it is clear that

$$
\operatorname{diam} E \leq \operatorname{diam} \bar{E} .
$$

Fix $\varepsilon>0$, and choose $p \in \bar{E}, q \in \bar{E}$. By the definition of $\bar{E}$, there are points $p^{\prime}, q^{\prime}$, in $E$ such that $d\left(p, p^{\prime}\right)<\varepsilon, d\left(q, q^{\prime}\right)<\varepsilon$. Hence

$$
\begin{aligned}
d(p, q) & \leq d\left(p, p^{\prime}\right)+d\left(p^{\prime} q^{\prime}\right)+d\left(q^{\prime}, q\right) \\
& <2 \varepsilon+d\left(p^{\prime}, q^{\prime}\right) \leq 2 \varepsilon+\operatorname{diam} E
\end{aligned}
$$

It follows that

$$
\operatorname{diam} \bar{E} \leq 2 \varepsilon+\operatorname{diam} E
$$

and since $\varepsilon$ was arbitrary, $(a)$ is proved.

(b) Put $K=\bigcap_{1}^{\infty} K_{n}$. By Theorem 2.36, $K$ is not empty. If $K$ contains more than one point, then diam $K>0$. But for each $n, K_{n} \supset K$, so that $\operatorname{diam} K_{n} \geq \operatorname{diam} K$. This contradicts the assumption that $\operatorname{diam} K_{n} \rightarrow 0$.

### 3.11 Theorem

(a) In any metric space $X$, every convergent sequence is a Cauchy sequence.

(b) If $X$ is a compact metric space and if $\left\{p_{n}\right\}$ is a Cauchy sequence in $X$, then $\left\{p_{n}\right\}$ converges to some point of $X$.

(c) In $R^{k}$, every Cauchy sequence converges.

Note: The difference between the definition of convergence and the definition of a Cauchy sequence is that the limit is explicitly involved in the former, but not in the latter. Thus Theorem 3.11(b) may enable us
to decide whether or not a given sequence converges without knowledge of the limit to which it may converge.

The fact (contained in Theorem 3.11) that a sequence converges in $R^{k}$ if and only if it is a Cauchy sequence is usually called the Cauchy criterion for convergence.

## Proof

(a) If $p_{n} \rightarrow p$ and if $\varepsilon>0$, there is an integer $N$ such that $d\left(p, p_{n}\right)<\varepsilon$ for all $n \geq N$. Hence

$$
d\left(p_{n}, p_{m}\right) \leq d\left(p_{n}, p\right)+d\left(p, p_{m}\right)<2 \varepsilon
$$

as soon as $n \geq N$ and $m \geq N$. Thus $\left\{p_{n}\right\}$ is a Cauchy sequence.

(b) Let $\left\{p_{n}\right\}$ be a Cauchy sequence in the compact space $X$. For $N=1,2,3, \ldots$, let $E_{N}$ be the set consisting of $p_{N}, p_{N+1}, p_{N+2}, \ldots$ Then

$$
\lim _{N \rightarrow \infty} \operatorname{diam} \bar{E}_{N}=0
$$

by Definition 3.9 and Theorem 3.10(a). Being a closed subset of the compact space $X$, each $\bar{E}_{N}$ is compact (Theorem 2.35). Also $E_{N} \supset E_{N+1}$, so that $\bar{E}_{N} \supset \bar{E}_{N+1}$.

Theorem $3.10(b)$ shows now that there is a unique $p \in X$ which lies in every $\bar{E}_{N}$.

Let $\varepsilon>0$ be given. By (3) there is an integer $N_{0}$ such that $\operatorname{diam} \bar{E}_{N}<\varepsilon$ if $N \geq N_{0}$. Since $p \in \bar{E}_{N}$, it follows that $d(p, q)<\varepsilon$ for every $q \in \bar{E}_{N}$, hence for every $q \in E_{N}$. In other words, $d\left(p, p_{n}\right)<\varepsilon$ if $n \geq N_{0}$. This says precisely that $p_{n} \rightarrow p$.

(c) Let $\left\{\mathbf{x}_{n}\right\}$ be a Cauchy sequence in $R^{k}$. Define $E_{N}$ as in $(b)$, with $\mathbf{x}_{i}$ in place of $p_{i}$. For some $N$, diam $E_{N}<1$. The range of $\left\{\mathbf{x}_{n}\right\}$ is the union of $E_{N}$ and the finite set $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N-1}\right\}$. Hence $\left\{\mathbf{x}_{n}\right\}$ is bounded. Since every bounded subset of $R^{k}$ has compact closure in $R^{k}$ (Theorem 2.41), $(c)$ follows from $(b)$.

3.12 Definition A metric space in which every Cauchy sequence converges is said to be complete.

Thus Theorem 3.11 says that all compact metric spaces and all Euclidean spaces are complete. Theorem 3.11 implies also that every closed subset $E$ of a complete metric space $X$ is complete. (Every Cauchy sequence in $E$ is a Cauchy sequence in $X$, hence it converges to some $p \in X$, and actually $p \in E$ since $E$ is closed.) An example of a metric space which is not complete is the space of all rational numbers, with $d(x, y)=|x-y|$.

Theorem 3.2(c) and example $(d)$ of Definition 3.1 show that convergent sequences are bounded, but that bounded sequences in $R^{k}$ need not converge. However, there is one important case in which convergence is equivalent to boundedness; this happens for monotonic sequences in $R^{\mathbf{1}}$.

3.13 Definition A sequence $\left\{s_{n}\right\}$ of real numbers is said to be

(a) monotonically increasing if $s_{n} \leq s_{n+1}(n=1,2,3, \ldots)$;

(b) monotonically decreasing if $s_{n} \geq s_{n+1}(n=1,2,3, \ldots)$.

The class of monotonic sequences consists of the increasing and the decreasing sequences.

3.14 Theorem Suppose $\left\{s_{n}\right\}$ is monotonic. Then $\left\{s_{n}\right\}$ converges if and only if it is bounded.

Proof Suppose $s_{n} \leq s_{n+1}$ (the proof is analogous in the other case). Let $E$ be the range of $\left\{s_{n}\right\}$. If $\left\{s_{n}\right\}$ is bounded, let $s$ be the least upper bound of $E$. Then

$$
s_{n} \leq s \quad(n=1,2,3, \ldots)
$$

For every $\varepsilon>0$, there is an integer $N$ such that

$$
s-\varepsilon<s_{N} \leq s
$$

for otherwise $s-\varepsilon$ would be an upper bound of $E$. Since $\left\{s_{n}\right\}$ increases, $n \geq N$ therefore implies

$$
s-\varepsilon<s_{n} \leq s,
$$

which shows that $\left\{s_{n}\right\}$ converges (to $s$ ).

The converse follows from Theorem 3.2(c).

## UPPER AND LOWER LIMITS

3.15 Definition Let $\left\{s_{n}\right\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_{n} \geq M$. We then write

$$
s_{n} \rightarrow+\infty \text {. }
$$

Similarly, if for every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_{n} \leq M$, we write

$$
s_{n} \rightarrow-\infty .
$$

It should be noted that we now use the symbol $\rightarrow$ (introduced in Definition 3.1) for certain types of divergent sequences, as well as for convergent sequences, but that the definitions of convergence and of limit, given in Definition 3.1 , are in no way changed.

3.16 Definition Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_{k}} \rightarrow x$ for some subsequence $\left\{s_{n_{k}}\right\}$. This set $E$ contains all subsequential limits as defined in Definition 3.5, plus possibly the numbers $+\infty,-\infty$.

We now recall Definitions 1.8 and 1.23 and put

$$
\begin{gathered}
s^{*}=\sup E, \\
s_{*}=\inf E .
\end{gathered}
$$

The numbers $s^{*}, s_{*}$ are called the upper and lower limits of $\left\{s_{n}\right\}$; we use the notation

$$
\limsup _{n \rightarrow \infty} s_{n}=s^{*}, \quad \liminf _{n \rightarrow \infty} s_{n}=s_{*} .
$$

3.17 Theorem Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ and $s^{*}$ have the same meaning as in Definition 3.16. Then $s^{*}$ has the following two properties:

(a) $s^{*} \in E$.

(b) If $x>s^{*}$, there is an integer $N$ such that $n \geq N$ implies $s_{n}<x$.

Moreover, $s^{*}$ is the only number with the properties $(a)$ and $(b)$.

Of course, an analogous result is true for $s_{*}$.

## Proof

(a) If $s^{*}=+\infty$, then $E$ is not bounded above; hence $\left\{s_{n}\right\}$ is not bounded above, and there is a subsequence $\left\{s_{n_{k}}\right\}$ such that $s_{n_{k}} \rightarrow+\infty$.

If $s^{*}$ is real, then $E$ is bounded above, and at least one subsequential limit exists, so that (a) follows from Theorems 3.7 and 2.28.

If $s^{*}=-\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Hence, for any real $M, s_{n}>M$ for at most a finite number of values of $n$, so that $s_{n} \rightarrow-\infty$.

This establishes $(a)$ in all cases.

(b) Suppose there is a number $x>s^{*}$ such that $s_{n} \geq x$ for infinitely many values of $n$. In that case, there is a number $y \in E$ such that $y \geq x>s^{*}$, contradicting the definition of $s^{*}$.

Thus $s^{*}$ satisfies $(a)$ and $(b)$.

To show the uniqueness, suppose there are two numbers, $p$ and $q$, which satisfy $(a)$ and $(b)$, and suppose $p<q$. Choose $x$ such that $p<x<q$. Since $p$ satisfies $(b)$, we have $s_{n}<x$ for $n \geq N$. But then $q$ cannot satisfy $(a)$.

### 3.18 Examples

(a) Let $\left\{s_{n}\right\}$ be a sequence containing all rationals. Then every real number is a subsequential limit, and

$$
\limsup _{n \rightarrow \infty} s_{n}=+\infty, \quad \liminf _{n \rightarrow \infty} s_{n}=-\infty
$$

(b) Let $s_{n}=\left(-1^{n}\right) /[1+(1 / n)]$. Then

$$
\limsup _{n \rightarrow \infty} s_{n}=1, \quad \liminf _{n \rightarrow \infty} s_{n}=-1 .
$$

(c) For a real-valued sequence $\left\{s_{n}\right\}, \lim _{n \rightarrow \infty} s_{n}=s$ if and only if

$$
\limsup _{n \rightarrow \infty} s_{n}=\liminf _{n \rightarrow \infty} s_{n}=s .
$$

We close this section with a theorem which is useful, and whose proof is quite trivial:

3.19 Theorem If $s_{n} \leq t_{n}$ for $n \geq N$, where $N$ is fixed, then

$$
\liminf _{n \rightarrow \infty} s_{n} \leq \liminf _{n \rightarrow \infty} t_{n},
$$

$$
\limsup _{n \rightarrow \infty} s_{n} \leq \limsup _{n \rightarrow \infty} t_{n} .
$$

## SOME SPECIAL SEQUENCES

We shall now compute the limits of some sequences which occur frequently. The proofs will all be based on the following remark: If $0 \leq x_{n} \leq s_{n}$ for $n \geq N$, where $N$ is some fixed number, and if $s_{n} \rightarrow 0$, then $x_{n} \rightarrow 0$.

### 3.20 Theorem

(a) If $p>0$, then $\lim _{n \rightarrow \infty} \frac{1}{n^{p}}=0$.

(b) If $p>0$, then $\lim _{n \rightarrow \infty} \sqrt[n]{p}=1$.

(c) $\lim _{n \rightarrow \infty} \sqrt[n]{n}=1$.

(d) If $p>0$ and $\alpha$ is real, then $\lim _{n \rightarrow \infty} \frac{n^{\alpha}}{(1+p)^{n}}=0$.

(e) If $|x|<1$, then $\lim _{n \rightarrow \infty} x^{n}=0$.

## Proof

(a) Take $n>(1 / \varepsilon)^{1 / p}$. (Note that the archimedean property of the real number system is used here.)

(b) If $p>1$, put $x_{n}=\sqrt[n]{p}-1$. Then $x_{n}>0$, and, by the binomial theorem,

$$
1+n x_{n} \leq\left(1+x_{n}\right)^{n}=p
$$

so that

$$
0<x_{n} \leq \frac{p-1}{n} \text {. }
$$

Hence $x_{n} \rightarrow 0$. If $p=1,(b)$ is trivial, and if $0<p<1$, the result is obtained by taking reciprocals.

(c) Put $x_{n}=\sqrt[n]{n}-1$. Then $x_{n} \geq 0$, and, by the binomial theorem,

$$
n=\left(1+x_{n}\right)^{n} \geq \frac{n(n-1)}{2} x_{n}^{2} \text {. }
$$

Hence

$$
0 \leq x_{n} \leq \sqrt{\frac{2}{n-1}} \quad(n \geq 2)
$$

(d) Let $k$ be an integer such that $k>\alpha, k>0$. For $n>2 k$,

$$
(1+p)^{n}>\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}=\frac{n(n-1) \cdots(n-k+1)}{k !} p^{k}>\frac{n^{k} p^{k}}{2^{k} k !}
$$

Hence

$$
0<\frac{n^{\alpha}}{(1+p)^{n}}<\frac{2^{k} k !}{p^{k}} n^{\alpha-k} \quad(n>2 k) .
$$

Since $\alpha-k<0, n^{\alpha-k} \rightarrow 0$, by $(a)$.

(e) Take $\alpha=0$ in $(d)$.

## SERIES

In the remainder of this chapter, all sequences and series under consideration will be complex-valued, unless the contrary is explicitly stated. Extensions of some of the theorems which follow, to series with terms in $R^{k}$, are mentioned in Exercise 15.

3.21 Definition Given a sequence $\left\{a_{n}\right\}$, we use the notation

$$
\sum_{n=p}^{q} a_{n} \quad(p \leq q)
$$

to denote the sum $a_{p}+a_{p+1}+\cdots+a_{q}$. With $\left\{a_{n}\right\}$ we associate a sequence $\left\{s_{n}\right\}$, where

$$
s_{n}=\sum_{k=1}^{n} a_{k}
$$

For $\left\{s_{n}\right\}$ we also use the symbolic expression

or, more concisely,

$$
a_{1}+a_{2}+a_{3}+\cdots
$$

$$
\sum_{n=1}^{\infty} a_{n}
$$

The symbol (4) we call an infinite series, or just a series. The numbers $s_{n}$ are called the partial sums of the series. If $\left\{s_{n}\right\}$ converges to $s$, we say that the series converges, and write

$$
\sum_{n=1}^{\infty} a_{n}=s
$$

The number $s$ is called the sum of the series; but it should be clearly understood that $s$ is the limit of a sequence of sums, and is not obtained simply by addition.

If $\left\{s_{n}\right\}$ diverges, the series is said to diverge.

Sometimes, for convenience of notation, we shall consider series of the form

$$
\sum_{n=0}^{\infty} a_{n}
$$

And frequently, when there is no possible ambiguity, or when the distinction is immaterial, we shall simply write $\Sigma a_{n}$ in place of (4) or (5).

It is clear that every theorem about sequences can be stated in terms of series (putting $a_{1}=s_{1}$, and $a_{n}=s_{n}-s_{n-1}$ for $n>1$ ), and vice versa. But it is nevertheless useful to consider both concepts.

The Cauchy criterion (Theorem 3.11) can be restated in the following form:

3.22 Theorem $\Sigma a_{n}$ converges if and only if for every $\varepsilon>0$ there is an integer $N$ such that

$$
\left|\sum_{k=n}^{m} a_{k}\right| \leq \varepsilon
$$

if $m \geq n \geq N$.

In particular, by taking $m=n$, (6) becomes

$$
\left|a_{n}\right| \leq \varepsilon \quad(n \geq N) .
$$

In other words:

3.23 Theorem If $\Sigma a_{n}$ converges, then $\lim _{n \rightarrow \infty} a_{n}=0$.

The condition $a_{n} \rightarrow 0$ is not, however, sufficient to ensure convergence of $\Sigma a_{n}$. For instance, the series

$$
\sum_{n=1}^{\infty} \frac{1}{n}
$$

diverges; for the proof we refer to Theorem 3.28.

Theorem 3.14, concerning monotonic sequences, also has an immediate counterpart for series.

3.24 Theorem $A$ series of nonnegative ${ }^{1}$ terms converges if and only if its partial sums form a bounded sequence.

We now turn to a convergence test of a different nature, the so-called "comparison test."

### 3.25 Theorem

(a) If $\left|a_{n}\right| \leq c_{n}$ for $n \geq N_{0}$, where $N_{0}$ is some fixed integer, and if $\Sigma c_{n}$ converges, then $\Sigma a_{n}$ converges.

(b) If $a_{n} \geq d_{n} \geq 0$ for $n \geq N_{0}$, and if $\Sigma d_{n}$ diverges, then $\Sigma a_{n}$ diverges.

Note that $(b)$ applies only to series of nonnegative terms $a_{n}$.

Proof Given $\varepsilon>0$, there exists $N \geq N_{0}$ such that $m \geq n \geq N$ implies

$$
\sum_{k=n}^{m} c_{k} \leq \varepsilon
$$

by the Cauchy criterion. Hence

$$
\left|\sum_{k=n}^{m} a_{k}\right| \leq \sum_{k=n}^{m}\left|a_{k}\right| \leq \sum_{k=n}^{m} c_{k} \leq \varepsilon
$$

and $(a)$ follows.

Next, $(b)$ follows from $(a)$, for if $\Sigma a_{n}$ converges, so must $\Sigma d_{n}$ [note that $(b)$ also follows from Theorem 3.24].

1 The expression "nonnegative" always refers to real numbers.

The comparison test is a very useful one; to use it efficiently, we have to become familiar with a number of series of nonnegative terms whose convergence or divergence is known.

## SERIES OF NONNEGATIVE TERMS

The simplest of all is perhaps the geometric series.

3.26 Theorem If $0 \leq x<1$, then

$$
\sum_{n=0}^{\infty} x^{n}=\frac{1}{1-x}
$$

If $x \geq 1$, the series diverges.

Proof If $x \neq 1$,

$$
s_{n}=\sum_{k=0}^{n} x^{k}=\frac{1-x^{n+1}}{1-x}
$$

The result follows if we let $n \rightarrow \infty$. For $x=1$, we get

$$
1+1+1+\cdots,
$$

which evidently diverges.

In many cases which occur in applications, the terms of the series decrease monotonically. The following theorem of Cauchy is therefore of particular interest. The striking feature of the theorem is that a rather "thin" subsequence of $\left\{a_{n}\right\}$ determines the convergence or divergence of $\Sigma a_{n}$.

3.27 Theorem Suppose $a_{1} \geq a_{2} \geq a_{3} \geq \cdots \geq 0$. Then the series $\sum_{n=1}^{\infty} a_{n}$ converges if and only if the series

$$
\sum_{k=0}^{\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\cdots
$$

converges.

Proof By Theorem 3.24, it suffices to consider boundedness of the partial sums. Let

$$
\begin{aligned}
s_{n} & =a_{1}+a_{2}+\cdots+a_{n}, \\
t_{k} & =a_{1}+2 a_{2}+\cdots+2^{k} a_{2^{k}} .
\end{aligned}
$$

For $n<2^{k}$,

$$
\begin{aligned}
s_{n} & \leq a_{1}+\left(a_{2}+a_{3}\right)+\cdots+\left(a_{2^{k}}+\cdots+a_{2^{k+1}-1}\right) \\
& \leq a_{1}+2 a_{2}+\cdots+2^{k} a_{2^{k}} \\
& =t_{k}
\end{aligned}
$$

so that

On the other hand, if $n>2^{k}$,

$$
s_{n} \leq t_{k} .
$$

$$
\begin{aligned}
s_{n} & \geq a_{1}+a_{2}+\left(a_{3}+a_{4}\right)+\cdots+\left(a_{2^{k-1}+1}+\cdots+a_{2^{k}}\right) \\
& \geq \frac{1}{2} a_{1}+a_{2}+2 a_{4}+\cdots+2^{k-1} a_{2^{k}} \\
& =\frac{1}{2} t_{k},
\end{aligned}
$$

so that

$$
2 s_{n} \geq t_{k} \text {. }
$$

By (8) and (9), the sequences $\left\{s_{n}\right\}$ and $\left\{t_{k}\right\}$ are either both bounded or both unbounded. This completes the proof.

3.28 Theorem $\sum \frac{1}{n^{p}}$ converges if $p>1$ and diverges if $p \leq 1$.

Proof If $p \leq 0$, divergence follows from Theorem 3.23. If $p>0$, Theorem 3.27 is applicable, and we are led to the series

$$
\sum_{k=0}^{\infty} 2^{k} \cdot \frac{1}{2^{k p}}=\sum_{k=0}^{\infty} 2^{(1-p) k}
$$

Now, $2^{1-p}<1$ if and only if $1-p<0$, and the result follows by comparison with the geometric series (take $x=2^{1-p}$ in Theorem 3.26).

As a further application of Theorem 3.27, we prove:

3.29 Theorem If $p>1$,

$$
\sum_{n=2}^{\infty} \frac{1}{n(\log n)^{p}}
$$

converges; if $p \leq 1$, the series diverges.

Remark " $\log n$ " denotes the logarithm of $n$ to the base $e$ (compare Exercise 7, Chap. 1); the number $e$ will be defined in a moment (see Definition 3.30). We let the series start with $n=2$, since $\log 1=0$.

Proof The monotonicity of the logarithmic function (which will be discussed in more detail in Chap. 8) implies that $\{\log n\}$ increases. Hence $\{1 / n \log n\}$ decreases, and we can apply Theorem 3.27 to (10); this leads us to the series

$$
\sum_{k=1}^{\infty} 2^{k} \cdot \frac{1}{2^{k}\left(\log 2^{k}\right)^{p}}=\sum_{k=1}^{\infty} \frac{1}{(k \log 2)^{p}}=\frac{1}{(\log 2)^{p}} \sum_{k=1}^{\infty} \frac{1}{k^{p}}
$$

and Theorem 3.29 follows from Theorem 3.28.

This procedure may evidently be continued. For instance,

$$
\sum_{n=3}^{\infty} \frac{1}{n \log n \log \log n}
$$

diverges, whereas

$$
\sum_{n=3}^{\infty} \frac{1}{n \log n(\log \log n)^{2}}
$$

converges.

We may now observe that the terms of the series (12) differ very little from those of (13). Still, one diverges, the other converges. If we continue the process which led us from Theorem 3.28 to Theorem 3.29, and then to (12) and (13), we get pairs of convergent and divergent series whose terms differ even less than those of (12) and (13). One might thus be led to the conjecture that there is a limiting situation of some sort, a "boundary" with all convergent series on one side, all divergent series on the other side-at least as far as series with monotonic coefficients are concerned. This notion of "boundary" is of course quite vague. The point we wish to make is this: No matter how we make this notion precise, the conjecture is false. Exercises $11(b)$ and $12(b)$ may serve as illustrations.

We do not wish to go any deeper into this aspect of convergence theory, and refer the reader to Knopp's "Theory and Application of Infinite Series," Chap. IX, particularly Sec. 41 .

## THE NUMBER $\boldsymbol{e}$

3.30 Definition $e=\sum_{n=0}^{\infty} \frac{1}{n !}$.

Here $n !=1 \cdot 2 \cdot 3 \cdots n$ if $n \geq 1$, and $0 !=1$.

Since

$$
\begin{aligned}
s_{n} & =1+1+\frac{1}{1 \cdot 2}+\frac{1}{1 \cdot 2 \cdot 3}+\cdots+\frac{1}{1 \cdot 2 \cdots n} \\
& <1+1+\frac{1}{2}+\frac{1}{2^{2}}+\cdots+\frac{1}{2^{n-1}}<3,
\end{aligned}
$$

the series converges, and the definition makes sense. In fact, the series converges very rapidly and allows us to compute $e$ with great accuracy.

It is of interest to note that $e$ can also be defined by means of another limit process; the proof provides a good illustration of operations with limits:

3.31 Theorem $\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$.

Proof Let

$$
s_{n}=\sum_{k=0}^{n} \frac{1}{k !}, \quad t_{n}=\left(1+\frac{1}{n}\right)^{n}
$$

By the binomial theorem,

$$
\begin{aligned}
t_{n}=1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\frac{1}{3 !}\left(1-\frac{1}{n}\right) & \left(1-\frac{2}{n}\right)+\cdots \\
& +\frac{1}{n !}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdots\left(1-\frac{n-1}{n}\right) .
\end{aligned}
$$

Hence $t_{n} \leq s_{n}$, so that

$$
\limsup _{n \rightarrow \infty} t_{n} \leq e,
$$

by Theorem 3.19. Next, if $n \geq m$,

$$
t_{n} \geq 1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\cdots+\frac{1}{m !}\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{m-1}{n}\right)
$$

Let $n \rightarrow \infty$, keeping $m$ fixed. We get

$$
\liminf _{n \rightarrow \infty} t_{n} \geq 1+1+\frac{1}{2 !}+\cdots+\frac{1}{m !},
$$

so that

$$
s_{m} \leq \liminf _{n \rightarrow \infty} t_{n} .
$$

Letting $m \rightarrow \infty$, we finally get

$$
e \leq \liminf _{n \rightarrow \infty} t_{n} .
$$

The theorem follows from (14) and (15).

The rapidity with which the series $\sum \frac{1}{n !}$ converges can be estimated as follows: If $s_{n}$ has the same meaning as above, we have

$$
\begin{aligned}
e-s_{n} & =\frac{1}{(n+1) !}+\frac{1}{(n+2) !}+\frac{1}{(n+3) !}+\cdots \\
& <\frac{1}{(n+1) !}\left\{1+\frac{1}{n+1}+\frac{1}{(n+1)^{2}}+\cdots\right\}=\frac{1}{n ! n}
\end{aligned}
$$

so that

$$
0<e-s_{n}<\frac{1}{n ! n}
$$

Thus $s_{10}$, for instance, approximates $e$ with an error less than $10^{-7}$. The inequality (16) is of theoretical interest as well, since it enables us to prove the irrationality of $e$ very easily.

3.32 Theorem $e$ is irrational.

Proof Suppose $e$ is rational. Then $e=p / q$, where $p$ and $q$ are positive integers. By (16),

$$
0<q !\left(e-s_{q}\right)<\frac{1}{q}
$$

By our assumption, $q ! e$ is an integer. Since

$$
q ! s_{q}=q !\left(1+1+\frac{1}{2 !}+\cdots+\frac{1}{q !}\right)
$$

is an integer, we see that $q !\left(e-s_{q}\right)$ is an integer.

Since $q \geq 1$, (17) implies the existence of an integer between 0 and 1 . We have thus reached a contradiction.

Actually, $e$ is not even an algebraic number. For a simple proof of this, see page 25 of Niven's book, or page 176 of Herstein's, cited in the Bibliography.

## THE ROOT AND RATIO TESTS

3.33 Theorem (Root Test) Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. Then
(a) if $\alpha<1, \Sigma a_{n}$ converges;
(b) if $\alpha>1, \Sigma a_{n}$ diverges;
(c) if $\alpha=1$, the test gives no information.

Proof If $\alpha<1$, we can choose $\beta$ so that $\alpha<\beta<1$, and an integer $N$ such that

$$
\sqrt[n]{\left|a_{n}\right|}<\beta
$$

for $n \geq N$ [by Theorem 3.17(b)]. That is, $n \geq N$ implies

$$
\left|a_{n}\right|<\beta^{n} .
$$

Since $0<\beta<1, \Sigma \beta^{n}$ converges. Convergence of $\Sigma a_{n}$ follows now from the comparison test.

If $\alpha>1$, then, again by Theorem 3.17, there is a sequence $\left\{n_{k}\right\}$ such that

$$
\sqrt[n_{k}]{\left|a_{n_{k}}\right|} \rightarrow \alpha .
$$

Hence $\left|a_{n}\right|>1$ for infinitely many values of $n$, so that the condition $a_{n} \rightarrow 0$, necessary for convergence of $\Sigma a_{n}$, does not hold (Theorem 3.23). To prove $(c)$, we consider the series

$$
\sum \frac{1}{n}, \sum \frac{1}{n^{2}}
$$

For each of these series $\alpha=1$, but the first diverges, the second converges.

### 3.34 Theorem (Ratio Test) The series $\Sigma a_{n}$

(a) converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$,

(b) diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.

Proof If condition (a) holds, we can find $\beta<1$, and an integer $N$, such that

$$
\left|\frac{a_{n+1}}{a_{n}}\right|<\beta
$$

for $n \geq N$. In particular,

$$
\begin{aligned}
& \left|a_{N+1}\right|<\beta\left|a_{N}\right|, \\
& \left|a_{N+2}\right|<\beta\left|a_{N+1}\right|<\beta^{2}\left|a_{N}\right|, \\
& \ldots \ldots \ldots \ldots \ldots \ldots \\
& \left|a_{N+p}\right|<\beta^{p}\left|a_{N}\right|
\end{aligned}
$$

That is,

$$
\left|a_{n}\right|<\left|a_{N}\right| \beta^{-N} \cdot \beta^{n}
$$

for $n \geq N$, and (a) follows from the comparison test, since $\Sigma \beta^{n}$ converges.

If $\left|a_{n+1}\right| \geq\left|a_{n}\right|$ for $n \geq n_{0}$, it is easily seen that the condition $a_{n} \rightarrow 0$ does not hold, and $(b)$ follows.

Note: The knowledge that $\lim a_{n+1} / a_{n}=1$ implies nothing about the convergence of $\Sigma a_{n}$. The series $\Sigma 1 / n$ and $\Sigma 1 / n^{2}$ demonstrate this.

### 3.35 Examples

(a) Consider the series

$$
\frac{1}{2}+\frac{1}{3}+\frac{1}{2^{2}}+\frac{1}{3^{2}}+\frac{1}{2^{3}}+\frac{1}{3^{3}}+\frac{1}{2^{4}}+\frac{1}{3^{4}}+\cdots
$$

for which

$$
\begin{aligned}
& \liminf _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=\lim _{n \rightarrow \infty}\left(\frac{2}{3}\right)^{n}=0 \\
& \liminf _{n \rightarrow \infty} \sqrt[n]{a_{n}}=\lim _{n \rightarrow \infty} \sqrt[2 n]{\frac{1}{3^{n}}}=\frac{1}{\sqrt{3}}, \\
& \limsup _{n \rightarrow \infty} \sqrt[n]{a_{n}}=\lim _{n \rightarrow \infty} \sqrt[2 n]{\frac{1}{2^{n}}}=\frac{1}{\sqrt{2}}, \\
& \limsup _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=\lim _{n \rightarrow \infty} \frac{1}{2}\left(\frac{3}{2}\right)^{n}=+\infty .
\end{aligned}
$$

The root test indicates convergence; the ratio test does not apply.

(b) The same is true for the series

$$
\frac{1}{2}+1+\frac{1}{8}+\frac{1}{4}+\frac{1}{32}+\frac{1}{16}+\frac{1}{128}+\frac{1}{64}+\cdots,
$$

where

$$
\begin{gathered}
\liminf _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=\frac{1}{8}, \\
\limsup _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=2,
\end{gathered}
$$

but

$$
\lim \sqrt[n]{a_{n}}=\frac{1}{2}
$$

3.36 Remarks The ratio test is frequently easier to apply than the root test, since it is usually easier to compute ratios than $n$th roots. However, the root test has wider scope. More precisely: Whenever the ratio test shows convergence, the root test does too; whenever the root test is inconclusive, the ratio test is too. This is a consequence of Theorem 3.37, and is illustrated by the above examples.

Neither of the two tests is subtle with regard to divergence. Both deduce divergence from the fact that $a_{n}$ does not tend to zero as $n \rightarrow \infty$.

3.37 Theorem For any sequence $\left\{c_{n}\right\}$ of positive numbers,

$$
\begin{aligned}
\liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} & \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} & \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$

Proof We shall prove the second inequality; the proof of the first is quite similar. Put

$$
\alpha=\limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
$$

If $\alpha=+\infty$, there is nothing to prove. If $\alpha$ is finite, choose $\beta>\alpha$. There is an integer $N$ such that

$$
\frac{c_{n+1}}{c_{n}} \leq \beta
$$

for $n \geq N$. In particular, for any $p>0$,

$$
c_{N+k+1} \leq \beta c_{N+k} \quad(k=0,1, \ldots, p-1)
$$

Multiplying these inequalities, we obtain

or

$$
c_{N+p} \leq \beta^{p} c_{N}
$$

$$
c_{n} \leq c_{N} \beta^{-N} \cdot \beta^{n} \quad(n \geq N)
$$

Hence

$$
\sqrt[n]{c_{n}} \leq \sqrt[n]{c_{N} \beta^{-N}} \cdot \beta
$$

so that

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \beta
$$

by Theorem 3.20(b). Since (18) is true for every $\beta>\alpha$, we have

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \alpha
$$

## POWER SERIES

3.38 Definition Given a sequence $\left\{c_{n}\right\}$ of complex numbers, the series

$$
\sum_{n=0}^{\infty} c_{n} z^{n}
$$

is called a power series. The numbers $c_{n}$ are called the coefficients of the series; $z$ is a complex number.

In general, the series will converge or diverge, depending on the choice of $z$. More specifically, with every power series there is associated a circle, the circle of convergence, such that (19) converges if $z$ is in the interior of the circle and diverges if $z$ is in the exterior (to cover all cases, we have to consider the plane as the interior of a circle of infinite radius, and a point as a circle of radius zero). The behavior on the circle of convergence is much more varied and cannot be described so simply.

3.39 Theorem Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}, \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.

Proof Put $a_{n}=c_{n} z^{n}$, and apply the root test:

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}=|z| \limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}=\frac{|z|}{R}
$$

Note: $R$ is called the radius of convergence of $\Sigma c_{n} z^{n}$.

### 3.40 Examples

(a) The series $\Sigma n^{n} z^{n}$ has $R=0$.

(b) The series $\sum \frac{z^{n}}{n !}$ has $R=+\infty$. (In this case the ratio test is easier to apply than the root test.)
(c) The series $\Sigma z^{n}$ has $R=1$. If $|z|=1$, the series diverges, since $\left\{z^{n}\right\}$ does not tend to 0 as $n \rightarrow \infty$.

(d) The series $\sum \frac{z^{n}}{n}$ has $R=1$. It diverges if $z=1$. It converges for all other $z$ with $|z|=1$. (The last assertion will be proved in Theorem 3.44.) (e) The series $\sum \frac{z^{n}}{n^{2}}$ has $R=1$. It converges for all $z$ with $|z|=1$, by the comparison test, since $\left|z^{n} / n^{2}\right|=1 / n^{2}$.

## SUMMATION BY PARTS

3.41 Theorem Given two sequences $\left\{a_{n}\right\},\left\{b_{n}\right\}$, put

$$
A_{n}=\sum_{k=0}^{n} a_{k}
$$

if $n \geq 0$; put $A_{-1}=0$. Then, if $0 \leq p \leq q$, we have

$$
\sum_{n=p}^{q} a_{n} b_{n}=\sum_{n=p}^{q-1} A_{n}\left(b_{n}-b_{n+1}\right)+A_{q} b_{q}-A_{p-1} b_{p}
$$

Proof

$$
\sum_{n=p}^{q} a_{n} b_{n}=\sum_{n=p}^{q}\left(A_{n}-A_{n-1}\right) b_{n}=\sum_{n=p}^{q} A_{n} b_{n}-\sum_{n=p-1}^{q-1} A_{n} b_{n+1}
$$

and the last expression on the right is clearly equal to the right side of (20).

Formula (20), the so-called "partial summation formula," is useful in the investigation of series of the form $\Sigma a_{n} b_{n}$, particularly when $\left\{b_{n}\right\}$ is monotonic. We shall now give applications.

### 3.42 Theorem Suppose

(a) the partial sums $A_{n}$ of $\Sigma a_{n}$ form a bounded sequence;

(b) $b_{0} \geq b_{1} \geq b_{2} \geq \cdots$;

(c) $\lim _{n \rightarrow \infty} b_{n}=0$.

Then $\Sigma a_{n} b_{n}$ converges.

Proof Choose $M$ such that $\left|A_{n}\right| \leq M$ for all $n$. Given $\varepsilon>0$, there is an integer $N$ such that $b_{N} \leq(\varepsilon / 2 M)$. For $N \leq p \leq q$, we have

$$
\begin{aligned}
\left|\sum_{n=p}^{q} a_{n} b_{n}\right| & =\left|\sum_{n=p}^{q-1} A_{n}\left(b_{n}-b_{n+1}\right)+A_{q} b_{q}-A_{p-1} b_{p}\right| \\
& \leq M\left|\sum_{n=p}^{q-1}\left(b_{n}-b_{n+1}\right)+b_{q}+b_{p}\right| \\
& =2 M b_{p} \leq 2 M b_{N} \leq \varepsilon .
\end{aligned}
$$

Convergence now follows from the Cauchy criterion. We note that the first inequality in the above chain depends of course on the fact that $b_{n}-b_{n+1} \geq 0$.

### 3.43 Theorem Suppose

(a) $\left|c_{1}\right| \geq\left|c_{2}\right| \geq\left|c_{3}\right| \geq \cdots$;

(b) $c_{2 m-1} \geq 0, c_{2 m} \leq 0 \quad(m=1,2,3, \ldots)$;

(c) $\lim _{n \rightarrow \infty} c_{n}=0$.

Then $\Sigma c_{n}$ converges.

Series for which $(b)$ holds are called "alternating series"; the theorem was known to Leibnitz.

Proof Apply Theorem 3.42, with $a_{n}=(-1)^{n+1}, b_{n}=\left|c_{n}\right|$.

3.44 Theorem Suppose the radius of convergence of $\Sigma c_{n} z^{n}$ is 1 , and suppose $c_{0} \geq c_{1} \geq c_{2} \geq \cdots, \lim _{n \rightarrow \infty} c_{n}=0$. Then $\Sigma c_{n} z^{n}$ converges at every point on the circle $|z|=1$, except possibly at $z=1$.

Proof Put $a_{n}=z^{n}, b_{n}=c_{n}$. The hypotheses of Theorem 3.42 are then satisfied, since

$$
\left|A_{n}\right|=\left|\sum_{m=0}^{n} z^{m}\right|=\left|\frac{1-z^{n+1}}{1-z}\right| \leq \frac{2}{|1-z|}
$$

if $|z|=1, z \neq 1$

## ABSOLUTE CONVERGENCE

The series $\Sigma a_{n}$ is said to converge absolutely if the series $\Sigma\left|a_{n}\right|$ converges.

3.45 Theorem If $\Sigma a_{n}$ converges absolutely, then $\Sigma a_{n}$ converges.

Proof The assertion follows from the inequality

$$
\left|\sum_{k=n}^{m} a_{k}\right| \leq \sum_{k=n}^{m}\left|a_{k}\right|
$$

plus the Cauchy criterion.

3.46 Remarks For series of positive terms, absolute convergence is the same as convergence.

If $\Sigma a_{n}$ converges, but $\Sigma\left|a_{n}\right|$ diverges, we say that $\Sigma a_{n}$ converges nonabsolutely. For instance, the series

$$
\sum \frac{(-1)^{n}}{n}
$$

converges nonabsolutely (Theorem 3.43).

The comparison test, as well as the root and ratio tests, is really a test for absolute convergence, and therefore cannot give any information about nonabsolutely convergent series. Summation by parts can sometimes be used to handle the latter. In particular, power series converge absolutely in the interior of the circle of convergence.

We shall see that we may operate with absolutely convergent series very much as with finite sums. We may multiply them term by term and we may change the order in which the additions are carried out, without affecting the sum of the series. But for nonabsolutely convergent series this is no longer true, and more care has to be taken when dealing with them.

## ADDITION AND MULTIPLICATION OF SERIES

3.47 Theorem If $\Sigma a_{n}=A$, and $\Sigma b_{n}=B$, then $\Sigma\left(a_{n}+b_{n}\right)=A+B$, and $\Sigma c a_{n}=c A$, for any fixed $c$.

Proof Let

$$
A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k}
$$

Then

$$
A_{n}+B_{n}=\sum_{k=0}^{n}\left(a_{k}+b_{k}\right)
$$

Since $\lim _{n \rightarrow \infty} A_{n}=A$ and $\lim _{n \rightarrow \infty} B_{n}=B$, we see that

$$
\lim _{n \rightarrow \infty}\left(A_{n}+B_{n}\right)=A+B .
$$

The proof of the second assertion is even simpler.

Thus two convergent series may be added term by term, and the resulting series converges to the sum of the two series. The situation becomes more complicated when we consider multiplication of two series. To begin with, we have to define the product. This can be done in several ways; we shall consider the so-called "Cauchy product."

3.48 Definition Given $\Sigma a_{n}$ and $\Sigma b_{n}$, we put

$$
c_{n}=\sum_{k=0}^{n} a_{k} b_{n-k} \quad(n=0,1,2, \ldots)
$$

and call $\Sigma c_{n}$ the product of the two given series.

This definition may be motivated as follows. If we take two power series $\Sigma a_{n} z^{n}$ and $\Sigma b_{n} z^{n}$, multiply them term by term, and collect terms containing the same power of $z$, we get

$$
\begin{aligned}
\sum_{n=0}^{\infty} a_{n} z^{n} \cdot \sum_{n=0}^{\infty} b_{n} z^{n} & =\left(a_{0}+a_{1} z+a_{2} z^{2}+\cdots\right)\left(b_{0}+b_{1} z+b_{2} z^{2}+\cdots\right) \\
& =a_{0} b_{0}+\left(a_{0} b_{1}+a_{1} b_{0}\right) z+\left(a_{0} b_{2}+a_{1} b_{1}+a_{2} b_{0}\right) z^{2}+\cdots \\
& =c_{0}+c_{1} z+c_{2} z^{2}+\cdots .
\end{aligned}
$$

Setting $z=1$, we arrive at the above definition.

3.49 Example If

$$
A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k}, \quad C_{n}=\sum_{k=0}^{n} c_{k}
$$

and $A_{n} \rightarrow A, B_{n} \rightarrow B$, then it is not at all clear that $\left\{C_{n}\right\}$ will converge to $A B$, since we do not have $C_{n}=A_{n} B_{n}$. The dependence of $\left\{C_{n}\right\}$ on $\left\{A_{n}\right\}$ and $\left\{B_{n}\right\}$ is quite a complicated one (see the proof of Theorem 3.50). We shall now show that the product of two convergent series may actually diverge.

The series

$$
\sum_{n=0}^{\infty} \frac{(-1)^{n}}{\sqrt{n+1}}=1-\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}-\frac{1}{\sqrt{4}}+\cdots
$$

converges (Theorem 3.43). We form the product of this series with itself and obtain

$$
\begin{aligned}
\sum_{n=0}^{\infty} c_{n}=1-\left(\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}\right)+\left(\frac{1}{\sqrt{3}}+\right. & \left.\frac{1}{\sqrt{2} \sqrt{2}}+\frac{1}{\sqrt{3}}\right) \\
& -\left(\frac{1}{\sqrt{4}}+\frac{1}{\sqrt{3} \sqrt{2}}+\frac{1}{\sqrt{2} \sqrt{3}}+\frac{1}{\sqrt{4}}\right)+\cdots,
\end{aligned}
$$

so that

Since

$$
c_{n}=(-1)^{n} \sum_{k=0}^{n} \frac{1}{\sqrt{(n-k+1)(k+1)}} \cdot
$$

$$
(n-k+1)(k+1)=\left(\frac{n}{2}+1\right)^{2}-\left(\frac{n}{2}-k\right)^{2} \leq\left(\frac{n}{2}+1\right)^{2} \text {. }
$$

we have

$$
\left|c_{n}\right| \geq \sum_{k=0}^{n} \frac{2}{n+2}=\frac{2(n+1)}{n+2}
$$

so that the condition $c_{n} \rightarrow 0$, which is necessary for the convergence of $\Sigma c_{n}$, is not satisfied.

In view of the next theorem, due to Mertens, we note that we have here considered the product of two nonabsolutely convergent series.

3.50 Theorem Suppose

Then

(a) $\sum_{n=0}^{\infty} a_{n}$ converges absolutely,

(b) $\sum_{n=0}^{\infty} a_{n}=A$,

(c) $\sum_{n=0}^{\infty} b_{n}=B$,

(d) $\quad c_{n}=\sum_{k=0}^{n} a_{k} b_{n-k} \quad(n=0,1,2, \ldots)$.

$$
\sum_{n=0}^{\infty} c_{n}=A B
$$

That is, the product of two convergent series converges, and to the right value, if at least one of the two series converges absolutely.

Proof Put

$$
A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k}, \quad C_{n}=\sum_{k=0}^{n} c_{k}, \quad \beta_{n}=B_{n}-B
$$

Then

$$
\begin{aligned}
C_{n} & =a_{0} b_{0}+\left(a_{0} b_{1}+a_{1} b_{0}\right)+\cdots+\left(a_{0} b_{n}+a_{1} b_{n-1}+\cdots+a_{n} b_{0}\right) \\
& =a_{0} B_{n}+a_{1} B_{n-1}+\cdots+a_{n} B_{0} \\
& =a_{0}\left(B+\beta_{n}\right)+a_{1}\left(B+\beta_{n-1}\right)+\cdots+a_{n}\left(B+\beta_{0}\right) \\
& =A_{n} B+a_{0} \beta_{n}+a_{1} \beta_{n-1}+\cdots+a_{n} \beta_{0}
\end{aligned}
$$

Put

$$
\gamma_{n}=a_{0} \beta_{n}+a_{1} \beta_{n-1}+\cdots+a_{n} \beta_{0} .
$$

We wish to show that $C_{n} \rightarrow A B$. Since $A_{n} B \rightarrow A B$, it suffices to show that

$$
\lim _{n \rightarrow \infty} \gamma_{n}=0
$$

Put

$$
\alpha=\sum_{n=0}^{\infty}\left|a_{n}\right|
$$

[It is here that we use $(a)$.] Let $\varepsilon>0$ be given. By $(c), \beta_{n} \rightarrow 0$. Hence we can choose $N$ such that $\left|\beta_{n}\right| \leq \varepsilon$ for $n \geq N$, in which case

$$
\begin{aligned}
\left|\gamma_{n}\right| & \leq\left|\beta_{0} a_{n}+\cdots+\beta_{N} a_{n-N}\right|+\left|\beta_{N+1} a_{n-N-1}+\cdots+\beta_{n} a_{0}\right| \\
& \leq\left|\beta_{0} a_{n}+\cdots+\beta_{N} a_{n-N}\right|+\varepsilon \alpha .
\end{aligned}
$$

Keeping $N$ fixed, and letting $n \rightarrow \infty$, we get

$$
\limsup _{n \rightarrow \infty}\left|\gamma_{n}\right| \leq \varepsilon \alpha
$$

since $a_{k} \rightarrow 0$ as $k \rightarrow \infty$. Since $\varepsilon$ is arbitrary, (21) follows.

Another question which may be asked is whether the series $\Sigma c_{n}$, if convergent, must have the sum $A B$. Abel showed that the answer is in the affirmative.

3.51 Theorem If the series $\Sigma a_{n}, \Sigma b_{n}, \Sigma c_{n}$ converge to $A, B, C$, and $c_{n}=a_{0} b_{n}+\cdots+a_{n} b_{0}$, then $C=A B$.

Here no assumption is made concerning absolute convergence. We shall give a simple proof (which depends on the continuity of power series) after Theorem 8.2.

## REARRANGEMENTS

3.52 Definition Let $\left\{k_{n}\right\}, n=1,2,3, \ldots$, be a sequence in which every positive integer appears once and only once (that is, $\left\{k_{n}\right\}$ is a 1-1 function from $J$ onto $J$, in the notation of Definition 2.2). Putting

$$
a_{n}^{\prime}=a_{k_{n}} \quad(n=1,2,3, \ldots)
$$

we say that $\Sigma a_{n}^{\prime}$ is a rearrangement of $\Sigma a_{n}$.

If $\left\{s_{n}\right\},\left\{s_{n}^{\prime}\right\}$ are the sequences of partial sums of $\Sigma a_{n}, \Sigma a_{n}^{\prime}$, it is easily seen that, in general, these two sequences consist of entirely different numbers. We are thus led to the problem of determining under what conditions all rearrangements of a convergent series will converge and whether the sums are necessarily the same.

3.53 Example Consider the convergent series

$$
1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\cdots
$$

and one of its rearrangements

$$
1+\frac{1}{3}-\frac{1}{2}+\frac{1}{3}+\frac{1}{7}-\frac{1}{4}+\frac{1}{9}+\frac{1}{11}-\frac{1}{6}+\cdots
$$

in which two positive terms are always followed by one negative. If $s$ is the sum of (22), then

$$
s<1-\frac{1}{2}+\frac{1}{3}=\frac{5}{6} .
$$

Since

$$
\frac{1}{4 k-3}+\frac{1}{4 k-1}-\frac{1}{2 k}>0
$$

for $k \geq 1$, we see that $s_{3}^{\prime}<s_{6}^{\prime}<s_{9}^{\prime}<\cdots$, where $s_{n}^{\prime}$ is $n$th partial sum of (23). Hence

$$
\limsup _{n \rightarrow \infty} s_{n}^{\prime}>s_{3}^{\prime}=\frac{5}{6}
$$

so that (23) certainly does not converge to $s$ [we leave it to the reader to verify that (23) does, however, converge].

This example illustrates the following theorem, due to Riemann.

3.54 Theorem Let $\Sigma a_{n}$ be a series of real numbers which converges, but not absolutely. Suppose

$$
-\infty \leq \alpha \leq \beta \leq \infty .
$$

Then there exists a rearrangement $\Sigma a_{n}^{\prime}$ with partial sums $s_{n}^{\prime}$ such that

$$
\liminf _{n \rightarrow \infty} s_{n}^{\prime}=\alpha, \quad \limsup _{n \rightarrow \infty} s_{n}^{\prime}=\beta .
$$

Proof Let

$$
p_{n}=\frac{\left|a_{n}\right|+a_{n}}{2}, \quad q_{n}=\frac{\left|a_{n}\right|-a_{n}}{2} \quad(n=1,2,3, \ldots)
$$

Then $p_{n}-q_{n}=a_{n}, p_{n}+q_{n}=\left|a_{n}\right|, p_{n} \geq 0, q_{n} \geq 0$. The series $\Sigma p_{n}, \Sigma q_{n}$ must both diverge.

For if both were convergent, then

$$
\Sigma\left(p_{n}+q_{n}\right)=\Sigma\left|a_{n}\right|
$$

would converge, contrary to hypothesis. Since

$$
\sum_{n=1}^{N} a_{n}=\sum_{n=1}^{N}\left(p_{n}-q_{n}\right)=\sum_{n=1}^{N} p_{n}-\sum_{n=1}^{N} q_{n}
$$

divergence of $\Sigma p_{n}$ and convergence of $\Sigma q_{n}$ (or vice versa) implies divergence of $\Sigma a_{n}$, again contrary to hypothesis.

Now let $P_{1}, P_{2}, P_{3}, \ldots$ denote the nonnegative terms of $\Sigma a_{n}$, in the order in which they occur, and let $Q_{1}, Q_{2}, Q_{3}, \ldots$ be the absolute values of the negative terms of $\Sigma a_{n}$, also in their original order.

The series $\Sigma P_{n}, \Sigma Q_{n}$ differ from $\Sigma p_{n}, \Sigma q_{n}$ only by zero terms, and are therefore divergent.

We shall construct sequences $\left\{m_{n}\right\},\left\{k_{n}\right\}$, such that the series

$$
\begin{aligned}
P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots & \\
& +P_{m_{2}}-Q_{k_{1}+1}-\cdots-Q_{k_{2}}+\cdots
\end{aligned}
$$

which clearly is a rearrangement of $\Sigma a_{n}$, satisfies (24).

Choose real-valued sequences $\left\{\alpha_{n}\right\},\left\{\beta_{n}\right\}$ such that $\alpha_{n} \rightarrow \alpha, \beta_{n} \rightarrow \beta$, $\alpha_{n}<\beta_{n}, \beta_{1}>0$.

Let $m_{1}, k_{1}$ be the smallest integers such that

$$
\begin{gathered}
P_{1}+\cdots+P_{m_{1}}>\beta_{1} \\
P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}<\alpha_{1}
\end{gathered}
$$

let $m_{2}, k_{2}$ be the smallest integers such that

$$
\begin{aligned}
& P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots+P_{m_{2}}>\beta_{2} \\
& P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots+P_{m_{2}}-Q_{k_{1}+1} \\
&
\end{aligned}
$$

and continue in this way. This is possible since $\Sigma P_{n}$ and $\Sigma Q_{n}$ diverge. If $x_{n}, y_{n}$ denote the partial sums of (25) whose last terms are $P_{m_{n}}$, $-Q_{k_{n}}$, then

$$
\left|x_{n}-\beta_{n}\right| \leq P_{m_{n}}, \quad\left|y_{n}-\alpha_{n}\right| \leq Q_{k_{n}} .
$$

Since $P_{n} \rightarrow 0$ and $Q_{n} \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_{n} \rightarrow \beta, y_{n} \rightarrow \alpha$.

Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25).

3.55 Theorem If $\Sigma a_{n}$ is a series of complex numbers which converges absolutely, then every rearrangement of $\Sigma a_{n}$ converges, and they all converge to the same sum.

Proof Let $\Sigma a_{n}^{\prime}$ be a rearrangement, with partial sums $s_{n}^{\prime}$. Given $\varepsilon>0$, there exists an integer $N$ such that $m \geq n \geq N$ implies

$$
\sum_{i=n}^{m}\left|a_{i}\right| \leq \varepsilon
$$

Now choose $p$ such that the integers $1,2, \ldots, N$ are all contained in the set $k_{1}, k_{2}, \ldots, k_{p}$ (we use the notation of Definition 3.52). Then if $n>p$, the numbers $a_{1}, \ldots, a_{N}$ will cancel in the difference $s_{n}-s_{n}^{\prime}$, so that $\left|s_{n}-s_{n}^{\prime}\right| \leq \varepsilon$, by (26). Hence $\left\{s_{n}^{\prime}\right\}$ converges to the same sum as $\left\{s_{n}\right\}$.

## EXERCISES

1. Prove that convergence of $\left\{s_{n}\right\}$ implies convergence of $\left\{\left|s_{n}\right|\right\}$. Is the converse true?
2. Calculate $\lim _{n \rightarrow \infty}\left(\sqrt{n^{2}+n}-n\right)$.
3. If $s_{1}=\sqrt{2}$, and

$$
s_{n+1}=\sqrt{2+\sqrt{s_{n}}} \quad(n=1,2,3, \ldots)
$$

prove that $\left\{s_{n}\right\}$ converges, and that $s_{n}<2$ for $n=1,2,3, \ldots$.

4. Find the upper and lower limits of the sequence $\left\{s_{n}\right\}$ defined by

$$
s_{1}=0 ; \quad s_{2 m}=\frac{s_{2 m-1}}{2} ; \quad s_{2 m+1}=\frac{1}{2}+s_{2 m}
$$

5. For any two real sequences $\left\{a_{n}\right\},\left\{b_{n}\right\}$, prove that

$$
\limsup _{n \rightarrow \infty}\left(a_{n}+b_{n}\right) \leq \limsup _{n \rightarrow \infty} a_{n}+\limsup _{n \rightarrow \infty} b_{n},
$$

provided the sum on the right is not of the form $\infty-\infty$.

6. Investigate the behavior (convergence or divergence) of $\Sigma a_{n}$ if

(a) $a_{n}=\sqrt{n+1}-\sqrt{n}$;

(b) $a_{n}=\frac{\sqrt{n+1}-\sqrt{n}}{n}$;

(c) $a_{n}=(\sqrt[n]{n}-1)^{n}$;

(d) $a_{n}=\frac{1}{1+z^{n}}, \quad$ for complex values of $z$.

7. Prove that the convergence of $\Sigma a_{n}$ implies the convergence of

$$
\sum \frac{\sqrt{a_{n}}}{n}
$$

if $a_{n} \geq 0$.

8. If $\Sigma a_{n}$ converges, and if $\left\{b_{n}\right\}$ is monotonic and bounded, prove that $\Sigma a_{n} b_{n}$ converges.
9. Find the radius of convergence of each of the following power series:
(a) $\sum n^{3} z^{n}$
(b) $\sum \frac{2^{n}}{n !} z^{n}$
(c) $\sum \frac{2^{n}}{n^{2}} z^{n}$
(d) $\sum \frac{n^{3}}{3^{n}} z^{n}$
10. Suppose that the coefficients of the power series $\sum a_{n} z^{n}$ are integers, infinitely many of which are distinct from zero. Prove that the radius of convergence is at most 1 .
11. Suppose $a_{n}>0, s_{n}=a_{1}+\cdots+a_{n}$, and $\Sigma a_{n}$ diverges.

(a) Prove that $\sum \frac{a_{n}}{1+a_{n}}$ diverges.

(b) Prove that

$$
\frac{a_{N+1}}{s_{N+1}}+\cdots+\frac{a_{N+k}}{s_{N+k}} \geq 1-\frac{s_{N}}{s_{N+k}}
$$

and deduce that $\sum \frac{a_{n}}{s_{n}}$ diverges.

(c) Prove that

$$
\frac{a_{n}}{s_{n}^{2}} \leq \frac{1}{s_{n-1}}-\frac{1}{s_{n}}
$$

and deduce that $\sum \frac{a_{n}}{s_{n}^{2}}$ converges.

(d) What can be said about

$$
\sum \frac{a_{n}}{1+n a_{n}} \text { and } \sum \frac{a_{n}}{1+n^{2} a_{n}} ?
$$

12. Suppose $a_{n}>0$ and $\Sigma a_{n}$ converges. Put

$$
r_{n}=\sum_{m=n}^{\infty} a_{m}
$$

(a) Prove that

$$
\frac{a_{m}}{r_{m}}+\cdots+\frac{a_{n}}{r_{n}}>1-\frac{r_{n}}{r_{m}}
$$

if $m<n$, and deduce that $\sum \frac{a_{n}}{r_{n}}$ diverges.
(b) Prove that

$$
\frac{a_{n}}{\sqrt{r_{n}}}<2\left(\sqrt{r_{n}}-\sqrt{r_{n+1}}\right)
$$

and deduce that $\sum \frac{a_{n}}{\sqrt{r_{n}}}=$ converges.

13. Prove that the Cauchy product of two absolutely convergent series converges absolutely.
14. If $\left\{s_{n}\right\}$ is a complex sequence, define its arithmetic means $\sigma_{n}$ by

$$
\sigma_{n}=\frac{s_{0}+s_{1}+\cdots+s_{n}}{n+1} \quad(n=0,1,2, \ldots) .
$$

(a) If $\lim s_{n}=s$, prove that $\lim \sigma_{n}=s$.

(b) Construct a sequence $\left\{s_{n}\right\}$ which does not converge, although $\lim \sigma_{n}=0$.

(c) Can it happen that $s_{n}>0$ for all $n$ and that $\lim \sup s_{n}=\infty$, although $\lim \sigma_{n}=0$ ?

(d) Put $a_{n}=s_{n}-s_{n-1}$, for $n \geq 1$. Show that

$$
s_{n}-\sigma_{n}=\frac{1}{n+1} \sum_{k=1}^{n} k a_{k}
$$

Assume that $\lim \left(n a_{n}\right)=0$ and that $\left\{\sigma_{n}\right\}$ converges. Prove that $\left\{s_{n}\right\}$ converges. [This gives a converse of $(a)$, but under the additional assumption that $n a_{n} \rightarrow 0$.]

(e) Derive the last conclusion from a weaker hypothesis: Assume $M<\infty$, $\left|n a_{n}\right| \leq M$ for all $n$, and $\lim \sigma_{n}=\sigma$. Prove that $\lim s_{n}=\sigma$, by completing the following outline:

If $m<n$, then

$$
s_{n}-\sigma_{n}=\frac{m+1}{n-m}\left(\sigma_{n}-\sigma_{m}\right)+\frac{1}{n-m} \sum_{l=m+1}^{n}\left(s_{n}-s_{l}\right) .
$$

For these $i$,

$$
\left|s_{n}-s_{i}\right| \leq \frac{(n-i) M}{i+1} \leq \frac{(n-m-1) M}{m+2} .
$$

Fix $\varepsilon>0$ and associate with each $n$ the integer $m$ that satisfies

$$
m \leq \frac{n-\varepsilon}{1+\varepsilon}<m+1
$$

Then $(m+1) /(n-m) \leq 1 / \varepsilon$ and $\left|s_{n}-s_{i}\right|<M \varepsilon$. Hence

$$
\limsup _{n \rightarrow \infty}\left|s_{n}-\sigma\right| \leq M \varepsilon \text {. }
$$

Since $\varepsilon$ was arbitrary, $\lim s_{n}=\sigma$.

15. Definition 3.21 can be extended to the case in which the $a_{n}$ lie in some fixed $R^{k}$. Absolute convergence is defined as convergence of $\Sigma\left|\mathbf{a}_{n}\right|$. Show that Theorems $3.22,3.23,3.25(a), 3.33,3.34,3.42,3.45,3.47$, and 3.55 are true in this more general setting. (Only slight modifications are required in any of the proofs.)
16. Fix a positive number $\alpha$. Choose $x_{1}>\sqrt{\alpha}$, and define $x_{2}, x_{3}, x_{4}, \ldots$, by the recursion formula

$$
x_{n+1}=\frac{1}{2}\left(x_{n}+\frac{\alpha}{x_{n}}\right)
$$

(a) Prove that $\left\{x_{n}\right\}$ decreases monotonically and that $\lim x_{n}=\sqrt{\alpha}$.

(b) Put $\varepsilon_{n}=x_{n}-\sqrt{\alpha}$, and show that

$$
\varepsilon_{n+1}=\frac{\varepsilon_{n}^{2}}{2 x_{n}}<\frac{\varepsilon_{n}^{2}}{2 \sqrt{\alpha}}
$$

so that, setting $\beta=2 \sqrt{\alpha}$,

$$
\varepsilon_{n+1}<\beta\left(\frac{\varepsilon_{1}}{\beta}\right)^{2^{n}} \quad(n=1,2,3, \ldots)
$$

(c) This is a good algorithm for computing square roots, since the recursion formula is simple and the convergence is extremely rapid. For example, if $\alpha=3$ and $x_{1}=2$, show that $\varepsilon_{1} / \beta<\frac{1}{10}$ and that therefore

$$
\varepsilon_{5}<4 \cdot 10^{-16}, \quad \varepsilon_{6}<4 \cdot 10^{-32} .
$$

17. Fix $\alpha>1$. Take $x_{1}>\sqrt{\alpha}$, and define

$$
x_{n+1}=\frac{\alpha+x_{n}}{1+x_{n}}=x_{n}+\frac{\alpha-x_{n}^{2}}{1+x_{n}}
$$

(a) Prove that $x_{1}>x_{3}>x_{5}>\cdots$.

(b) Prove that $x_{2}<x_{4}<x_{6}<\cdots$.

(c) Prove that $\lim x_{n}=\sqrt{\alpha}$.

(d) Compare the rapidity of convergence of this process with the one described in Exercise 16.

18. Replace the recursion formula of Exercise 16 by

$$
x_{n+1}=\frac{p-1}{p} x_{n}+\frac{\alpha}{p} x_{n}^{-p+1}
$$

where $p$ is a fixed positive integer, and describe the behavior of the resulting sequences $\left\{x_{n}\right\}$.

19. Associate to each sequence $a=\left\{\alpha_{n}\right\}$, in which $\alpha_{n}$ is 0 or 2 , the real number

$$
x(a)=\sum_{n=1}^{\infty} \frac{\alpha_{n}}{3^{n}}
$$

Prove that the set of all $x(a)$ is precisely the Cantor set described in Sec. 2.44.

20. Suppose $\left\{p_{n}\right\}$ is a Cauchy sequence in a metric space $X$, and some subsequence $\left\{p_{n\}}\right\}$ converges to a point $p \in X$. Prove that the full sequence $\left\{p_{n}\right\}$ converges to $p$.
21. Prove the following analogue of Theorem $3.10(b)$ : If $\left\{E_{n}\right\}$ is a sequence of closed nonempty and bounded sets in a complete metric space $X$, if $E_{n} \supset E_{n+1}$, and if

$$
\lim _{n \rightarrow \infty} \operatorname{diam} E_{n}=0 \text {, }
$$

then $\bigcap_{1}^{\infty} E_{n}$ consists of exactly one point.

22. Suppose $X$ is a nonempty complete metric space, and $\left\{G_{n}\right\}$ is a sequence of dense open subsets of $X$. Prove Baire's theorem, namely, that $\bigcap_{1}^{\infty} G_{n}$ is not empty. (In fact, it is dense in $X$.) Hint: Find a shrinking sequence of neighborhoods $E_{n}$ such that $E_{n} \subset G_{n}$, and apply Exercise 21 .
23. Suppose $\left\{p_{n}\right\}$ and $\left\{q_{n}\right\}$ are Cauchy sequences in a metric space $X$. Show that the sequence $\left\{d\left(p_{n}, q_{n}\right)\right\}$ converges. Hint: For any $m, n$,

$$
d\left(p_{n}, q_{n}\right) \leq d\left(p_{n}, p_{m}\right)+d\left(p_{m}, q_{m}\right)+d\left(q_{m}, q_{n}\right)
$$

it follows that

$$
\left|d\left(p_{n}, q_{n}\right)-d\left(p_{m}, q_{m}\right)\right|
$$

is small if $m$ and $n$ are large.

24. Let $X$ be a metric space.

(a) Call two Cauchy sequences $\left\{p_{n}\right\},\left\{q_{n}\right\}$ in $X$ equivalent if

$$
\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)=0
$$

Prove that this is an equivalence relation.

(b) Let $X^{*}$ be the set of all equivalence classes so obtained. If $P \in X^{*}, Q \in X^{*}$, $\left\{p_{n}\right\} \in P,\left\{q_{n}\right\} \in Q$, define

$$
\Delta(P, Q)=\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)
$$

by Exercise 23, this limit exists. Show that the number $\Delta(P, Q)$ is unchanged if $\left\{\boldsymbol{p}_{n}\right\}$ and $\left\{q_{n}\right\}$ are replaced by equivalent sequences, and hence that $\Delta$ is a distance function in $X^{*}$.

(c) Prove that the resulting metric space $X^{*}$ is complete.

(d) For each $p \in X$, there is a Cauchy sequence all of whose terms are $p$; let $P_{p}$ be the element of $X^{*}$ which contains this sequence. Prove that

$$
\Delta\left(\boldsymbol{P}_{p}, \boldsymbol{P}_{q}\right)=d(p, q)
$$

for all $p, q \in X$. In other words, the mapping $\varphi$ defined by $\varphi(p)=P_{p}$ is an isometry (i.e., a distance-preserving mapping) of $X$ into $X^{*}$.

(e) Prove that $\varphi(X)$ is dense in $X^{*}$, and that $\varphi(X)=X^{*}$ if $X$ is complete. By $(d)$, we may identify $X$ and $\varphi(X)$ and thus regard $X$ as embedded in the complete metric space $X^{*}$. We call $X^{*}$ the completion of $X$.

25. Let $X$ be the metric space whose points are the rational numbers, with the metric $d(x, y)=|x-y|$. What is the completion of this space? (Compare Exercise 24.)

## 4

## CONTINUITY

The function concept and some of the related terminology were introduced in Definitions 2.1 and 2.2. Although we shall (in later chapters) be mainly interested in real and complex functions (i.e., in functions whose values are real or complex numbers) we shall also discuss vector-valued functions (i.e., functions with values in $R^{k}$ ) and functions with values in an arbitrary metric space. The theorems we shall discuss in this general setting would not become any easier if we restricted ourselves to real functions, for instance, and it actually simplifies and clarifies the picture to discard unnecessary hypotheses and to state and prove theorems in an appropriately general context.

The domains of definition of our functions will also be metric spaces, suitably specialized in various instances.

## LIMITS OF FUNCTIONS

4.1 Definition Let $X$ and $Y$ be metric spaces; suppose $E \subset X, f$ maps $E$ into $Y$, and $p$ is a limit point of $E$. We write $f(x) \rightarrow q$ as $x \rightarrow p$, or

$$
\lim _{x \rightarrow p} f(x)=q
$$

if there is a point $q \in Y$ with the following property: For every $\varepsilon>0$ there exists a $\delta>0$ such that

$$
d_{Y}(f(x), q)<\varepsilon
$$

for all points $x \in E$ for which

$$
0<d_{x}(x, p)<\delta
$$

The symbols $d_{X}$ and $d_{Y}$ refer to the distances in $X$ and $Y$, respectively.

If $X$ and/or $Y$ are replaced by the real line, the complex plane, or by some euclidean space $R^{k}$, the distances $d_{X}, d_{Y}$ are of course replaced by absolute values, or by norms of differences (see Sec. 2.16).

It should be noted that $p \in X$, but that $p$ need not be a point of $E$ in the above definition. Moreover, even if $p \in E$, we may very well have $f(p) \neq \lim _{x \rightarrow p} f(x)$.

We can recast this definition in terms of limits of sequences:

4.2 Theorem Let $X, Y, E, f$, and $p$ be as in Definition 4.1. Then

$$
\lim _{x \rightarrow p} f(x)=q
$$

if and only if

$$
\lim _{n \rightarrow \infty} f\left(p_{n}\right)=q
$$

for every sequence $\left\{p_{n}\right\}$ in $E$ such that

$$
p_{n} \neq p, \quad \lim _{n \rightarrow \infty} p_{n}=p .
$$

Proof Suppose (4) holds. Choose $\left\{p_{n}\right\}$ in $E$ satisfying (6). Let $\varepsilon>0$ be given. Then there exists $\delta>0$ such that $d_{Y}(f(x), q)<\varepsilon$ if $x \in E$ and $0<d_{x}(x, p)<\delta$. Also, there exists $N$ such that $n>N$ implies $0<d_{x}\left(p_{n}, p\right)<\delta$. Thus, for $n>N$, we have $d_{Y}\left(f\left(p_{n}\right), q\right)<\varepsilon$, which shows that (5) holds.

Conversely, suppose (4) is false. Then there exists some $\varepsilon>0$ such that for every $\delta>0$ there exists a point $x \in E$ (depending on $\delta$ ), for which $d_{Y}(f(x), q) \geq \varepsilon$ but $0<d_{X}(x, p)<\delta$. Taking $\delta_{n}=1 / n(n=1,2,3, \ldots)$, we thus find a sequence in $E$ satisfying (6) for which (5) is false.

Corollary If $f$ has a limit at $p$, this limit is unique.

This follows from Theorems 3.2(b) and 4.2.

4.3 Definition Suppose we have two complex functions, $f$ and $g$, both defined on $E$. By $f+g$ we mean the function which assigns to each point $x$ of $E$ the number $f(x)+g(x)$. Similarly we define the difference $f-g$, the product $f g$, and the quotient $f / g$ of the two functions, with the understanding that the quotient is defined only at those points $x$ of $E$ at which $g(x) \neq 0$. If $f$ assigns to each point $x$ of $E$ the same number $c$, then $f$ is said to be a constant function, or simply a constant, and we write $f=c$. If $f$ and $g$ are real functions, and if $f(x) \geq g(x)$ for every $x \in E$, we shall sometimes write $f \geq g$, for brevity.

Similarly, if $\mathbf{f}$ and $\mathbf{g}$ map $E$ into $R^{k}$, we define $\mathbf{f}+\mathbf{g}$ and $\mathbf{f} \cdot \mathbf{g}$ by

$$
(\mathbf{f}+\mathbf{g})(x)=\mathbf{f}(x)+\mathbf{g}(x), \quad(\mathbf{f} \cdot \mathbf{g})(x)=\mathbf{f}(x) \cdot \mathbf{g}(x)
$$

and if $\lambda$ is a real number, $(\lambda \mathbf{f})(x)=\lambda \mathbf{f}(x)$.

4.4 Theorem Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then (a) $\lim _{x \rightarrow p}(f+g)(x)=A+B$;

(b) $\lim _{x \rightarrow p}(f g)(x)=A B$;

(c) $\lim _{x \rightarrow p}\left(\frac{f}{g}\right)(x)=\frac{A}{B}$, if $B \neq 0$.

Proof In view of Theorem 4.2, these assertions follow immediately from the analogous properties of sequences (Theorem 3.3).

Remark If $\mathbf{f}$ and $\mathbf{g}$ map $E$ into $R^{k}$, then $(a)$ remains true, and $(b)$ becomes

(b') $\lim _{x \rightarrow p}(\mathbf{f} \cdot \mathbf{g})(x)=\mathbf{A} \cdot \mathbf{B}$.

(Compare Theorem 3.4.)

## CONTINUOUS FUNCTIONS

4.5 Definition Suppose $X$ and $Y$ are metric spaces, $E \subset X, p \in E$, and $f$ maps $E$ into $Y$. Then $f$ is said to be continuous at $p$ if for every $\varepsilon>0$ there exists a $\delta>0$ such that

$$
d_{Y}(f(x), f(p))<\varepsilon
$$

for all points $x \in E$ for which $d_{x}(x, p)<\delta$.

If $f$ is continuous at every point of $E$, then $f$ is said to be continuous on $E$.

It should be noted that $f$ has to be defined at the point $p$ in order to be continuous at $p$. (Compare this with the remark following Definition 4.1.)

If $p$ is an isolated point of $E$, then our definition implies that every function $f$ which has $E$ as its domain of definition is continuous at $p$. For, no matter which $\varepsilon>0$ we choose, we can pick $\delta>0$ so that the only point $x \in E$ for which $d_{x}(x, p)<\delta$ is $x=p$; then

$$
d_{Y}(f(x), f(p))=0<\varepsilon .
$$

4.6 Theorem In the situation given in Definition 4.5, assume also that $p$ is a limit point of $E$. Then $f$ is continuous at $p$ if and only if $\lim _{x \rightarrow p} f(x)=f(p)$.

Proof This is clear if we compare Definitions 4.1 and 4.5.

We now turn to compositions of functions. A brief statement of the following theorem is that a continuous function of a continuous function is continuous.

4.7 Theorem Suppose $X, Y, Z$ are metric spaces, $E \subset X, f$ maps $E$ into $Y, g$ maps the range of $f, f(E)$, into $Z$, and $h$ is the mapping of $E$ into $Z$ defined by

$$
h(x)=g(f(x)) \quad(x \in E)
$$

If $f$ is continuous at a point $p \in E$ and if $g$ is continuous at the point $f(p)$, then $h$ is continuous at $p$.

This function $h$ is called the composition or the composite of $f$ and $g$. The notation

$$
h=g \circ f
$$

is frequently used in this context.

Proof Let $\varepsilon>0$ be given. Since $g$ is continuous at $f(p)$, there exists $\eta>0$ such that

$$
d_{Z}(g(y), g(f(p)))<\varepsilon \text { if } d_{Y}(y, f(p))<\eta \text { and } y \in f(E) \text {. }
$$

Since $f$ is continuous at $p$, there exists $\delta>0$ such that

$$
d_{Y}(f(x), f(p))<\eta \text { if } d_{x}(x, p)<\delta \text { and } x \in E
$$

It follows that

$$
d_{\mathrm{Z}}(h(x), h(p))=d_{\mathrm{Z}}(g(f(x)), g(f(p)))<\varepsilon
$$

if $d_{x}(x, p)<\delta$ and $x \in E$. Thus $h$ is continuous at $p$.

4.8 Theorem $A$ mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous on $X$ if and only if $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$.

(Inverse images are defined in Definition 2.2.) This is a very useful characterization of continuity.

Proof Suppose $f$ is continuous on $X$ and $V$ is an open set in $Y$. We have to show that every point of $f^{-1}(V)$ is an interior point of $f^{-1}(V)$. So, suppose $p \in X$ and $f(p) \in V$. Since $V$ is open, there exists $\varepsilon>0$ such that $y \in V$ if $d_{Y}(f(p), y)<\varepsilon$; and since $f$ is continuous at $p$, there exists $\delta>0$ such that $d_{\boldsymbol{Y}}(f(x), f(p))<\varepsilon$ if $d_{\boldsymbol{X}}(x, p)<\delta$. Thus $x \in f^{-1}(V)$ as soon as $d_{x}(x, p)<\delta$.

Conversely, suppose $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$. Fix $p \in X$ and $\varepsilon>0$, let $V$ be the set of all $y \in Y$ such that $d_{Y}(y, f(p))<\varepsilon$. Then $V$ is open; hence $f^{-1}(V)$ is open; hence there exists $\delta>0$ such that $x \in f^{-1}(V)$ as soon as $d_{x}(p, x)<\delta$. But if $x \in f^{-1}(V)$, then $f(x) \in V$, so that $d_{Y}(f(x), f(p))<\varepsilon$.

This completes the proof.

Corollary $A$ mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous if and only if $f^{-1}(C)$ is closed in $X$ for every closed set $C$ in $Y$.

This follows from the theorem, since a set is closed if and only if its complement is open, and since $f^{-1}\left(E^{c}\right)=\left[f^{-1}(E)\right]^{c}$ for every $E \subset Y$.

We now turn to complex-valued and vector-valued functions, and to functions defined on subsets of $R^{k}$.

4.9 Theorem Let $f$ and $g$ be complex continuous functions on a metric space $X$. Then $f+g, f g$, and $f / g$ are continuous on $X$.

In the last case, we must of course assume that $g(x) \neq 0$, for all $x \in X$.

Proof At isolated points of $X$ there is nothing to prove. At limit points, the statement follows from Theorems 4.4 and 4.6.

### 4.10 Theorem

(a) Let $f_{1}, \ldots, f_{k}$ be real functions on a metric space $X$, and let $\mathbf{f}$ be the mapping of $X$ into $R^{k}$ defined by

$$
\mathbf{f}(x)=\left(f_{1}(x), \ldots, f_{k}(x)\right) \quad(x \in X)
$$

then $\mathbf{f}$ is continuous if and only if each of the functions $f_{1}, \ldots, f_{k}$ is continuous. (b) If $\mathbf{f}$ and $\mathbf{g}$ are continuous mappings of $X$ into $R^{k}$, then $\mathbf{f}+\mathbf{g}$ and $\mathbf{f} \cdot \mathbf{g}$ are continuous on $X$.

The functions $f_{1}, \ldots, f_{k}$ are called the components of $\mathbf{f}$. Note that $\mathbf{f}+\mathbf{g}$ is a mapping into $R^{k}$, whereas $\mathbf{f} \cdot \mathbf{g}$ is a real function on $X$.

Proof Part $(a)$ follows from the inequalities

$$
\left|f_{j}(x)-f_{j}(y)\right| \leq|\mathbf{f}(x)-\mathbf{f}(y)|=\left\{\sum_{i=1}^{k}\left|f_{i}(x)-f_{i}(y)\right|^{2}\right\}^{\frac{t}{2}},
$$

for $j=1, \ldots, k$. Part $(b)$ follows from $(a)$ and Theorem 4.9.

4.11 Examples If $x_{1}, \ldots, x_{k}$ are the coordinates of the point $\mathrm{x} \in R^{k}$, the functions $\phi_{i}$ defined by

$$
\phi_{i}(\mathbf{x})=x_{i} \quad\left(\mathbf{x} \in R^{k}\right)
$$

are continuous on $R^{k}$, since the inequality

$$
\left|\phi_{i}(\mathbf{x})-\phi_{l}(\mathbf{y})\right| \leq|\mathbf{x}-\mathbf{y}|
$$

shows that we may take $\delta=\varepsilon$ in Definition 4.5. The functions $\phi_{l}$ are sometimes called the coordinate functions.

Repeated application of Theorem 4.9 then shows that every monomial

$$
x_{1}^{n_{1}} x_{2}^{n_{2}} \ldots x_{k}^{n_{k}}
$$

where $n_{1}, \ldots, n_{k}$ are nonnegative integers, is continuous on $R^{k}$. The same is true of constant multiples of (9), since constants are evidently continuous. It follows that every polynomial $P$, given by

$$
P(\mathbf{x})=\Sigma c_{n_{1}} \cdots n_{k} x_{1}^{n_{1}} \ldots x_{k}^{n_{k}} \quad\left(\mathbf{x} \in R^{k}\right)
$$

is continuous on $R^{k}$. Here the coefficients $c_{n_{1}} \cdots n_{k}$ are complex numbers, $n_{1}, \ldots, n_{k}$ are nonnegative integers, and the sum in (10) has finitely many terms.

Furthermore, every rational function in $x_{1}, \ldots, x_{k}$, that is, every quotient of two polynomials of the form (10), is continuous on $R^{k}$ wherever the denominator is different from zero.

From the triangle inequality one sees easily that

$$
|| \mathbf{x}|-| \mathbf{y}|| \leq|\mathbf{x}-\mathbf{y}| \quad\left(\mathbf{x}, \mathbf{y} \in R^{k}\right)
$$

Hence the mapping $\mathbf{x} \rightarrow|\mathbf{x}|$ is a continuous real function on $R^{k}$.

If now $\mathrm{f}$ is a continuous mapping from a metric space $X$ into $R^{k}$, and if $\phi$ is defined on $X$ by setting $\phi(p)=|\mathbf{f}(p)|$, it follows, by Theorem 4.7, that $\phi$ is a continuous real function on $X$.

4.12 Remark We defined the notion of continuity for functions defined on a subset $E$ of a metric space $X$. However, the complement of $E$ in $X$ plays no role whatever in this definition (note that the situation was somewhat different for limits of functions). Accordingly, we lose nothing of interest by discarding the complement of the domain of $f$. This means that we may just as well talk only about continuous mappings of one metric space into another, rather than
of mappings of subsets. This simplifies statements and proofs of some theorems. We have already made use of this principle in Theorems 4.8 to 4.10 , and will continue to do so in the following section on compactness.

## CONTINUITY AND COMPACTNESS

4.13 Definition A mapping $\mathrm{f}$ of a set $E$ into $R^{k}$ is said to be bounded if there is a real number $M$ such that $|\mathrm{f}(x)| \leq M$ for all $x \in E$.

4.14 Theorem Suppose $f$ is a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f(X)$ is compact.

Proof Let $\left\{V_{\alpha}\right\}$ be an open cover of $f(X)$. Since $f$ is continuous, Theorem 4.8 shows that each of the sets $f^{-1}\left(V_{a}\right)$ is open. Since $X$ is compact, there are finitely many indices, say $\alpha_{1}, \ldots, \alpha_{n}$, such that

$$
X \subset f^{-1}\left(V_{\alpha_{1}}\right) \cup \cdots \cup f^{-1}\left(V_{\alpha_{n}}\right) .
$$

Since $f\left(f^{-1}(E)\right) \subset E$ for every $E \subset Y$, (12) implies that

$$
f(X) \subset V_{a_{1}} \cup \cdots \cup V_{a_{n}} .
$$

This completes the proof.

Note: We have used the relation $f\left(f^{-1}(E)\right) \subset E$, valid for $E \subset Y$. If $E \subset X$, then $f^{-1}(f(E)) \supset E$; equality need not hold in either case.

We shall now deduce some consequences of Theorem 4.14.

4.15 Theorem If $\mathbf{f}$ is a continuous mapping of a compact metric space $X$ into $R^{k}$, then $\mathbf{f}(X)$ is closed and bounded. Thus, $\mathbf{f}$ is bounded.

This follows from Theorem 2.41. The result is particularly important when $f$ is real:

4.16 Theorem Suppose $f$ is a continuous real function on a compact metric space $X$, and

$$
M=\sup _{\boldsymbol{p} \in \boldsymbol{X}} f(p), \quad m=\inf _{\boldsymbol{p} \in \boldsymbol{X}} f(p)
$$

Then there exist points $p, q \in X$ such that $f(p)=M$ and $f(q)=m$.

The notation in (14) means that $M$ is the least upper bound of the set of all numbers $f(p)$, where $p$ ranges over $X$, and that $m$ is the greatest lower bound of this set of numbers.

The conclusion may also be stated as follows: There exist points $p$ and $q$ in $X$ such that $f(q) \leq f(x) \leq f(p)$ for all $x \in X$; that is, $f$ attains its maximum (at $p$ ) and its minimum (at $q$ ).

Proof By Theorem 4.15, $f(X)$ is a closed and bounded set of real numbers; hence $f(X)$ contains

$$
M=\sup f(X) \quad \text { and } \quad m=\inf f(X)
$$

by Theorem 2.28.

4.17 Theorem Suppose $f$ is a continuous 1-1 mapping of a compact metric space $X$ onto a metric space $Y$. Then the inverse mapping $f^{-1}$ defined on $Y$ by

$$
f^{-1}(f(x))=x \quad(x \in X)
$$

is a continuous mapping of $Y$ onto $X$.

Proof Applying Theorem 4.8 to $f^{-1}$ in place of $f$, we see that it suffices to prove that $f(V)$ is an open set in $Y$ for every open set $V$ in $X$. Fix such a set $V$.

The complement $V^{c}$ of $V$ is closed in $X$, hence compact (Theorem 2.35); hence $f\left(V^{c}\right)$ is a compact subset of $Y$ (Theorem 4.14) and so is closed in $Y$ (Theorem 2.34). Since $f$ is one-to-one and onto, $f(V)$ is the complement of $f\left(V^{c}\right)$. Hence $f(V)$ is open.

4.18 Definition Let $f$ be a mapping of a metric space $X$ into a metric space $Y$. We say that $f$ is uniformly continuous on $X$ if for every $\varepsilon>0$ there exists $\delta>0$ such that

$$
d_{Y}(f(p), f(q))<\varepsilon
$$

for all $p$ and $q$ in $X$ for which $d_{\mathbf{X}}(p, q)<\delta$.

Let us consider the differences between the concepts of continuity and of uniform continuity. First, uniform continuity is a property of a function on a set, whereas continuity can be defined at a single point. To ask whether a given function is uniformly continuous at a certain point is meaningless. Second, if $f$ is continuous on $X$, then it is possible to find, for each $\varepsilon>0$ and for each point $p$ of $X$, a number $\delta>0$ having the property specified in Definition 4.5. This $\delta$ depends on $\varepsilon$ and on $p$. If $f$ is, however, uniformly continuous on $X$, then it is possible, for each $\varepsilon>0$, to find one number $\delta>0$ which will do for all points $p$ of $X$.

Evidently, every uniformly continuous function is continuous. That the two concepts are equivalent on compact sets follows from the next theorem.

4.19 Theorem Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f$ is uniformly continuous on $X$.

Proof Let $\varepsilon>0$ be given. Since $f$ is continuous, we can associate to each point $p \in X$ a positive number $\phi(p)$ such that

$$
q \in X, d_{X}(p, q)<\phi(p) \quad \text { implies } \quad d_{Y}(f(p), f(q))<\frac{\varepsilon}{2}
$$

Let $J(p)$ be the set of all $q \in X$ for which

We put

$$
d_{\mathbf{X}}(p, q)<\frac{1}{2} \phi(p)
$$

Since $p \in J(p)$, the collection of all sets $J(p)$ is an open cover of $X$; and since $X$ is compact, there is a finite set of points $p_{1}, \ldots, p_{n}$ in $X$, such that

$$
X \subset J\left(p_{1}\right) \cup \cdots \cup J\left(p_{n}\right)
$$

$$
\delta=\frac{1}{2} \min \left[\phi\left(p_{1}\right), \ldots, \phi\left(p_{n}\right)\right]
$$

Then $\delta>0$. (This is one point where the finiteness of the covering, inherent in the definition of compactness, is essential. The minimum of a finite set of positive numbers is positive, whereas the inf of an infinite set of positive numbers may very well be 0 .)

Now let $q$ and $p$ be points of $X$, such that $d_{x}(p, q)<\delta$. By (18), there is an integer $m, 1 \leq m \leq n$, such that $p \in J\left(p_{m}\right)$; hence

$$
d_{X}\left(p, p_{m}\right)<\frac{1}{2} \phi\left(p_{m}\right)
$$

and we also have

$$
d_{x}\left(q, p_{m}\right) \leq d_{x}(p, q)+d_{x}\left(p, p_{m}\right)<\delta+\frac{1}{2} \phi\left(p_{m}\right) \leq \phi\left(p_{m}\right)
$$

Finally, (16) shows that therefore

$$
d_{Y}(f(p), f(q)) \leq d_{Y}\left(f(p), f\left(p_{m}\right)\right)+d_{Y}\left(f(q), f\left(p_{m}\right)\right)<\varepsilon
$$

This completes the proof.

An alternative proof is sketched in Exercise 10.

We now proceed to show that compactness is essential in the hypotheses of Theorems 4.14, 4.15, 4.16, and 4.19.

4.20 Theorem Let $E$ be a noncompact set in $R^{1}$. Then

(a) there exists a continuous function on $E$ which is not bounded;

(b) there exists a continuous and bounded function on $E$ which has no maximum.

If, in addition, $E$ is bounded, then
(c) there exists a continuous function on $E$ which is not uniformly continuous.

Proof Suppose first that $E$ is bounded, so that there exists a limit point $x_{0}$ of $E$ which is not a point of $E$. Consider

$$
f(x)=\frac{1}{x-x_{0}} \quad(x \in E)
$$

This is continuous on $E$ (Theorem 4.9), but evidently unbounded. To see that (21) is not uniformly continuous, let $\varepsilon>0$ and $\delta>0$ be arbitrary, and choose a point $x \in E$ such that $\left|x-x_{0}\right|<\delta$. Taking $t$ close enough to $x_{0}$, we can then make the difference $|f(t)-f(x)|$ greater than $\varepsilon$, although $|t-x|<\delta$. Since this is true for every $\delta>0, f$ is not uniformly continuous on $E$.

The function $g$ given by

$$
g(x)=\frac{1}{1+\left(x-x_{0}\right)^{2}} \quad(x \in E)
$$

is continuous on $E$, and is bounded, since $0<g(x)<1$. It is clear that

$$
\sup _{x \in E} g(x)=1
$$

whereas $g(x)<1$ for all $x \in E$. Thus $g$ has no maximum on $E$.

Having proved the theorem for bounded sets $E$, let us now suppose that $E$ is unbounded. Then $f(x)=x$ establishes $(a)$, whereas

$$
h(x)=\frac{x^{2}}{1+x^{2}} \quad(x \in E)
$$

establishes $(b)$, since

$$
\sup _{x \in E} h(x)=1
$$

and $h(x)<1$ for all $x \in E$.

Assertion (c) would be false if boundedness were omitted from the hypotheses. For, let $E$ be the set of all integers. Then every function defined on $E$ is uniformly continuous on $E$. To see this, we need merely take $\delta<1$ in Definition 4.18.

We conclude this section by showing that compactness is also essential in Theorem 4.17.

4.21 Example Let $X$ be the half-open interval $[0,2 \pi)$ on the real line, and let $\mathrm{f}$ be the mapping of $X$ onto the circle $Y$ consisting of all points whose distance from the origin is 1 , given by

$$
\mathbf{f}(t)=(\cos t, \sin t) \quad(0 \leq t<2 \pi)
$$

The continuity of the trigonometric functions cosine and sine, as well as their periodicity properties, will be established in Chap. 8. These results show that $f$ is a continuous 1-1 mapping of $X$ onto $Y$.

However, the inverse mapping (which exists, since $f$ is one-to-one and onto) fails to be continuous at the point $(1,0)=\mathbf{f}(0)$. Of course, $X$ is not compact in this example. (It may be of interest to observe that $\mathbf{f}^{-1}$ fails to be continuous in spite of the fact that $Y$ is compact!)

## CONTINUITY AND CONNECTEDNESS

4.22 Theorem If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and if $E$ is a connected subset of $X$, then $f(E)$ is connected.

Proof Assume, on the contrary, that $f(E)=A \cup B$, where $A$ and $B$ are nonempty separated subsets of $Y$. Put $G=E \cap f^{-1}(A), H=E \cap f^{-1}(B)$.

Then $E=G \cup H$, and neither $G$ nor $H$ is empty.

Since $A \subset \bar{A}$ (the closure of $A$ ), we have $G \subset f^{-1}(\bar{A})$; the latter set is closed, since $f$ is continuous; hence $\bar{G} \subset f^{-1}(\bar{A})$. It follows that $f(\bar{G}) \subset \bar{A}$. Since $f(H)=B$ and $\bar{A} \cap B$ is empty, we conclude that $\bar{G} \cap H$ is empty.

The same argument shows that $G \cap \bar{H}$ is empty. Thus $G$ and $H$ are separated. This is impossible if $E$ is connected.

4.23 Theorem Let $f$ be a continuous real function on the interval $[a, b]$. If $f(a)<f(b)$ and if $c$ is a number such that $f(a)<c<f(b)$, then there exists a point $x \in(a, b)$ such that $f(x)=c$.

A similar result holds, of course, if $f(a)>f(b)$. Roughly speaking, the theorem says that a continuous real function assumes all intermediate values on an interval.

Proof By Theorem 2.47, $[a, b]$ is connected; hence Theorem 4.22 shows that $f([a, b])$ is a connected subset of $R^{1}$, and the assertion follows if we appeal once more to Theorem 2.47.

4.24 Remark At first glance, it might seem that Theorem 4.23 has a converse. That is, one might think that if for any two points $x_{1}<x_{2}$ and for any number $c$ between $f\left(x_{1}\right)$ and $f\left(x_{2}\right)$ there is a point $x$ in $\left(x_{1}, x_{2}\right)$ such that $f(x)=c$, then $f$ must be continuous.

That this is not so may be concluded from Example 4.27(d).

## DISCONTINUITIES

If $x$ is a point in the domain of definition of the function $f$ at which $f$ is not continuous, we say that $f$ is discontinuous at $x$, or that $f$ has a discontinuity at $x$. If $f$ is defined on an interval or on a segment, it is customary to divide discontinuities into two types. Before giving this classification, we have to define the right-hand and the left-hand limits of $f$ at $x$, which we denote by $f(x+)$ and $f(x-)$, respectively.

4.25 Definition Let $f$ be defined on $(a, b)$. Consider any point $x$ such that $a \leq x<b$. We write

$$
f(x+)=q
$$

if $f\left(t_{n}\right) \rightarrow q$ as $n \rightarrow \infty$, for all sequences $\left\{t_{n}\right\}$ in $(x, b)$ such that $t_{n} \rightarrow x$. To obtain the definition of $f(x-)$, for $a<x \leq b$, we restrict ourselves to sequences $\left\{t_{n}\right\}$ in $(a, x)$.

It is clear that any point $x$ of $(a, b), \lim _{t \rightarrow x} f(t)$ exists if and only if

$$
f(x+)=f(x-)=\lim _{t \rightarrow x} f(t)
$$

4.26 Definition Let $f$ be defined on $(a, b)$. If $f$ is discontinuous at a point $x$, and if $f(x+)$ and $f(x-)$ exist, then $f$ is said to have a discontinuity of the first $k i n d$, or a simple discontinuity, at $x$. Otherwise the discontinuity is said to be of the second kind.

There are two ways in which a function can have a simple discontinuity: either $f(x+) \neq f(x-)$ [in which case the value $f(x)$ is immaterial], or $f(x+)=$ $f(x-) \neq f(x)$.

### 4.27 Examples

(a) Define

$$
f(x)= \begin{cases}1 & (x \text { rational }) \\ 0 & (x \text { irrational })\end{cases}
$$

Then $f$ has a discontinuity of the second kind at every point $x$, since neither $f(x+)$ nor $f(x-)$ exists.

(b) Define

$$
f(x)= \begin{cases}x & (x \text { rational }) \\ 0 & (x \text { irrational })\end{cases}
$$

Then $f$ is continuous at $x=0$ and has a discontinuity of the second kind at every other point.

(c) Define

$$
f(x)= \begin{cases}x+2 & (-3<x<-2) \\ -x-2 & (-2 \leq x<0) \\ x+2 & (0 \leq x<1)\end{cases}
$$

Then $f$ has a simple discontinuity at $x=0$ and is continuous at every other point of $(-3,1)$.

(d) Define

$$
f(x)= \begin{cases}\sin \frac{1}{x} & (x \neq 0) \\ 0 & (x=0)\end{cases}
$$

Since neither $f(0+)$ nor $f(0-)$ exists, $f$ has a discontinuity of the second kind at $x=0$. We have not yet shown that $\sin x$ is a continuous function. If we assume this result for the moment, Theorem 4.7 implies that $f$ is continuous at every point $x \neq 0$.

## MONOTONIC FUNCTIONS

We shall now study those functions which never decrease (or never increase) on a given segment.

4.28 Definition Let $f$ be real on $(a, b)$. Then $f$ is said to be monotonically increasing on $(a, b)$ if $a<x<y<b$ implies $f(x) \leq f(y)$. If the last inequality is reversed, we obtain the definition of a monotonically decreasing function. The class of monotonic functions consists of both the increasing and the decreasing functions.

4.29 Theorem Let $f$ be monotonically increasing on $(a, b)$. Then $f(x+)$ and $f(x-)$ exist at every point of $x$ of $(a, b)$. More precisely,

$$
\sup _{a<t<x} f(t)=f(x-) \leq f(x) \leq f(x+)=\inf _{x<t<b} f(t) .
$$

Furthermore, if $a<x<y<b$, then

$$
f(x+) \leq f(y-) .
$$

Analogous results evidently hold for monotonically decreasing functions.

Proof By hypothesis, the set of numbers $f(t)$, where $a<t<x$, is bounded above by the number $f(x)$, and therefore has a least upper bound which we shall denote by $A$. Evidently $A \leq f(x)$. We have to show that $A=f(x-)$.

Let $\varepsilon>0$ be given. It follows from the definition of $A$ as a least upper bound that there exists $\delta>0$ such that $a<x-\delta<x$ and

Since $f$ is monotonic, we have

$$
f(x-\delta) \leq f(t) \leq A \quad(x-\delta<t<x)
$$

Combining (27) and (28), we see that

$$
|f(t)-A|<\varepsilon \quad(x-\delta<t<x) .
$$

Hence $f(x-)=A$.

The second half of (25) is proved in precisely the same way.

Next, if $a<x<y<b$, we see from (25) that

$$
f(x+)=\inf _{x<t<b} f(t)=\inf _{x<t<y} f(t) .
$$

The last equality is obtained by applying (25) to $(a, y)$ in place of $(a, b)$. Similarly,

$$
f(y-)=\sup _{a<t<y} f(t)=\sup _{x<t<y} f(t) .
$$

Comparison of (29) and (30) gives (26).

Corollary Monotonic functions have no discontinuities of the second kind.

This corollary implies that every monotonic function is discontinuous at a countable set of points at most. Instead of appealing to the general theorem whose proof is sketched in Exercise 17, we give here a simple proof which is applicable to monotonic functions.

4.30 Theorem Let $f$ be monotonic on $(a, b)$. Then the set of points of $(a, b)$ at which $f$ is discontinuous is at most countable.

Proof Suppose, for the sake of definiteness, that $f$ is increasing, and let $E$ be the set of points at which $f$ is discontinuous. that

With every point $x$ of $E$ we associate a rational number $r(x)$ such

$$
f(x-)<r(x)<f(x+) .
$$

Since $x_{1}<x_{2}$ implies $f\left(x_{1}+\right) \leq f\left(x_{2}-\right)$, we see that $r\left(x_{1}\right) \neq r\left(x_{2}\right)$ if $x_{1} \neq x_{2}$.

We have thus established a 1-1 correspondence between the set $E$ and a subset of the set of rational numbers. The latter, as we know, is countable.

4.31 Remark It should be noted that the discontinuities of a monotonic function need not be isolated. In fact, given any countable subset $E$ of $(a, b)$, which may even be dense, we can construct a function $f$, monotonic on $(a, b)$, discontinuous at every point of $E$, and at no other point of $(a, b)$.

To show this, let the points of $E$ be arranged in a sequence $\left\{x_{n}\right\}$, $n=1,2,3, \ldots$ Let $\left\{c_{n}\right\}$ be a sequence of positive numbers such that $\Sigma c_{n}$ converges. Define

$$
f(x)=\sum_{x_{n}<x} c_{n} \quad(a<x<b) .
$$

The summation is to be understood as follows: Sum over those indices $n$ for which $x_{n}<x$. If there are no points $x_{n}$ to the left of $x$, the sum is empty; following the usual convention, we define it to be zero. Since (31) converges absolutely, the order in which the terms are arranged is immaterial.

We leave the verification of the following properties of $f$ to the reader:

(a) $f$ is monotonically increasing on $(a, b)$;

(b) $f$ is discontinuous at every point of $E$; in fact,

$$
f\left(x_{n}+\right)-f\left(x_{n}-\right)=c_{n} .
$$

(c) $f$ is continuous at every other point of $(a, b)$.

Moreover, it is not hard to see that $f(x-)=f(x)$ at all points of $(a, b)$. If a function satisfies this condition, we say that $f$ is continuous from the left. If the summation in (31) were taken over all indices $n$ for which $x_{n} \leq x$, we would have $f(x+)=f(x)$ at every point of $(a, b)$; that is, $f$ would be continuous from the right.

Functions of this sort can also be defined by another method; for an example we refer to Theorem 6.16.

## INFINITE LIMITS AND LIMITS AT INFINITY

To enable us to operate in the extended real number system, we shall now enlarge the scope of Definition 4.1, by reformulating it in terms of neighborhoods.

For any real number $x$, we have already defined a neighborhood of $x$ to be any segment $(x-\delta, x+\delta)$.

4.32 Definition For any real $c$, the set of real numbers $x$ such that $x>c$ is called a neighborhood of $+\infty$ and is written $(c,+\infty)$. Similarly, the set $(-\infty, c)$ is a neighborhood of $-\infty$.

4.33 Definition Let $f$ be a real function defined on $E \subset R$. We say that

$$
f(t) \rightarrow A \text { as } t \rightarrow x
$$

where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E, t \neq x$.

A moment's consideration will show that this coincides with Definition 4.1 when $A$ and $x$ are real.

The analogue of Theorem 4.4 is still true, and the proof offers nothing new. We state it, for the sake of completeness.

### 4.34 Theorem Let $f$ and $g$ be defined on $E \subset R$. Suppose

Then

$$
f(t) \rightarrow A, \quad g(t) \rightarrow B \quad \text { as } t \rightarrow x
$$

(a) $f(t) \rightarrow A^{\prime}$ implies $A^{\prime}=A$.

(b) $(f+g)(t) \rightarrow A+B$

(c) $(f g)(t) \rightarrow A B$,

(d) $(f / g)(t) \rightarrow A / B$

provided the right members of $(b),(c)$, and $(d)$ are defined.

Note that $\infty-\infty, 0 \cdot \infty, \infty / \infty, A / 0$ are not defined (see Definition 1.23).

## EXERCISES

1. Suppose $f$ is a real function defined on $R^{1}$ which satisfies

$$
\lim _{h \rightarrow 0}[f(x+h)-f(x-h)]=0
$$

for every $x \in R^{1}$. Does this imply that $f$ is continuous?

2. If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, prove that

$$
f(E) \subset \overline{f(E)}
$$

for every set $E \subset X$. ( $E$ denotes the closure of $E$.) Show, by an example, that $f(E)$ can be a proper subset of $\overline{f(E)}$.

3. Let $f$ be a continuous real function on a metric space $X$. Let $Z(f)$ (the zero set of $f$ ) be the set of all $p \in X$ at which $f(p)=0$. Prove that $Z(f)$ is closed.
4. Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$,
and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p)=f(p)$ for all $p \in E$, prove that $g(p)=f(p)$ for all $p \in X$. (In other words, a continuous mapping is determined by its values on a dense subset of its domain.)
5. If $f$ is a real continuous function defined on a closed set $E \subset R^{1}$, prove that there exist continuous real functions $g$ on $R^{1}$ such that $g(x)=f(x)$ for all $x \in E$. (Such functions $g$ are called continuous extensions of $f$ from $E$ to $R^{1}$.) Show that the result becomes false if the word "closed" is omitted. Extend the result to vectorvalued functions. Hint: Let the graph of $g$ be a straight line on each of the segments which constitute the complement of $E$ (compare Exercise 29, Chap. 2). The result remains true if $R^{1}$ is replaced by any metric space, but the proof is not so simple.
6. If $f$ is defined on $E$, the graph of $f$ is the set of points $(x, f(x))$, for $x \in E$. In particular, if $E$ is a set of real numbers, and $f$ is real-valued, the graph of $f$ is a subset of the plane.

Suppose $E$ is compact, and prove that $f$ is continuous on $E$ if and only if its graph is compact.

7. If $E \subset X$ and if $f$ is a function defined on $X$, the restriction of $f$ to $E$ is the function $g$ whose domain of definition is $E$, such that $g(p)=f(p)$ for $p \in E$. Define $f$ and $g$ on $R^{2}$ by: $f(0,0)=g(0,0)=0, f(x, y)=x y^{2} /\left(x^{2}+y^{4}\right), g(x, y)=x y^{2} /\left(x^{2}+y^{6}\right)$ if $(x, y) \neq(0,0)$. Prove that $f$ is bounded on $R^{2}$, that $g$ is unbounded in every neighborhood of $(0,0)$, and that $f$ is not continuous at $(0,0)$; nevertheless, the restrictions of both $f$ and $g$ to every straight line in $R^{2}$ are continuous!
8. Let $f$ be a real uniformly continuous function on the bounded set $E$ in $R^{1}$. Prove that $f$ is bounded on $E$.

Show that the conclusion is false if boundedness of $E$ is omitted from the hypothesis.

9. Show that the requirement in the definition of uniform continuity can be rephrased as follows, in terms of diameters of sets: To every $\varepsilon>0$ there exists a $\delta>0$ such that $\operatorname{diam} f(E)<\varepsilon$ for all $E \subset X$ with $\operatorname{diam} E<\delta$.
10. Complete the details of the following alternative proof of Theorem 4.19: If $f$ is not uniformly continuous, then for some $\varepsilon>0$ there are sequences $\left\{p_{n}\right\},\left\{q_{n}\right\}$ in $X$ such that $d_{x}\left(p_{n}, q_{n}\right) \rightarrow 0$ but $d_{y}\left(f\left(p_{n}\right), f\left(q_{n}\right)\right)>\varepsilon$. Use Theorem 2.37 to obtain a contradiction.
11. Suppose $f$ is a uniformly continuous mapping of a metric space $X$ into a metric space $Y$ and prove that $\left\{f\left(x_{n}\right)\right\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence $\left\{x_{n}\right\}$ in $X$. Use this result to give an alternative proof of the theorem stated in Exercise 13.
12. A uniformly continuous function of a uniformly continuous function is uniformly continuous.

State this more precisely and prove it.

13. Let $E$ be a dense subset of a metric space $X$, and let $f$ be a uniformly continuous real function defined on $E$. Prove that $f$ has a continuous extension from $E$ to $X$
(see Exercise 5 for terminology). (Uniqueness follows from Exercise 4.) Hint: For each $p \in X$ and each positive integer $n$, let $V_{n}(p)$ be the set of all $q \in E$ with $d(p, q)<1 / n$. Use Exercise 9 to show that the intersection of the closures of the sets $f\left(V_{1}(p)\right), f\left(V_{2}(p)\right), \ldots$, consists of a single point, say $g(p)$, of $R^{1}$. Prove that the function $g$ so defined on $X$ is the desired extension of $f$.

Could the range space $R^{1}$ be replaced by $R^{k}$ ? By any compact metric space? By any complete metric space? By any metric space?

14. Let $I=[0,1]$ be the closed unit interval. Suppose $f$ is a continuous mapping of $I$ into $I$. Prove that $f(x)=x$ for at least one $x \in I$.
15. Call a mapping of $X$ into $Y$ open if $f(V)$ is an open set in $Y$ whenever $V$ is an open set in $X$.

Prove that every continuous open mapping of $R^{1}$ into $R^{1}$ is monotonic.

16. Let $[x]$ denote the largest integer contained in $x$, that is, $[x]$ is the integer such that $x-1<[x] \leq x$; and let $(x)=x-[x]$ denote the fractional part of $x$. What discontinuities do the functions $[x]$ and $(x)$ have?
17. Let $f$ be a real function defined on $(a, b)$. Prove that the set of points at which $f$ has a simple discontinuity is at most countable. Hint: Let $E$ be the set on which $f(x-)<f(x+)$. With each point $x$ of $E$, associate a triple $(p, q, r)$ of rational numbers such that

(a) $f(x-)<p<f(x+)$,

(b) $a<q<t<x$ implies $f(t)<p$,

(c) $x<t<r<b$ implies $f(t)>p$.

The set of all such triples is countable. Show that each triple is associated with at most one point of $E$. Deal similarly with the other possible types of simple discontinuities.

18. Every rational $x$ can be written in the form $x=m / n$, where $n>0$, and $m$ and $n$ are integers without any common divisors. When $x=0$, we take $n=1$. Consider the function $f$ defined on $R^{1}$ by

$$
f(x)= \begin{cases}0 & (x \text { irrational }) \\ \frac{1}{n} & \left(x=\frac{m}{n}\right)\end{cases}
$$

Prove that $f$ is continuous at every irrational point, and that $f$ has a simple discontinuity at every rational point.

19. Suppose $f$ is a real function with domain $R^{1}$ which has the intermediate value property: If $f(a)<c<f(b)$, then $f(x)=c$ for some $x$ between $a$ and $b$.

Suppose also, for every rational $r$, that the set of all $x$ with $f(x)=r$ is closed.

Prove that $f$ is continuous.

Hint: If $x_{n} \rightarrow x_{0}$ but $f\left(x_{n}\right)>r>f\left(x_{0}\right)$ for some $r$ and all $n$, then $f\left(t_{n}\right)=r$ for some $t_{n}$ between $x_{0}$ and $x_{n}$; thus $t_{n} \rightarrow x_{0}$. Find a contradiction. (N. J. Fine, Amer. Math. Monthly, vol. 73, 1966, p. 782.)

20. If $E$ is a nonempty subset of a metric space $X$, define the distance from $x \in X$ to $E$ by

$$
\rho_{E}(x)=\inf _{z \in E} d(x, z)
$$

(a) Prove that $\rho_{E}(x)=0$ if and only if $x \in E$.

(b) Prove that $\rho_{E}$ is a uniformly continuous function on $X$, by showing that

$$
\left|\rho_{E}(x)-\rho_{E}(y)\right| \leq d(x, y)
$$

for all $x \in X, y \in X$.

Hint: $\rho_{E}(x) \leq d(x, z) \leq d(x, y)+d(y, z)$, so that

$$
\rho_{E}(x) \leq d(x, y)+\rho_{E}(y)
$$

21. Suppose $K$ and $F$ are disjoint sets in a metric space $X, K$ is compact, $F$ is closed. Prove that there exists $\delta>0$ such that $d(p, q)>\delta$ if $p \in K, q \in F$. Hint: $\rho_{F}$ is a continuous positive function on $K$.

Show that the conclusion may fail for two disjoint closed sets if neither is compact.

22. Let $A$ and $B$ be disjoint nonempty closed sets in a metric space $X$, and define

$$
f(p)=\frac{\rho_{A}(p)}{\rho_{A}(p)+\rho_{B}(p)} \quad(p \in X)
$$

Show that $f$ is a continuous function on $X$ whose range lies in $[0,1]$, that $f(p)=0$ precisely on $A$ and $f(p)=1$ precisely on $B$. This establishes a converse of Exercise 3: Every closed set $A \subset X$ is $Z(f)$ for some continuous real $f$ on $X$. Setting

$$
V=f^{-1}\left(\left[0, \frac{1}{2}\right)\right), \quad W=f^{-1}\left(\left(\frac{1}{2}, 1\right]\right),
$$

show that $V$ and $W$ are open and disjoint, and that $A \subset V, B \subset W$. (Thus pairs of disjoint closed sets in a metric space can be covered by pairs of disjoint open sets. This property of metric spaces is called normality.)

23. A real-valued function $f$ defined in $(a, b)$ is said to be convex if

$$
f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)
$$

whenever $a<x<b, a<y<b, 0<\lambda<1$. Prove that every convex function is continuous. Prove that every increasing convex function of a convex function is convex. (For example, if $f$ is convex, so is $e^{f}$.)

If $f$ is convex in $(a, b)$ and if $a<s<t<u<b$, show that

$$
\frac{f(t)-f(s)}{t-s} \leq \frac{f(u)-f(s)}{u-s} \leq \frac{f(u)-f(t)}{u-t}
$$

24. Assume that $f$ is a continuous real function defined in $(a, b)$ such that

$$
f\left(\frac{x+y}{2}\right) \leq \frac{f(x)+f(y)}{2}
$$

for all $x, y \in(a, b)$. Prove that $f$ is convex.

25. If $A \subset R^{k}$ and $B \subset R^{k}$, define $A+B$ to be the set of all sums $\mathbf{x}+\mathbf{y}$ with $\mathbf{x} \in A$, $\mathbf{y} \in B$.

(a) If $K$ is compact and $C$ is closed in $R^{k}$, prove that $K+C$ is closed.

Hint: Take $\mathbf{z} \notin K+C$, put $F=\mathbf{z}-C$, the set of all $\mathbf{z}-\mathbf{y}$ with $\mathbf{y} \in C$. Then $K$ and $F$ are disjoint. Choose $\delta$ as in Exercise 21. Show that the open ball with center $\mathbf{z}$ and radius $\delta$ does not intersect $K+C$.

(b) Let $\alpha$ be an irrational real number. Let $C_{1}$ be the set of all integers, let $C_{2}$ be the set of all $n \alpha$ with $n \in C_{1}$. Show that $C_{1}$ and $C_{2}$ are closed subsets of $R^{1}$ whose sum $C_{1}+C_{2}$ is not closed, by showing that $C_{1}+C_{2}$ is a countable dense subset of $R^{1}$.

26. Suppose $X, Y, Z$ are metric spaces, and $Y$ is compact. Let $f$ map $X$ into $Y$, let $g$ be a continuous one-to-one mapping of $Y$ into $Z$, and put $h(x)=g(f(x))$ for $x \in X$.

Prove that $f$ is uniformly continuous if $h$ is uniformly continuous.

Hint: $g^{-1}$ has compact domain $g(Y)$, and $f(x)=g^{-1}(h(x))$.

Prove also that $f$ is continuous if $h$ is continuous.

Show (by modifying Example 4.21, or by finding a different example) that the compactness of $Y$ cannot be omitted from the hypotheses, even when $X$ and $Z$ are compact.

## DIFFERENTIATION

In this chapter we shall (except in the final section) confine our attention to real functions defined on intervals or segments. This is not just a matter of convenience, since genuine differences appear when we pass from real functions to vector-valued ones. Differentiation of functions defined on $R^{k}$ will be discussed in Chap. 9.

## THE DERIVATIVE OF A REAL FUNCTION

5.1 Definition Let $f$ be defined (and real-valued) on $[a, b]$. For any $x \in[a, b]$ form the quotient

$$
\phi(t)=\frac{f(t)-f(x)}{t-x} \quad(a<t<b, t \neq x)
$$

and define

$$
f^{\prime}(x)=\lim _{t \rightarrow x} \phi(t)
$$

provided this limit exists in accordance with Definition 4.1.

We thus associate with the function $f$ a function $f^{\prime}$ whose domain is the set of points $x$ at which the limit (2) exists; $f^{\prime}$ is called the derivative of $f$.

If $f^{\prime}$ is defined at a point $x$, we say that $f$ is differentiable at $x$. If $f^{\prime}$ is defined at every point of a set $E \subset[a, b]$, we say that $f$ is differentiable on $E$.

It is possible to consider right-hand and left-hand limits in (2); this leads to the definition of right-hand and left-hand derivatives. In particular, at the endpoints $a$ and $b$, the derivative, if it exists, is a right-hand or left-hand derivative, respectively. We shall not, however, discuss one-sided derivatives in any detail.

If $f$ is defined on a segment $(a, b)$ and if $a<x<b$, then $f^{\prime}(x)$ is defined by (1) and (2), as above. But $f^{\prime}(a)$ and $f^{\prime}(b)$ are not defined in this case.

5.2 Theorem Let $f$ be defined on $[a, b]$. If $f$ is differentiable at a point $x \in[a, b]$, then $f$ is continuous at $x$.

Proof As $t \rightarrow x$, we have, by Theorem 4.4,

$$
f(t)-f(x)=\frac{f(t)-f(x)}{t-x} \cdot(t-x) \rightarrow f^{\prime}(x) \cdot 0=0
$$

The converse of this theorem is not true. It is easy to construct continuous functions which fail to be differentiable at isolated points. In Chap. 7 we shall even become acquainted with a function which is continuous on the whole line without being differentiable at any point!

5.3 Theorem Suppose $f$ and $g$ are defined on $[a, b]$ and are differentiable at $a$ point $x \in[a, b]$. Then $f+g, f g$, and $f / g$ are differentiable at $x$, and

(a) $(f+g)^{\prime}(x)=f^{\prime}(x)+g^{\prime}(x)$;

(b) $(f g)^{\prime}(x)=f^{\prime}(x) g(x)+f(x) g^{\prime}(x)$;

(c) $\left(\frac{f}{g}\right)^{\prime}(x)=\frac{g(x) f^{\prime}(x)-g^{\prime}(x) f(x)}{g^{2}(x)}$.

In $(c)$, we assume of course that $g(x) \neq 0$.

Proof $(a)$ is clear, by Theorem 4.4. Let $h=f g$. Then

$$
h(t)-h(x)=f(t)[g(t)-g(x)]+g(x)[f(t)-f(x)]
$$

If we divide this by $t-x$ and note that $f(t) \rightarrow f(x)$ as $t \rightarrow x$ (Theorem 5.2), (b) follows. Next, let $h=f / g$. Then

$$
\frac{h(t)-h(x)}{t-x}=\frac{1}{g(t) g(x)}\left[g(x) \frac{f(t)-f(x)}{t-x}-f(x) \frac{g(t)-g(x)}{t-x}\right] \text {. }
$$

Letting $t \rightarrow x$, and applying Theorems 4.4 and 5.2, we obtain $(c)$.

5.4 Examples The derivative of any constant is clearly zero. If $f$ is defined by $f(x)=x$, then $f^{\prime}(x)=1$. Repeated application of $(b)$ and $(c)$ then shows that $x^{n}$ is differentiable, and that its derivative is $n x^{n-1}$, for any integer $n$ (if $n<0$, we have to restrict ourselves to $x \neq 0$ ). Thus every polynomial is differentiable, and so is every rational function, except at the points where the denominator is zero.

The following theorem is known as the "chain rule" for differentiation. It deals with differentiation of composite functions and is probably the most important theorem about derivatives. We shall meet more general versions of it in Chap. 9.

5.5 Theorem Suppose $f$ is continuous on $[a, b], f^{\prime}(x)$ exists at some point $x \in[a, b], g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the point $f(x)$. If

$$
h(t)=g(f(t)) \quad(a \leq t \leq b)
$$

then $h$ is differentiable at $x$, and

$$
h^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x)
$$

Proof Let $y=f(x)$. By the definition of the derivative, we have

$$
\begin{aligned}
f(t)-f(x) & =(t-x)\left[f^{\prime}(x)+u(t)\right], \\
g(s)-g(y) & =(s-y)\left[g^{\prime}(y)+v(s)\right],
\end{aligned}
$$

where $t \in[a, b], s \in I$, and $u(t) \rightarrow 0$ as $t \rightarrow x, v(s) \rightarrow 0$ as $s \rightarrow y$. Let $s=f(t)$.

Using first (5) and then (4), we obtain

or, if $t \neq x$,

$$
\begin{aligned}
h(t)-h(x) & =g(f(t))-g(f(x)) \\
& =[f(t)-f(x)] \cdot\left[g^{\prime}(y)+v(s)\right] \\
& =(t-x) \cdot\left[f^{\prime}(x)+u(t)\right] \cdot\left[g^{\prime}(y)+v(s)\right]
\end{aligned}
$$

$$
\frac{h(t)-h(x)}{t-x}=\left[g^{\prime}(y)+v(s)\right] \cdot\left[f^{\prime}(x)+u(t)\right]
$$

Letting $t \rightarrow x$, we see that $s \rightarrow y$, by the continuity of $f$, so that the right side of (6) tends to $g^{\prime}(y) f^{\prime}(x)$, which gives (3).

### 5.6 Examples

(a) Let $f$ be defined by

$$
f(x)= \begin{cases}x \sin \frac{1}{x} & (x \neq 0), \\ 0 & (x=0) .\end{cases}
$$

Taking for granted that the derivative of $\sin x$ is $\cos x$ (we shall discuss the trigonometric functions in Chap. 8), we can apply Theorems 5.3 and 5.5 whenever $x \neq 0$, and obtain

$$
f^{\prime}(x)=\sin \frac{1}{x}-\frac{1}{x} \cos \frac{1}{x} \quad(x \neq 0)
$$

At $x=0$, these theorems do not apply any longer, since $1 / x$ is not defined there, and we appeal directly to the definition: for $t \neq 0$,

$$
\frac{f(t)-f(0)}{t-0}=\sin \frac{1}{t}
$$

As $t \rightarrow 0$, this does not tend to any limit, so that $f^{\prime}(0)$ does not exist.

(b) Let $f$ be defined by

$$
f(x)= \begin{cases}x^{2} \sin \frac{1}{x} & (x \neq 0) \\ 0 & (x=0)\end{cases}
$$

As above, we obtain

$$
f^{\prime}(x)=2 x \sin \frac{1}{x}-\cos \frac{1}{x} \quad(x \neq 0)
$$

At $x=0$, we appeal to the definition, and obtain

$$
\left|\frac{f(t)-f(0)}{t-0}\right|=\left|t \sin \frac{1}{t}\right| \leq|t| \quad(t \neq 0)
$$

letting $t \rightarrow 0$, we see that

$$
f^{\prime}(0)=0
$$

Thus $f$ is differentiable at all points $x$, but $f^{\prime}$ is not a continuous function, since $\cos (1 / x)$ in (10) does not tend to a limit as $x \rightarrow 0$.

## MEAN VALUE THEOREMS

5.7 Definition Let $f$ be a real function defined on a metric space $X$. We say that $f$ has a local maximum at a point $p \in X$ if there exists $\delta>0$ such that $f(q) \leq$ $f(p)$ for all $q \in X$ with $d(p, q)<\delta$.

Local minima are defined likewise.

Our next theorem is the basis of many applications of differentiation.

5.8 Theorem Let $f$ be defined on $[a, b]$; if $f$ has a local maximum at a point $x \in(a, b)$, and if $f^{\prime}(x)$ exists, then $f^{\prime}(x)=0$.

The analogous statement for local minima is of course also true.

Proof Choose $\delta$ in accordance with Definition 5.7, so that

$$
a<x-\delta<x<x+\delta<b .
$$

If $x-\delta<t<x$, then

$$
\frac{f(t)-f(x)}{t-x} \geq 0
$$

Letting $t \rightarrow x$, we see that $f^{\prime}(x) \geq 0$.

$$
\text { If } x<t<x+\delta \text {, then }
$$

$$
\frac{f(t)-f(x)}{t-x} \leq 0
$$

which shows that $f^{\prime}(x) \leq 0$. Hence $f^{\prime}(x)=0$.

5.9 Theorem If $f$ and $g$ are continuous real functions on $[a, b]$ which are differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
[f(b)-f(a)] g^{\prime}(x)=[g(b)-g(a)] f^{\prime}(x)
$$

Note that differentiability is not required at the endpoints.

Proof Put

$$
h(t)=[f(b)-f(a)] g(t)-[g(b)-g(a)] f(t) \quad(a \leq t \leq b)
$$

Then $h$ is continuous on $[a, b], h$ is differentiable in $(a, b)$, and

$$
h(a)=f(b) g(a)-f(a) g(b)=h(b)
$$

To prove the theorem, we have to show that $h^{\prime}(x)=0$ for some $x \in(a, b)$.

If $h$ is constant, this holds for every $x \in(a, b)$. If $h(t)>h(a)$ for some $t \in(a, b)$, let $x$ be a point on $[a, b]$ at which $h$ attains its maximum

(Theorem 4.16). By (12), $x \in(a, b)$, and Theorem 5.8 shows that $h^{\prime}(x)=0$. If $h(t)<h(a)$ for some $t \in(a, b)$, the same argument applies if we choose for $x$ a point on $[a, b]$ where $h$ attains its minimum.

This theorem is often called a generalized mean value theorem; the following special case is usually referred to as "the" mean value theorem:

5.10 Theorem If $f$ is a real continuous function on $[a, b]$ which is differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
f(b)-f(a)=(b-a) f^{\prime}(x) .
$$

Proof Take $g(x)=x$ in Theorem 5.9.

### 5.11 Theorem Suppose $f$ is differentiable in $(a, b)$.

(a) If $f^{\prime}(x) \geq 0$ for all $x \in(a, b)$, then $f$ is monotonically increasing.

(b) If $f^{\prime}(x)=0$ for all $x \in(a, b)$, then $f$ is constant.

(c) If $f^{\prime}(x) \leq 0$ for all $x \in(a, b)$, then $f$ is monotonically decreasing.

Proof All conclusions can be read off from the equation

$$
f\left(x_{2}\right)-f\left(x_{1}\right)=\left(x_{2}-x_{1}\right) f^{\prime}(x)
$$

which is valid, for each pair of numbers $x_{1}, x_{2}$ in $(a, b)$, for some $x$ between $x_{1}$ and $x_{2}$.

## THE CONTINUITY OF DERIVATIVES

We have already seen [Example 5.6(b)] that a function $f$ may have a derivative $f^{\prime}$ which exists at every point, but is discontinuous at some point. However, not every function is a derivative. In particular, derivatives which exist at every point of an interval have one important property in common with functions which are continuous on an interval: Intermediate values are assumed (compare Theorem 4.23). The precise statement follows.

5.12 Theorem Suppose $f$ is a real differentiable function on $[a, b]$ and suppose $f^{\prime}(a)<\lambda<f^{\prime}(b)$. Then there is a point $x \in(a, b)$ such that $f^{\prime}(x)=\lambda$.

A similar result holds of course if $f^{\prime}(a)>f^{\prime}(b)$.

Proof Put $g(t)=f(t)-\lambda t$. Then $g^{\prime}(a)<0$, so that $g\left(t_{1}\right)<g(a)$ for some $t_{1} \in(a, b)$, and $g^{\prime}(b)>0$, so that $g\left(t_{2}\right)<g(b)$ for some $t_{2} \in(a, b)$. Hence $g$ attains its minimum on $[a, b]$ (Theorem 4.16) at some point $x$ such that $a<x<b$. By Theorem 5.8, $g^{\prime}(x)=0$. Hence $f^{\prime}(x)=\lambda$.

Corollary If $f$ is differentiable on $[a, b]$, then $f^{\prime}$ cannot have any simple discontinuities on $[a, b]$.

But $f^{\prime}$ may very well have discontinuities of the second kind.

## L'HOSPITAL'S RULE

The following theorem is frequently useful in the evaluation of limits.

5.13 Theorem Suppose $f$ and $g$ are real and differentiable in $(a, b)$, and $g^{\prime}(x) \neq 0$ for all $x \in(a, b)$, where $-\infty \leq a<b \leq+\infty$. Suppose

$$
\frac{f^{\prime}(x)}{g^{\prime}(x)} \rightarrow A \text { as } x \rightarrow a
$$

If

$$
f(x) \rightarrow 0 \text { and } g(x) \rightarrow 0 \text { as } x \rightarrow a \text {, }
$$

or if

$$
g(x) \rightarrow+\infty \text { as } x \rightarrow a \text {, }
$$

then

$$
\frac{f(x)}{g(x)} \rightarrow A \text { as } x \rightarrow a .
$$

The analogous statement is of course also true if $x \rightarrow b$, or if $g(x) \rightarrow-\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33.

Proof We first consider the case in which $-\infty \leq A<+\infty$. Choose a real number $q$ such that $A<q$, and then choose $r$ such that $A<r<q$. By (13) there is a point $c \in(a, b)$ such that $a<x<c$ implies

$$
\frac{f^{\prime}(x)}{g^{\prime}(x)}<r
$$

If $a<x<y<c$, then Theorem 5.9 shows that there is a point $t \in(x, y)$ such that

$$
\frac{f(x)-f(y)}{g(x)-g(y)}=\frac{f^{\prime}(t)}{g^{\prime}(t)}<r
$$

Suppose (14) holds. Letting $x \rightarrow a$ in (18), we see that

$$
\frac{f(y)}{g(y)} \leq r<q \quad(a<y<c)
$$

Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choose a point $c_{1} \in(a, y)$ such that $g(x)>g(y)$ and $g(x)>0$ if $a<x<c_{1}$. Multiplying (18) by $[g(x)-g(y)] / g(x)$, we obtain

$$
\frac{f(x)}{g(x)}<r-r \frac{g(y)}{g(x)}+\frac{f(y)}{g(x)} \quad\left(a<x<c_{1}\right)
$$

If we let $x \rightarrow a$ in (20), (15) shows that there is a point $c_{2} \in\left(a, c_{1}\right)$ such that

$$
\frac{f(x)}{g(x)}<q \quad\left(a<x<c_{2}\right)
$$

Summing up, (19) and (21) show that for any $q$, subject only to the condition $A<q$, there is a point $c_{2}$ such that $f(x) / g(x)<q$ if $a<x<c_{2}$.

In the same manner, if $-\infty<A \leq+\infty$, and $p$ is chosen so that $p<A$, we can find a point $c_{3}$ such that

$$
p<\frac{f(x)}{g(x)} \quad\left(a<x<c_{3}\right)
$$

and (16) follows from these two statements.

## DERIVATIVES OF HIGHER ORDER

5.14 Definition If $f$ has a derivative $f^{\prime}$ on an interval, and if $f^{\prime}$ is itself differentiable, we denote the derivative of $f^{\prime}$ by $f^{\prime \prime}$ and call $f^{\prime \prime}$ the second derivative of $f$. Continuing in this manner, we obtain functions

$$
f, f^{\prime}, f^{\prime \prime}, f^{(3)}, \ldots, f^{(n)}
$$

each of which is the derivative of the preceding one. $f^{(n)}$ is called the $n$th derivative, or the derivative of order $n$, of $f$.

In order for $f^{(n)}(x)$ to exist at a point $x, f^{(n-1)}(t)$ must exist in a neighborhood of $x$ (or in a one-sided neighborhood, if $x$ is an endpoint of the interval on which $f$ is defined), and $f^{(n-1)}$ must be differentiable at $x$. Since $f^{(n-1)}$ must exist in a neighborhood of $x, f^{(n-2)}$ must be differentiable in that neighborhood.

## TAYLOR'S THEOREM

5.15 Theorem Suppose $f$ is a real function on $[a, b], n$ is a positive integer, $f^{(n-1)}$ is continuous on $[a, b], f^{(n)}(t)$ exists for every $t \in(a, b)$. Let $\alpha, \beta$ be distinct points of $[a, b]$, and define

$$
P(t)=\sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k !}(t-\alpha)^{k}
$$

Then there exists a point $x$ between $\alpha$ and $\beta$ such that

$$
f(\beta)=P(\beta)+\frac{f^{(n)}(x)}{n !}(\beta-\alpha)^{n}
$$

For $n=1$, this is just the mean value theorem. In general, the theorem shows that $f$ can be approximated by a polynomial of degree $n-1$, and that (24) allows us to estimate the error, if we know bounds on $\left|f^{(n)}(x)\right|$.

Proof Let $M$ be the number defined by

$$
f(\beta)=P(\beta)+M(\beta-\alpha)^{n}
$$

and put

$$
g(t)=f(t)-P(t)-M(t-\alpha)^{n} \quad(a \leq t \leq b) .
$$

We have to show that $n ! M=f^{(n)}(x)$ for some $x$ between $\alpha$ and $\beta$. By (23) and (26),

$$
g^{(n)}(t)=f^{(n)}(t)-n ! M \quad(a<t<b)
$$

Hence the proof will be complete if we can show that $g^{(n)}(x)=0$ for some $x$ between $\alpha$ and $\beta$.

Since $P^{(k)}(\alpha)=f^{(k)}(\alpha)$ for $k=0, \ldots, n-1$, we have

$$
g(\alpha)=g^{\prime}(\alpha)=\cdots=g^{(n-1)}(\alpha)=0 .
$$

Our choice of $M$ shows that $g(\beta)=0$, so that $g^{\prime}\left(x_{1}\right)=0$ for some $x_{1}$ between $\alpha$ and $\beta$, by the mean value theorem. Since $g^{\prime}(\alpha)=0$, we conclude similarly that $g^{\prime \prime}\left(x_{2}\right)=0$ for some $x_{2}$ between $\alpha$ and $x_{1}$. After $n$ steps we arrive at the conclusion that $g^{(n)}\left(x_{n}\right)=0$ for some $x_{n}$ between $\alpha$ and $x_{n-1}$, that is, between $\alpha$ and $\beta$.

## DIFFERENTIATION OF VECTOR-VALUED FUNCTIONS

5.16 Remarks Definition 5.1 applies without any change to complex functions $f$ defined on $[a, b]$, and Theorems 5.2 and 5.3 , as well as their proofs, remain valid. If $f_{1}$ and $f_{2}$ are the real and imaginary parts of $f$, that is, if

$$
f(t)=f_{1}(t)+i f_{2}(t)
$$

for $a \leq t \leq b$, where $f_{1}(t)$ and $f_{2}(t)$ are real, then we clearly have

$$
f^{\prime}(x)=f_{1}^{\prime}(x)+i f_{2}^{\prime}(x)
$$

also, $f$ is differentiable at $x$ if and only if both $f_{1}$ and $f_{2}$ are differentiable at $x$.

Passing to vector-valued functions in general, i.e., to functions $\mathbf{f}$ which map $[a, b]$ into some $R^{k}$, we may still apply Definition 5.1 to define $f^{\prime}(x)$. The term $\phi(t)$ in (1) is now, for each $t$, a point in $R^{k}$, and the limit in (2) is taken with respect to the norm of $R^{k}$. In other words, $f^{\prime}(x)$ is that point of $R^{k}$ (if there is one) for which

$$
\lim _{t \rightarrow x}\left|\frac{\mathbf{f}(t)-\mathbf{f}(x)}{t-x}-\mathbf{f}^{\prime}(x)\right|=0,
$$

and $\mathbf{f}^{\prime}$ is again a function with values in $R^{k}$.

If $f_{1}, \ldots, f_{k}$ are the components of $f$, as defined in Theorem 4.10, then

$$
\mathbf{f}^{\prime}=\left(f_{1}^{\prime}, \ldots, f_{k}^{\prime}\right)
$$

and $\mathbf{f}$ is differentiable at a point $x$ if and only if each of the functions $f_{1}, \ldots, f_{k}$ is differentiable at $x$.

Theorem 5.2 is true in this context as well, and so is Theorem 5.3(a) and (b), if $f g$ is replaced by the inner product $\mathbf{f} \cdot \mathbf{g}$ (see Definition 4.3).

When we turn to the mean value theorem, however, and to one of its consequences, namely, L'Hospital's rule, the situation changes. The next two examples will show that each of these results fails to be true for complex-valued functions.

5.17 Example Define, for real $x$,

$$
f(x)=e^{i x}=\cos x+i \sin x
$$

(The last expression may be taken as the definition of the complex exponential $e^{i x}$; see Chap. 8 for a full discussion of these functions.) Then

$$
f(2 \pi)-f(0)=1-1=0,
$$

but

$$
f^{\prime}(x)=i e^{i x},
$$

so that $\left|f^{\prime}(x)\right|=1$ for all real $x$.

Thus Theorem 5.10 fails to hold in this case.

5.18 Example On the segment $(0,1)$, define $f(x)=x$ and

$$
g(x)=x+x^{2} e^{i / x^{2}} .
$$

Since $\left|e^{i t}\right|=1$ for all real $t$, we see that

$$
\lim _{x \rightarrow 0} \frac{f(x)}{g(x)}=1
$$

Next,

$$
g^{\prime}(x)=1+\left\{2 x-\frac{2 i}{x}\right\} e^{i / x^{2}} \quad(0<x<1)
$$

so that

$$
\left|g^{\prime}(x)\right| \geq\left|2 x-\frac{2 i}{x}\right|-1 \geq \frac{2}{x}-1
$$

Hence

and so

$$
\left|\frac{f^{\prime}(x)}{g^{\prime}(x)}\right|=\frac{1}{\left|g^{\prime}(x)\right|} \leq \frac{x}{2-x}
$$

$$
\lim _{x \rightarrow 0} \frac{f^{\prime}(x)}{g^{\prime}(x)}=0
$$

By (36) and (40), L'Hospital's rule fails in this case. Note also that $g^{\prime}(x) \neq 0$ on $(0,1)$, by $(38)$.

However, there is a consequence of the mean value theorem which, for purposes of applications, is almost as useful as Theorem 5.10, and which remains true for vector-valued functions: From Theorem 5.10 it follows that

$$
|f(b)-f(a)| \leq(b-a) \sup _{a<x<b}\left|f^{\prime}(x)\right| .
$$

5.19 Theorem Suppose $\mathbf{f}$ is a continuous mapping of $[a, b]$ into $R^{k}$ and $\mathbf{f}$ is differentiable in $(a, b)$. Then there exists $x \in(a, b)$ such that

$$
|\mathbf{f}(b)-\mathbf{f}(a)| \leq(b-a)\left|\mathbf{f}^{\prime}(x)\right|
$$

Proof $^{1} \quad$ Put $\mathbf{z}=\mathbf{f}(b)-\mathbf{f}(a)$, and define

$$
\varphi(t)=\mathbf{z} \cdot \mathbf{f}(t) \quad(a \leq t \leq b)
$$

Then $\varphi$ is a real-valued continuous function on $[a, b]$ which is differentiable in $(a, b)$. The mean value theorem shows therefore that

$$
\varphi(b)-\varphi(a)=(b-a) \varphi^{\prime}(x)=(b-a) \mathbf{z} \cdot \mathbf{f}^{\prime}(x)
$$

for some $x \in(a, b)$. On the other hand,

$$
\varphi(b)-\varphi(a)=\mathbf{z} \cdot \mathbf{f}(b)-\mathbf{z} \cdot \mathbf{f}(a)=\mathbf{z} \cdot \mathbf{z}=|\mathbf{z}|^{2}
$$

The Schwarz inequality now gives

$$
|\mathbf{z}|^{2}=(b-a)\left|\mathbf{z} \cdot \mathbf{f}^{\prime}(x)\right| \leq(b-a)|\mathbf{z}|\left|\mathbf{f}^{\prime}(x)\right|
$$

Hence $|\mathbf{z}| \leq(b-a)\left|\mathbf{f}^{\prime}(x)\right|$, which is the desired conclusion.

1 V. P. Havin translated the second edition of this book into Russian and added this proof to the original one.

## EXERCISES

1. Let $f$ be defined for all real $x$, and suppose that

$$
|f(x)-f(y)| \leq(x-y)^{2}
$$

for all real $x$ and $y$. Prove that $f$ is constant.

2. Suppose $f^{\prime}(x)>0$ in $(a, b)$. Prove that $f$ is strictly increasing in $(a, b)$, and let $g$ be its inverse function. Prove that $g$ is differentiable, and that

$$
g^{\prime}(f(x))=\frac{1}{f^{\prime}(x)} \quad(a<x<b)
$$

3. Suppose $g$ is a real function on $R^{1}$, with bounded derivative (say $\left|g^{\prime}\right| \leq M$ ). Fix $\varepsilon>0$, and define $f(x)=x+\varepsilon g(x)$. Prove that $f$ is one-to-one if $\varepsilon$ is small enough. (A set of admissible values of $\varepsilon$ can be determined which depends only on $M$.)
4. If

$$
C_{0}+\frac{C_{1}}{2}+\cdots+\frac{C_{n-1}}{n}+\frac{C_{n}}{n+1}=0
$$

where $C_{0}, \ldots, C_{n}$ are real constants, prove that the equation

$$
C_{0}+C_{1} x+\cdots+C_{n-1} x^{n-1}+C_{n} x^{n}=0
$$

has at least one real root between 0 and 1 .

5. Suppose $f$ is defined and differentiable for every $x>0$, and $f^{\prime}(x) \rightarrow 0$ as $x \rightarrow+\infty$.

Put $g(x)=f(x+1)-f(x)$. Prove that $g(x) \rightarrow 0$ as $x \rightarrow+\infty$.

6. Suppose

(a) $f$ is continuous for $x \geq 0$,

(b) $f^{\prime}(x)$ exists for $x>0$,

(c) $f(0)=0$,

(d) $f^{\prime}$ is monotonically increasing.

Put

$$
g(x)=\frac{f(x)}{x} \quad(x>0)
$$

and prove that $g$ is monotonically increasing.

7. Suppose $f^{\prime}(x), g^{\prime}(x)$ exist, $g^{\prime}(x) \neq 0$, and $f(x)=g(x)=0$. Prove that

$$
\lim _{t \rightarrow x} \frac{f(t)}{g(t)}=\frac{f^{\prime}(x)}{g^{\prime}(x)}
$$

(This holds also for complex functions.)

8. Suppose $f^{\prime}$ is continuous on $[a, b]$ and $\varepsilon>0$. Prove that there exists $\delta>0$ such that

$$
\left|\frac{f(t)-f(x)}{t-x}-f^{\prime}(x)\right|<\varepsilon
$$

whenever $0<|t-x|<\delta, a \leq x \leq b, a \leq t \leq b$. (This could be expressed by saying that $f$ is uniformly differentiable on $[a, b]$ if $f^{\prime}$ is continuous on $[a, b]$.) Does this hold for vector-valued functions too?

9. Let $f$ be a continuous real function on $R^{1}$, of which it is known that $f^{\prime}(x)$ exists for all $x \neq 0$ and that $f^{\prime}(x) \rightarrow 3$ as $x \rightarrow 0$. Does it follow that $f^{\prime}(0)$ exists?
10. Suppose $f$ and $g$ are complex differentiable functions on $(0,1), f(x) \rightarrow 0, g(x) \rightarrow 0$, $f^{\prime}(x) \rightarrow A, g^{\prime}(x) \rightarrow B$ as $x \rightarrow 0$, where $A$ and $B$ are complex numbers, $B \neq 0$. Prove that

$$
\lim _{x \rightarrow 0} \frac{f(x)}{g(x)}=\frac{A}{B}
$$

Compare with Example 5.18. Hint:

$$
\frac{f(x)}{g(x)}=\left\{\frac{f(x)}{x}-A\right\} \cdot \frac{x}{g(x)}+A \cdot \frac{x}{g(x)}
$$

Apply Theorem 5.13 to the real and imaginary parts of $f(x) / x$ and $g(x) / x$.

11. Suppose $f$ is defined in a neighborhood of $x$, and suppose $f^{\prime \prime}(x)$ exists. Show that

$$
\lim _{h \rightarrow 0} \frac{f(x+h)+f(x-h)-2 f(x)}{h^{2}}=f^{\prime \prime}(x) .
$$

Show by an example that the limit may exist even if $f^{\prime \prime}(x)$ does not.

Hint: Use Theorem 5.13.

12. If $f(x)=|x|^{3}$, compute $f^{\prime}(x), f^{\prime \prime}(x)$ for all real $x$, and show that $f^{(3)}(0)$ does not exist.
13. Suppose $a$ and $c$ are real numbers, $c>0$, and $f$ is defined on $[-1,1]$ by

$$
f(x)= \begin{cases}x^{a} \sin \left(|x|^{-c}\right) & (\text { if } x \neq 0) \\ 0 & \text { (if } x=0)\end{cases}
$$

Prove the following statements:

(a) $f$ is continuous if and only if $a>0$.

(b) $f^{\prime}(0)$ exists if and only if $a>1$.

(c) $f^{\prime}$ is bounded if and only if $a \geq 1+c$.

(d) $f^{\prime}$ is continuous if and only if $a>1+c$.

(e) $f^{\prime \prime}(0)$ exists if and only if $a>2+c$.

$(f) f^{\prime \prime}$ is bounded if and only if $a \geq 2+2 c$.

(g) $f^{\prime \prime}$ is continuous if and only if $a>2+2 c$.

14. Let $f$ be a differentiable real function defined in $(a, b)$. Prove that $f$ is convex if and only if $f^{\prime}$ is monotonically increasing. Assume next that $f^{\prime \prime}(x)$ exists for every $x \in(a, b)$, and prove that $f$ is convex if and only if $f^{\prime \prime}(x) \geq 0$ for all $x \in(a, b)$.
15. Suppose $a \in R^{1}, f$ is a twice-differentiable real function on $(a, \infty)$, and $M_{0}, M_{1}, M_{2}$ are the least upper bounds of $|f(x)|,\left|f^{\prime}(x)\right|,\left|f^{\prime \prime}(x)\right|$, respectively, on $(a, \infty)$. Prove that

$$
M_{1}^{2} \leq 4 M_{0} M_{2} .
$$

Hint: If $h>0$, Taylor's theorem shows that

$$
f^{\prime}(x)=\frac{1}{2 h}[f(x+2 h)-f(x)]-h f^{\prime \prime}(\xi)
$$

for some $\xi \in(x, x+2 h)$. Hence

$$
\left|f^{\prime}(x)\right| \leq h M_{2}+\frac{M_{0}}{h} \text {. }
$$

To show that $M_{1}^{2}=4 M_{0} M_{2}$ can actually happen, take $a=-1$, define

$$
f(x)= \begin{cases}2 x^{2}-1 & (-1<x<0) \\ \frac{x^{2}-1}{x^{2}+1} & (0 \leq x<\infty)\end{cases}
$$

and show that $M_{0}=1, M_{1}=4, M_{2}=4$.

Does $M_{1}^{2} \leq 4 M_{0} M_{2}$ hold for vector-valued functions too?

16. Suppose $f$ is twice-differentiable on $(0, \infty), f^{\prime \prime}$ is bounded on $(0, \infty)$, and $f(x) \rightarrow 0$ as $x \rightarrow \infty$. Prove that $f^{\prime}(x) \rightarrow 0$ as $x \rightarrow \infty$.

Hint: Let $a \rightarrow \infty$ in Exercise 15.

17. Suppose $f$ is a real, three times differentiable function on $[-1,1]$, such that

$$
f(-1)=0, \quad f(0)=0, \quad f(1)=1, \quad f^{\prime}(0)=0
$$

Prove that $f^{(3)}(x) \geq 3$ for some $x \in(-1,1)$.

Note that equality holds for $\frac{1}{2}\left(x^{3}+x^{2}\right)$.

Hint: Use Theorem 5.15, with $\alpha=0$ and $\beta= \pm 1$, to show that there exist $s \in(0,1)$ and $t \in(-1,0)$ such that

$$
f^{(3)}(s)+f^{(3)}(t)=6
$$

18. Suppose $f$ is a real function on $[a, b], n$ is a positive integer, and $f^{(n-1)}$ exists for every $t \in[a, b]$. Let $\alpha, \beta$, and $P$ be as in Taylor's theorem (5.15). Define

$$
Q(t)=\frac{f(t)-f(\beta)}{t-\beta}
$$

for $t \in[a, b], t \neq \beta$, differentiate

$$
f(t)-f(\beta)=(t-\beta) Q(t)
$$

$n-1$ times at $t=\alpha$, and derive the following version of Taylor's theorem:

$$
f(\beta)=P(\beta)+\frac{Q^{(n-1)}(\alpha)}{(n-1) !}(\beta-\alpha)^{n}
$$

19. Suppose $f$ is defined in $(-1,1)$ and $f^{\prime}(0)$ exists. Suppose $-1<\alpha_{n}<\beta_{n}<1$, $\alpha_{n} \rightarrow 0$, and $\beta_{n} \rightarrow 0$ as $n \rightarrow \infty$. Define the difference quotients

$$
D_{n}=\frac{f\left(\beta_{n}\right)-f\left(\alpha_{n}\right)}{\beta_{n}-\alpha_{n}}
$$

Prove the following statements:

(a) If $\alpha_{n}<0<\beta_{n}$, then $\lim D_{n}=f^{\prime}(0)$.

(b) If $0<\alpha_{n}<\beta_{n}$ and $\left\{\beta_{n} /\left(\beta_{n}-\alpha_{n}\right)\right\}$ is bounded, then $\lim D_{n}=f^{\prime}(0)$.

(c) If $f^{\prime}$ is continuous in $(-1,1)$, then $\lim D_{n}=f^{\prime}(0)$.

Give an example in which $f$ is differentiable in $(-1,1)$ (but $f^{\prime}$ is not continuous at 0 ) and in which $\alpha_{n}, \beta_{n}$ tend to 0 in such a way that $\lim D_{n}$ exists but is different from $f^{\prime}(0)$.

20. Formulate and prove an inequality which follows from Taylor's theorem and which remains valid for vector-valued functions.
21. Let $E$ be a closed subset of $R^{1}$. We saw in Exercise 22, Chap. 4, that there is a real continuous function $f$ on $R^{1}$ whose zero set is $E$. Is it possible, for each closed set $E$, to find such an $f$ which is differentiable on $R^{1}$, or one which is $n$ times differentiable, or even one which has derivatives of all orders on $R^{1}$ ?
22. Suppose $f$ is a real function on $(-\infty, \infty)$. Call $x$ a fixed point of $f$ if $f(x)=x$.

(a) If $f$ is differentiable and $f^{\prime}(t) \neq 1$ for every real $t$, prove that $f$ has at most one fixed point.

(b) Show that the function $f$ defined by

$$
f(t)=t+\left(1+e^{t}\right)^{-1}
$$

has no fixed point, although $0<f^{\prime}(t)<1$ for all real $t$.

(c) However, if there is a constant $A<1$ such that $\left|f^{\prime}(t)\right| \leq A$ for all real $t$, prove that a fixed point $x$ of $f$ exists, and that $x=\lim x_{n}$, where $x_{1}$ is an arbitrary real number and

$$
x_{n+1}=f\left(x_{n}\right)
$$

for $n=1,2,3, \ldots$.

(d) Show that the process described in (c) can be visualized by the zig-zag path

$$
\left(x_{1}, x_{2}\right) \rightarrow\left(x_{2}, x_{2}\right) \rightarrow\left(x_{2}, x_{3}\right) \rightarrow\left(x_{3}, x_{3}\right) \rightarrow\left(x_{3}, x_{4}\right) \rightarrow \cdots .
$$

23. The function $f$ defined by

$$
f(x)=\frac{x^{3}+1}{3}
$$

has three fixed points, say $\alpha, \beta, \gamma$, where

$$
-2<\alpha<-1, \quad 0<\beta<1, \quad 1<\gamma<2 .
$$

For arbitrarily chosen $x_{1}$, define $\left\{x_{n}\right\}$ by setting $x_{n+1}=f\left(x_{n}\right)$.

(a) If $x_{1}<\alpha$, prove that $x_{n} \rightarrow-\infty$ as $n \rightarrow \infty$.

(b) If $\alpha<x_{1}<\gamma$, prove that $x_{n} \rightarrow \beta$ as $n \rightarrow \infty$.

(c) If $\gamma<x_{1}$, prove that $x_{n} \rightarrow+\infty$ as $n \rightarrow \infty$.

Thus $\beta$ can be located by this method, but $\alpha$ and $\gamma$ cannot.

24. The process described in part $(c)$ of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$.

Fix some $\alpha>1$, and put

$$
f(x)=\frac{1}{2}\left(x+\frac{\alpha}{x}\right), \quad g(x)=\frac{\alpha+x}{1+x}
$$

Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$. Try to explain, on the basis of properties of $f$ and $g$, why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^{\prime}$ and $g^{\prime}$, draw the zig-zags suggested in Exercise 22.)

Do the same when $0<\alpha<1$.

25. Suppose $f$ is twice differentiable on $[a, b], f(a)<0, f(b)>0, f^{\prime}(x) \geq \delta>0$, and $0 \leq f^{\prime \prime}(x) \leq M$ for all $x \in[a, b]$. Let $\xi$ be the unique point in $(a, b)$ at which $f(\xi)=0$.

Complete the details in the following outline of Newton's method for computing $\xi$.

(a) Choose $x_{1} \in(\xi, b)$, and define $\left\{x_{n}\right\}$ by

$$
x_{n+1}=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)} \text {. }
$$

Interpret this geometrically, in terms of a tangent to the graph of $f$.

(b) Prove that $x_{n+1}<x_{n}$ and that

$$
\lim _{n \rightarrow \infty} x_{n}=\xi .
$$

(c) Use Taylor's theorem to show that

$$
x_{n+1}-\xi=\frac{f^{\prime \prime}\left(t_{n}\right)}{2 f^{\prime}\left(x_{n}\right)}\left(x_{n}-\xi\right)^{2}
$$

for some $t_{n} \in\left(\xi, x_{n}\right)$.

(d) If $A=M / 2 \delta$, deduce that

$$
0 \leq x_{n+1}-\xi \leq \frac{1}{A}\left[A\left(x_{1}-\xi\right)\right]^{2 n}
$$

(Compare with Exercises 16 and 18, Chap. 3.)

(e) Show that Newton's method amounts to finding a fixed point of the function $g$ defined by

$$
g(x)=x-\frac{f(x)}{f^{\prime}(x)} \text {. }
$$

How does $g^{\prime}(x)$ behave for $x$ near $\xi$ ?

( $f$ ) Put $f(x)=x^{1 / 3}$ on $(-\infty, \infty)$ and try Newton's method. What happens?

26. Suppose $f$ is differentiable on $[a, b], f(a)=0$, and there is a real number $A$ such that $\left|f^{\prime}(x)\right| \leq A|f(x)|$ on $[a, b]$. Prove that $f(x)=0$ for all $x \in[a, b]$. Hint: Fix $x_{0} \in[a, b]$, let

$$
M_{0}=\sup |f(x)|, \quad M_{1}=\sup \left|f^{\prime}(x)\right|
$$

for $a \leq x \leq x_{0}$. For any such $x$,

$$
|f(x)| \leq M_{1}\left(x_{0}-a\right) \leq A\left(x_{0}-a\right) M_{0} .
$$

Hence $M_{0}=0$ if $A\left(x_{0}-a\right)<1$. That is, $f=0$ on $\left[a, x_{0}\right]$. Proceed.

27. Let $\phi$ be a real function defined on a rectangle $R$ in the plane, given by $a \leq x \leq b$, $\alpha \leq y \leq \beta$. A solution of the initial-value problem

$$
y^{\prime}=\phi(x, y), \quad y(a)=c \quad(\alpha \leq c \leq \beta)
$$

is, by definition, a differentiable function $f$ on $[a, b]$ such that $f(a)=c, \alpha \leq f(x) \leq \beta$, and

$$
f^{\prime}(x)=\phi(x, f(x)) \quad(a \leq x \leq b)
$$

Prove that such a problem has at most one solution if there is a constant $A$ such that

$$
\left|\phi\left(x, y_{2}\right)-\phi\left(x, y_{1}\right)\right| \leq A\left|y_{2}-y_{1}\right|
$$

whenever $\left(x, y_{1}\right) \in R$ and $\left(x, y_{2}\right) \in R$.

Hint: Apply Exercise 26 to the difference of two solutions. Note that this uniqueness theorem does not hold for the initial-value problem

$$
y^{\prime}=y^{1 / 2}, \quad y(0)=0
$$

which has two solutions: $f(x)=0$ and $f(x)=x^{2} / 4$. Find all other solutions.

28. Formulate and prove an analogous uniqueness theorem for systems of differential equations of the form

$$
y_{\jmath}^{\prime}=\phi_{\jmath}\left(x, y_{1}, \ldots, y_{k}\right), \quad y_{\jmath}(a)=c_{J} \quad(j=1, \ldots, k)
$$

Note that this can be rewritten in the form

$$
\mathbf{y}^{\prime}=\boldsymbol{\phi}(x, \mathbf{y}), \quad \mathbf{y}(a)=\mathbf{c}
$$

where $\mathbf{y}=\left(y_{1}, \ldots, y_{k}\right)$ ranges over a $k$-cell, $\boldsymbol{\phi}$ is the mapping of a $(k+1)$-cell into the Euclidean $k$-space whose components are the functions $\phi_{1}, \ldots, \phi_{k}$, and $\mathbf{c}$ is the vector $\left(c_{1}, \ldots, c_{k}\right)$. Use Exercise 26 , for vector-valued functions.

29. Specialize Exercise 28 by considering the system

$$
\begin{aligned}
& y_{j}^{\prime}=y_{\jmath+1} \quad(j=1, \ldots, k-1) \\
& y_{k}^{\prime}=f(x)-\sum_{j=1}^{k} g_{\jmath}(x) y_{\jmath}
\end{aligned}
$$

where $f, g_{1}, \ldots, g_{k}$ are continuous real functions on $[a, b]$, and derive a uniqueness theorem for solutions of the equation

$$
y^{(k)}+g_{k}(x) y^{(k-1)}+\cdots+g_{2}(x) y^{\prime}+g_{1}(x) y=f(x)
$$

subject to initial conditions

$$
y(a)=c_{1}, \quad y^{\prime}(a)=c_{2}, \quad \ldots, \quad y^{(k-1)}(a)=c_{k}
$$

## 6

## THE RIEMANN-STIELTJES INTEGRAL

The present chapter is based on a definition of the Riemann integral which depends very explicitly on the order structure of the real line. Accordingly, we begin by discussing integration of real-valued functions on intervals. Extensions to complex- and vector-valued functions on intervals follow in later sections. Integration over sets other than intervals is discussed in Chaps. 10 and 11 .

## DEFINITION AND EXISTENCE OF THE INTEGRAL

6.1 Definition Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_{0}, x_{1}, \ldots, x_{n}$, where

$$
a=x_{0} \leq x_{1} \leq \cdots \leq x_{n-1} \leq x_{n}=b .
$$

We write

$$
\Delta x_{i}=x_{i}-x_{i-1} \quad(i=1, \ldots, n)
$$

Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put

$$
\begin{array}{rlrl}
M_{i} & =\sup f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
m_{i} & =\inf f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
U(P, f) & =\sum_{i=1}^{n} M_{i} \Delta x_{i}, & \\
L(P, f) & =\sum_{i=1}^{n} m_{i} \Delta x_{i}, &
\end{array}
$$

and finally

$$
\begin{aligned}
& \int_{a}^{b} f d x=\inf U(P, f), \\
& \int_{a}^{b} f d x=\sup L(P, f),
\end{aligned}
$$

where the inf and the sup are taken over all partitions $P$ of $[a, b]$. The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively.

If the upper and lower integrals are equal, we say that $f$ is Riemannintegrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemannintegrable functions), and we denote the common value of (1) and (2) by

$$
\int_{a}^{b} f d x
$$

or by

$$
\int_{a}^{b} f(x) d x .
$$

This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that

$$
m \leq f(x) \leq M \quad(a \leq x \leq b)
$$

Hence, for every $P$,

$$
m(b-a) \leq L(P, f) \leq U(P, f) \leq M(b-a)
$$

so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. The question of their equality, and hence the question of the integrability of $f$, is a more delicate one. Instead of investigating it separately for the Riemann integral, we shall immediately consider a more general situation.

6.2 Definition Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b])$. Corresponding to each partition $P$ of $[a, b]$, we write

$$
\Delta \alpha_{i}=\alpha\left(x_{i}\right)-\alpha\left(x_{i-1}\right)
$$

It is clear that $\Delta \alpha_{i} \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put

$$
\begin{aligned}
& U(P, f, \alpha)=\sum_{i=1}^{n} M_{i} \Delta \alpha_{i} \\
& L(P, f, \alpha)=\sum_{i=1}^{n} m_{i} \Delta \alpha_{i}
\end{aligned}
$$

where $M_{i}, m_{i}$ have the same meaning as in Definition 6.1, and we define

$$
\begin{aligned}
& \int_{a}^{b} f d \alpha=\inf U(P, f, \alpha) \\
& \int_{a}^{b} f d \alpha=\sup L(P, f, \alpha)
\end{aligned}
$$

the inf and sup again being taken over all partitions.

If the left members of (5) and (6) are equal, we denote their common value by

$$
\int_{a}^{b} f d \alpha
$$

or sometimes by

$$
\int_{a}^{b} f(x) d \alpha(x)
$$

This is the Riemann-Stieltjes integral (or simply the Stieltjes integral) of $f$ with respect to $\alpha$, over $[a, b]$.

If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$.

By taking $\alpha(x)=x$, the Riemann integral is seen to be a special case of the Riemann-Stieltjes integral. Let us mention explicitly, however, that in the general case $\alpha$ need not even be continuous.

A few words should be said about the notation. We prefer (7) to (8), since the letter $x$ which appears in (8) adds nothing to the content of (7). It is immaterial which letter we use to represent the so-called "variable of integration." For instance, (8) is the same as

$$
\int_{a}^{b} f(y) d \alpha(y)
$$

The integral depends on $f, \alpha, a$ and $b$, but not on the variable of integration, which may as well be omitted.

The role played by the variable of integration is quite analogous to that of the index of summation: The two symbols

$$
\sum_{i=1}^{n} c_{i}, \quad \sum_{k=1}^{n} c_{k}
$$

mean the same thing, since each means $c_{1}+c_{2}+\cdots+c_{n}$.

Of course, no harm is done by inserting the variable of integration, and in many cases it is actually convenient to do so.

We shall now investigate the existence of the integral (7). Without saying so every time, $f$ will be assumed real and bounded, and $\alpha$ monotonically increasing on $[a, b]$; and, when there can be no misunderstanding, we shall write $\int$ in place of $\int_{a}^{b}$.

6.3 Definition We say that the partition $P^{*}$ is a refinement of $P$ if $P^{*} \supset P$ (that is, if every point of $P$ is a point of $P^{*}$ ). Given two partitions, $P_{1}$ and $P_{2}$, we say that $P^{*}$ is their common refinement if $P^{*}=P_{1} \cup P_{2}$.

6.4 Theorem If $P^{*}$ is a refinement of $P$, then

and

$$
L(P, f, \alpha) \leq L\left(P^{*}, f, \alpha\right)
$$

$$
U\left(P^{*}, f, \alpha\right) \leq U(P, f, \alpha) .
$$

Proof To prove (9), suppose first that $P^{*}$ contains just one point more than $P$. Let this extra point be $x^{*}$, and suppose $x_{i-1}<x^{*}<x_{i}$, where $x_{i-1}$ and $x_{i}$ are two consecutive points of $P$. Put

$$
\begin{array}{ll}
w_{1}=\inf f(x) & \left(x_{i-1} \leq x \leq x^{*}\right) \\
w_{2}=\inf f(x) & \left(x^{*} \leq x \leq x_{i}\right)
\end{array}
$$

Clearly $w_{1} \geq m_{i}$ and $w_{2} \geq m_{i}$, where, as before,

Hence

$$
m_{i}=\inf f(x) \quad\left(x_{i-1} \leq x \leq x_{i}\right)
$$

$$
\begin{aligned}
& L\left(P^{*}, f, \alpha\right)-L(P, f, \alpha) \\
& \quad=w_{1}\left[\alpha\left(x^{*}\right)-\alpha\left(x_{i-1}\right)\right]+w_{2}\left[\alpha\left(x_{i}\right)-\alpha\left(x^{*}\right)\right]-m_{i}\left[\alpha\left(x_{i}\right)-\alpha\left(x_{i-1}\right)\right] \\
& \quad=\left(w_{1}-m_{i}\right)\left[\alpha\left(x^{*}\right)-\alpha\left(x_{i-1}\right)\right]+\left(w_{2}-m_{i}\right)\left[\alpha\left(x_{i}\right)-\alpha\left(x^{*}\right)\right] \geq 0 .
\end{aligned}
$$

If $P^{*}$ contains $k$ points more than $P$, we repeat this reasoning $k$ times, and arrive at (9). The proof of (10) is analogous.

6.5 Theorem $\int_{a}^{b} f d \alpha \leq \int_{a}^{b} f d \alpha$.

Proof Let $P^{*}$ be the common refinement of two partitions $P_{1}$ and $P_{2}$. By Theorem 6.4,

$$
L\left(P_{1}, f, \alpha\right) \leq L\left(P^{*}, f . \alpha\right) \leq U\left(P^{*}, f, \alpha\right) \leq U\left(P_{2}, f, \alpha\right)
$$

Hence

$$
L\left(P_{1}, f, \alpha\right) \leq U\left(P_{2}, f, \alpha\right)
$$

If $P_{2}$ is fixed and the sup is taken over all $P_{1}$, (11) gives

$$
\int_{\underline{1}} f d \alpha \leq U\left(P_{2}, f, \alpha\right) .
$$

The theorem follows by taking the inf over all $P_{2}$ in (12).

6.6 Theorem $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$

Proof For every $P$ we have

$$
L(P, f, \alpha) \leq \int_{-} f d \alpha \leq \bar{\int} f d \alpha \leq U(P, f, \alpha)
$$

Thus (13) implies

$$
0 \leq \bar{\int} f d \alpha-\int f d \alpha<\varepsilon .
$$

Hence, if (13) can be satisfied for every $\varepsilon>0$, we have

$$
\bar{\int} f d \alpha=\int f d \alpha
$$

that is, $f \in \mathscr{R}(\alpha)$.

Conversely, suppose $f \in \mathscr{R}(\alpha)$, and let $\varepsilon>0$ be given. Then there exist partitions $P_{1}$ and $P_{2}$ such that

$$
\begin{aligned}
& U\left(P_{2}, f, \alpha\right)-\int f d \alpha<\frac{\varepsilon}{2} \\
& \int f d \alpha-L\left(P_{1}, f, \alpha\right)<\frac{\varepsilon}{2}
\end{aligned}
$$

We choose $P$ to be the common refinement of $P_{1}$ and $P_{2}$. Then Theorem 6.4 , together with (14) and (15), shows that

$$
U(P, f, \alpha) \leq U\left(P_{2}, f, \alpha\right)<\int f d \alpha+\frac{\varepsilon}{2}<L\left(P_{1}, f, \alpha\right)+\varepsilon \leq L(P, f, \alpha)+\varepsilon
$$

so that (13) holds for this partition $P$.

Theorem 6.6 furnishes a convenient criterion for integrability. Before we apply it, we state some closely related facts.

### 6.7 Theorem

(a) If (13) holds for some $P$ and some $\varepsilon$, then (13) holds (with the same $\varepsilon$ ) for every refinement of $P$.

(b) If (13) holds for $P=\left\{x_{0}, \ldots, x_{n}\right\}$ and if $s_{i}, t_{i}$ are arbitrary points in $\left[x_{i-1}, x_{i}\right]$, then

$$
\sum_{i=1}^{n}\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \Delta \alpha_{i}<\varepsilon
$$

(c) If $f \in \mathscr{R}(\alpha)$ and the hypotheses of (b) hold, then

$$
\left|\sum_{i=1}^{n} f\left(t_{i}\right) \Delta \alpha_{i}-\int_{a}^{b} f d \alpha\right|<\varepsilon
$$

Proof Theorem 6.4 implies $(a)$. Under the assumptions made in $(b)$, both $f\left(s_{i}\right)$ and $f\left(t_{i}\right)$ lie in $\left[m_{i}, M_{i}\right]$, so that $\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \leq M_{i}-m_{i}$. Thus

$$
\sum_{i=1}^{n}\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \Delta \alpha_{i} \leq U(P, f, \alpha)-L(P, f, \alpha)
$$

which proves $(b)$. The obvious inequalities

and

$$
L(P, f, \alpha) \leq \sum f\left(t_{i}\right) \Delta \alpha_{i} \leq U(P, f, \alpha)
$$

prove $(c)$.

$$
L(P, f, \alpha) \leq \int f d \alpha \leq U(P, f, \alpha)
$$

6.8 Theorem If $f$ is continuous on $[a, b]$ then $f \in \mathscr{R}(\alpha)$ on $[a, b]$.

Proof Let $\varepsilon>0$ be given. Choose $\eta>0$ so that

$$
[\alpha(b)-\alpha(a)] \eta<\varepsilon
$$

Since $f$ is uniformly continuous on $[a, b]$ (Theorem 4.19), there exists a $\delta>0$ such that

$$
|f(x)-f(t)|<\eta
$$

if $x \in[a, b], t \in[a, b]$, and $|x-t|<\delta$.

If $P$ is any partition of $[a, b]$ such that $\Delta x_{i}<\delta$ for all $i$, then (16) implies that

$$
M_{i}-m_{i} \leq \eta \quad(i-1, \ldots, n)
$$

and therefore

$$
\begin{aligned}
U(P, f, \alpha)-L(P, f, \alpha) & =\sum_{i=1}^{n}\left(M_{i}-m_{i}\right) \Delta \alpha_{i} \\
\leq \eta \sum_{i=1}^{n} \Delta \alpha_{i} & =\eta[\alpha(b)-\alpha(a)]<\varepsilon
\end{aligned}
$$

By Theorem 6.6, $f \in \mathscr{R}(\alpha)$.

6.9 Theorem If $f$ is monotonic on $[a, b]$, and if $\alpha$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$. (We still assume, of course, that $\alpha$ is monotonic.)

Proof Let $\varepsilon>0$ be given. For any positive integer $n$, choose a partition such that

$$
\Delta \alpha_{i}=\frac{\alpha(b)-\alpha(a)}{n} \quad(i=1, \ldots, n)
$$

This is possible since $\alpha$ is continuous (Theorem 4.23).

We suppose that $f$ is monotonically increasing (the proof is analogous in the other case). Then

so that

$$
M_{i}=f\left(x_{i}\right), \quad m_{i}=f\left(x_{i-1}\right) \quad(i=1, \ldots, n)
$$

$$
\begin{aligned}
U(P, f, \alpha)-L(P, f, \alpha) & =\frac{\alpha(b)-\alpha(a)}{n} \sum_{i=1}^{n}\left[f\left(x_{i}\right)-f\left(x_{i-1}\right)\right] \\
& =\frac{\alpha(b)-\alpha(a)}{n} \cdot[f(b)-f(a)]<\varepsilon
\end{aligned}
$$

if $n$ is taken large enough. By Theorem 6.6, $f \in \mathscr{R}(\alpha)$.

6.10 Theorem Suppose $f$ is bounded on $[a, b], f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$.

Proof Let $\varepsilon>0$ be given. Put $M=\sup |f(x)|$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[u_{j}, v_{j}\right] \subset$ $[a, b]$ such that the sum of the corresponding differences $\alpha\left(v_{j}\right)-\alpha\left(u_{j}\right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap(a, b)$ lies in the interior of some $\left[u_{j}, v_{j}\right]$.

Remove the segments $\left(u_{j}, v_{j}\right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta>0$ such that $|f(s)-f(t)|<\varepsilon$ if $s \in K, t \in K,|s-t|<\delta$.

Now form a partition $P=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$ of $[a, b]$, as follows: Each $u_{j}$ occurs in $P$. Each $v_{j}$ occurs in $P$. No point of any segment $\left(u_{j}, v_{j}\right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_{j}$, then $\Delta x_{i}<\delta$.

Note that $M_{i}-m_{i} \leq 2 M$ for every $i$, and that $M_{i}-m_{i} \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_{j}$. Hence, as in the proof of Theorem 6.8,

$$
U(P, f, \alpha)-L(P, f, \alpha) \leq[\alpha(b)-\alpha(a)] \varepsilon+2 M \varepsilon
$$

Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$.

Note: If $f$ and $\alpha$ have a common point of discontinuity, then $f$ need not be in $\mathscr{R}(\alpha)$. Exercise 3 shows this.

6.11 Theorem Suppose $f \in \mathscr{R}(\alpha)$ on $[a, b], m \leq f \leq M, \phi$ is continuous on $[m, M]$, and $h(x)=\phi(f(x))$ on $[a, b]$. Then $h \in \mathscr{R}(\alpha)$ on $[a, b]$.

Proof Choose $\varepsilon>0$. Since $\phi$ is uniformly continuous on $[m, M]$, there exists $\delta>0$ such that $\delta<\varepsilon$ and $|\phi(s)-\phi(t)|<\varepsilon$ if $|s-t| \leq \delta$ and $s, t \in[m, M]$.

Since $f \in \mathscr{R}(\alpha)$, there is a partition $P=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$ of $[a, b]$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\delta^{2}
$$

Let $M_{i}, m_{i}$ have the same meaning as in Definition 6.1, and let $M_{i}^{*}, m_{i}^{*}$ be the analogous numbers for $h$. Divide the numbers $1, \ldots, n$ into two classes: $i \in A$ if $M_{i}-m_{i}<\delta, i \in B$ if $M_{i}-m_{i} \geq \delta$.

For $i \in A$, our choice of $\delta$ shows that $M_{i}^{*}-m_{i}^{*} \leq \varepsilon$.

For $i \in B, M_{i}^{*}-m_{i}^{*} \leq 2 K$, where $K=\sup |\phi(t)|, m \leq t \leq M$. By (18), we have

$$
\delta \sum_{i \in B} \Delta \alpha_{i} \leq \sum_{i \in B}\left(M_{i}-m_{i}\right) \Delta \alpha_{i}<\delta^{2}
$$

so that $\sum_{i \in B} \Delta \alpha_{i}<\delta$. It follows that

$$
\begin{aligned}
U(P, h, \alpha)-L(P, h, \alpha) & =\sum_{i \in A}\left(M_{i}^{*}-m_{i}^{*}\right) \Delta \alpha_{i}+\sum_{i \in B}\left(M_{i}^{*}-m_{i}^{*}\right) \Delta \alpha_{i} \\
& \leq \varepsilon[\alpha(b)-\alpha(a)]+2 K \delta<\varepsilon[\alpha(b)-\alpha(a)+2 K]
\end{aligned}
$$

Since $\varepsilon$ was arbitrary, Theorem 6.6 implies that $h \in \mathscr{R}(\alpha)$.

Remark: This theorem suggests the question: Just what functions are Riemann-integrable? The answer is given by Theorem $11.33(b)$.

## PROPERTIES OF THE INTEGRAL

### 6.12 Theorem

(a) If $f_{1} \in \mathscr{R}(\alpha)$ and $f_{2} \in \mathscr{R}(\alpha)$ on $[a, b]$, then

$$
f_{1}+f_{2} \in \mathscr{R}(\alpha)
$$

$c f \in \mathscr{R}(\alpha)$ for every constant $c$, and

$$
\begin{gathered}
\int_{a}^{b}\left(f_{1}+f_{2}\right) d \alpha=\int_{a}^{b} f_{1} d \alpha+\int_{a}^{b} f_{2} d \alpha \\
\int_{a}^{b} c f d \alpha=c \int_{a}^{b} f d \alpha .
\end{gathered}
$$

(b) If $f_{1}(x) \leq f_{2}(x)$ on $[a, b]$, then

$$
\int_{a}^{b} f_{1} d \alpha \leq \int_{a}^{b} f_{2} d \alpha .
$$

(c) If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c<b$, then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$, and

$$
\int_{a}^{c} f d \alpha+\int_{c}^{b} f d \alpha=\int_{a}^{b} f d \alpha .
$$

(d) If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $|f(x)| \leq M$ on $[a, b]$, then

$$
\left|\int_{a}^{b} f d \alpha\right| \leq M[\alpha(b)-\alpha(a)]
$$

(e) If $f \in \mathscr{R}\left(\alpha_{1}\right)$ and $f \in \mathscr{R}\left(\alpha_{2}\right)$, then $f \in \mathscr{R}\left(\alpha_{1}+\alpha_{2}\right)$ and

$$
\int_{a}^{b} f d\left(\alpha_{1}+\alpha_{2}\right)=\int_{a}^{b} f d \alpha_{1}+\int_{a}^{b} f d \alpha_{2}
$$

if $f \in \mathscr{R}(\alpha)$ and $c$ is a positive constant, then $f \in \mathscr{R}(c \alpha)$ and

$$
\int_{a}^{b} f d(c \alpha)=c \int_{a}^{b} f d \alpha
$$

Proof If $f=f_{1}+f_{2}$ and $P$ is any partition of $[a, b]$, we have

$$
\begin{aligned}
L\left(P, f_{1}, \alpha\right)+L\left(P, f_{2}, \alpha\right) \leq L(P, & f, \alpha) \\
& \leq U(P, f, \alpha) \leq U\left(P, f_{1}, \alpha\right)+U\left(P, f_{2}, \alpha\right)
\end{aligned}
$$

If $f_{1} \in \mathscr{R}(\alpha)$ and $f_{2} \in \mathscr{R}(\alpha)$, let $\varepsilon>0$ be given. There are partitions $P_{j}$ $(j=1,2)$ such that

$$
U\left(P_{j}, f_{j}, \alpha\right)-L\left(P_{j}, f_{j}, \alpha\right)<\varepsilon
$$

These inequalities persist if $P_{1}$ and $P_{2}$ are replaced by their common refinement $P$. Then (20) implies

$$
U(P, f, \alpha)-L(P, f, \alpha)<2 \varepsilon
$$

which proves that $f \in \mathscr{R}(\alpha)$.

With this same $P$ we have

$$
U\left(P, f_{J}, \alpha\right)<\int f_{j} d \alpha+\varepsilon \quad(j=1,2)
$$

hence (20) implies

$$
\int f d \alpha \leq U(P, f, \alpha)<\int f_{1} d \alpha+\int f_{2} d \alpha+2 \varepsilon \text {. }
$$

Since $\varepsilon$ was arbitrary, we conclude that

$$
\int f d \alpha \leq \int f_{1} d \alpha+\int f_{2} d \alpha
$$

If we replace $f_{1}$ and $f_{2}$ in (21) by $-f_{1}$ and $-f_{2}$, the inequality is reversed, and the equality is proved.

The proofs of the other assertions of Theorem 6.12 are so similar that we omit the details. In part $(c)$ the point is that (by passing to refinements) we may restrict ourselves to partitions which contain the point $c$, in approximating $\int f d \alpha$.

6.13 Theorem If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$, then

(a) $f g \in \mathscr{R}(\alpha)$;

(b) $|f| \in \mathscr{R}(\alpha)$ and $\left|\int_{a}^{b} f d \alpha\right| \leq \int_{a}^{b}|f| d \alpha$.

Proof If we take $\phi(t)=t^{2}$, Theorem 6.11 shows that $f^{2} \in \mathscr{R}(\alpha)$ if $f \in \mathscr{R}(a)$. The identity

$$
4 f g=(f+g)^{2}-(f-g)^{2}
$$

completes the proof of $(a)$.

If we take $\phi(t)=|t|$, Theorem 6.11 shows similarly that $|f| \in \mathscr{R}(\alpha)$. Choose $c= \pm 1$, so that

Then

$$
c \int f d \alpha \geq 0
$$

$$
\left|\int f d \alpha\right|=c \int f d \alpha=\int c f d \alpha \leq \int|f| d \alpha
$$

since $c f \leq|f|$.

6.14 Definition The unit step function $I$ is defined by

$$
I(x)= \begin{cases}0 & (x \leq 0) \\ 1 & (x>0)\end{cases}
$$

6.15 Theorem If $a<s<b, f$ is bounded on $[a, b], f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then

$$
\int_{a}^{b} f d \alpha=f(s)
$$

Proof Consider partitions $P=\left\{x_{0}, x_{1}, x_{2}, x_{3}\right\}$, where $x_{0}=a$, and $x_{1}=s<x_{2}<x_{3}=b$. Then

$$
U(P, f, \alpha)=M_{2}, \quad L(P, f, \alpha)=m_{2}
$$

Since $f$ is continuous at $s$, we see that $M_{2}$ and $m_{2}$ converge to $f(s)$ as $x_{2} \rightarrow s$.

6.16 Theorem Suppose $c_{n} \geq 0$ for $1,2,3, \ldots, \Sigma c_{n}$ converges, $\left\{s_{n}\right\}$ is a sequence of distinct points in $(a, b)$, and

$$
\alpha(x)=\sum_{n=1}^{\infty} c_{n} I\left(x-s_{n}\right)
$$

Let $f$ be continuous on $[a, b]$. Then

$$
\int_{a}^{b} f d \alpha=\sum_{n=1}^{\infty} c_{n} f\left(s_{n}\right)
$$

Proof The comparison test shows that the series (22) converges for every $x$. Its sum $\alpha(x)$ is evidently monotonic, and $\alpha(a)=0, \alpha(b)=\Sigma c_{n}$. (This is the type of function that occurred in Remark 4.31.)

Let $\varepsilon>0$ be given, and choose $N$ so that

$$
\sum_{N+1}^{\infty} c_{n}<\varepsilon
$$

Put

$$
\alpha_{1}(x)=\sum_{n=1}^{N} c_{n} I\left(x-s_{n}\right), \quad \alpha_{2}(x)=\sum_{N+1}^{\infty} c_{n} I\left(x-s_{n}\right)
$$

By Theorems 6.12 and 6.15,

$$
\int_{a}^{b} f d \alpha_{1}=\sum_{i=1}^{N} c_{n} f\left(s_{n}\right)
$$

Since $\alpha_{2}(b)-\alpha_{2}(a)<\varepsilon$,

$$
\left|\int_{a}^{b} f d \alpha_{2}\right| \leq M \varepsilon
$$

where $M=\sup |f(x)|$. Since $\alpha=\alpha_{1}+\alpha_{2}$, it follows from (24) and (25) that

(26)

$$
\left|\int_{a}^{b} f d \alpha-\sum_{i=1}^{N} c_{n} f\left(s_{n}\right)\right| \leq M \varepsilon
$$

If we let $N \rightarrow \infty$, we obtain (23).

6.17 Theorem Assume $\alpha$ increases monotonically and $\alpha^{\prime} \in \mathscr{R}$ on $[a, b]$. Let $f$ be a bounded real function on $[a, b]$.

Then $f \in \mathscr{R}(\alpha)$ if and only if $f \alpha^{\prime} \in \mathscr{R}$. In that case

$$
\int_{a}^{b} f d \alpha=\int_{a}^{b} f(x) \alpha^{\prime}(x) d x
$$

Proof Let $\varepsilon>0$ be given and apply Theorem 6.6 to $\alpha^{\prime}$ : There is a partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ such that

$$
U\left(P, \alpha^{\prime}\right)-L\left(P, \alpha^{\prime}\right)<\varepsilon .
$$

The mean value theorem furnishes points $t_{i} \in\left[x_{i-1}, x_{i}\right]$ such that

$$
\Delta \alpha_{i}=\alpha^{\prime}\left(t_{i}\right) \Delta x_{i}
$$

for $i=1, \ldots, n$. If $s_{i} \in\left[x_{i-1}, x_{i}\right]$, then

$$
\sum_{i=1}^{n}\left|\alpha^{\prime}\left(s_{i}\right)-\alpha^{\prime}\left(t_{i}\right)\right| \Delta x_{i}<\varepsilon
$$

by (28) and Theorem 6.7(b). Put $M=\sup |f(x)|$. Since

$$
\sum_{i=1}^{n} f\left(s_{i}\right) \Delta \alpha_{i}=\sum_{i=1}^{n} f\left(s_{i}\right) \alpha^{\prime}\left(t_{i}\right) \Delta x_{i}
$$

it follows from (29) that

$$
\left|\sum_{i=1}^{n} f\left(s_{i}\right) \Delta \alpha_{i}-\sum_{i=1}^{n} f\left(s_{i}\right) \alpha^{\prime}\left(s_{i}\right) \Delta x_{i}\right| \leq M \varepsilon
$$

In particular,

$$
\sum_{i=1}^{n} f\left(s_{i}\right) \Delta \alpha_{i} \leq U\left(P, f \alpha^{\prime}\right)+M \varepsilon
$$

for all choices of $s_{i} \in\left[x_{i-1}, x_{i}\right]$, so that

$$
U(P, f, \alpha) \leq U\left(P, f \alpha^{\prime}\right)+M \varepsilon
$$

The same argument leads from (30) to

Thus

$$
U\left(P, f \alpha^{\prime}\right) \leq U(P, f, \alpha)+M \varepsilon
$$

$$
\left|U(P, f, \alpha)-U\left(P, f \alpha^{\prime}\right)\right| \leq M \varepsilon
$$

Now note that (28) remains true if $P$ is replaced by any refinement. Hence (31) also remains true. We conclude that

$$
\left|\bar{\int}_{a}^{b} f d \alpha-\bar{\int}_{a}^{b} f(x) \alpha^{\prime}(x) d x\right| \leq M \varepsilon
$$

But $\varepsilon$ is arbitrary. Hence

$$
\int_{a}^{b} f d \alpha=\bar{\int}_{a}^{b} f(x) \alpha^{\prime}(x) d x
$$

for any bounded $f$. The equality of the lower integrals follows from (30) in exactly the same way. The theorem follows.

6.18 Remark The two preceding theorems illustrate the generality and flexibility which are inherent in the Stieltjes process of integration. If $\alpha$ is a pure step function [this is the name often given to functions of the form (22)], the integral reduces to a finite or infinite series. If $\alpha$ has an integrable derivative, the integral reduces to an ordinary Riemann integral. This makes it possible in many cases to study series and integrals simultaneously, rather than separately.

To illustrate this point, consider a physical example. The moment of inertia of a straight wire of unit length, about an axis through an endpoint, at right angles to the wire, is

$$
\int_{0}^{1} x^{2} d m
$$

where $m(x)$ is the mass contained in the interval $[0, x]$. If the wire is regarded as having a continuous density $\rho$, that is, if $m^{\prime}(x)=\rho(x)$, then (33) turns into

$$
\int_{0}^{1} x^{2} \rho(x) d x
$$

On the other hand, if the wire is composed of masses $m_{i}$ concentrated at points $x_{i}$, (33) becomes

$$
\sum_{i} x_{i}^{2} m_{i}
$$

Thus (33) contains (34) and (35) as special cases, but it contains much more; for instance, the case in which $m$ is continuous but not everywhere differentiable.

6.19 Theorem (change of variable) Suppose $\varphi$ is a strictly increasing continuous function that maps an interval $[A, B]$ onto $[a, b]$. Suppose $\alpha$ is monotonically increasing on $[a, b]$ and $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Define $\beta$ and $g$ on $[A, B]$ by

$$
\beta(y)=\alpha(\varphi(y)), \quad g(y)=f(\varphi(y)) .
$$

Then $g \in \mathscr{R}(\beta)$ and

$$
\int_{A}^{B} g d \beta=\int_{a}^{b} f d \alpha
$$

Proof To each partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ corresponds a partition $Q=\left\{y_{0}, \ldots, y_{n}\right\}$ of $[A, B]$, so that $x_{i}=\varphi\left(y_{i}\right)$. All partitions of $[A, B]$ are obtained in this way. Since the values taken by $f$ on $\left[x_{i-1}, x_{i}\right]$ are exactly the same as those taken by $g$ on $\left[y_{i-1}, y_{i}\right]$, we see that

$$
U(Q, g, \beta)=U(P, f, \alpha), \quad L(Q, g, \beta)=L(P, f, \alpha)
$$

Since $f \in \mathscr{R}(\alpha), P$ can be chosen so that both $U(P, f, \alpha)$ and $L(P, f, \alpha)$ are close to $\int f d \alpha$. Hence (38), combined with Theorem 6.6 , shows that $g \in \mathscr{R}(\beta)$ and that (37) holds. This completes the proof.

Let us note the following special case:

Take $\alpha(x)=x$. Then $\beta=\varphi$. Assume $\varphi^{\prime} \in \mathscr{R}$ on $[A, B]$. If Theorem 6.17 is applied to the left side of (37), we obtain

$$
\int_{a}^{b} f(x) d x=\int_{A}^{B} f(\varphi(y)) \varphi^{\prime}(y) d y .
$$

## INTEGRATION AND DIFFERENTIATION

We still confine ourselves to real functions in this section. We shall show that integration and differentiation are, in a certain sense, inverse operations.

6.20 Theorem Let $f \in \mathscr{R}$ on $[a, b]$. For $a \leq x \leq b$, put

$$
F(x)=\int_{a}^{x} f(t) d t
$$

Then $F$ is continuous on $[a, b]$; furthermore, if $f$ is continuous at a point $x_{0}$ of $[a, b]$, then $F$ is differentiable at $x_{0}$, and

$$
F^{\prime}\left(x_{0}\right)=f\left(x_{0}\right) \text {. }
$$

Proof Since $f \in \mathscr{R}, f$ is bounded. Suppose $|f(t)| \leq M$ for $a \leq t \leq b$. If $a \leq x<y \leq b$, then

$$
|F(y)-F(x)|=\left|\int_{x}^{y} f(t) d t\right| \leq M(y-x)
$$

by Theorem $6.12(c)$ and $(d)$. Given $\varepsilon>0$, we see that

$$
|F(y)-F(x)|<\varepsilon,
$$

provided that $|y-x|<\varepsilon / M$. This proves continuity (and, in fact, uniform continuity) of $F$.

Now suppose $f$ is continuous at $x_{0}$. Given $\varepsilon>0$, choose $\delta>0$ such that

$$
\left|f(t)-f\left(x_{0}\right)\right|<\varepsilon
$$

if $\left|t-x_{0}\right|<\delta$, and $a \leq t \leq b$. Hence, if

$$
x_{0}-\delta<s \leq x_{0} \leq t<x_{0}+\delta \quad \text { and } \quad a \leq s<t \leq b
$$

we have, by Theorem $6.12(d)$,

$$
\left|\frac{F(t)-F(s)}{t-s}-f\left(x_{0}\right)\right|=\left|\frac{1}{t-s} \int_{s}^{t}\left[f(u)-f\left(x_{0}\right)\right] d u\right|<\varepsilon .
$$

It follows that $F^{\prime}\left(x_{0}\right)=f\left(x_{0}\right)$.

6.21 The fundamental theorem of calculus If $f \in \mathscr{R}$ on $[a, b]$ and if there is a differentiable function $F$ on $[a, b]$ such that $F^{\prime}=f$, then

$$
\int_{a}^{b} f(x) d x=F(b)-F(a)
$$

Proof Let $\varepsilon>0$ be given. Choose a partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ so that $U(P, f)-L(P, f)<\varepsilon$. The mean value theorem furnishes points $t_{i} \in\left[x_{i-1}, x_{i}\right]$ such that

$$
F\left(x_{i}\right)-F\left(x_{i-1}\right)=f\left(t_{i}\right) \Delta x_{i}
$$

for $i=1, \ldots, n$. Thus

$$
\sum_{i=1}^{n} f\left(t_{i}\right) \Delta x_{i}=F(b)-F(a)
$$

It now follows from Theorem 6.7(c) that

$$
\left|F(b)-F(a)-\int_{a}^{b} f(x) d x\right|<\varepsilon
$$

Since this holds for every $\varepsilon>0$, the proof is complete.

6.22 Theorem (integration by parts) Suppose $F$ and $G$ are differentiable functions on $[a, b], F^{\prime}=f \in \mathscr{R}$, and $G^{\prime}=g \in \mathscr{R}$. Then

$$
\int_{a}^{b} F(x) g(x) d x=F(b) G(b)-F(a) G(a)-\int_{a}^{b} f(x) G(x) d x .
$$

Proof Put $H(x)=F(x) G(x)$ and apply Theorem 6.21 to $H$ and its derivative. Note that $H^{\prime} \in \mathscr{R}$, by Theorem 6.13.

## INTEGRATION OF VECTOR-VALUED FUNCTIONS

6.23 Definition Let $f_{1}, \ldots, f_{k}$ be real functions on $[a, b]$, and let $\mathbf{f}=\left(f_{1}, \ldots, f_{k}\right)$ be the corresponding mapping of $[a, b]$ into $R^{k}$. If $\alpha$ increases monotonically on $[a, b]$, to say that $\mathbf{f} \in \mathscr{R}(\alpha)$ means that $f_{j} \in \mathscr{R}(\alpha)$ for $j=1, \ldots, k$. If this is the case, we define

$$
\int_{a}^{b} \mathbf{f} d \alpha=\left(\int_{a}^{b} f_{1} d \alpha, \ldots, \int_{a}^{b} f_{k} d \alpha\right)
$$

In other words, $\int f d \alpha$ is the point in $R^{k}$ whose $j$ th coordinate is $\int f_{j} d \alpha$.

It is clear that parts $(a),(c)$, and $(e)$ of Theorem 6.12 are valid for these vector-valued integrals; we simply apply the earlier results to each coordinate. The same is true of Theorems 6.17, 6.20, and 6.21. To illustrate, we state the analogue of Theorem 6.21.

6.24 Theorem If $\mathbf{f}$ and $\mathbf{F}$ map $[a, b]$ into $R^{k}$, if $\mathbf{f} \in \mathscr{R}$ on $[a, b]$, and if $\mathbf{F}^{\prime}=\mathbf{f}$, then

$$
\int_{a}^{b} \mathbf{f}(t) d t=\mathbf{F}(b)-\mathbf{F}(a) .
$$

The analogue of Theorem $6.13(b)$ offers some new features, however, at least in its proof.

6.25 Theorem If $\mathbf{f}$ maps $[a, b]$ into $R^{k}$ and if $\mathbf{f} \in \mathscr{R}(\alpha)$ for some monotonically increasing function $\alpha$ on $[a, b]$, then $|\mathbf{f}| \in \mathscr{R}(\alpha)$, and

$$
\left|\int_{a}^{b} \mathbf{f} d \alpha\right| \leq \int_{a}^{b}|\mathbf{f}| d \alpha
$$

Proof If $f_{1}, \ldots, f_{k}$ are the components of $\mathbf{f}$, then

$$
|\mathbf{f}|=\left(f_{1}^{2}+\cdots+f_{k}^{2}\right)^{1 / 2} .
$$

By Theorem 6.11, each of the functions $f_{i}^{2}$ belongs to $\mathscr{R}(\alpha)$; hence so does their sum. Since $x^{2}$ is a continuous function of $x$, Theorem 4.17 shows that the square-root function is continuous on $[0, M]$, for every real $M$. If we apply Theorem 6.11 once more, (41) shows that $|\mathbf{f}| \in \mathscr{R}(\alpha)$.

To prove (40), put $\mathbf{y}=\left(y_{1}, \ldots, y_{k}\right)$, where $y_{j}=\int f_{j} d \alpha$. Then we have $\mathbf{y}=\int \mathbf{f} d \alpha$, and

$$
|\mathbf{y}|^{2}=\sum y_{i}^{2}=\sum y_{j} \int f_{j} d \alpha=\int\left(\sum y_{j} f_{j}\right) d \alpha .
$$

By the Schwarz inequality,

$$
\sum y_{J} f_{j}(t) \leq|\mathbf{y}||\mathbf{f}(t)| \quad(a \leq t \leq b)
$$

hence Theorem 6.12(b) implies

$$
|\mathbf{y}|^{2} \leq|\mathbf{y}| \int|\mathbf{f}| d \alpha
$$

If $\mathbf{y}=\mathbf{0}$, (40) is trivial. If $\mathbf{y} \neq \mathbf{0}$, division of (43) by $|\mathbf{y}|$ gives (40).

## RECTIFIABLE CURVES

We conclude this chapter with a topic of geometric interest which provides an application of some of the preceding theory. The case $k=2$ (i.e., the case of plane curves) is of considerable importance in the study of analytic functions of a complex variable.

6.26 Definition A continuous mapping $\gamma$ of an interval $[a, b]$ into $R^{k}$ is called a curve in $R^{k}$. To emphasize the parameter interval $[a, b]$, we may also say that $\gamma$ is a curve on $[a, b]$.

If $\gamma$ is one-to-one, $\gamma$ is called an arc.

If $\gamma(a)=\gamma(b), \gamma$ is said to be a closed curve.

It should be noted that we define a curve to be a mapping, not a point set. Of course, with each curve $\gamma$ in $R^{k}$ there is associated a subset of $R^{k}$, namely the range of $\gamma$, but different curves may have the same rarige.

We associate to each partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ and to each curve $\gamma$ on $[a, b]$ the number

$$
\Lambda(P, \gamma)=\sum_{i=1}^{n}\left|\gamma\left(x_{i}\right)-\gamma\left(x_{i-1}\right)\right|
$$

The $i$ th term in this sum is the distance (in $R^{k}$ ) between the points $\gamma\left(x_{i-1}\right)$ and $\gamma\left(x_{i}\right)$. Hence $\Lambda(P, \gamma)$ is the length of a polygonal path with vertices at $\gamma\left(x_{0}\right)$, $\gamma\left(x_{1}\right), \ldots, \gamma\left(x_{n}\right)$, in this order. As our partition becomes finer and finer, this polygon approaches the range of $\gamma$ more and more closely. This makes it seem reasonable to define the length of $\gamma$ as

$$
\Lambda(\gamma)=\sup \Lambda(P, \gamma)
$$

where the supremum is taken over all partitions of $[a, b]$.

If $\Lambda(\gamma)<\infty$, we say that $\gamma$ is rectifiable.

In certain cases, $\Lambda(\gamma)$ is given by a Riemann integral. We shall prove this for continuously differentiable curves, i.e., for curves $\gamma$ whose derivative $\gamma^{\prime}$ is continuous.

6.27 Theorem If $\gamma^{\prime}$ is continuous on $[a, b]$, then $\gamma$ is rectifiable, and

$$
\Lambda(\gamma)=\int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t
$$

Proof If $a \leq x_{i-1}<x_{i} \leq b$, then

Hence

$$
\left|\gamma\left(x_{i}\right)-\gamma\left(x_{i-1}\right)\right|=\left|\int_{x_{i-1}}^{x_{i}} \gamma^{\prime}(t) d t\right| \leq \int_{x_{i-1}}^{x_{i}}\left|\gamma^{\prime}(t)\right| d t
$$

$$
\Lambda(P, \gamma) \leq \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t
$$

for every partition $P$ of $[a, b]$. Consequently,

$$
\Lambda(\gamma) \leq \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t
$$

To prove the opposite inequality, let $\varepsilon>0$ be given. Since $\gamma^{\prime}$ is uniformly continuous on $[a, b]$, there exists $\delta>0$ such that

$$
\left|\gamma^{\prime}(s)-\gamma^{\prime}(t)\right|<\varepsilon \quad \text { if }|s-t|<\delta \text {. }
$$

Let $P=\left\{x_{0}, \ldots, x_{n}\right\}$ be a partition of $[a, b]$, with $\Delta x_{i}<\delta$ for all $i$. If $x_{i-1} \leq t \leq x_{i}$, it follows that

Hence

$$
\left|\gamma^{\prime}(t)\right| \leq\left|\gamma^{\prime}\left(x_{i}\right)\right|+\varepsilon .
$$

$$
\begin{aligned}
\int_{x_{l-1}}^{x_{i}}\left|\gamma^{\prime}(t)\right| d t & \leq\left|\gamma^{\prime}\left(x_{i}\right)\right| \Delta x_{i}+\varepsilon \Delta x_{i} \\
& =\left|\int_{x_{t-1}}^{x_{i}}\left[\gamma^{\prime}(t)+\gamma^{\prime}\left(x_{i}\right)-\gamma^{\prime}(t)\right] d t\right|+\varepsilon \Delta x_{i} \\
& \leq\left|\int_{x_{l-1}}^{x_{i}} \gamma^{\prime}(t) d t\right|+\left|\int_{x_{i-1}}^{x_{i}}\left[\gamma^{\prime}\left(x_{i}\right)-\gamma^{\prime}(t)\right] d t\right|+\varepsilon \Delta x_{i} \\
& \leq\left|\gamma\left(x_{i}\right)-\gamma\left(x_{i-1}\right)\right|+2 \varepsilon \Delta x_{i} .
\end{aligned}
$$

If we add these inequalities, we obtain

Since $\varepsilon$ was arbitrary,

$$
\begin{aligned}
\int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t & \leq \Lambda(P, \gamma)+2 \varepsilon(b-a) \\
& \leq \Lambda(\gamma)+2 \varepsilon(b-a) .
\end{aligned}
$$

$$
\int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t \leq \Lambda(\gamma) .
$$

This completes the proof.

## EXERCISES

1. Suppose $\alpha$ increases on $[a, b], a \leq x_{0} \leq b, \alpha$ is continuous at $x_{0}, f\left(x_{0}\right)=1$, and $f(x)=0$ if $x \neq x_{0}$. Prove that $f \in \mathscr{R}(\alpha)$ and that $\int f d \alpha=0$.
2. Suppose $f \geq 0, f$ is continuous on $[a, b]$, and $\int_{a}^{b} f(x) d x=0$. Prove that $f(x)=0$ for all $x \in[a, b]$. (Compare this with Exercise 1.)
3. Define three functions $\beta_{1}, \beta_{2}, \beta_{3}$ as follows: $\beta_{\jmath}(x)=0$ if $x<0, \beta_{\jmath}(x)=1$ if $x>0$ for $j=1,2,3$; and $\beta_{1}(0)=0, \beta_{2}(0)=1, \beta_{3}(0)=\frac{1}{2}$. Let $f$ be a bounded function on $[-1,1]$.

(a) Prove that $f \in \mathscr{R}\left(\beta_{1}\right)$ if and only if $f(0+)=f(0)$ and that then

$$
\int f d \beta_{1}=f(0)
$$

(b) State and prove a similar result for $\beta_{2}$.

(c) Prove that $f \in \mathscr{R}\left(\beta_{3}\right)$ if and only if $f$ is continuous at 0 .

(d) If $f$ is continuous at 0 prove that

$$
\int f d \beta_{1}=\int f d \beta_{2}=\int f d \beta_{3}=f(0) .
$$

4. If $f(x)=0$ for all irrational $x, f(x)=1$ for all rational $x$, prove that $f \notin \mathscr{R}$ on $[a, b]$ for any $a<b$.
5. Suppose $f$ is a bounded real function on $[a, b]$, and $f^{2} \in \mathscr{R}$ on $[a, b]$. Does it follow that $f \in \mathscr{R}$ ? Does the answer change if we assume that $f^{3} \in \mathscr{R}$ ?
6. Let $P$ be the Cantor set constructed in Sec. 2.44. Let $f$ be a bounded real function on $[0,1]$ which is continuous at every point outside $P$. Prove that $f \in \mathscr{R}$ on $[0,1]$. Hint: $P$ can be covered by finitely many segments whose total length can be made as small as desired. Proceed as in Theorem 6.10.
7. Suppose $f$ is a real function on $(0,1]$ and $f \in \mathscr{R}$ on $[c, 1]$ for every $c>0$. Define

$$
\int_{0}^{1} f(x) d x=\lim _{c \rightarrow 0} \int_{c}^{1} f(x) d x
$$

if this limit exists (and is finite).

(a) If $f \in \mathscr{R}$ on $[0,1]$, show that this definition of the integral agrees with the old one.

(b) Construct a function $f$ such that the above limit exists, although it fails to exist with $|f|$ in place of $f$.

8. Suppose $f \in \mathscr{R}$ on $[a, b]$ for every $b>a$ where $a$ is fixed. Define

$$
\int_{a}^{\infty} f(x) d x=\lim _{b \rightarrow \infty} \int_{a}^{b} f(x) d x
$$

if this limit exists (and is finite). In that case, we say that the integral on the left converges. If it also converges after $f$ has been replaced by $|f|$, it is said to converge absolutely.

Assume that $f(x) \geq 0$ and that $f$ decreases monotonically on $[1, \infty)$. Prove that

$$
\int_{1}^{\infty} f(x) d x
$$

converges if and only if

$$
\sum_{n=1}^{\infty} f(n)
$$

converges. (This is the so-called "integral test" for convergence of series.)

9. Show that integration by parts can sometimes be applied to the "improper" integrals defined in Exercises 7 and 8. (State appropriate hypotheses, formulate a theorem, and prove it.) For instance show that

$$
\int_{0}^{\infty} \frac{\cos x}{1+x} d x=\int_{0}^{\infty} \frac{\sin x}{(1+x)^{2}} d x
$$

Show that one of these integrals converges absolutely, but that the other does not.

10. Let $p$ and $q$ be positive real numbers such that

$$
\frac{1}{p}+\frac{1}{q}=1
$$

Prove the following statements.

(a) If $u \geq 0$ and $v \geq 0$, then

$$
u v \leq \frac{u^{p}}{p}+\frac{v^{q}}{q} .
$$

Equality holds if and only if $u^{p}=v^{q}$.

(b) If $f \in \mathscr{R}(\alpha), g \in \mathscr{R}(\alpha), f \geq 0, g \geq 0$, and

$$
\int_{a}^{b} f^{p} d \alpha=1=\int_{a}^{b} g^{q} d \alpha
$$

then

$$
\int_{a}^{b} f g d \alpha \leq 1
$$

(c) If $f$ and $g$ are complex functions in $\mathscr{R}(\alpha)$, then

$$
\left|\int_{a}^{b} f g d \alpha\right| \leq\left\{\int_{a}^{b}|f|^{p} d \alpha\right\}^{1 / p}\left\{\int_{a}^{b}|g|^{a} d \alpha\right\}^{1 / a}
$$

This is H√∂lder's inequality. When $p=q=2$ it is usually called the Schwarz inequality. (Note that Theorem 1.35 is a very special case of this.)

(d) Show that H√∂lder's inequality is also true for the "improper" integrals described in Exercises 7 and 8.

11. Let $\alpha$ be a fixed increasing function on $[a, b]$. For $u \in \mathscr{R}(\alpha)$, define

$$
\|u\|_{2}=\left\{\int_{a}^{b}|u|^{2} d \alpha\right\}^{1 / 2}
$$

Suppose $f, g, h \in \mathscr{R}(\alpha)$, and prove the triangle inequality

$$
\|f-h\|_{2} \leq\|f-g\|_{2}+\|g-h\|_{2}
$$

as a consequence of the Schwarz inequality, as in the proof of Theorem 1.37.

12. With the notations of Exercise 11, suppose $f \in \mathscr{R}(\alpha)$ and $\varepsilon>0$. Prove that there exists a continuous function $g$ on $[a, b]$ such that $\|f-g\|_{2}<\varepsilon$.

Hint: Let $P=\left\{x_{0}, \ldots, x_{n}\right\}$ be a suitable partition of $[a, b]$, define

$$
g(t)=\frac{x_{i}-t}{\Delta x_{i}} f\left(x_{i-1}\right)+\frac{t-x_{i-1}}{\Delta x_{i}} f\left(x_{i}\right)
$$

if $x_{t-1} \leq t \leq x_{i}$.

13. Define

$$
f(x)=\int_{x}^{x+1} \sin \left(t^{2}\right) d t
$$

(a) Prove that $|f(x)|<1 / x$ if $x>0$.

Hint: Put $t^{2}=u$ and integrate by parts, to show that $f(x)$ is equal to

$$
\frac{\cos \left(x^{2}\right)}{2 x}-\frac{\cos \left[(x+1)^{2}\right]}{2(x+1)}-\int_{x^{2}}^{(x+1)^{2}} \frac{\cos u}{4 u^{3 / 2}} d u .
$$

Replace $\cos u$ by -1 .

(b) Prove that

$$
2 x f(x)=\cos \left(x^{2}\right)-\cos \left[(x+1)^{2}\right]+r(x)
$$

where $|r(x)|<c / x$ and $c$ is a constant.

(c) Find the upper and lower limits of $x f(x)$, as $x \rightarrow \infty$.

(d) Does $\int_{0}^{\infty} \sin \left(t^{2}\right) d t$ converge?

14. Deal similarly with

$$
f(x)=\int_{x}^{x+1} \sin \left(e^{t}\right) d t
$$

Show that

$$
e^{x}|f(x)|<2
$$

and that

$$
e^{x} f(x)=\cos \left(e^{x}\right)-e^{-1} \cos \left(e^{x+1}\right)+r(x)
$$

where $|r(x)|<C e^{-x}$, for some constant $C$.

15. Suppose $f$ is a real, continuously differentiable function on $[a, b], f(a)=f(b)=0$, and

Prove that

$$
\int_{a}^{b} f^{2}(x) d x=1
$$

$$
\int_{a}^{b} x f(x) f^{\prime}(x) d x=-\frac{1}{2}
$$

and that

$$
\int_{a}^{b}\left[f^{\prime}(x)\right]^{2} d x \cdot \int_{a}^{b} x^{2} f^{2}(x) d x>1
$$

16. For $1<s<\infty$, define

$$
\zeta(s)=\sum_{n=1}^{\infty} \frac{1}{n^{s}}
$$

(This is Riemann's zeta function, of great importance in the study of the distribution of prime numbers.) Prove that

(a) $\zeta(s)=s \int_{1}^{\infty} \frac{[x]}{x^{s+1}} d x$

and that

(b) $\zeta(s)=\frac{s}{s-1}-s \int_{1}^{\infty} \frac{x-[x]}{x^{s+1}} d x$,

where $[x]$ denotes the greatest integer $\leq x$.

Prove that the integral in $(b)$ converges for all $s>0$.

Hint: To prove (a), compute the difference between the integral over $[1, N]$ and the $N$ th partial sum of the series that defines $\zeta(s)$.

17. Suppose $\alpha$ increases monotonically on $[a, b], g$ is continuous, and $g(x)=G^{\prime}(x)$ for $a \leq x \leq b$. Prove that

$$
\int_{a}^{b} \alpha(x) g(x) d x=G(b) \alpha(b)-G(a) \alpha(a)-\int_{a}^{b} G d \alpha .
$$

Hint: Take $g$ real, without loss of generality. Given $P=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$, choose $t_{t} \in\left(x_{t-1}, x_{t}\right)$ so that $g\left(t_{t}\right) \Delta x_{t}=G\left(x_{t}\right)-G\left(x_{t-1}\right)$. Show that

$$
\sum_{i=1}^{n} \alpha\left(x_{i}\right) g\left(t_{t}\right) \Delta x_{t}=G(b) \alpha(b)-G(a) \alpha(a)-\sum_{i=1}^{n} G\left(x_{i-1}\right) \Delta \alpha_{t} .
$$

18. Let $\gamma_{1}, \gamma_{2}, \gamma_{3}$ be curves in the complex plane, defined on $[0,2 \pi]$ by

$$
\gamma_{1}(t)=e^{i t}, \quad \gamma_{2}(t)=e^{2 t t}, \quad \gamma_{3}(t)=e^{2 \pi t t \sin (1 / t)}
$$

Show that these three curves have the same range, that $\gamma_{1}$ and $\gamma_{2}$ are rectifiable, that the length of $\gamma_{1}$ is $2 \pi$, that the length of $\gamma_{2}$ is $4 \pi$, and that $\gamma_{3}$ is not rectifiable.

19. Let $\gamma_{1}$ be a curve in $R^{k}$, defined on $[a, b]$; let $\phi$ be a continuous 1-1 mapping of $[c, d]$ onto $[a, b]$, such that $\phi(c)=a$; and define $\gamma_{2}(s)=\gamma_{1}(\phi(s))$. Prove that $\gamma_{2}$ is an arc, a closed curve, or a rectifiable curve if and only if the same is true of $\gamma_{1}$. Prove that $\gamma_{2}$ and $\gamma_{1}$ have the same length.

## SEQUENCES AND SERIES OF FUNCTIONS

In the present chapter we confine our attention to complex-valued functions (including the real-valued ones, of course), although many of the theorems and proofs which follow extend without difficulty to vector-valued functions, and even to mappings into general metric spaces. We choose to stay within this simple framework in order to focus attention on the most important aspects of the problems that arise when limit processes are interchanged.

## DISCUSSION OF MAIN PROBLEM

7.1 Definition Suppose $\left\{f_{n}\right\}, n=1,2,3, \ldots$, is a sequence of functions defined on a set $E$, and suppose that the sequence of numbers $\left\{f_{n}(x)\right\}$ converges for every $x \in E$. We can then define a function $f$ by

$$
f(x)=\lim _{n \rightarrow \infty} f_{n}(x) \quad(x \in E)
$$

Under these circumstances we say that $\left\{f_{n}\right\}$ converges on $E$ and that $f$ is the limit, or the limit function, of $\left\{f_{n}\right\}$. Sometimes we shall use a more descriptive terminology and shall say that " $\left\{f_{n}\right\}$ converges to $f$ pointwise on $E$ " if (1) holds. Similarly, if $\Sigma f_{n}(x)$ converges for every $x \in E$, and if we define

$$
f(x)=\sum_{n=1}^{\infty} f_{n}(x) \quad(x \in E)
$$

the function $f$ is called the sum of the series $\Sigma f_{n}$.

The main problem which arises is to determine whether important properties of functions are preserved under the limit operations (1) and (2). For instance, if the functions $f_{n}$ are continuous, or differentiable, or integrable, is the same true of the limit function? What are the relations between $f_{n}^{\prime}$ and $f^{\prime}$, say, or between the integrals of $f_{n}$ and that of $f$ ?

To say that $f$ is continuous at a limit point $x$ means

$$
\lim _{t \rightarrow x} f(t)=f(x) .
$$

Hence, to ask whether the limit of a sequence of continuous functions is continuous is the same as to ask whether

$$
\lim _{t \rightarrow x} \lim _{n \rightarrow \infty} f_{n}(t)=\lim _{n \rightarrow \infty} \lim _{t \rightarrow x} f_{n}(t),
$$

i.e., whether the order in which limit processes are carried out is immaterial. On the left side of (3), we first let $n \rightarrow \infty$, then $t \rightarrow x$; on the right side, $t \rightarrow x$ first, then $n \rightarrow \infty$.

We shall now show by means of several examples that limit processes cannot in general be interchanged without affecting the result. Afterward, we shall prove that under certain conditions the order in which limit operations are carried out is immaterial.

Our first example, and the simplest one, concerns a "double sequence."

7.2 Example For $m=1,2,3, \ldots, n=1,2,3, \ldots$, let

$$
s_{m, n}=\frac{m}{m+n} .
$$

Then, for every fixed $n$,

$$
\lim _{m \rightarrow \infty} s_{m, n}=1
$$

so that

$$
\lim _{n \rightarrow \infty} \lim _{m \rightarrow \infty} s_{m, n}=1 \text {. }
$$

On the other hand, for every fixed $m$,

$$
\lim _{n \rightarrow \infty} s_{m, n}=0
$$

so that

$$
\lim _{m \rightarrow \infty} \lim _{n \rightarrow \infty} s_{m, n}=0
$$

### 7.3 Example Let

$$
f_{n}(x)=\frac{x^{2}}{\left(1+x^{2}\right)^{n}} \quad(x \text { real } ; n=0,1,2, \ldots)
$$

and consider

$$
f(x)=\sum_{n=0}^{\infty} f_{n}(x)=\sum_{n=0}^{\infty} \frac{x^{2}}{\left(1+x^{2}\right)^{n}}
$$

Since $f_{n}(0)=0$, we have $f(0)=0$. For $x \neq 0$, the last series in (6) is a convergent geometric series with sum $1+x^{2}$ (Theorem 3.26). Hence

$$
f(x)= \begin{cases}0 & (x=0) \\ 1+x^{2} & (x \neq 0)\end{cases}
$$

so that a convergent series of continuous functions may have a discontinuous sum.

### 7.4 Example For $m=1,2,3, \ldots$, put

$$
f_{m}(x)=\lim _{n \rightarrow \infty}(\cos m ! \pi x)^{2 n}
$$

When $m ! x$ is an integer, $f_{m}(x)=1$. For all other values of $x, f_{m}(x)=0$. Now let

$$
f(x)=\lim _{m \rightarrow \infty} f_{m}(x)
$$

For irrational $x, f_{m}(x)=0$ for every $m$; hence $f(x)=0$. For rational $x$, say $x=p / q$, where $p$ and $q$ are integers, we see that $m ! x$ is an integer if $m \geq q$, so that $f(x)=1$. Hence

$$
\lim _{m \rightarrow \infty} \lim _{n \rightarrow \infty}(\cos m ! \pi x)^{2 n}= \begin{cases}0 & (x \text { irrational }) \\ 1 & (x \text { rational })\end{cases}
$$

We have thus obtained an everywhere discontinuous limit function, which is not Riemann-integrable (Exercise 4, Chap. 6).

### 7.5 Example Let

and

$$
f_{n}(x)=\frac{\sin n x}{\sqrt{n}} \quad(x \text { real, } n=1,2,3, \ldots)
$$

$$
f(x)=\lim _{n \rightarrow \infty} f_{n}(x)=0 .
$$

Then $f^{\prime}(x)=0$, and

$$
f_{n}^{\prime}(x)=\sqrt{n} \cos n x,
$$

so that $\left\{f_{n}^{\prime}\right\}$ does not converge to $f^{\prime}$. For instance,

$$
f_{n}^{\prime}(0)=\sqrt{n} \rightarrow+\infty
$$

as $n \rightarrow \infty$, whereas $f^{\prime}(0)=0$.

### 7.6 Example Let

$$
f_{n}(x)=n^{2} x\left(1-x^{2}\right)^{n} \quad(0 \leq x \leq 1, n=1,2,3, \ldots)
$$

For $0<x \leq 1$, we have

$$
\lim _{n \rightarrow \infty} f_{n}(x)=0
$$

by Theorem $3.20(d)$. Since $f_{n}(0)=0$, we see that

$$
\lim _{n \rightarrow \infty} f_{n}(x)=0 \quad(0 \leq x \leq 1) .
$$

A simple calculation shows that

$$
\int_{0}^{1} x\left(1-x^{2}\right)^{n} d x=\frac{1}{2 n+2}
$$

Thus, in spite of (11),

$$
\int_{0}^{1} f_{n}(x) d x=\frac{n^{2}}{2 n+2} \rightarrow+\infty
$$

as $n \rightarrow \infty$.

If, in (10), we replace $n^{2}$ by $n$, (11) still holds, but we now have

$$
\lim _{n \rightarrow \infty} \int_{0}^{1} f_{n}(x) d x=\lim _{n \rightarrow \infty} \frac{n}{2 n+2}=\frac{1}{2},
$$

whereas

$$
\int_{0}^{1}\left[\lim _{n \rightarrow \infty} f_{n}(x)\right] d x=0
$$

Thus the limit of the integral need not be equal to the integral of the limit, even if both are finite.

After these examples, which show what can go wrong if limit processes are interchanged carelessly, we now define a new mode of convergence, stronger than pointwise convergence as defined in Definition 7.1, which will enable us to arrive at positive results.

## UNIFORM CONVERGENCE

7.7 Definition We say that a sequence of functions $\left\{f_{n}\right\}, n=1,2,3, \ldots$, converges uniformly on $E$ to a function $f$ if for every $\varepsilon>0$ there is an integer $N$ such that $n \geq N$ implies

$$
\left|f_{n}(x)-f(x)\right| \leq \varepsilon
$$

for all $x \in E$.

It is clear that every uniformly convergent sequence is pointwise convergent. Quite explicitly, the difference between the two concepts is this: If $\left\{f_{n}\right\}$ converges pointwise on $E$, then there exists a function $f$ such that, for every $\varepsilon>0$, and for every $x \in E$, there is an integer $N$, depending on $\varepsilon$ and on $x$, such that (12) holds if $n \geq N$; if $\left\{f_{n}\right\}$ converges uniformly on $E$, it is possible, for each $\varepsilon>0$, to find one integer $N$ which will do for all $x \in E$.

We say that the series $\Sigma f_{n}(x)$ converges uniformly on $E$ if the sequence $\left\{s_{n}\right\}$ of partial sums defined by

$$
\sum_{i=1}^{n} f_{i}(x)=s_{n}(x)
$$

converges uniformly on $E$.

The Cauchy criterion for uniform convergence is as follows.

7.8 Theorem The sequence of functions $\left\{f_{n}\right\}$, defined on $E$, converges uniformly on $E$ if and only if for every $\varepsilon>0$ there exists an integer $N$ such that $m \geq N$, $n \geq N, x \in E$ implies

$$
\left|f_{n}(x)-f_{m}(x)\right| \leq \varepsilon
$$

Proof Suppose $\left\{f_{n}\right\}$ converges uniformly on $E$, and let $f$ be the limit function. Then there is an integer $N$ such that $n \geq N, x \in E$ implies

so that

$$
\left|f_{n}(x)-f(x)\right| \leq \frac{\varepsilon}{2},
$$

$$
\left|f_{n}(x)-f_{m}(x)\right| \leq\left|f_{n}(x)-f(x)\right|+\left|f(x)-f_{m}(x)\right| \leq \varepsilon
$$

if $n \geq N, m \geq N, x \in E$.

Conversely, suppose the Cauchy condition holds. By Theorem 3.11, the sequence $\left\{f_{n}(x)\right\}$ converges, for every $x$, to a limit which we may call $f(x)$. Thus the sequence $\left\{f_{n}\right\}$ converges on $E$, to $f$. We have to prove that the convergence is uniform.

Let $\varepsilon>0$ be given, and choose $N$ such that (13) holds. Fix $n$, and let $m \rightarrow \infty$ in (13). Since $f_{m}(x) \rightarrow f(x)$ as $m \rightarrow \infty$, this gives

$$
\left|f_{n}(x)-f(x)\right| \leq \varepsilon
$$

for every $n \geq N$ and every $x \in E$, which completes the proof.

The following criterion is sometimes useful.

### 7.9 Theorem Suppose

$$
\lim _{n \rightarrow \infty} f_{n}(x)=f(x) \quad(x \in E) .
$$

Put

$$
M_{n}=\sup _{x \in E}\left|f_{n}(x)-f(x)\right| .
$$

Then $f_{n} \rightarrow f$ uniformly on $E$ if and only if $M_{n} \rightarrow 0$ as $n \rightarrow \infty$.

Since this is an immediate consequence of Definition 7.7, we omit the details of the proof.

For series, there is a very convenient test for uniform convergence, due to Weierstrass.

7.10 Theorem Suppose $\left\{f_{n}\right\}$ is a sequence of functions defined on $E$, and suppose

$$
\left|f_{n}(x)\right| \leq M_{n} \quad(x \in E, n=1,2,3, \ldots)
$$

Then $\Sigma f_{n}$ converges uniformly on $E$ if $\Sigma M_{n}$ converges.

Note that the converse is not asserted (and is, in fact, not true).

Proof If $\Sigma M_{n}$ converges, then, for arbitrary $\varepsilon>0$,

$$
\left|\sum_{i=n}^{m} f_{i}(x)\right| \leq \sum_{i=n}^{m} M_{i} \leq \varepsilon \quad(x \in E)
$$

provided $m$ and $n$ are large enough. Uniform convergence now follows from Theorem 7.8.

## UNIFORM CONVERGENCE AND CONTINUITY

7.11 Theorem Suppose $f_{n} \rightarrow f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that

$$
\lim _{t \rightarrow x} f_{n}(t)=A_{n} \quad(n=1,2,3, \ldots)
$$

Then $\left\{A_{n}\right\}$ converges, and

$$
\lim _{t \rightarrow x} f(t)=\lim _{n \rightarrow \infty} A_{n} .
$$

In other words, the conclusion is that

$$
\lim _{t \rightarrow x} \lim _{n \rightarrow \infty} f_{n}(t)=\lim _{n \rightarrow \infty} \lim _{t \rightarrow x} f_{n}(t)
$$

Proof Let $\varepsilon>0$ be given. By the uniform convergence of $\left\{f_{n}\right\}$, there exists $N$ such that $n \geq N, m \geq N, t \in E$ imply

$$
\left|f_{n}(t)-f_{m}(t)\right| \leq \varepsilon
$$

Letting $t \rightarrow x$ in (18), we obtain

$$
\left|A_{n}-A_{m}\right| \leq \varepsilon
$$

for $n \geq N, m \geq N$, so that $\left\{A_{n}\right\}$ is a Cauchy sequence and therefore converges, say to $A$.

Next,

$$
|f(t)-A| \leq\left|f(t)-f_{n}(t)\right|+\left|f_{n}(t)-A_{n}\right|+\left|A_{n}-A\right| .
$$

We first choose $n$ such that

$$
\left|f(t)-f_{n}(t)\right| \leq \frac{\varepsilon}{3}
$$

for all $t \in E$ (this is possible by the uniform convergence), and such that

$$
\left|A_{n}-A\right| \leq \frac{\varepsilon}{3} \text {. }
$$

Then, for this $n$, we choose a neighborhood $V$ of $x$ such that

$$
\left|f_{n}(t)-A_{n}\right| \leq \frac{\varepsilon}{3}
$$

if $t \in V \cap E, t \neq x$.

Substituting the inequalities (20) to (22) into (19), we see that

$$
|f(t)-A| \leq \varepsilon
$$

provided $t \in V \cap E, t \neq x$. This is equivalent to (16).

7.12 Theorem If $\left\{f_{n}\right\}$ is a sequence of continuous functions on $E$, and if $f_{n} \rightarrow f$ uniformly on $E$, then $f$ is continuous on $E$.

This very important result is an immediate corollary of Theorem 7.11.

The converse is not true; that is, a sequence of continuous functions may converge to a continuous function, although the convergence is not uniform. Example 7.6 is of this kind (to see this, apply Theorem 7.9). But there is a case in which we can assert the converse.

### 7.13 Theorem Suppose $K$ is compact, and

(a) $\left\{f_{n}\right\}$ is a sequence of continuous functions on $K$,

(b) $\left\{f_{n}\right\}$ converges pointwise to a continuous function $f$ on $K$,

(c) $f_{n}(x) \geq f_{n+1}(x)$ for all $x \in K, n=1,2,3, \ldots$

Then $f_{n} \rightarrow f$ uniformly on $K$.

Proof Put $g_{n}=f_{n}-f$. Then $g_{n}$ is continuous, $g_{n} \rightarrow 0$ pointwise, and $g_{n} \geq g_{n+1}$. We have to prove that $g_{n} \rightarrow 0$ uniformly on $K$.

Let $\varepsilon>0$ be given. Let $K_{n}$ be the set of all $x \in K$ with $g_{n}(x) \geq \varepsilon$.

Since $g_{n}$ is continuous, $K_{n}$ is closed (Theorem 4.8), hence compact (Theorem 2.35). Since $g_{n} \geq g_{n+1}$, we have $K_{n} \supset K_{n+1}$. Fix $x \in K$. Since $g_{n}(x) \rightarrow 0$, we see that $x \notin K_{n}$ if $n$ is sufficiently large. Thus $x \notin \bigcap K_{n}$. In other words, $\bigcap K_{n}$ is empty. Hence $K_{N}$ is empty for some $N$ (Theorem 2.36). It follows that $0 \leq g_{n}(x)<\varepsilon$ for all $x \in K$ and for all $n \geq N$. This proves the theorem.

Let us note that compactness is really needed here. For instance, if

$$
f_{n}(x)=\frac{1}{n x+1} \quad(0<x<1 ; n=1,2,3, \ldots)
$$

then $f_{n}(x) \rightarrow 0$ monotonically in $(0,1)$, but the convergence is not uniform.

7.14 Definition If $X$ is a metric space, $\mathscr{C}(X)$ will denote the set of all complexvalued, continuous, bounded functions with domain $X$.

[Note that boundedness is redundant if $X$ is compact (Theorem 4.15). Thus $\mathscr{C}(X)$ consists of all complex continuous functions on $X$ if $X$ is compact.]

We associate with each $f \in \mathscr{C}(X)$ its supremum norm

$$
\|f\|=\sup _{x \in X}|f(x)| .
$$

Since $f$ is assumed to be bounded, $\|f\|<\infty$. It is obvious that $\|f\|=0$ only if $f(x)=0$ for every $x \in X$, that is, only if $f=0$. If $h=f+g$, then

$$
|h(x)| \leq|f(x)|+|g(x)| \leq\|f\|+\|g\|
$$

for all $x \in X$; hence

$$
\|f+g\| \leq\|f\|+\|g\| .
$$

If we define the distance between $f \in \mathscr{C}(X)$ and $g \in \mathscr{C}(X)$ to be $\|f-g\|$, it follows that Axioms 2.15 for a metric are satisfied.

We have thus made $\mathscr{C}(X)$ into a metric space.

Theorem 7.9 can be rephrased as follows:

$A$ sequence $\left\{f_{n}\right\}$ converges to $f$ with respect to the metric of $\mathscr{C}(X)$ if and only if $f_{n} \rightarrow f$ uniformly on $X$.

Accordingly, closed subsets of $\mathscr{C}(X)$ are sometimes called uniformly closed, the closure of a set $\mathscr{A} \subset \mathscr{C}(X)$ is called its uniform closure, and so on.

7.15 Theorem The above metric makes $\mathscr{C}(X)$ into a complete metric space.

Proof Let $\left\{f_{n}\right\}$ be a Cauchy sequence in $\mathscr{C}(X)$. This means that to each $\varepsilon>0$ corresponds an $N$ such that $\left\|f_{n}-f_{m}\right\|<\varepsilon$ if $n \geq N$ and $m \geq N$. It follows (by Theorem 7.8) that there is a function $f$ with domain $X$ to which $\left\{f_{n}\right\}$ converges uniformly. By Theorem 7.12, $f$ is continuous. Moreover, $f$ is bounded, since there is an $n$ such that $\left|f(x)-f_{n}(x)\right|<1$ for all $x \in X$, and $f_{n}$ is bounded.

Thus $f \in \mathscr{C}(X)$, and since $f_{n} \rightarrow f$ uniformly on $X$, we have $\left\|f-f_{n}\right\| \rightarrow 0$ as $n \rightarrow \infty$.

## UNIFORM CONVERGENCE AND INTEGRATION

7.16 Theorem Let $\alpha$ be monotonically increasing on $[a, b]$. Suppose $f_{n} \in \mathscr{R}(\alpha)$ on $[a, b]$, for $n=1,2,3, \ldots$, and suppose $f_{n} \rightarrow f$ uniformly on $[a, b]$. Then $f \in \mathscr{R}(\alpha)$ on $[a, b]$, and

$$
\int_{a}^{b} f d \alpha=\lim _{n \rightarrow \infty} \int_{a}^{b} f_{n} d \alpha
$$

(The existence of the limit is part of the conclusion.)

Proof It suffices to prove this for real $f_{n}$. Put

$$
\varepsilon_{n}=\sup \left|f_{n}(x)-f(x)\right|,
$$

the supremum being taken over $a \leq x \leq b$. Then

$$
f_{n}-\varepsilon_{n} \leq f \leq f_{n}+\varepsilon_{n},
$$

so that the upper and lower integrals of $f$ (see Definition 6.2) satisfy

Hence

$$
\int_{a}^{b}\left(f_{n}-\varepsilon_{n}\right) d \alpha \leq \int_{-} f d \alpha \leq \bar{\int} f d \alpha \leq \int_{a}^{b}\left(f_{n}+\varepsilon_{n}\right) d \alpha
$$

$$
0 \leq \bar{\int} f d \alpha-\int_{-} f d \alpha \leq 2 \varepsilon_{n}[\alpha(b)-\alpha(a)] .
$$

Since $\varepsilon_{n} \rightarrow 0$ as $n \rightarrow \infty$ (Theorem 7.9), the upper and lower integrals of $f$ are equal.

Thus $f \in \mathscr{R}(\alpha)$. Another application of (25) now yields

$$
\left|\int_{a}^{b} f d \alpha-\int_{a}^{b} f_{n} d \alpha\right| \leq \varepsilon_{n}[\alpha(b)-\alpha(a)]
$$

This implies (23).

Corollary If $f_{n} \in \mathscr{R}(\alpha)$ on $[a, b]$ and if

$$
f(x)=\sum_{n=1}^{\infty} f_{n}(x) \quad(a \leq x \leq b)
$$

the series converging uniformly on $[a, b]$, then

$$
\int_{a}^{b} f d \alpha=\sum_{n=1}^{\infty} \int_{a}^{b} f_{n} d \alpha
$$

In other words, the series may be integrated term by term.

## UNIFORM CONVERGENCE AND DIFFERENTIATION

We have already seen, in Example 7.5, that uniform convergence of $\left\{f_{n}\right\}$ implies nothing about the sequence $\left\{f_{n}^{\prime}\right\}$. Thus stronger hypotheses are required for the assertion that $f_{n}^{\prime} \rightarrow f^{\prime}$ if $f_{n} \rightarrow f$.

7.17 Theorem Suppose $\left\{f_{n}\right\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\left\{f_{n}\left(x_{0}\right)\right\}$ converges for some point $x_{0}$ on $[a, b]$. If $\left\{f_{n}^{\prime}\right\}$ converges uniformly on $[a, b]$, then $\left\{f_{n}\right\}$ converges uniformly on $[a, b]$, to a function $f$, and

$$
f^{\prime}(x)=\lim _{n \rightarrow \infty} f_{n}^{\prime}(x) \quad(a \leq x \leq b)
$$

Proof Let $\varepsilon>0$ be given. Choose $N$ such that $n \geq N, m \geq N$, implies

$$
\left|f_{n}\left(x_{0}\right)-f_{m}\left(x_{0}\right)\right|<\frac{\varepsilon}{2}
$$

and

$$
\left|f_{n}^{\prime}(t)-f_{m}^{\prime}(t)\right|<\frac{\varepsilon}{2(b-a)} \quad(a \leq t \leq b)
$$

If we apply the mean value theorem 5.19 to the function $f_{n}-f_{m},(29)$ shows that

$$
\left|f_{n}(x)-f_{m}(x)-f_{n}(t)+f_{m}(t)\right| \leq \frac{|x-t| \varepsilon}{2(b-a)} \leq \frac{\varepsilon}{2}
$$

for any $x$ and $t$ on $[a, b]$, if $n \geq N, m \geq N$. The inequality

$$
\left|f_{n}(x)-f_{m}(x)\right| \leq\left|f_{n}(x)-f_{m}(x)-f_{n}\left(x_{0}\right)+f_{m}\left(x_{0}\right)\right|+\left|f_{n}\left(x_{0}\right)-f_{m}\left(x_{0}\right)\right|
$$

implies, by (28) and (30), that

$$
\left|f_{n}(x)-f_{m}(x)\right|<\varepsilon \quad(a \leq x \leq b, n \geq N, m \geq N)
$$

so that $\left\{f_{n}\right\}$ converges uniformly on $[a, b]$. Let

$$
f(x)=\lim _{n \rightarrow \infty} f_{n}(x) \quad(a \leq x \leq b) .
$$

Let us now fix a point $x$ on $[a, b]$ and define

$$
\phi_{n}(t)=\frac{f_{n}(t)-f_{n}(x)}{t-x}, \quad \phi(t)=\frac{f(t)-f(x)}{t-x}
$$

for $a \leq t \leq b, t \neq x$. Then

$$
\lim _{t \rightarrow x} \phi_{n}(t)=f_{n}^{\prime}(x) \quad(n=1,2,3, \ldots)
$$

The first inequality in (30) shows that

$$
\left|\phi_{n}(t)-\phi_{m}(t)\right| \leq \frac{\varepsilon}{2(b-a)} \quad(n \geq N, m \geq N)
$$

so that $\left\{\phi_{n}\right\}$ converges uniformly, for $t \neq x$. Since $\left\{f_{n}\right\}$ converges to $f$, we conclude from (31) that

$$
\lim _{n \rightarrow \infty} \phi_{n}(t)=\phi(t)
$$

uniformly for $a \leq t \leq b, t \neq x$.

If we now apply Theorem 7.11 to $\left\{\phi_{n}\right\}$, (32) and (33) show that

$$
\lim _{t \rightarrow x} \phi(t)=\lim _{n \rightarrow \infty} f_{n}^{\prime}(x)
$$

and this is (27), by the definition of $\phi(t)$.

Remark: If the continuity of the functions $f_{n}^{\prime}$ is assumed in addition to the above hypotheses, then a much shorter proof of (27) can be based on Theorem 7.16 and the fundamental theorem of calculus.

7.18 Theorem There exists a real continuous function on the real line which is nowhere differentiable.

Proof Define

$$
\varphi(x)=|x| \quad(-1 \leq x \leq 1)
$$

and extend the definition of $\varphi(x)$ to all real $x$ by requiring that

$$
\varphi(x+2)=\varphi(x)
$$

Then, for all $s$ and $t$,

$$
|\varphi(s)-\varphi(t)| \leq|s-t|
$$

In particular, $\varphi$ is continuous on $R^{1}$. Define

$$
f(x)=\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n} \varphi\left(4^{n} x\right)
$$

Since $0 \leq \varphi \leq 1$, Theorem 7.10 shows that the series (37) converges uniformly on $R^{1}$. By Theorem 7.12, $f$ is continuous on $R^{1}$.

Now fix a real number $x$ and a positive integer $m$. Put

$$
\delta_{m}= \pm \frac{1}{2} \cdot 4^{-m}
$$

where the sign is so chosen that no integer lies between $4^{m} x$ and $4^{m}\left(x+\delta_{m}\right)$. This can be done, since $4^{m}\left|\delta_{m}\right|=\frac{1}{2}$. Define

$$
\gamma_{n}=\frac{\varphi\left(4^{n}\left(x+\delta_{m}\right)\right)-\varphi\left(4^{n} x\right)}{\delta_{m}}
$$

When $n>m$, then $4^{n} \delta_{m}$ is an even integer, so that $\gamma_{n}=0$. When $0 \leq n \leq m$, (36) implies that $\left|\gamma_{n}\right| \leq 4^{n}$.

Since $\left|\gamma_{m}\right|=4^{m}$, we conclude that

$$
\begin{aligned}
\left|\frac{f\left(x+\delta_{m}\right)-f(x)}{\delta_{m}}\right| & =\left|\sum_{n=0}^{m}\left(\frac{3}{4}\right)^{n} \gamma_{n}\right| \\
& \geq 3^{m}-\sum_{n=0}^{m-1} 3^{n} \\
& =\frac{1}{2}\left(3^{m}+1\right) .
\end{aligned}
$$

As $m \rightarrow \infty, \delta_{m} \rightarrow 0$. It follows that $f$ is not differentiable at $x$.

## EQUICONTINUOUS FAMILIES OF FUNCTIONS

In Theorem 3.6 we saw that every bounded sequence of complex numbers contains a convergent subsequence, and the question arises whether something similar is true for sequences of functions. To make the question more precise, we shall define two kinds of boundedness.

7.19 Definition Let $\left\{f_{n}\right\}$ be a sequence of functions defined on a set $E$.

We say that $\left\{f_{n}\right\}$ is pointwise bounded on $E$ if the sequence $\left\{f_{n}(x)\right\}$ is bounded for every $x \in E$, that is, if there exists a finite-valued function $\phi$ defined on $E$ such that

$$
\left|f_{n}(x)\right|<\phi(x) \quad(x \in E, n=1,2,3, \ldots)
$$

We say that $\left\{f_{n}\right\}$ is uniformly bounded on $E$ if there exists a number $M$ such that

$$
\left|f_{n}(x)\right|<M \quad(x \in E, n=1,2,3, \ldots)
$$

Now if $\left\{f_{n}\right\}$ is pointwise bounded on $E$ and $E_{1}$ is a countable subset of $E$, it is always possible to find a subsequence $\left\{f_{n_{k}}\right\}$ such that $\left\{f_{n_{k}}(x)\right\}$ converges for every $x \in E_{1}$. This can be done by the diagonal process which is used in the proof of Theorem 7.23.

However, even if $\left\{f_{n}\right\}$ is a uniformly bounded sequence of continuous functions on a compact set $E$, there need not exist a subsequence which converges pointwise on $E$. In the following example, this would be quite troublesome to prove with the equipment which we have at hand so far, but the proof is quite simple if we appeal to a theorem from Chap. 11.

7.20 Example Let

$$
f_{n}(x)=\sin n x \quad(0 \leq x \leq 2 \pi, n=1,2,3, \ldots)
$$

Suppose there exists a sequence $\left\{n_{k}\right\}$ such that $\left\{\sin n_{k} x\right\}$ converges, for every $x \in[0,2 \pi]$. In that case we must have

$$
\lim _{k \rightarrow \infty}\left(\sin n_{k} x-\sin n_{k+1} x\right)=0 \quad(0 \leq x \leq 2 \pi)
$$

hence

$$
\lim _{k \rightarrow \infty}\left(\sin n_{k} x-\sin n_{k+1} x\right)^{2}=0 \quad(0 \leq x \leq 2 \pi)
$$

By Lebesgue's theorem concerning integration of boundedly convergent sequences (Theorem 11.32), (40) implies

$$
\lim _{k \rightarrow \infty} \int_{0}^{2 \pi}\left(\sin n_{k} x-\sin n_{k+1} x\right)^{2} d x=0
$$

But a simple calculation shows that

$$
\int_{0}^{2 \pi}\left(\sin n_{k} x-\sin n_{k+1} x\right)^{2} d x=2 \pi
$$

which contradicts (41).

Another question is whether every convergent sequence contains a uniformly convergent subsequence. Our next example will show that this need not be so, even if the sequence is uniformly bounded on a compact set. (Example 7.6 shows that a sequence of bounded functions may converge without being uniformly bounded; but it is trivial to see that uniform convergence of a sequence of bounded functions implies uniform boundedness.)

### 7.21 Example Let

$$
f_{n}(x)=\frac{x^{2}}{x^{2}+(1-n x)^{2}} \quad(0 \leq x \leq 1, n=1,2,3, \ldots) .
$$

Then $\left|f_{n}(x)\right| \leq 1$, so that $\left\{f_{n}\right\}$ is uniformly bounded on $[0,1]$. Also

$$
\lim _{n \rightarrow \infty} f_{n}(x)=0 \quad(0 \leq x \leq 1)
$$

but

$$
f_{n}\left(\frac{1}{n}\right)=1 \quad(n=1,2,3, \ldots)
$$

so that no subsequence can converge uniformly on $[0,1]$.

The concept which is needed in this connection is that of equicontinuity; it is given in the following definition.

7.22 Definition A family $\mathscr{F}$ of complex functions $f$ defined on a set $E$ in a metric space $X$ is said to be equicontinuous on $E$ if for every $\varepsilon>0$ there exists a $\delta>0$ such that

$$
|f(x)-f(y)|<\varepsilon
$$

whenever $d(x, y)<\delta, x \in E, y \in E$, and $f \in \mathscr{F}$. Here $d$ denotes the metric of $X$.

It is clear that every member of an equicontinuous family is uniformly continuous.

The sequence of Example 7.21 is not equicontinuous.

Theorems 7.24 and 7.25 will show that there is a very close relation between equicontinuity, on the one hand, and uniform convergence of sequences of continuous functions, on the other. But first we describe a selection process which has nothing to do with continuity.

7.23 Theorem If $\left\{f_{n}\right\}$ is a pointwise bounded sequence of complex functions on a countable set $E$, then $\left\{f_{n}\right\}$ has a subsequence $\left\{f_{n_{k}}\right\}$ such that $\left\{f_{n_{k}}(x)\right\}$ converges for every $x \in E$.

Proof Let $\left\{x_{i}\right\}, i=1,2,3, \ldots$, be the points of $E$, arranged in a sequence. Since $\left\{f_{n}\left(x_{1}\right)\right\}$ is bounded, there exists a subsequence, which we shall denote by $\left\{f_{1, k}\right\}$, such that $\left\{f_{1, k}\left(x_{1}\right)\right\}$ converges as $k \rightarrow \infty$.

Let us now consider sequences $S_{1}, S_{2}, S_{3}, \ldots$, which we represent by the array

$$
\begin{array}{cccccc}
S_{1}: & f_{1,1} & f_{1,2} & f_{1,3} & f_{1,4} & \cdots \\
S_{2}: & f_{2,1} & f_{2,2} & f_{2,3} & f_{2,4} & \cdots \\
S_{3}: & f_{3,1} & f_{3,2} & f_{3,3} & f_{3,4} & \cdots
\end{array}
$$

and which have the following properties:

(a) $S_{n}$ is a subsequence of $S_{n-1}$, for $n=2,3,4, \ldots$

(b) $\left\{f_{n, k}\left(x_{n}\right)\right\}$ converges, as $k \rightarrow \infty$ (the boundedness of $\left\{f_{n}\left(x_{n}\right)\right\}$ makes it possible to choose $S_{n}$ in this way);

(c) The order in which the functions appear is the same in each sequence; i.e., if one function precedes another in $S_{1}$, they are in the same relation in every $S_{n}$, until one or the other is deleted. Hence, when going from one row in the above array to the next below, functions may move to the left but never to the right.

We now go down the diagonal of the array; i.e., we consider the sequence

$$
S: f_{1,1} \quad f_{2,2} \quad f_{3,3} \quad f_{4,4} \cdots \text {. }
$$

By $(c)$, the sequence $S$ (except possibly its first $n-1$ terms) is a subsequence of $S_{n}$, for $n=1,2,3, \ldots$. Hence $(b)$ implies that $\left\{f_{n, n}\left(x_{i}\right)\right\}$ converges, as $n \rightarrow \infty$, for every $x_{i} \in E$.

7.24 Theorem If $K$ is a compact metric space, if $f_{n} \in \mathscr{C}(K)$ for $n=1,2,3, \ldots$, and if $\left\{f_{n}\right\}$ converges uniformly on $K$, then $\left\{f_{n}\right\}$ is equicontinuous on $K$.

Proof Let $\varepsilon>0$ be given. Since $\left\{f_{n}\right\}$ converges uniformly, there is an integer $N$ such that

$$
\left\|f_{n}-f_{N}\right\|<\varepsilon \quad(n>N)
$$

(See Definition 7.14.) Since continuous functions are uniformly continuous on compact sets, there is a $\delta>0$ such that

$$
\left|f_{i}(x)-f_{i}(y)\right|<\varepsilon
$$

if $1 \leq i \leq N$ and $d(x, y)<\delta$.

If $n>N$ and $d(x, y)<\delta$, it follows that

$$
\left|f_{n}(x)-f_{n}(y)\right| \leq\left|f_{n}(x)-f_{N}(x)\right|+\left|f_{N}(x)-f_{N}(y)\right|+\left|f_{N}(y)-f_{n}(y)\right|<3 \varepsilon
$$

In conjunction with (43), this proves the theorem.

7.25 Theorem If $K$ is compact, if $f_{n} \in \mathscr{C}(K)$ for $n=1,2,3, \ldots$, and if $\left\{f_{n}\right\}$ is pointwise bounded and equicontinuous on $K$, then

(a) $\left\{f_{n}\right\}$ is uniformly bounded on $K$,

(b) $\left\{f_{n}\right\}$ contains a uniformly convergent subsequence.

## Proof

(a) Let $\varepsilon>0$ be given and choose $\delta>0$, in accordance with Definition 7.22 , so that

$$
\left|f_{n}(x)-f_{n}(y)\right|<\varepsilon
$$

for all $n$, provided that $d(x, y)<\delta$.

Since $K$ is compact, there are finitely many points $p_{1}, \ldots, p_{r}$ in $K$ such that to every $x \in K$ corresponds at least one $p_{i}$ with $d\left(x, p_{i}\right)<\delta$. Since $\left\{f_{n}\right\}$ is pointwise bounded, there exist $M_{i}<\infty$ such that $\left|f_{n}\left(p_{i}\right)\right|<M_{i}$ for all $n$. If $M=\max \left(M_{1}, \ldots, M_{r}\right)$, then $\left|f_{n}(x)\right|<M+\varepsilon$ for every $x \in K$. This proves $(a)$.

(b) Let $E$ be a countable dense subset of $K$. (For the existence of such a set $E$, see Exercise 25, Chap. 2.) Theorem 7.23 shows that $\left\{f_{n}\right\}$ has a subsequence $\left\{f_{n_{l}}\right\}$ such that $\left\{f_{n_{t}}(x)\right\}$ converges for every $x \in E$.

Put $f_{n_{i}}=g_{i}$, to simplify the notation. We shall prove that $\left\{g_{i}\right\}$ converges uniformly on $K$.

Let $\varepsilon>0$, and pick $\delta>0$ as in the beginning of this proof. Let $V(x, \delta)$ be the set of all $y \in K$ with $d(x, y)<\delta$. Since $E$ is dense in $K$, and $K$ is compact, there are finitely many points $x_{1}, \ldots, x_{m}$ in $E$ such that

$$
K \subset V\left(x_{1}, \delta\right) \cup \cdots \cup V\left(x_{m}, \delta\right)
$$

Since $\left\{g_{i}(x)\right\}$ converges for every $x \in E$, there is an integer $N$ such that

$$
\left|g_{l}\left(x_{s}\right)-g_{j}\left(x_{s}\right)\right|<\varepsilon
$$

whenever $i \geq N, j \geq N, 1 \leq s \leq m$.

If $x \in K$, (45) shows that $x \in V\left(x_{s}, \delta\right)$ for some $s$, so that

$$
\left|g_{l}(x)-g_{l}\left(x_{s}\right)\right|<\varepsilon
$$

for every $i$. If $i \geq N$ and $j \geq N$, it follows from (46) that

$$
\begin{aligned}
\left|g_{i}(x)-g_{j}(x)\right| & \leq\left|g_{i}(x)-g_{i}\left(x_{s}\right)\right|+\left|g_{i}\left(x_{s}\right)-g_{j}\left(x_{s}\right)\right|+\left|g_{j}\left(x_{s}\right)-g_{j}(x)\right| \\
& <3 \varepsilon .
\end{aligned}
$$

This completes the proof.

## THE STONE-WEIERSTRASS THEOREM

7.26 Theorem If $f$ is a continuous complex function on $[a, b]$, there exists a sequence of polynomials $P_{n}$ such that

$$
\lim _{n \rightarrow \infty} P_{n}(x)=f(x)
$$

uniformly on $[a, b]$. If $f$ is real, the $P_{n}$ may be taken real.

This is the form in which the theorem was originally discovered by Weierstrass.

Proof We may assume, without loss of generality, that $[a, b]=[0,1]$.

We may also assume that $f(0)=f(1)=0$. For if the theorem is proved for this case, consider

$$
g(x)=f(x)-f(0)-x[f(1)-f(0)] \quad(0 \leq x \leq 1)
$$

Here $g(0)=g(1)=0$, and if $g$ can be obtained as the limit of a uniformly convergent sequence of polynomials, it is clear that the same is true for $f$, since $f-g$ is a polynomial.

Furthermore, we define $f(x)$ to be zero for $x$ outside $[0,1]$. Then $f$ is uniformly continuous on the whole line.

We put

$$
Q_{n}(x)=c_{n}\left(1-x^{2}\right)^{n} \quad(n=1,2,3, \ldots)
$$

where $c_{n}$ is chosen so that

$$
\int_{-1}^{1} Q_{n}(x) d x=1 \quad(n=1,2,3, \ldots)
$$

We need some information about the order of magnitude of $c_{n}$. Since

$$
\begin{aligned}
\int_{-1}^{1}\left(1-x^{2}\right)^{n} d x=2 \int_{0}^{1}\left(1-x^{2}\right)^{n} d x & \geq 2 \int_{0}^{1 / \sqrt{n}}\left(1-x^{2}\right)^{n} d x \\
& \geq 2 \int_{0}^{1 / \sqrt{n}}\left(1-n x^{2}\right) d x \\
& =\frac{4}{3 \sqrt{n}} \\
& >\frac{1}{\sqrt{n}},
\end{aligned}
$$

it follows from (48) that

$$
c_{n}<\sqrt{n}
$$

The inequality $\left(1-x^{2}\right)^{n} \geq 1-n x^{2}$ which we used above is easily shown to be true by considering the function

$$
\left(1-x^{2}\right)^{n}-1+n x^{2}
$$

which is zero at $x=0$ and whose derivative is positive in $(0,1)$.

For any $\delta>0$, (49) implies

$$
Q_{n}(x) \leq \sqrt{n}\left(1-\delta^{2}\right)^{n} \quad(\delta \leq|x| \leq 1)
$$

so that $Q_{n} \rightarrow 0$ uniformly in $\delta \leq|x| \leq 1$.

Now set

$$
P_{n}(x)=\int_{-1}^{1} f(x+t) Q_{n}(t) d t \quad(0 \leq x \leq 1)
$$

Our assumptions about $f$ show, by a simple change of variable, that

$$
P_{n}(x)=\int_{-x}^{1-x} f(x+t) Q_{n}(t) d t=\int_{0}^{1} f(t) Q_{n}(t-x) d t
$$

and the last integral is clearly a polynomial in $x$. Thus $\left\{P_{n}\right\}$ is a sequence of polynomials, which are real if $f$ is real.

Given $\varepsilon>0$, we choose $\delta>0$ such that $|y-x|<\delta$ implies

$$
|f(y)-f(x)|<\frac{\varepsilon}{2}
$$

Let $M=\sup |f(x)|$. Using (48), (50), and the fact that $Q_{n}(x) \geq 0$, we see that for $0 \leq x \leq 1$,

$$
\begin{aligned}
\left|P_{n}(x)-f(x)\right| & =\left|\int_{-1}^{1}[f(x+t)-f(x)] Q_{n}(t) d t\right| \\
& \leq \int_{-1}^{1}|f(x+t)-f(x)| Q_{n}(t) d t \\
& \leq 2 M \int_{-1}^{-\delta} Q_{n}(t) d t+\frac{\varepsilon}{2} \int_{-\delta}^{\delta} Q_{n}(t) d t+2 M \int_{\delta}^{1} Q_{n}(t) d t \\
& \leq 4 M \sqrt{n}\left(1-\delta^{2}\right)^{n}+\frac{\varepsilon}{2} \\
& <\varepsilon
\end{aligned}
$$

for all large enough $n$, which proves the theorem.

It is instructive to sketch the graphs of $Q_{n}$ for a few values of $n$; also, note that we needed uniform continuity of $f$ to deduce uniform convergence of $\left\{P_{n}\right\}$.

In the proof of Theorem 7.32 we shall not need the full strength of Theorem 7.26, but only the following special case, which we state as a corollary.

7.27 Corollary For every interval $[-a, a]$ there is a sequence of real polynomials $P_{n}$ such that $P_{n}(0)=0$ and such that

uniformly on $[-a, a]$.

$$
\lim _{n \rightarrow \infty} P_{n}(x)=|x|
$$

Proof By Theorem 7.26, there exists a sequence $\left\{P_{n}^{*}\right\}$ of real polynomials which converges to $|x|$ uniformly on $[-a, a]$. In particular, $P_{n}^{*}(0) \rightarrow 0$ as $n \rightarrow \infty$. The polynomials

$$
P_{n}(x)=P_{n}^{*}(x)-P_{n}^{*}(0) \quad(n=1,2,3, \ldots)
$$

have desired properties.

We shall now isolate those properties of the polynomials which make the Weierstrass theorem possible.

7.28 Definition A family $\mathscr{A}$ of complex functions defined on a set $E$ is said to be an algebra if (i) $f+g \in \mathscr{A}$, (ii) $f g \in \mathscr{A}$, and (iii) $c f \in \mathscr{A}$ for all $f \in \mathscr{A}, g \in \mathscr{A}$ and for all complex constants $c$, that is, if $\mathscr{A}$ is closed under addition, multiplication, and scalar multiplication. We shall also have to consider algebras of real functions; in this case, (iii) is of course only required to hold for all real $c$.

If $\mathscr{A}$ has the property that $f \in \mathscr{A}$ whenever $f_{n} \in \mathscr{A}(n=1,2,3, \ldots)$ and $f_{n} \rightarrow f$ uniformly on $E$, then $\mathscr{A}$ is said to be uniformly closed.

Let $\mathscr{B}$ be the set of all functions which are limits of uniformly convergent sequences of members of $\mathscr{A}$. Then $\mathscr{B}$ is called the uniform closure of $\mathscr{A}$. (See Definition 7.14.)

For example, the set of all polynomials is an algebra, and the Weierstrass theorem may be stated by saying that the set of continuous functions on $[a, b]$ is the uniform closure of the set of polynomials on $[a, b]$.

7.29 Theorem Let $\mathscr{B}$ be the uniform closure of an algebra $\mathscr{A}$ of bounded functions. Then $\mathscr{B}$ is a uniformly closed algebra.

Proof If $f \in \mathscr{B}$ and $g \in \mathscr{B}$, there exist uniformly convergent sequences $\left\{f_{n}\right\},\left\{g_{n}\right\}$ such that $f_{n} \rightarrow f, g_{n} \rightarrow g$ and $f_{n} \in \mathscr{A}, g_{n} \in \mathscr{A}$. Since we are dealing with bounded functions, it is easy to show that

$$
f_{n}+g_{n} \rightarrow f+g, \quad f_{n} g_{n} \rightarrow f g, \quad c f_{n} \rightarrow c f
$$

where $c$ is any constant, the convergence being uniform in each case.

Hence $f+g \in \mathscr{B}, f g \in \mathscr{B}$, and $c f \in \mathscr{B}$, so that $\mathscr{B}$ is an algebra.

By Theorem 2.27, $\mathscr{B}$ is (uniformly) closed.

7.30 Definition Let $\mathscr{A}$ be a family of functions on a set $E$. Then $\mathscr{A}$ is said to separate points on $E$ if to every pair of distinct points $x_{1}, x_{2} \in E$ there corresponds a function $f \in \mathscr{A}$ such that $f\left(x_{1}\right) \neq f\left(x_{2}\right)$.

If to each $x \in E$ there corresponds a function $g \in \mathscr{A}$ such that $g(x) \neq 0$, we say that $\mathscr{A}$ vanishes at no point of $E$.

The algebra of all polynomials in one variable clearly has these properties on $R^{1}$. An example of an algebra which does not separate points is the set of all even polynomials, say on $[-1,1]$, since $f(-x)=f(x)$ for every even function $f$.

The following theorem will illustrate these concepts further.

7.31 Theorem Suppose $\mathscr{A}$ is an algebra of functions on a set $E, \mathscr{A}$ separates points on $E$, and $\mathscr{A}$ vanishes at no point of $E$. Suppose $x_{1}, x_{2}$ are distinct points of $E$, and $c_{1}, c_{2}$ are constants (real if $\mathscr{A}$ is a real algebra). Then $\mathscr{A}$ contains a function $f$ such that

$$
f\left(x_{1}\right)=c_{1}, \quad f\left(x_{2}\right)=c_{2} .
$$

Proof The assumptions show that $\mathscr{A}$ contains functions $g, h$, and $k$ such that

$$
g\left(x_{1}\right) \neq g\left(x_{2}\right), \quad h\left(x_{1}\right) \neq 0, \quad k\left(x_{2}\right) \neq 0
$$

Put

$$
u=g k-g\left(x_{1}\right) k, \quad v=g h-g\left(x_{2}\right) h .
$$

Then $u \in \mathscr{A}, v \in \mathscr{A}, u\left(x_{1}\right)=v\left(x_{2}\right)=0, u\left(x_{2}\right) \neq 0$, and $v\left(x_{1}\right) \neq 0$. Therefore

$$
f=\frac{c_{1} v}{v\left(x_{1}\right)}+\frac{c_{2} u}{u\left(x_{2}\right)}
$$

has the desired properties.

We now have all the material needed for Stone's generalization of the Weierstrass theorem.

7.32 Theorem Let $\mathscr{A}$ be an algebra of real continuous functions on a compact set $K$. If $\mathscr{A}$ separates points on $K$ and if $\mathscr{A}$ vanishes at no point of $K$, then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all real continuous functions on $K$.

We shall divide the proof into four steps.

STEP 1 If $f \in \mathscr{B}$, then $|f| \in \mathscr{B}$.

Proof Let

$$
a=\sup |f(x)| \quad(x \in K)
$$

and let $\varepsilon>0$ be given. By Corollary 7.27 there exist real numbers $c_{1}, \ldots, c_{n}$ such that

$$
\left|\sum_{i=1}^{n} c_{i} y^{i}-\right| y||<\varepsilon \quad(-a \leq y \leq a)
$$

Since $\mathscr{B}$ is an algebra, the function

$$
g=\sum_{i=1}^{n} c_{i} f^{i}
$$

is a member of $\mathscr{B}$. By (52) and (53), we have

$$
|g(x)-| f(x)||<\varepsilon \quad(x \in K)
$$

Since $\mathscr{B}$ is uniformly closed, this shows that $|f| \in \mathscr{B}$.

STEP 2 If $f \in \mathscr{B}$ and $g \in \mathscr{B}$, then $\max (f, g) \in \mathscr{B}$ and $\min (f, g) \in \mathscr{B}$.

By $\max (f, g)$ we mean the function $h$ defined by

$$
h(x)= \begin{cases}f(x) & \text { if } f(x) \geq g(x), \\ g(x) & \text { if } f(x)<g(x)\end{cases}
$$

and $\min (f, g)$ is defined likewise.

Proof Step 2 follows from step 1 and the identities

$$
\begin{aligned}
& \max (f, g)=\frac{f+g}{2}+\frac{|f-g|}{2}, \\
& \min (f, g)=\frac{f+g}{2}-\frac{|f-g|}{2} .
\end{aligned}
$$

By iteration, the result can of course be extended to any finite set of functions: If $f_{1}, \ldots, f_{n} \in \mathscr{B}$, then $\max \left(f_{1}, \ldots, f_{n}\right) \in \mathscr{B}$, and

$$
\min \left(f_{1}, \ldots, f_{n}\right) \in \mathscr{B}
$$

STEP 3 Given a real function $f$, continuous on $K$, a point $x \in K$, and $\varepsilon>0$, there exists a function $g_{x} \in \mathscr{B}$ such that $g_{x}(x)=f(x)$ and

$$
g_{x}(t)>f(t)-\varepsilon \quad(t \in K) .
$$

Proof Since $\mathscr{A} \subset \mathscr{B}$ and $\mathscr{A}$ satisfies the hypotheses of Theorem 7.31 so does $\mathscr{B}$. Hence, for every $y \in K$, we can find a function $h_{y} \in \mathscr{B}$ such that

$$
h_{y}(x)=f(x), \quad h_{y}(y)=f(y) .
$$

such that

By the continuity of $h_{y}$ there exists an open set $J_{y}$, containing $y$,

$$
h_{y}(t)>f(t)-\varepsilon \quad\left(t \in J_{y}\right) .
$$

Since $K$ is compact, there is a finite set of points $y_{1}, \ldots, y_{n}$ such that

$$
K \subset J_{y_{1}} \cup \cdots \cup J_{y_{n}} .
$$

Put

$$
g_{x}=\max \left(h_{y_{1}}, \ldots, h_{y_{n}}\right)
$$

By step 2, $g_{x} \in \mathscr{B}$, and the relations (55) to (57) show that $g_{x}$ has the other required properties.

STEP 4 Given a real function $f$, continuous on $K$, and $\varepsilon>0$, there exists a function $h \in \mathscr{B}$ such that

$$
|h(x)-f(x)|<\varepsilon \quad(x \in K)
$$

Since $\mathscr{B}$ is uniformly closed, this statement is equivalent to the conclusion of the theorem.

Proof Let us consider the functions $g_{x}$, for each $x \in K$, constructed in step 3. By the continuity of $g_{x}$, there exist open sets $V_{x}$ containing $x$, such that

$$
g_{x}(t)<f(t)+\varepsilon \quad\left(t \in V_{x}\right)
$$

Since $K$ is compact, there exists a finite set of points $x_{1}, \ldots, x_{m}$ such that

$$
K \subset V_{x_{1}} \cup \cdots \cup V_{x_{m}} .
$$

Put

$$
h=\min \left(g_{x_{1}}, \ldots, g_{x_{m}}\right)
$$

By step 2, $h \in \mathscr{B}$, and (54) implies

$$
h(t)>f(t)-\varepsilon \quad(t \in K)
$$

whereas (59) and (60) imply

$$
h(t)<f(t)+\varepsilon \quad(t \in K)
$$

Finally, (58) follows from (61) and (62).

Theorem 7.32 does not hold for complex algebras. A counterexample is given in Exercise 21. However, the conclusion of the theorem does hold, even for complex algebras, if an extra condition is imposed on $\mathscr{A}$, namely, that $\mathscr{A}$ be self-adjoint. This means that for every $f \in \mathscr{A}$ its complex conjugate $f$ must also belong to $\mathscr{A} ; f$ is defined by $f(x)=\overline{f(x)}$.

7.33 Theorem Suppose $\mathscr{A}$ is a self-adjoint algebra of complex continuous functions on a compact set $K, \mathscr{A}$ separates points on $K$, and $\mathscr{A}$ vanishes at no point of $K$. Then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all complex continuous functions on $K$. In other words, $\mathscr{A}$ is dense $\mathscr{C}(K)$.

Proof Let $\mathscr{A}_{R}$ be the set of all real functions on $K$ which belong to $\mathscr{A}$. If $f \in \mathscr{A}$ and $f=u+i v$, with $u, v$ real, then $2 u=f+f$, and since $\mathscr{A}$ is self-adjoint, we see that $u \in \mathscr{A}_{R}$. If $x_{1} \neq x_{2}$, there exists $f \in \mathscr{A}$ such that $f\left(x_{1}\right)=1, f\left(x_{2}\right)=0$; hence $0=u\left(x_{2}\right) \neq u\left(x_{1}\right)=1$, which shows that $\mathscr{A}_{\mathrm{R}}$ separates points on $K$. If $x \in K$, then $g(x) \neq 0$ for some $g \in \mathscr{A}$, and there is a complex number $\lambda$ such that $\lambda g(x)>0$; if $f=\lambda g, f=u+i v$, it follows that $u(x)>0$; hence $\mathscr{A}_{R}$ vanishes at no point of $K$.

Thus $\mathscr{A}_{R}$ satisfies the hypotheses of Theorem 7.32. It follows that

every real continuous function on $K$ lies in the uniform closure of $\mathscr{A}_{R}$, hence lies in $\mathscr{B}$. If $f$ is a complex continuous function on $K, f=u+i v$, then $u \in \mathscr{B}, v \in \mathscr{B}$, hence $f \in \mathscr{B}$. This completes the proof.

## EXERCISES

1. Prove that every uniformly convergent sequence of bounded functions is uniformly bounded.
2. If $\left\{f_{n}\right\}$ and $\left\{g_{n}\right\}$ converge uniformly on a set $E$, prove that $\left\{f_{n}+g_{n}\right\}$ converges uniformly on $E$. If, in addition, $\left\{f_{n}\right\}$ and $\left\{g_{n}\right\}$ are sequences of bounded functions, prove that $\left\{f_{n} g_{n}\right\}$ converges uniformly on $E$.
3. Construct sequences $\left\{f_{n}\right\},\left\{g_{n}\right\}$ which converge uniformly on some set $E$, but such that $\left\{f_{n} g_{n}\right\}$ does not converge uniformly on $E$ (of course, $\left\{f_{n} g_{n}\right\}$ must converge on E).
4. Consider

$$
f(x)=\sum_{n=1}^{\infty} \frac{1}{1+n^{2} x} .
$$

For what values of $x$ does the series converge absolutely? On what intervals does it converge uniformly? On what intervals does it fail to converge uniformly? Is $f$ continuous wherever the series converges? Is $f$ bounded?

5. Let

$$
f_{n}(x)= \begin{cases}0 & \left(x<\frac{1}{n+1}\right) \\ \sin ^{2} \frac{\pi}{x} & \left(\frac{1}{n+1} \leq x \leq \frac{1}{n}\right) \\ 0 & \left(\frac{1}{n}<x\right)\end{cases}
$$

Show that $\left\{f_{n}\right\}$ converges to a continuous function, but not uniformly. Use the series $\Sigma f_{n}$ to show that absolute convergence, even for all $x$, does not imply uniform convergence.

6. Prove that the series

$$
\sum_{n=1}^{\infty}(-1)^{n} \frac{x^{2}+n}{n^{2}}
$$

converges uniformly in every bounded interval, but does not converge absolutely for any value of $x$.

7. For $n=1,2,3, \ldots, x$ real, put

$$
f_{n}(x)=\frac{x}{1+n x^{2}}
$$

Show that $\left\{f_{n}\right\}$ converges uniformly to a function $f$, and that the equation

$$
f^{\prime}(x)=\lim _{n \rightarrow \infty} f_{n}^{\prime}(x)
$$

is correct if $x \neq 0$, but false if $x=0$.

8. If

$$
I(x)= \begin{cases}0 & (x \leq 0) \\ 1 & (x>0)\end{cases}
$$

if $\left\{x_{n}\right\}$ is a sequence of distinct points of $(a, b)$, and if $\Sigma\left|c_{n}\right|$ converges, prove that the series

$$
f(x)=\sum_{n=1}^{\infty} c_{n} I\left(x-x_{n}\right) \quad(a \leq x \leq b)
$$

converges uniformly, and that $f$ is continuous for every $x \neq x_{n}$.

9. Let $\left\{f_{n}\right\}$ be a sequence of continuous functions which converges uniformly to a function $f$ on a set $E$. Prove that

$$
\lim _{n \rightarrow \infty} f_{n}\left(x_{n}\right)=f(x)
$$

for every sequence of points $x_{n} \in E$ such that $x_{n} \rightarrow x$, and $x \in E$. Is the converse of this true?

10. Letting $(x)$ denote the fractional part of the real number $x$ (see Exercise 16, Chap. 4 , for the definition), consider the function

$$
f(x)=\sum_{n=1}^{\infty} \frac{(n x)}{n^{2}} \quad(x \text { real })
$$

Find all discontinuities of $f$, and show that they form a countable dense set. Show that $f$ is nevertheless Riemann-integrable on every bounded interval.

11. Suppose $\left\{f_{n}\right\},\left\{g_{n}\right\}$ are defined on $E$, and

(a) $\Sigma f_{n}$ has uniformly bounded partial sums;

(b) $g_{n} \rightarrow 0$ uniformly on $E$;

(c) $g_{1}(x) \geq g_{2}(x) \geq g_{3}(x) \geq \cdots$ for every $x \in E$.

Prove that $\Sigma f_{n} g_{n}$ converges uniformly on $E$. Hint: Compare with Theorem 3.42 .

12. Suppose $g$ and $f_{n}(n=1,2,3, \ldots)$ are defined on $(0, \infty)$, are Riemann-integrable on $[t, T]$ whenever $0<t<T<\infty,\left|f_{n}\right| \leq g, f_{n} \rightarrow f$ uniformly on every compact subset of $(0, \infty)$, and

$$
\int_{0}^{\infty} g(x) d x<\infty
$$

Prove that

$$
\lim _{n \rightarrow \infty} \int_{0}^{\infty} f_{n}(x) d x=\int_{0}^{\infty} f(x) d x
$$

(See Exercises 7 and 8 of Chap. 6 for the relevant definitions.)

This is a rather weak form of Lebesgue's dominated convergence theorem (Theorem 11.32). Even in the context of the Riemann integral, uniform convergence can be replaced by pointwise convergence if it is assumed that $f \in \mathscr{R}$. (See the articles by F. Cunningham in Math. Mag., vol. 40, 1967, pp. 179-186, and by H. Kestelman in Amer. Math. Monthly, vol. 77, 1970, pp. 182-187.)

13. Assume that $\left\{f_{n}\right\}$ is a sequence of monotonically increasing functions on $R^{1}$ with $0 \leq f_{n}(x) \leq 1$ for all $x$ and all $n$.

(a) Prove that there is a function $f$ and a sequence $\left\{n_{k}\right\}$ such that

$$
f(x)=\lim _{k \rightarrow \infty} f_{n k}(x)
$$

for every $x \in R^{1}$. (The existence of such a pointwise convergent subsequence is usually called Helly's selection theorem.)

(b) If, moreover, $f$ is continuous, prove that $f_{n_{k}} \rightarrow f$ uniformly on compact sets.

Hint: (i) Some subsequence $\left\{f_{n}\right\}$ converges at all rational points $r$, say, to $f(r)$. (ii) Define $f(x)$, for any $x \in R^{1}$, to be $\sup f(r)$, the sup being taken over all $r \leq x$. (iii) Show that $f_{n s}(x) \rightarrow f(x)$ at every $x$ at which $f$ is continuous. (This is where monotonicity is strongly used.) (iv) A subsequence of $\left\{f_{n}\right\}$ converges at every point of discontinuity of $f$ since there are at most countably many such points. This proves $(a)$. To prove $(b)$, modify your proof of (iii) appropriately.

14. Let $f$ be a continuous real function on $R^{1}$ with the following properties: $0 \leq f(t) \leq 1, f(t+2)=f(t)$ for every $t$, and

$$
f(t)= \begin{cases}0 & (0 \leq t \leq t) \\ 1 & \left(\frac{8}{3} \leq t \leq 1\right) .\end{cases}
$$

Put $\Phi(t)=(x(t), y(t))$, where

$$
x(t)=\sum_{n=1}^{\infty} 2^{-n} f\left(3^{2 n-1} t\right), \quad y(t)=\sum_{n=1}^{\infty} 2^{-n} f\left(3^{2 n} t\right) .
$$

Prove that $\Phi$ is continuous and that $\Phi$ maps $I=[0,1]$ onto the unit square $I^{2} \subset R^{2}$. If fact, show that $\Phi$ maps the Cantor set onto $I^{2}$.

Hint: Each $\left(x_{0}, y_{0}\right) \in \boldsymbol{I}^{2}$ has the form

$$
x_{0}=\sum_{n=1}^{\infty} 2^{-n} a_{2 n-1}, \quad y_{0}=\sum_{n=1}^{\infty} 2^{-n} a_{2 n}
$$

where each $a_{t}$ is 0 or 1 . If

$$
t_{0}=\sum_{t=1}^{\infty} 3^{-t-1}\left(2 a_{t}\right)
$$

show that $f\left(3^{k} t_{0}\right)=a_{k}$, and hence that $x\left(t_{0}\right)=x_{0}, y\left(t_{0}\right)=y_{0}$.

(This simple example of a so-called "space-filling curve" is due to I. J. Schoenberg, Bull. A.M.S., vol. 44, 1938, pp. 519.)

15. Suppose $f$ is a real continuous function on $R^{1}, f_{n}(t)=f(n t)$ for $n=1,2,3, \ldots$, and $\left\{f_{n}\right\}$ is equicontinuous on $[0,1]$. What conclusion can you draw about $f$ ?
16. Suppose $\left\{f_{n}\right\}$ is an equicontinuous sequence of functions on a compact set $K$, and $\left\{f_{n}\right\}$ converges pointwise on $K$. Prove that $\left\{f_{n}\right\}$ converges uniformly on $K$.
17. Define the notions of uniform convergence and equicontinuity for mappings into any metric space. Show that Theorems 7.9 and 7.12 are valid for mappings into any metric space, that Theorems 7.8 and 7.11 are valid for mappings into any complete metric space, and that Theorems 7.10, 7.16, 7.17, 7.24, and 7.25 hold for vector-valued functions, that is, for mappings into any $R^{k}$.
18. Let $\left\{f_{n}\right\}$ be a uniformly bounded sequence of functions which are Riemann-integrable on $[a, b]$, and put

$$
F_{n}(x)=\int_{a}^{x} f_{n}(t) d t \quad(a \leq x \leq b)
$$

Prove that there exists a subsequence $\left\{F_{n_{k}}\right\}$ which converges uniformly on $[a, b]$.

19. Let $K$ be a compact metric space, let $S$ be a subset of $\mathscr{C}(K)$. Prove that $S$ is compact (with respect to the metric defined in Section 7.14) if and only if $S$ is uniformly closed, pointwise bounded, and equicontinuous. (If $S$ is not equicontinuous, then $S$ contains a sequence which has no equicontinuous subsequence, hence has no subsequence that converges uniformly on $K$.)
20. If $f$ is continuous on $[0,1]$ and if

$$
\int_{0}^{1} f(x) x^{n} d x=0 \quad(n=0,1,2, \ldots)
$$

prove that $f(x)=0$ on $[0,1]$. Hint: The integral of the product of $f$ with any polynomial is zero. Use the Weierstrass theorem to show that $\int_{0}^{1} f^{2}(x) d x=0$.

21. Let $K$ be the unit circle in the complex plane (i.e., the set of all $z$ with $|z|=1$ ), and let $\mathscr{A}$ be the algebra of all functions of the form

$$
f\left(e^{i \theta}\right)=\sum_{n=0}^{N} c_{n} e^{i n \theta} \quad(\theta \text { real })
$$

Then $\mathscr{A}$ separates points on $K$ and $\mathscr{A}$ vanishes at no point of $K$, but nevertheless there are continuous functions on $K$ which are not in the uniform closure of $\mathscr{A}$. Hint: For every $f \in \mathscr{A}$

$$
\int_{0}^{2 \pi} f\left(e^{\theta \theta}\right) e^{i \theta} d \theta=0
$$

and this is also true for every $f$ in the closure of $\mathscr{A}$.

22. Assume $f \in \mathscr{R}(\alpha)$ on $[a, b]$, and prove that there are polynomials $P_{n}$ such that

$$
\lim _{n \rightarrow \infty} \int_{a}^{b}\left|f-P_{n}\right|^{2} d \alpha=0
$$

(Compare with Exercise 12, Chap. 6.)

23. Put $P_{0}=0$, and define, for $n=0,1,2, \ldots$,

$$
P_{n+1}(x)=P_{n}(x)+\frac{x^{2}-P_{n}^{2}(x)}{2}
$$

Prove that

$$
\lim _{n \rightarrow \infty} P_{n}(x)=|x|,
$$

uniformly on $[-1,1]$.

(This makes it possible to prove the Stone-Weierstrass theorem without first proving Theorem 7.26.)

Hint: Use the identity

$$
|x|-P_{n+1}(x)=\left[|x|-P_{n}(x)\right]\left[1-\frac{|x|+P_{n}(x)}{2}\right]
$$

to prove that $0 \leq P_{n}(x) \leq P_{n+1}(x) \leq|x|$ if $|x| \leq 1$, and that

$$
|x|-P_{n}(x) \leq|x|\left(1-\frac{|x|}{2}\right)^{n}<\frac{2}{n+1}
$$

if $|x| \leq 1$.

24. Let $X$ be a metric space, with metric $d$. Fix a point $a \in X$. Assign to each $p \in X$ the function $f_{p}$ defined by

$$
f_{p}(x)=d(x, p)-d(x, a) \quad(x \in X) .
$$

Prove that $\left|f_{p}(x)\right| \leq d(a, p)$ for all $x \in X$, and that therefore $f_{p} \in \mathscr{C}(X)$.

Prove that

$$
\left\|f_{p}-f_{q}\right\|=d(p, q)
$$

for all $p, q \in X$.

If $\Phi(p)=f_{p}$ it follows that $\Phi$ is an isometry (a distance-preserving mapping) of $X$ onto $\Phi(X) \subset \mathscr{C}(X)$.

Let $Y$ be the closure of $\Phi(X)$ in $\mathscr{C}(X)$. Show that $Y$ is complete.

Conclusion: $X$ is isometric to a dense subset of a complete metric space $Y$. (Exercise 24, Chap. 3 contains a different proof of this.)

25. Suppose $\phi$ is a continuous bounded real function in the strip defined by $0 \leq x \leq 1,-\infty<y<\infty$. Prove that the initial-value problem

$$
y^{\prime}=\phi(x, y), \quad y(0)=c
$$

has a solution. (Note that the hypotheses of this existence theorem are less stringent than those of the corresponding uniqueness theorem; see Exercise 27, Chap. 5.)

Hint: Fix $n$. For $i=0, \ldots, n$, put $x_{t}=i / n$. Let $f_{n}$ be a continuous function on $[0,1]$ such that $f_{n}(0)=c$,

$$
f_{n}^{\prime}(t)=\phi\left(x_{l}, f_{n}\left(x_{l}\right)\right) \quad \text { if } x_{l}<t<x_{l+1},
$$

and put

$$
\Delta_{n}(t)=f_{n}^{\prime}(t)-\phi\left(t, f_{n}(t)\right)
$$

except at the points $x_{t}$, where $\Delta_{n}(t)=0$. Then

$$
f_{n}(x)=c+\int_{0}^{x}\left[\phi\left(t, f_{n}(t)\right)+\Delta_{n}(t)\right] d t .
$$

Choose $M<\infty$ so that $|\phi| \leq M$. Verify the following assertions.

(a) $\left|f_{n}^{\prime}\right| \leq M,\left|\Delta_{n}\right| \leq 2 M, \Delta_{n} \in \mathscr{R}$, and $\left|f_{n}\right| \leq|c|+M=M_{1}$, say, on [0, 1], for all $n$.

(b) $\left\{f_{n}\right\}$ is equicontinuous on $[0,1]$, since $\left|f_{n}^{\prime}\right| \leq M$.

(c) Some $\left\{f_{n_{k}}\right\}$ converges to some $f$, uniformly on $[0,1]$.

(d) Since $\phi$ is uniformly continuous on the rectangle $0 \leq x \leq 1,|y| \leq M_{1}$,

$$
\phi\left(t, f_{n_{k}}(t)\right) \rightarrow \phi(t, f(t))
$$

uniformly on $[0,1]$.

(e) $\Delta_{n}(t) \rightarrow 0$ uniformly on $[0,1]$, since

$$
\Delta_{n}(t)=\phi\left(x_{i}, f_{n}\left(x_{i}\right)\right)-\phi\left(t, f_{n}(t)\right)
$$

in $\left(x_{i}, x_{l+1}\right)$.

( $f$ ) Hence

$$
f(x)=c+\int_{0}^{x} \phi(t, f(t)) d t
$$

This $f$ is a solution of the given problem.

26. Prove an analogous existence theorem for the initial-value problem

$$
\mathbf{y}^{\prime}=\Phi(x, y), \quad y(0)=c
$$

where now $\mathrm{c} \in R^{k}, \mathrm{y} \in R^{k}$, and $\Phi$ is a continuous bounded mapping of the part of $R^{k+1}$ defined by $0 \leq x \leq 1, y \in R^{k}$ into $R^{k}$. (Compare Exercise 28, Chap. 5.) Hint: Use the vector-valued version of Theorem 7.25.

## SOME SPECIAL FUNCTIONS

## POWER SERIES

In this section we shall derive some properties of functions which are represented by power series, i.e., functions of the form

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n}
$$

or, more generally,

$$
f(x)=\sum_{n=0}^{\infty} c_{n}(x-a)^{n}
$$

These are called analytic functions.

We shall restrict ourselves to real values of $x$. Instead of circles of convergence (see Theorem 3.39) we shall therefore encounter intervals of convergence.

If (1) converges for all $x$ in $(-R, R)$, for some $R>0(R$ may be $+\infty)$, we say that $f$ is expanded in a power series about the point $x=0$. Similarly, if (2) converges for $|x-a|<R, f$ is said to be expanded in a power series about the point $x=a$. As a matter of convenience, we shall often take $a=0$ without any loss of generality.

8.1 Theorem Suppose the series

$$
\sum_{n=0}^{\infty} c_{n} x^{n}
$$

converges for $|x|<R$, and define

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n} \quad(|x|<R)
$$

Then (3) converges uniformly on $[-R+\varepsilon, R-\varepsilon]$, no matter which $\varepsilon>0$ is chosen. The function $f$ is continuous and differentiable in $(-R, R)$, and

$$
f^{\prime}(x)=\sum_{n=1}^{\infty} n c_{n} x^{n-1} \quad(|x|<R)
$$

Proof Let $\varepsilon>0$ be given. For $|x| \leq R-\varepsilon$, we have

$$
\left|c_{n} x^{n}\right| \leq\left|c_{n}(R-\varepsilon)^{n}\right|
$$

and since

$$
\Sigma c_{n}(R-\varepsilon)^{n}
$$

converges absolutely (every power series converges absolutely in the interior of its interval of convergence, by the root test), Theorem 7.10 shows the uniform convergence of (3) on $[-R+\varepsilon, R-\varepsilon]$.

Since $\sqrt[n]{n} \rightarrow 1$ as $n \rightarrow \infty$, we have

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{n\left|c_{n}\right|}=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|},
$$

so that the series (4) and (5) have the same interval of convergence.

Since (5) is a power series, it converges uniformly in $[-R+\varepsilon$, $R-\varepsilon$ ], for every $\varepsilon>0$, and we can apply Theorem 7.17 (for series instead of sequences). It follows that (5) holds if $|x| \leq R-\varepsilon$.

But, given any $x$ such that $|x|<R$, we can find an $\varepsilon>0$ such that $|x|<R-\varepsilon$. This shows that (5) holds for $|x|<R$.

Continuity of $f$ follows from the existence of $f^{\prime}$ (Theorem 5.2).

Corollary Under the hypotheses of Theorem 8.1, $f$ has derivatives of all orders in $(-R, R)$, which are given by

In particular,

$$
f^{(k)}(x)=\sum_{n=k}^{\infty} n(n-1) \cdots(n-k+1) c_{n} x^{n-k}
$$

$$
f^{(k)}(0)=k ! c_{k} \quad(k=0,1,2, \ldots)
$$

(Here $f^{(0)}$ means $f$, and $f^{(k)}$ is the $k$ th derivative of $f$, for $k=1,2,3, \ldots$ ).

Proof Equation (6) follows if we apply Theorem 8.1 successively to $f$, $f^{\prime}, f^{\prime \prime}, \ldots$ Putting $x=0$ in (6), we obtain (7).

Formula (7) is very interesting. It shows, on the one hand, that the coefficients of the power series development of $f$ are determined by the values of $f$ and of its derivatives at a single point. On the other hand, if the coefficients are given, the values of the derivatives of $f$ at the center of the interval of convergence can be read off immediately from the power series.

Note, however, that although a function $f$ may have derivatives of all orders, the series $\Sigma c_{n} x^{n}$, where $c_{n}$ is computed by (7), need not converge to $f(x)$ for any $x \neq 0$. In this case, $f$ cannot be expanded in a power series about $x=0$. For if we had $f(x)=\Sigma a_{n} x^{n}$, we should have

$$
n ! a_{n}=f^{(n)}(0)
$$

hence $a_{n}=c_{n}$. An example of this situation is given in Exercise 1 .

If the series (3) converges at an endpoint, say at $x=R$, then $f$ is continuous not only in $(-R, R)$, but also at $x=R$. This follows from Abel's theorem (for simplicity of notation, we take $R=1)$ :

8.2 Theorem Suppose $\Sigma c_{n}$ converges. Put

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n} \quad(-1<x<1) .
$$

Then

$$
\lim _{x \rightarrow 1} f(x)=\sum_{n=0}^{\infty} c_{n}
$$

Proof Let $s_{n}=c_{0}+\cdots+c_{n}, s_{-1}=0$. Then

$$
\sum_{n=0}^{m} c_{n} x^{n}=\sum_{n=0}^{m}\left(s_{n}-s_{n-1}\right) x^{n}=(1-x) \sum_{n=0}^{m-1} s_{n} x^{n}+s_{m} x^{m}
$$

For $|x|<1$, we let $m \rightarrow \infty$ and obtain

$$
f(x)=(1-x) \sum_{n=0}^{\infty} s_{n} x^{n}
$$

Suppose $s=\lim _{n \rightarrow \infty} s_{n}$. Let $\varepsilon>0$ be given. Choose $N$ so that $n>N$ implies

$$
\left|s-s_{n}\right|<\frac{\varepsilon}{2}
$$

Then, since

$$
(1-x) \sum_{n=0}^{\infty} x^{n}=1 \quad(|x|<1)
$$

we obtain from (9)

$$
|f(x)-s|=\left|(1-x) \sum_{n=0}^{\infty}\left(s_{n}-s\right) x^{n}\right| \leq(1-x) \sum_{n=0}^{N}\left|s_{n}-s\right||x|^{n}+\frac{\varepsilon}{2} \leq \varepsilon
$$

if $x>1-\delta$, for some suitably chosen $\delta>0$. This implies (8).

As an application, let us prove Theorem 3.51, which asserts: If $\Sigma a_{n}, \Sigma b_{n}$, $\Sigma c_{n}$, converge to $A, B, C$, and if $c_{n}=a_{0} b_{n}+\cdots+a_{n} b_{0}$, then $C=A B$. We let

$$
f(x)=\sum_{n=0}^{\infty} a_{n} x^{n}, \quad g(x)=\sum_{n=0}^{\infty} b_{n} x^{n}, \quad h(x)=\sum_{n=0}^{\infty} c_{n} x^{n}
$$

for $0 \leq x \leq 1$. For $x<1$, these series converge absolutely and hence may be multiplied according to Definition 3.48; when the multiplication is carried out, we see that

$$
f(x) \cdot g(x)=h(x) \quad(0 \leq x<1) .
$$

By Theorem 8.2,

$$
f(x) \rightarrow A, \quad g(x) \rightarrow B, \quad h(x) \rightarrow C
$$

as $x \rightarrow 1$. Equations (10) and (11) imply $A B=C$.

We now require a theorem concerning an inversion in the order of summation. (See Exercises 2 and 3.)

8.3 Theorem Given a double sequence $\left\{a_{i j}\right\}, i=1,2,3, \ldots, j=1,2,3, \ldots$, suppose that

$$
\sum_{j=1}^{\infty}\left|a_{i j}\right|=b_{i} \quad(i=1,2,3, \ldots)
$$

and $\Sigma b_{i}$ converges. Then

$$
\sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{i j}=\sum_{j=1}^{\infty} \sum_{i=1}^{\infty} a_{i j}
$$

Proof We could establish (13) by a direct procedure similar to (although more involved than) the one used in Theorem 3.55. However, the following method seems more interesting.

Let $E$ be a countable set, consisting of the points $x_{0}, x_{1}, x_{2}, \ldots$, and suppose $x_{n} \rightarrow x_{0}$ as $n \rightarrow \infty$. Define

$$
\begin{aligned}
f_{i}\left(x_{0}\right)=\sum_{j=1}^{\infty} a_{i j} & (i=1,2,3, \ldots), \\
f_{i}\left(x_{n}\right)=\sum_{j=1}^{n} a_{i j} & (i, n=1,2,3, \ldots), \\
g(x)=\sum_{i=1}^{\infty} f_{i}(x) & (x \in E) .
\end{aligned}
$$

Now, (14) and (15), together with (12), show that each $f_{i}$ is continuous at $x_{0}$. Since $\left|f_{i}(x)\right| \leq b_{i}$ for $x \in E$, (16) converges uniformly, so that $g$ is continuous at $x_{0}$ (Theorem 7.11). It follows that

$$
\begin{aligned}
\sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{i j} & =\sum_{i=1}^{\infty} f_{i}\left(x_{0}\right)=g\left(x_{0}\right)=\lim _{n \rightarrow \infty} g\left(x_{n}\right) \\
& =\lim _{n \rightarrow \infty} \sum_{i=1}^{\infty} f_{i}\left(x_{n}\right)=\lim _{n \rightarrow \infty} \sum_{i=1}^{\infty} \sum_{j=1}^{n} a_{i j} \\
& =\lim _{n \rightarrow \infty} \sum_{j=1}^{n} \sum_{i=1}^{\infty} a_{i j}=\sum_{j=1}^{\infty} \sum_{i=1}^{\infty} a_{i j} .
\end{aligned}
$$

### 8.4 Theorem Suppose

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n}
$$

the series converging in $|x|<R$. If $-R<a<R$, then $f$ can be expanded in a power series about the point $x=a$ which converges in $|x-a|<R-|a|$, and

$$
f(x)=\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^{n} \quad(|x-a|<R-|a|) .
$$

This is an extension of Theorem 5.15 and is also known as Taylor's theorem.

Proof We have

$$
\begin{aligned}
f(x) & =\sum_{n=0}^{\infty} c_{n}[(x-a)+a]^{n} \\
& =\sum_{n=0}^{\infty} c_{n} \sum_{m=0}^{n}\left(\begin{array}{l}
n \\
m
\end{array}\right) a^{n-m}(x-a)^{m} \\
& =\sum_{m=0}^{\infty}\left[\sum_{n=m}^{\infty}\left(\begin{array}{l}
n \\
m
\end{array}\right) c_{n} a^{n-m}\right](x-a)^{m} .
\end{aligned}
$$

This is the desired expansion about the point $x=a$. To prove its validity, we have to justify the change which was made in the order of summation. Theorem 8.3 shows that this is permissible if

$$
\sum_{n=0}^{\infty} \sum_{m=0}^{n}\left|c_{n}\left(\begin{array}{l}
n \\
m
\end{array}\right) a^{n-m}(x-a)^{m}\right|
$$

converges. But (18) is the same as

$$
\sum_{n=0}^{\infty}\left|c_{n}\right| \cdot(|x-a|+|a|)^{n}
$$

and (19) converges if $|x-a|+|a|<R$.

Finally, the form of the coefficients in (17) follows from (7).

It should be noted that (17) may actually converge in a larger interval than the one given by $|x-a|<R-|a|$.

If two power series converge to the same function in $(-R, R),(7)$ shows that the two series must be identical, i.e., they must have the same coefficients. It is interesting that the same conclusion can be deduced from much weaker hypotheses:

8.5 Theorem Suppose the series $\Sigma a_{n} x^{n}$ and $\Sigma b_{n} x^{n}$ converge in the segment $S=(-R, R)$. Let $E$ be the set of all $x \in S$ at which

$$
\sum_{n=0}^{\infty} a_{n} x^{n}=\sum_{n=0}^{\infty} b_{n} x^{n}
$$

If $E$ has a limit point in $S$, then $a_{n}=b_{n}$ for $n=0,1,2, \ldots$ Hence (20) holds for all $x \in S$.

Proof Put $c_{n}=a_{n}-b_{n}$ and

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n} \quad(x \in S)
$$

Then $f(x)=0$ on $E$.

Let $A$ be the set of all limit points of $E$ in $S$, and let $B$ consist of all other points of $S$. It is clear from the definition of "limit point" that $B$ is open. Suppose we can prove that $A$ is open. Then $A$ and $B$ are disjoint open sets. Hence they are separated (Definition 2.45). Since $S=A \cup B$, and $S$ is connected, one of $A$ and $B$ must be empty. By hypothesis, $A$ is not empty. Hence $B$ is empty, and $A=S$. Since $f$ is continuous in $S$, $A \subset E$. Thus $E=S$, and (7) shows that $c_{n}=0$ for $n=0,1,2, \ldots$, which is the desired conclusion.
that

Thus we have to prove that $A$ is open. If $x_{0} \in A$, Theorem 8.4 shows

$$
f(x)=\sum_{n=0}^{\infty} d_{n}\left(x-x_{0}\right)^{n} \quad\left(\left|x-x_{0}\right|<R-\left|x_{0}\right|\right) .
$$

We claim that $d_{n}=0$ for all $n$. Otherwise, let $k$ be the smallest nonnegative integer such that $d_{k} \neq 0$. Then

$$
f(x)=\left(x-x_{0}\right)^{k} g(x) \quad\left(\left|x-x_{0}\right|<R-\left|x_{0}\right|\right)
$$

where

$$
g(x)=\sum_{m=0}^{\infty} d_{k+m}\left(x-x_{0}\right)^{m}
$$

Since $g$ is continuous at $x_{0}$ and

$$
g\left(x_{0}\right)=d_{k} \neq 0,
$$

there exists a $\delta>0$ such that $g(x) \neq 0$ if $\left|x-x_{0}\right|<\delta$. It follows from (23) that $f(x) \neq 0$ if $0<\left|x-x_{0}\right|<\delta$. But this contradicts the fact that $x_{0}$ is a limit point of $E$.

Thus $d_{n}=0$ for all $n$, so that $f(x)=0$ for all $x$ for which (22) holds, i.e., in a neighborhood of $x_{0}$. This shows that $A$ is open, and completes the proof.

## THE EXPONENTIAL AND LOGARITHMIC FUNCTIONS

We define

$$
E(z)=\sum_{n=0}^{\infty} \frac{z^{n}}{n !}
$$

The ratio test shows that this series converges for every complex $z$. Applying Theorem 3.50 on multiplication of absolutely convergent series, we obtain

$$
\begin{aligned}
E(z) E(w) & =\sum_{n=0}^{\infty} \frac{z^{n}}{n !} \sum_{m=0}^{\infty} \frac{w^{m}}{m !}=\sum_{n=0}^{\infty} \sum_{k=0}^{n} \frac{z^{k} w^{n-k}}{k !(n-k) !} \\
& =\sum_{n=0}^{\infty} \frac{1}{n !} \sum_{k=0}^{n}\left(\begin{array}{l}
n \\
k
\end{array}\right) z^{k} w^{n-k}=\sum_{n=0}^{\infty} \frac{(z+w)^{n}}{n !},
\end{aligned}
$$

which gives us the important addition formula

$$
E(z+w)=E(z) E(w) \quad(z, w \text { complex })
$$

One consequence is that

$$
E(z) E(-z)=E(z-z)=E(0)=1 \quad(z \text { complex })
$$

This shows that $E(z) \neq 0$ for all $z$. By (25), $E(x)>0$ if $x>0$; hence (27) shows that $E(x)>0$ for all real $x$. By (25), $E(x) \rightarrow+\infty$ as $x \rightarrow+\infty$; hence (27) shows that $E(x) \rightarrow 0$ as $x \rightarrow-\infty$ along the real axis. By (25), $0<x<y$ implies that $E(x)<E(y)$; by (27), it follows that $E(-y)<E(-x)$; hence $E$ is strictly increasing on the whole real axis.

The addition formula also shows that

$$
\lim _{h=0} \frac{E(z+h)-E(z)}{h}=E(z) \lim _{h=0} \frac{E(h)-1}{h}=E(z)
$$

the last equality follows directly from (25).

Iteration of (26) gives

$$
E\left(z_{1}+\cdots+z_{n}\right)=E\left(z_{1}\right) \cdots E\left(z_{n}\right) .
$$

Let us take $z_{1}=\cdots=z_{n}=1$. Since $E(1)=e$, where $e$ is the number defined in Definition 3.30, we obtain

$$
E(n)=e^{n} \quad(n=1,2,3, \ldots) .
$$

If $p=n / m$, where $n, m$ are positive integers, then

$$
[E(p)]^{m}=E(m p)=E(n)=e^{n}
$$

so that

$$
E(p)=e^{p} \quad(p>0, p \text { rational })
$$

It follows from (27) that $E(-p)=e^{-p}$ if $p$ is positive and rational. Thus (32) holds for all rational $p$.

In Exercise 6, Chap. 1, we suggested the definition

$$
x^{y}=\sup x^{p}
$$

where the sup is taken over all rational $p$ such that $p<y$, for any real $y$, and $x>1$. If we thus define, for any real $x$,

$$
e^{x}=\sup e^{p} \quad(p<x, p \text { rational })
$$

the continuity and monotonicity properties of $E$, together with (32), show that

$$
E(x)=e^{x}
$$

for all real $x$. Equation (35) explains why $E$ is called the exponential function.

The notation $\exp (x)$ is often used in place of $e^{x}$, expecially when $x$ is a complicated expression.

Actually one may very well use (35) instead of (34) as the definition of $e^{x}$; (35) is a much more convenient starting point for the investigation of the properties of $e^{x}$. We shall see presently that (33) may also be replaced by a more convenient definition [see (43)].

We now revert to the customary notation, $e^{x}$, in place of $E(x)$, and summarize what we have proved so far.

8.6 Theorem Let $e^{x}$ be defined on $R^{1}$ by (35) and (25). Then

(a) $e^{x}$ is continuous and differentiable for all $x$;

(b) $\left(e^{x}\right)^{\prime}=e^{x}$;

(c) $e^{x}$ is a strictly increasing function of $x$, and $e^{x}>0$;

(d) $e^{x+y}=e^{x} e^{y}$;

(e) $e^{x} \rightarrow+\infty$ as $x \rightarrow+\infty, e^{x} \rightarrow 0$ as $x \rightarrow-\infty$;

(f) $\lim _{x \rightarrow+\infty} x^{n} e^{-x}=0$, for every $n$.

Proof We have already proved $(a)$ to $(e) ;(25)$ shows that

$$
e^{x}>\frac{x^{n+1}}{(n+1) !}
$$

for $x>0$, so that

$$
x^{n} e^{-x}<\frac{(n+1) !}{x}
$$

and $(f)$ follows. Part $(f)$ shows that $e^{x}$ tends to $+\infty$ "faster" than any power of $x$, as $x \rightarrow+\infty$.

Since $E$ is strictly increasing and differentiable on $R^{1}$, it has an inverse function $L$ which is also strictly increasing and differentiable and whose domain is $E\left(R^{1}\right)$, that is, the set of all positive numbers. $L$ is defined by

$$
E(L(y))=y \quad(y>0)
$$

or, equivalently, by

$$
L(E(x))=x \quad(x \text { real })
$$

Differentiating (37), we get (compare Theorem 5.5)

$$
L^{\prime}(E(x)) \cdot E(x)=1 .
$$

Writing $y=E(x)$, this gives us

$$
L^{\prime}(y)=\frac{1}{y} \quad(y>0)
$$

Taking $x=0$ in (37), we see that $L(1)=0$. Hence (38) implies

$$
L(y)=\int_{1}^{y} \frac{d x}{x}
$$

Quite frequently, (39) is taken as the starting point of the theory of the logarithm and the exponential function. Writing $u=E(x), v=E(y),(26)$ gives

$$
L(u v)=L(E(x) \cdot E(y))=L(E(x+y))=x+y,
$$

so that

$$
L(u v)=L(u)+L(v) \quad(u>0, v>0)
$$

This shows that $L$ has the familiar property which makes logarithms useful tools for computation. The customary notation for $L(x)$ is of course $\log x$.

As to the behavior of $\log x$ as $x \rightarrow+\infty$ and as $x \rightarrow 0$, Theorem 8.6(e) shows that

$$
\begin{array}{ll}
\log x \rightarrow+\infty & \text { as } x \rightarrow+\infty, \\
\log x \rightarrow-\infty & \text { as } x \rightarrow 0 .
\end{array}
$$

It is easily seen that

$$
x^{n}=E(n L(x))
$$

if $x>0$ and $n$ is an integer. Similarly, if $m$ is a positive integer, we have

$$
x^{1 / m}=E\left(\frac{1}{m} L(x)\right)
$$

since each term of (42), when raised to the $m$ th power, yields the corresponding term of (36). Combining (41) and (42), we obtain

$$
x^{\alpha}=E(\alpha L(x))=e^{\alpha \log x}
$$

for any rational $\alpha$.

We now define $x^{\alpha}$, for any real $\alpha$ and any $x>0$, by (43). The continuity and monotonicity of $E$ and $L$ show that this definition leads to the same result as the previously suggested one. The facts stated in Exercise 6 of Chap. 1, are trivial consequences of (43).

If we differentiate (43), we obtain, by Theorem 5.5,

$$
\left(x^{\alpha}\right)^{\prime}=E(\alpha L(x)) \cdot \frac{\alpha}{x}=\alpha x^{\alpha-1}
$$

Note that we have previously used (44) only for integral values of $\alpha$, in which case (44) follows easily from Theorem $5.3(b)$. To prove (44) directly from the definition of the derivative, if $x^{\alpha}$ is defined by (33) and $\alpha$ is irrational, is quite troublesome.

The well-known integration formula for $x^{\alpha}$ follows from (44) if $\alpha \neq-1$, and from (38) if $\alpha=-1$. We wish to demonstrate one more property of $\log x$, namely,

$$
\lim _{x \rightarrow+\infty} x^{-\alpha} \log x=0
$$

for every $\alpha>0$. That is, $\log x \rightarrow+\infty$ "slower" than any positive power of $x$, as $x \rightarrow+\infty$.

For if $0<\varepsilon<\alpha$, and $x>1$, then

$$
\begin{aligned}
x^{-\alpha} \log x & =x^{-\alpha} \int_{1}^{x} t^{-1} d t<x^{-\alpha} \int_{1}^{x} t^{\varepsilon-1} d t \\
& =x^{-\alpha} \cdot \frac{x^{\varepsilon}-1}{\varepsilon}<\frac{x^{\varepsilon-\alpha}}{\varepsilon},
\end{aligned}
$$

and (45) follows. We could also have used Theorem 8.6( $f$ ) to derive (45).

## THE TRIGONOMETRIC FUNCTIONS

Let us define

$$
C(x)=\frac{1}{2}[E(i x)+E(-i x)], \quad S(x)=\frac{1}{2 i}[E(i x)-E(-i x)]
$$

We shall show that $C(x)$ and $S(x)$ coincide with the functions $\cos x$ and $\sin x$, whose definition is usually based on geometric considerations. By $(25), E(\bar{z})=$ $\overline{E(z)}$. Hence (46) shows that $C(x)$ and $S(x)$ are real for real $x$. Also,

$$
E(i x)=C(x)+i S(x) \text {. }
$$

Thus $C(x)$ and $S(x)$ are the real and imaginary parts, respectively, of $E(i x)$, if $x$ is real. By (27),

$$
|E(i x)|^{2}=E(i x) \overline{E(i x)}=E(i x) E(-i x)=1,
$$

so that

$$
|E(i x)|=1 \quad(x \text { real }) .
$$

From (46) we can read off that $C(0)=1, S(0)=0$, and (28) shows that

$$
C^{\prime}(x)=-S(x), \quad S^{\prime}(x)=C(x)
$$

We assert that there exist positive numbers $x$ such that $C(x)=0$. For suppose this is not so. Since $C(0)=1$, it then follows that $C(x)>0$ for all $x>0$, hence $S^{\prime}(x)>0$, by (49), hence $S$ is strictly increasing; and since $S(0)=0$, we have $S(x)>0$ if $x>0$. Hence if $0<x<y$, we have

$$
S(x)(y-x)<\int_{x}^{y} S(t) d t=C(x)-C(y) \leq 2 .
$$

The last inequality follows from (48) and (47). Since $S(x)>0,(50)$ cannot be true for large $y$, and we have a contradiction.

Let $x_{0}$ be the smallest positive number such that $C\left(x_{0}\right)=0$. This exists, since the set of zeros of a continuous function is closed, and $C(0) \neq 0$. We define the number $\pi$ by

$$
\pi=2 x_{0} \text {. }
$$

Then $C(\pi / 2)=0$, and (48) shows that $S(\pi / 2)= \pm 1$. Since $C(x)>0$ in $(0, \pi / 2), S$ is increasing in $(0, \pi / 2)$; hence $S(\pi / 2)=1$. Thus

$$
E\left(\frac{\pi i}{2}\right)=i
$$

and the addition formula gives

$$
E(\pi i)=-1, \quad E(2 \pi i)=1
$$

hence

$$
E(z+2 \pi i)=E(z) \quad(z \text { complex })
$$

### 8.7 Theorem

(a) The function $E$ is periodic, with period $2 \pi i$.

(b) The functions $C$ and $S$ are periodic, with period $2 \pi$.

(c) If $0<t<2 \pi$, then $E($ it $) \neq 1$.

(d) If $z$ is a complex number with $|z|=1$, there is a unique $t$ in $[0,2 \pi)$ such that $E(i t)=z$.

Proof By (53), (a) holds; and (b) follows from (a) and (46).

Suppose $0<t<\pi / 2$ and $E(i t)=x+i y$, with $x, y$ real. Our preceding work shows that $0<x<1,0<y<1$. Note that

$$
E(4 i t)=(x+i y)^{4}=x^{4}-6 x^{2} y^{2}+y^{4}+4 i x y\left(x^{2}-y^{2}\right) .
$$

If $E(4 i t)$ is real, it follows that $x^{2}-y^{2}=0$; since $x^{2}+y^{2}=1$, by (48), we have $x^{2}=y^{2}=\frac{1}{2}$, hence $E(4 i t)=-1$. This proves $(c)$.

If $0 \leq t_{1}<t_{2}<2 \pi$, then

$$
E\left(i t_{2}\right)\left[E\left(i t_{1}\right)\right]^{-1}=E\left(i t_{2}-i t_{1}\right) \neq 1,
$$

by $(c)$. This establishes the uniqueness assertion in $(d)$.

To prove the existence assertion in (d), fix $z$ so that $|z|=1$. Write $z=x+i y$, with $x$ and $y$ real. Suppose first that $x \geq 0$ and $y \geq 0$. On $[0, \pi / 2], C$ decreases from 1 to 0 . Hence $C(t)=x$ for some $t \in[0, \pi / 2]$. Since $C^{2}+S^{2}=1$ and $S \geq 0$ on $[0, \pi / 2]$, it follows that $z=E(i t)$.

If $x<0$ and $y \geq 0$, the preceding conditions are satisfied by $-i z$.

Hence $-i z=E(i t)$ for some $t \in[0, \pi / 2]$, and since $i=E(\pi i / 2)$, we obtain $z=E(i(t+\pi / 2))$. Finally, if $y<0$, the preceding two cases show that
$-z=E(i t)$ for some $t \in(0, \pi)$. Hence $z=-E(i t)=E(i(t+\pi))$. This proves $(d)$, and hence the theorem.

It follows from $(d)$ and (48) that the curve $\gamma$ defined by

$$
\gamma(t)=E(i t) \quad(0 \leq t \leq 2 \pi)
$$

is a simple closed curve whose range is the unit circle in the plane. Since $\gamma^{\prime}(t)=i E(i t)$, the length of $\gamma$ is

$$
\int_{0}^{2 \pi}\left|\gamma^{\prime}(t)\right| d t=2 \pi
$$

by Theorem 6.27. This is of course the expected result for the circumference of a circle of radius 1 . It shows that $\pi$, defined by (51), has the usual geometric significance.

In the same way we see that the point $\gamma(t)$ describes a circular arc of length $t_{0}$ as $t$ increases from 0 to $t_{0}$. Consideration of the triangle whose vertices are

$$
z_{1}=0, \quad z_{2}=\gamma\left(t_{0}\right), \quad z_{3}=C\left(t_{0}\right)
$$

shows that $C(t)$ and $S(t)$ are indeed identical with $\cos t$ and $\sin t$, if the latter are defined in the usual way as ratios of the sides of a right triangle.

It should be stressed that we derived the basic properties of the trigonometric functions from (46) and (25), without any appeal to the geometric notion of angle. There are other nongeometric approaches to these functions. The papers by W. F. Eberlein (Amer. Math. Monthly, vol. 74, 1967, pp. 1223-1225) and by G. B. Robison (Math. Mag., vol. 41, 1968, pp. 66-70) deal with these topics.

## THE ALGEBRAIC COMPLETENESS OF THE COMPLEX FIELD

We are now in a position to give a simple proof of the fact that the complex field is algebraically complete, that is to say, that every nonconstant polynomial with complex coefficients has a complex root.

8.8 Theorem Suppose $a_{0}, \ldots, a_{n}$ are complex numbers, $n \geq 1, a_{n} \neq 0$,

$$
P(z)=\sum_{0}^{n} a_{k} z^{k}
$$

Then $P(z)=0$ for some complex number $z$.

Proof Without loss of generality, assume $a_{n}=1$. Put

If $|z|=R$, then

$$
\mu=\inf |P(z)| \quad(z \text { complex })
$$

$$
|P(z)| \geq R^{n}\left[1-\left|a_{n-1}\right| R^{-1}-\cdots-\left|a_{0}\right| R^{-n}\right]
$$

The right side of (56) tends to $\infty$ as $R \rightarrow \infty$. Hence there exists $R_{0}$ such that $|P(z)|>\mu$ if $|z|>R_{0}$. Since $|P|$ is continuous on the closed disc with center at 0 and radius $R_{0}$, Theorem 4.16 shows that $\left|P\left(z_{0}\right)\right|=\mu$ for some $z_{0}$.

We claim that $\mu=0$.

If not, put $Q(z)=P\left(z+z_{0}\right) / P\left(z_{0}\right)$. Then $Q$ is a nonconstant polynomial, $Q(0)=1$, and $|Q(z)| \geq 1$ for all $z$. There is a smallest integer $k$, $1 \leq k \leq n$, such that

$$
Q(z)=1+b_{k} z^{k}+\cdots+b_{n} z^{n}, \quad b_{k} \neq 0 .
$$

By Theorem 8.7(d) there is a real $\theta$ such that

$$
e^{i k \theta} b_{k}=-\left|b_{k}\right|
$$

If $r>0$ and $r^{k}\left|b_{k}\right|<1$, (58) implies

$$
\left|1+b_{k} r^{k} e^{i k \theta}\right|=1-r^{k}\left|b_{k}\right|
$$

so that

$$
\left|Q\left(r e^{i \theta}\right)\right| \leq 1-r^{k}\left\{\left|b_{k}\right|-r\left|b_{k+1}\right|-\cdots-r^{n-k}\left|b_{n}\right|\right\}
$$

For sufficiently small $r$, the expression in braces is positive; hence $\left|Q\left(r e^{i \theta}\right)\right|<1$, a contradiction.

Thus $\mu=0$, that is, $P\left(z_{0}\right)=0$.

Exercise 27 contains a more general result.

## FOURIER SERIES

8.9 Definition A trigonometric polynomial is a finite sum of the form

$$
f(x)=a_{0}+\sum_{n=1}^{N}\left(a_{n} \cos n x+b_{n} \sin n x\right) \quad(x \text { real })
$$

where $a_{0}, \ldots, a_{N}, b_{1}, \ldots, b_{N}$ are complex numbers. On account of the identities (46), (59) can also be written in the form

$$
f(x)=\sum_{-N}^{N} c_{n} e^{i n x} \quad(x \text { real })
$$

which is more convenient for most purposes. It is clear that every trigonometric polynomial is periodic, with period $2 \pi$.

If $n$ is a nonzero integer, $e^{i n x}$ is the derivative of $e^{i n x} /$ in, which also has period $2 \pi$. Hence

$$
\frac{1}{2 \pi} \int_{-\pi}^{\pi} e^{i n x} d x= \begin{cases}1 & (\text { if } n=0) \\ 0 & (\text { if } n= \pm 1, \pm 2, \ldots)\end{cases}
$$

Let us multiply (60) by $e^{-i m x}$, where $m$ is an integer; if we integrate the product, (61) shows that

$$
c_{m}=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) e^{-i m x} d x
$$

for $|m| \leq N$. If $|m|>N$, the integral in (62) is 0 .

The following observation can be read off from (60) and (62): The trigonometric polynomial $f$, given by (60), is real if and only if $c_{-n}=\overline{c_{n}}$ for $n=0, \ldots, N$.

In agreement with (60), we define a trigonometric series to be a series of the form

$$
\sum_{-\infty}^{\infty} c_{n} e^{i n x} \quad(x \text { real })
$$

the $N$ th partial sum of (63) is defined to be the right side of (60).

If $f$ is an integrable function on $[-\pi, \pi]$, the numbers $c_{m}$ defined by (62) for all integers $m$ are called the Fourier coefficients of $f$, and the series (63) formed with these coefficients is called the Fourier series of $f$.

The natural question which now arises is whether the Fourier series of $f$ converges to $f$, or, more generally, whether $f$ is determined by its Fourier series. That is to say, if we know the Fourier coefficients of a function, can we find the function, and if so, how?

The study of such series, and, in particular, the problem of representing a given function by a trigonometric series, originated in physical problems such as the theory of oscillations and the theory of heat conduction (Fourier's "Th√©orie analytique de la chaleur" was published in 1822). The many difficult: and delicate problems which arose during this study caused a thorough revision and reformulation of the whole theory of functions of a real variable. Among many prominent names, those of Riemann, Cantor, and Lebesgue are intimately connected with this field, which nowadays, with all its generalizations and ramifications, may well be said to occupy a central position in the whole of analysis.

We shall be content to derive some basic theorems which are easily accessible by the methods developed in the preceding chapters. For more thorough investigations, the Lebesgue integral is a natural and indispensable tool.

We shall first study more general systems of functions which share a property analogous to (61).

8.10 Definition Let $\left\{\phi_{n}\right\}(n=1,2,3, \ldots)$ be a sequence of complex functions on $[a, b]$, such that

$$
\int_{a}^{b} \phi_{n}(x) \overline{\phi_{m}(x)} d x=0 \quad(n \neq m)
$$

Then $\left\{\phi_{n}\right\}$ is said to be an orthogonal system of functions on $[a, b]$. If, in addition,

$$
\int_{a}^{b}\left|\phi_{n}(x)\right|^{2} d x=1
$$

for all $n,\left\{\phi_{n}\right\}$ is said to be orthonormal.

For example, the functions $(2 \pi)^{-\frac{1}{2}} e^{i n x}$ form an orthonormal system on $[-\pi, \pi]$. So do the real functions

$$
\frac{1}{\sqrt{2 \pi}}, \frac{\cos x}{\sqrt{\pi}}, \frac{\sin x}{\sqrt{\pi}}, \frac{\cos 2 x}{\sqrt{\pi}}, \frac{\sin 2 x}{\sqrt{\pi}}, \cdots
$$

If $\left\{\phi_{n}\right\}$ is orthonormal on $[a, b]$ and if

$$
c_{n}=\int_{a}^{b} f(t) \overline{\phi_{n}(t)} d t \quad(n=1,2,3, \ldots)
$$

we call $c_{n}$ the $n$th Fourier coefficient of $f$ relative to $\left\{\phi_{n}\right\}$. We write

$$
f(x) \sim \sum_{1}^{\infty} c_{n} \phi_{n}(x)
$$

and call this series the Fourier series of $f$ (relative to $\left\{\phi_{n}\right\}$ ).

Note that the symbol $\sim$ used in (67) implies nothing about the convergence of the series; it merely says that the coefficients are given by (66).

The following theorems show that the partial sums of the Fourier series of $f$ have a certain minimum property. We shall assume here and in the rest of this chapter that $f \in \mathscr{R}$, although this hypothesis can be weakened.

8.11 Theorem Let $\left\{\phi_{n}\right\}$ be orthonormal on $[a, b]$. Let

$$
s_{n}(x)=\sum_{m=1}^{n} c_{m} \phi_{m}(x)
$$

be the nth partial sum of the Fourier series of $f$, and suppose

$$
t_{n}(x)=\sum_{m=1}^{n} \gamma_{m} \phi_{m}(x)
$$

Then

$$
\int_{a}^{b}\left|f-s_{n}\right|^{2} d x \leq \int_{a}^{b}\left|f-t_{n}\right|^{2} d x
$$

and equality holds if and only if

$$
\gamma_{m}=c_{m} \quad(m=1, \ldots, n)
$$

That is to say, among all functions $t_{n}, s_{n}$ gives the best possible mean square approximation to $f$.

Proof Let $\int$ denote the integral over $[a, b], \Sigma$ the sum from 1 to $n$. Then

$$
\int f \bar{t}_{n}=\int f \sum \bar{\gamma}_{m} \bar{\phi}_{m}=\sum c_{m} \bar{\gamma}_{m}
$$

by the definition of $\left\{c_{m}\right\}$,

$$
\int\left|t_{n}\right|^{2}=\int t_{n} \bar{t}_{n}=\int \sum \gamma_{m} \phi_{m} \sum \bar{\gamma}_{k} \bar{\phi}_{k}=\sum\left|\gamma_{m}\right|^{2}
$$

since $\left\{\phi_{m}\right\}$ is orthonormal, and so

$$
\begin{aligned}
\int\left|f-t_{n}\right|^{2} & =\int|f|^{2}-\int f \bar{t}_{n}-\int f t_{n}+\int\left|t_{n}\right|^{2} \\
& =\int|f|^{2}-\sum c_{m} \bar{\gamma}_{m}-\sum \bar{c}_{m} \gamma_{m}+\sum \gamma_{m} \bar{\gamma}_{m} \\
& =\int|f|^{2}-\sum\left|c_{m}\right|^{2}+\sum\left|\gamma_{m}-c_{m}\right|^{2}
\end{aligned}
$$

which is evidently minimized if and only if $\gamma_{m}=c_{m}$.

Putting $\gamma_{m}=c_{m}$ in this calculation, we obtain

$$
\int_{a}^{b}\left|s_{n}(x)\right|^{2} d x=\sum_{1}^{n}\left|c_{m}\right|^{2} \leq \int_{a}^{b}|f(x)|^{2} d x
$$

since $\int\left|f-t_{n}\right|^{2} \geq 0$.

8.12 Theorem If $\left\{\phi_{n}\right\}$ is orthonormal on $[a, b]$, and if

then

$$
f(x) \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}(x)
$$

$$
\sum_{n=1}^{\infty}\left|c_{n}\right|^{2} \leq \int_{a}^{b}|f(x)|^{2} d x
$$

In particular,

$$
\lim _{n \rightarrow \infty} c_{n}=0
$$

Proof Letting $n \rightarrow \infty$ in (72), we obtain (73), the so-called "Bessel inequality."

8.13 Trigonometric series From now on we shall deal only with the trigonometric system. We shall consider functions $f$ that have period $2 \pi$ and that are Riemann-integrable on $[-\pi, \pi]$ (and hence on every bounded interval). The Fourier series of $f$ is then the series (63) whose coefficients $c_{n}$ are given by the integrals (62), and

$$
s_{N}(x)=s_{N}(f ; x)=\sum_{-N}^{N} c_{n} e^{i n x}
$$

is the $N$ th partial sum of the Fourier series of $f$. The inequality (72) now takes the form

$$
\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left|s_{N}(x)\right|^{2} d x=\sum_{-N}^{N}\left|c_{n}\right|^{2} \leq \frac{1}{2 \pi} \int_{-\pi}^{\pi}|f(x)|^{2} d x
$$

In order to obtain an expression for $s_{N}$ that is more manageable than (75) we introduce the Dirichlet kernel

$$
D_{N}(x)=\sum_{n=-N}^{N} e^{i n x}=\frac{\sin \left(N+\frac{1}{2}\right) x}{\sin (x / 2)}
$$

The first of these equalities is the definition of $D_{N}(x)$. The second follows if both sides of the identity

$$
\left(e^{i x}-1\right) D_{N}(x)=e^{i(N+1) x}-e^{-i N x}
$$

are multiplied by $e^{-i x / 2}$.

By (62) and (75), we have

$$
\begin{aligned}
s_{N}(f ; x) & =\sum_{-N}^{N} \frac{1}{2 \pi} \int_{-\pi}^{\pi} f(t) e^{-i n t} d t e^{i n x} \\
& =\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(t) \sum_{-N}^{N} e^{i n(x-t)} d t,
\end{aligned}
$$

so that

$$
s_{N}(f ; x)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(t) D_{N}(x-t) d t=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x-t) D_{N}(t) d t
$$

The periodicity of all functions involved shows that it is immaterial over which interval we integrate, as long as its length is $2 \pi$. This shows that the two integrals in (78) are equal.

We shall prove just one theorem about the pointwise convergence of Fourier series.

8.14 Theorem If, for some $x$, there are constants $\delta>0$ and $M<\infty$ such that

$$
|f(x+t)-f(x)| \leq M|t|
$$

for all $t \in(-\delta, \delta)$, then

$$
\lim _{N \rightarrow \infty} s_{N}(f ; x)=f(x)
$$

Proof Define

$$
g(t)=\frac{f(x-t)-f(x)}{\sin (t / 2)}
$$

for $0<|t| \leq \pi$, and put $g(0)=0$. By the definition (77),

$$
\frac{1}{2 \pi} \int_{-\pi}^{\pi} D_{N}(x) d x=1
$$

Hence (78) shows that

$$
\begin{aligned}
s_{N}(f ; x) & -f(x)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} g(t) \sin \left(N+\frac{1}{2}\right) t d t \\
& =\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left[g(t) \cos \frac{t}{2}\right] \sin N t d t+\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left[g(t) \sin \frac{t}{2}\right] \cos N t d t .
\end{aligned}
$$

By (79) and (81), $g(t) \cos (t / 2)$ and $g(t) \sin (t / 2)$ are bounded. The last two integrals thus tend to 0 as $N \rightarrow \infty$, by (74). This proves (80).

Corollary If $f(x)=0$ for all $x$ in some segment $J$, then $\lim s_{N}(f ; x)=0$ for every $x \in J$.

Here is another formulation of this corollary:

$$
\begin{aligned}
\text { If } f(t)= & g(t) \text { for all } t \text { in some neighborhood of } x \text {, then } \\
& s_{N}(f ; x)-s_{N}(g ; x)=s_{N}(f-g ; x) \rightarrow 0 \text { as } N \rightarrow \infty .
\end{aligned}
$$

This is usually called the localization theorem. It shows that the behavior of the sequence $\left\{s_{N}(f ; x)\right\}$, as far as convergence is concerned, depends only on the values of $f$ in some (arbitrarily small) neighborhood of $x$. Two Fourier series may thus have the same behavior in one interval, but may behave in entirely different ways in some other interval. We have here a very striking contrast between Fourier series and power series (Theorem 8.5).

We conclude with two other approximation theorems.

8.15 Theorem If $f$ is continuous (with period $2 \pi$ ) and if $\varepsilon>0$, then there is a trigonometric polynomial $P$ such that

for all real $x$.

$$
|P(x)-f(x)|<\varepsilon
$$

Proof If we identify $x$ and $x+2 \pi$, we may regard the $2 \pi$-periodic functions on $R^{1}$ as functions on the unit circle $T$, by means of the mapping $x \rightarrow e^{i x}$. The trigonometric polynomials, i.e., the functions of the form (60), form a self-adjoint algebra $\mathscr{A}$, which separates points on $T$, and which vanishes at no point of $T$. Since $T$ is compact, Theorem 7.33 tells us that $\mathscr{A}$ is dense in $\mathscr{C}(T)$. This is exactly what the theorem asserts.

A more precise form of this theorem appears in Exercise 15.

8.16 Parseval's theorem Suppose $f$ and $g$ are Riemann-integrable functions with period $2 \pi$, and

$$
f(x) \sim \sum_{-\infty}^{\infty} c_{n} e^{i n x}, \quad g(x) \sim \sum_{-\infty}^{\infty} \gamma_{n} e^{i n x}
$$

Then

$$
\begin{aligned}
\lim _{N \rightarrow \infty} \frac{1}{2 \pi} \int_{-\pi}^{\pi}\left|f(x)-s_{N}(f ; x)\right|^{2} d x & =0, \\
\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) \overline{g(x)} d x & =\sum_{-\infty}^{\infty} c_{n} \bar{\gamma}_{n}, \\
\frac{1}{2 \pi} \int_{-\pi}^{\pi}|f(x)|^{2} d x & =\sum_{-\infty}^{\infty}\left|c_{n}\right|^{2} .
\end{aligned}
$$

Proof Let us use the notation

$$
\|h\|_{2}=\left\{\frac{1}{2 \pi} \int_{-\pi}^{\pi}|h(x)|^{2} d x\right\}^{1 / 2} .
$$

Let $\varepsilon>0$ be given. Since $f \in \mathscr{R}$ and $f(\pi)=f(-\pi)$, the construction described in Exercise 12 of Chap. 6 yields a continuous $2 \pi$-periodic function $h$ with

$$
\|f-h\|_{2}<\varepsilon \text {. }
$$

By Theorem 8.15 , there is a trigonometric polynomial $P$ such that $|h(x)-P(x)|<\varepsilon$ for all $x$. Hence $\|h-P\|_{2}<\varepsilon$. If $P$ has degree $N_{0}$, Theorem 8.11 shows that

$$
\left\|h-s_{N}(h)\right\|_{2} \leq\|h-P\|_{2}<\varepsilon
$$

for all $N \geq N_{0}$. By (72), with $h-f$ in place of $f$,

$$
\left\|s_{N}(h)-s_{N}(f)\right\|_{2}=\left\|s_{N}(h-f)\right\|_{2} \leq\|h-f\|_{2}<\varepsilon .
$$

Now the triangle inequality (Exercise 11, Chap. 6), combined with (87), (88), and (89), shows that

$$
\left\|f-s_{N}(f)\right\|_{2}<3 \varepsilon \quad\left(N \geq N_{0}\right) .
$$

This proves (83). Next,

$$
\frac{1}{2 \pi} \int_{-\pi}^{\pi} s_{N}(f) \bar{g} d x=\sum_{-N}^{N} c_{n} \frac{1}{2 \pi} \int_{-\pi}^{\pi} e^{i n x} \overline{g(x)} d x=\sum_{-N}^{N} c_{n} \bar{\gamma}_{n},
$$

and the Schwarz inequality shows that

$$
\left|\int f \bar{g}-\int s_{N}(f) \bar{g}\right| \leq \int\left|f-s_{N}(f) \| g\right| \leq\left\{\int\left|f-s_{N}\right|^{2} \int|g|^{2}\right\}^{1 / 2},
$$

which tends to 0 , as $N \rightarrow \infty$, by (83). Comparison of (91) and (92) gives (84). Finally, (85) is the special case $g=f$ of (84).

A more general version of Theorem 8.16 appears in Chap. 11 .

## THE GAMMA FUNCTION

This function is closely related to factorials and crops up in many unexpected places in analysis. Its origin, history, and development are very well described in an interesting article by P. J. Davis (Amer. Math. Monthly, vol. 66, 1959, pp. 849-869). Artin's book (cited in the Bibliography) is another good elementary introduction.

Our presentation will be very condensed, with only a few comments after each theorem. This section may thus be regarded as a large exercise, and as an opportunity to apply some of the material that has been presented so far.

8.17 Definition For $0<x<\infty$,

$$
\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} d t
$$

The integral converges for these $x$. (When $x<1$, both 0 and $\infty$ have to be looked at.)

### 8.18 Theorem

(a) The functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds if $0<x<\infty$.

(b) $\Gamma(n+1)=n !$ for $n=1,2,3, \ldots$

(c) $\log \Gamma$ is convex on $(0, \infty)$.

Proof An integration by parts proves $(a)$. Since $\Gamma(1)=1,(a)$ implies (b), by induction. If $1<p<\infty$ and $(1 / p)+(1 / q)=1$, apply H√∂lder's inequality (Exercise 10, Chap. 6) to (93), and obtain

$$
\Gamma\left(\frac{x}{p}+\frac{y}{q}\right) \leq \Gamma(x)^{1 / p} \Gamma(y)^{1 / q}
$$

This is equivalent to $(c)$.

It is a rather surprising fact, discovered by Bohr and Mollerup, that these three properties characterize $\Gamma$ completely.

8.19 Theorem If $f$ is a positive function on $(0, \infty)$ such that

(a) $f(x+1)=x f(x)$,

(b) $f(1)=1$,

(c) $\log f$ is convex,

then $f(x)=\Gamma(x)$.

Proof Since $\Gamma$ satisfies $(a),(b)$, and $(c)$, it is enough to prove that $f(x)$ is uniquely determined by $(a),(b),(c)$, for all $x>0$. By $(a)$, it is enough to do this for $x \in(0,1)$.

Put $\varphi=\log f$. Then

$$
\varphi(x+1)=\varphi(x)+\log x \quad(0<x<\infty)
$$

$\varphi(1)=0$, and $\varphi$ is convex. Suppose $0<x<1$, and $n$ is a positive integer. By (94), $\varphi(n+1)=\log (n !)$. Consider the difference quotients of $\varphi$ on the intervals $[n, n+1],[n+1, n+1+x],[n+1, n+2]$. Since $\varphi$ is convex

$$
\log n \leq \frac{\varphi(n+1+x)-\varphi(n+1)}{x} \leq \log (n+1)
$$

Repeated application of (94) gives

$$
\varphi(n+1+x)=\varphi(x)+\log [x(x+1) \cdots(x+n)]
$$

Thus

$$
0 \leq \varphi(x)-\log \left[\frac{n ! n^{x}}{x(x+1) \cdots(x+n)}\right] \leq x \log \left(1+\frac{1}{n}\right)
$$

The last expression tends to 0 as $n \rightarrow \infty$. Hence $\varphi(x)$ is determined, and the proof is complete.

As a by-product we obtain the relation

$$
\Gamma(x)=\lim _{n \rightarrow \infty} \frac{n ! n^{x}}{x(x+1) \cdots(x+n)}
$$

at least when $0<x<1$; from this one can deduce that (95) holds for all $x>0$, since $\Gamma(x+1)=x \Gamma(x)$.

8.20 Theorem If $x>0$ and $y>0$, then

$$
\int_{0}^{1} t^{x-1}(1-t)^{y-1} d t=\frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}
$$

This integral is the so-called beta function $B(x, y)$.

Proof Note that $B(1, y)=1 / y$, that $\log B(x, y)$ is a convex function of $x$, for each fixed $y$, by H√∂lder's inequality, as in Theorem 8.18, and that

$$
B(x+1, y)=\frac{x}{x+y} B(x, y)
$$

To prove (97), perform an integration by parts on

$$
B(x+1, y)=\int_{0}^{1}\left(\frac{t}{1-t}\right)^{x}(1-t)^{x+y-1} d t
$$

These three properties of $B(x, y)$ show, for each $y$, that Theorem 8.19 applies to the function $f$ defined by

Hence $f(x)=\Gamma(x)$.

$$
f(x)=\frac{\Gamma(x+y)}{\Gamma(y)} B(x, y)
$$

8.21 Some consequences The substitution $t=\sin ^{2} \theta$ turns (96) into

$$
2 \int_{0}^{\pi / 2}(\sin \theta)^{2 x-1}(\cos \theta)^{2 y-1} d \theta=\frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}
$$

The special case $x=y=\frac{1}{2}$ gives

$$
\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi} .
$$

The substitution $t=s^{2}$ turns (93) into

$$
\Gamma(x)=2 \int_{0}^{\infty} s^{2 x-1} e^{-s^{2}} d s \quad(0<x<\infty)
$$

The special case $x=\frac{1}{2}$ gives

$$
\int_{-\infty}^{\infty} e^{-s^{2}} d s=\sqrt{\pi}
$$

By (99), the identity

$$
\Gamma(x)=\frac{2^{x-1}}{\sqrt{\pi}} \Gamma\left(\frac{x}{2}\right) \Gamma\left(\frac{x+1}{2}\right)
$$

follows directly from Theorem 8.19.

8.22 Stirling's formula This provides a simple approximate expression for $\Gamma(x+1)$ when $x$ is large (hence for $n !$ when $n$ is large). The formula is

$$
\lim _{x \rightarrow \infty} \frac{\Gamma(x+1)}{(x / e)^{x} \sqrt{2 \pi x}}=1
$$

Here is a proof. Put $t=x(1+u)$ in (93). This gives

$$
\Gamma(x+1)=x^{x+1} e^{-x} \int_{-1}^{\infty}\left[(1+u) e^{-u}\right]^{x} d u
$$

Determine $h(u)$ so that $h(0)=1$ and

$$
(1+u) e^{-u}=\exp \left[-\frac{u^{2}}{2} h(u)\right]
$$

if $-1<u<\infty, u \neq 0$. Then

$$
h(u)=\frac{2}{u^{2}}[u-\log (1+u)]
$$

It follows that $h$ is continuous, and that $h(u)$ decreases monotonically from $\infty$ to 0 as $u$ increases from -1 to $\infty$.

The substitution $u=s \sqrt{2 / x}$ turns (104) into

$$
\Gamma(x+1)=x^{x} e^{-x} \sqrt{2 x} \int_{-\infty}^{\infty} \psi_{x}(s) d s
$$

where

$$
\psi_{x}(s)= \begin{cases}\exp \left[-s^{2} h(s \sqrt{2 / x})\right] & (-\sqrt{x / 2}<s<\infty) \\ 0 & (s \leq-\sqrt{x / 2})\end{cases}
$$

Note the following facts about $\psi_{x}(s)$ :

(a) For every $s, \psi_{x}(s) \rightarrow e^{-s^{2}}$ as $x \rightarrow \infty$.

(b) The convergence in $(a)$ is uniform on $[-A, A]$, for every $A<\infty$.

(c) When $s<0$, then $0<\psi_{x}(s)<e^{-s^{2}}$.

(d) When $s>0$ and $x>1$, then $0<\psi_{x}(s)<\psi_{1}(s)$.

(e) $\int_{0}^{\infty} \psi_{1}(s) d s<\infty$.

The convergence theorem stated in Exercise 12 of Chap. 7 can therefore be applied to the integral (107), and shows that this integral converges to $\sqrt{\pi}$ as $x \rightarrow \infty$, by (101). This proves (103).

A more detailed version of this proof may be found in R. C. Buck's "Advanced Calculus," pp. 216-218. For two other, entirely different, proofs, see W. Feller's article in Amer. Math. Monthly, vol. 74, 1967, pp. 1223-1225 (with a correction in vol. 75, 1968, p. 518) and pp. 20-24 of Artin's book.

Exercise 20 gives a simpler proof of a less precise result.

## EXERCISES

1. Define

$$
f(x)= \begin{cases}e^{-1 / x^{2}} & (x \neq 0) \\ 0 & (x=0)\end{cases}
$$

Prove that $f$ has derivatives of all orders at $x=0$, and that $f^{(n)}(0)=0$ for $n=1,2,3, \ldots$.

2. Let $a_{i j}$ be the number in the $i$ th row and $j$ th column of the array

$$
\begin{array}{rrrrr}
-1 & 0 & 0 & 0 & \cdots \\
\frac{1}{2} & -1 & 0 & 0 & \cdots \\
\frac{1}{4} & \frac{1}{2} & -1 & 0 & \cdots \\
\frac{1}{8} & \frac{1}{4} & \frac{1}{2} & -1 & \cdots
\end{array}
$$

so that

$$
a_{i j}= \begin{cases}0 & (i<j) \\ -1 & (i=j) \\ 2^{j-i} & (i>j)\end{cases}
$$

Prove that

$$
\sum_{i} \sum_{j} a_{i j}=-2, \quad \sum_{j} \sum_{i} a_{i j}=0 .
$$

3. Prove that

$$
\sum_{i} \sum_{J} a_{i j}=\sum_{J} \sum_{T} a_{i j}
$$

if $a_{l j} \geq 0$ for all $i$ and $j$ (the case $+\infty=+\infty$ may occur).

4. Prove the following limit relations:

(a) $\lim _{x \rightarrow 0} \frac{b^{x}-1}{x}=\log b \quad(b>0)$

(b) $\lim _{x \rightarrow 0} \frac{\log (1+x)}{x}=1$.

(c) $\lim _{x \rightarrow 0}(1+x)^{1 / x}=e$.

(d) $\lim _{n \rightarrow \infty}\left(1+\frac{x}{n}\right)^{n}=e^{x}$.

5. Find the following limits
(a) $\lim _{x \rightarrow 0} \frac{e-(1+x)^{1 / x}}{x}$.
(b) $\lim _{n \rightarrow \infty} \frac{n}{\log n}\left[n^{1 / n}-1\right]$.
(c) $\lim _{x \rightarrow 0} \frac{\tan x-x}{x(1-\cos x)}$.
(d) $\lim _{x \rightarrow 0} \frac{x-\sin x}{\tan x-x}$.
6. Suppose $f(x) f(y)=f(x+y)$ for all real $x$ and $y$.

(a) Assuming that $f$ is differentiable and not zero, prove that

$$
f(x)=e^{c x}
$$

where $c$ is a constant.

(b) Prove the same thing, assuming only that $f$ is continuous.

7. If $0<x<\frac{\pi}{2}$, prove that

$$
\frac{2}{\pi}<\frac{\sin x}{x}<1
$$

8. For $n=0,1,2, \ldots$, and $x$ real, prove that

$$
|\sin n x| \leq n|\sin x|
$$

Note that this inequality may be false for other values of $n$. For instance,

$$
\left|\sin \frac{1}{2} \pi\right|>\frac{1}{2}|\sin \pi| \text {. }
$$

9. (a) Put $s_{N}=1+\left(\frac{1}{2}\right)+\cdots+(1 / N)$. Prove that

$$
\lim _{N \rightarrow \infty}\left(s_{N}-\log N\right)
$$

exists. (The limit, often denoted by $\gamma$, is called Euler's constant. Its numerical value is $0.5772 \ldots$... It is not known whether $\gamma$ is rational or not.)

(b) Roughly how large must $m$ be so that $N=10^{m}$ satisfies $s_{N}>100$ ?

10. Prove that $\sum 1 / p$ diverges; the sum extends over all primes.

(This shows that the primes form a fairly substantial subset of the positive integers.)

Hint: Given $N$, let $p_{1}, \ldots, p_{k}$ be those primes that divide at least one integer $\leq N$. Then

$$
\begin{aligned}
\sum_{n=1}^{N} \frac{1}{n} & \leq \prod_{j=1}^{k}\left(1+\frac{1}{p_{j}}+\frac{1}{p_{j}^{2}}+\cdots\right) \\
& =\prod_{j=1}^{k}\left(1-\frac{1}{p_{j}}\right)^{-1} \\
& \leq \exp \sum_{j=1}^{k} \frac{2}{p_{j}} .
\end{aligned}
$$

The last inequality holds bocause

$$
(1-x)^{-1} \leq e^{2 x}
$$

if $0 \leq x \leq \frac{1}{2}$.

(There are many proofs of this result. See, for instance, the article by I. Niven in Amer. Math. Monthly, vol. 78, 1971, pp. 272-273, and the one by R. Bellman in Amer. Math. Monthly, vol. 50, 1943, pp. 318-319.)

11. Suppose $f \in \mathscr{R}$ on $[0, A]$ for all $A<\infty$, and $f(x) \rightarrow 1$ as $x \rightarrow+\infty$. Prove that

$$
\lim _{t \rightarrow 0} t \int_{0}^{\infty} e^{-t x} f(x) d x=1 \quad(t>0) .
$$

12. Suppose $0<\delta<\pi, f(x)=1$ if $|x| \leq \delta, f(x)=0$ if $\delta<|x| \leq \pi$, and $f(x+2 \pi)=$ $f(x)$ for all $x$.

(a) Compute the Fourier coefficients of $f$.

(b) Conclude that

$$
\sum_{n=1}^{\infty} \frac{\sin (n \delta)}{n}=\frac{\pi-\delta}{2} \quad(0<\delta<\pi) .
$$

(c) Deduce from Parseval's theorem that

$$
\sum_{n=1}^{\infty} \frac{\sin ^{2}(n \delta)}{n^{2} \delta}=\frac{\pi-\delta}{2}
$$

(d) Let $\delta \rightarrow 0$ and prove that

$$
\int_{0}^{\infty}\left(\frac{\sin x}{x}\right)^{2} d x=\frac{\pi}{2}
$$

(e) Put $\delta=\pi / 2$ in (c). What do you get?

13. Put $f(x)=x$ if $0 \leq x<2 \pi$, and apply Parseval's theorem to conclude that

$$
\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}
$$

14. If $f(x)=(\pi-|x|)^{2}$ on $[-\pi, \pi]$, prove that

$$
f(x)=\frac{\pi^{2}}{3}+\sum_{n=1}^{\infty} \frac{4}{n^{2}} \cos n x
$$

and deduce that

$$
\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}, \quad \sum_{n=1}^{\infty} \frac{1}{n^{4}}=\frac{\pi^{4}}{90}
$$

(A recent article by E. L. Stark contains many references to series of the form $\sum n^{-3}$, where $s$ is a positive integer. See Math. Mag., vol. 47, 1974, pp. 197-202.) 15. With $D_{n}$ as defined in (77), put

$$
K_{N}(x)=\frac{1}{N+1} \sum_{n=0}^{N} D_{n}(x)
$$

Prove that

$$
K_{N}(x)=\frac{1}{N+1} \cdot \frac{1-\cos (N+1) x}{1-\cos x}
$$

and that

(a) $K_{N} \geq 0$,

(b) $\frac{1}{2 \pi} \int_{-\pi}^{\pi} K_{N}(x) d x=1$,

(c) $K_{N}(x) \leq \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} \quad$ if $0<\delta \leq|x| \leq \pi$.

If $s_{N}=s_{N}(f ; x)$ is the $N$ th partial sum of the Fourier series of $f$, consider the arithmetic means

$$
\sigma_{N}=\frac{s_{0}+s_{1}+\cdots+s_{N}}{N+1}
$$

Prove that

$$
\sigma_{N}(f ; x)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x-t) K_{N}(t) d t
$$

and hence prove Fej√©r's theorem:

If $f$ is continuous, with period $2 \pi$, then $\sigma_{N}(f ; x) \rightarrow f(x)$ uniformly on $[-\pi, \pi]$.

Hint: Use properties $(a),(b),(c)$ to proceed as in Theorem 7.26.

16. Prove a pointwise version of Fej√©r's theorem:

If $f \in \mathscr{R}$ and $f(x+), f(x-)$ exist for some $x$, then

$$
\lim _{N \rightarrow \infty} \sigma_{N}(f ; x)=\frac{1}{d}[f(x+)+f(x-)] .
$$

17. Assume $f$ is bounded and monotonic on $[-\pi, \pi)$, with Fourier coefficients $c_{n}$, as given by (62).

(a) Use Exercise 17 of Chap. 6 to prove that $\left\{n c_{n}\right\}$ is a bounded sequence.

(b) Combine (a) with Exercise 16 and with Exercise 14(e) of Chap. 3, to conclude that

$$
\lim _{N \rightarrow \infty} s_{N}(f ; x)=\frac{1}{2}[f(x+)+f(x-)]
$$

for every $x$.

(c) Assume only that $f \in \mathscr{R}$ on $[-\pi, \pi]$ and that $f$ is monotonic in some segment $(\alpha, \beta) \subset[-\pi, \pi]$. Prove that the conclusion of $(b)$ holds for every $x \in(\alpha, \beta)$. (This is an application of the localization theorem.)

18. Define

$$
\begin{aligned}
& f(x)=x^{3}-\sin ^{2} x \tan x \\
& g(x)=2 x^{2}-\sin ^{2} x-x \tan x
\end{aligned}
$$

Find out, for each of these two functions, whether it is positive or negative for all $x \in(0, \pi / 2)$, or whether it changes sign. Prove your answer.

19. Suppose $f$ is a continuous function on $R^{1}, f(x+2 \pi)=f(x)$, and $\alpha / \pi$ is irrational. Prove that

$$
\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^{N} f(x+n \alpha)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(t) d t
$$

for every $x$. Hint: Do it first for $f(x)=e^{i k x}$.

20. The following simple computation yields a good approximation to Stirling's formula.

For $m=1,2,3, \ldots$, define

$$
f(x)=(m+1-x) \log m+(x-m) \log (m+1)
$$

if $m \leq x \leq m+1$, and define

$$
g(x)=\frac{x}{m}-1+\log m
$$

if $m-\frac{1}{2} \leq x<m+\frac{1}{2}$. Draw the graphs of $f$ and $g$. Note that $f(x) \leq \log x \leq g(x)$ if $x \geq 1$ and that

$$
\int_{1}^{n} f(x) d x=\log (n !)-\frac{1}{2} \log n>-\frac{1}{8}+\int_{1}^{n} g(x) d x
$$

Integrate $\log x$ over $[1, n]$. Conclude that

$$
\frac{7}{8}<\log (n !)-\left(n+\frac{1}{2}\right) \log n+n<1
$$

for $n=2,3,4, \ldots$ (Note: $\log \sqrt{2 \pi} \sim 0.918 \ldots)$ Thus

$$
e^{7 / 8}<\frac{n !}{(n / e)^{n} \sqrt{n}}<e
$$

21. Let

$$
L_{n}=\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left|D_{n}(t)\right| d t \quad(n=1,2,3, \ldots)
$$

Prove that there exists a constant $C>0$ such that

$$
L_{n}>C \log n \quad(n=1,2,3, \ldots)
$$

or, more precisely, that the sequence

$$
\left\{L_{n}-\frac{4}{\pi^{2}} \log n\right\}
$$

is bounded.

22. If $\alpha$ is real and $-1<x<1$, prove Newton's binomial theorem

$$
(1+x)^{\alpha}=1+\sum_{n=1}^{\infty} \frac{\alpha(\alpha-1) \cdots(\alpha-n+1)}{n !} x^{n} .
$$

Hint: Denote the right side by $f(x)$. Prove that the series converges. Prove that

$$
(1+x) f^{\prime}(x)=\alpha f(x)
$$

and solve this differential equation.

Show also that

$$
(1-x)^{-\alpha}=\sum_{n=0}^{\infty} \frac{\Gamma(n+\alpha)}{n ! \Gamma(\alpha)} x^{n}
$$

if $-1<x<1$ and $\alpha>0$.

23. Let $\gamma$ be a continuously differentiable closed curve in the complex plane, with parameter interval $[a, b]$, and assume that $\gamma(t) \neq 0$ for every $t \in[a, b]$. Define the index of $\gamma$ to be

$$
\text { Ind }(\gamma)=\frac{1}{2 \pi i} \int_{a}^{* b} \frac{\gamma^{\prime}(t)}{\gamma(t)} d t
$$

Prove that Ind $(\gamma)$ is always an integer.

Hint: There exists $\varphi$ on $[a, b]$ with $\varphi^{\prime}=\gamma^{\prime} / \gamma, \varphi(a)=0$. Hence $\gamma \exp (-\varphi)$ is constant. Since $\gamma(a)=\gamma(b)$ it follows that $\exp \varphi(b)=\exp \varphi(a)=1$. Note that $\varphi(b)=2 \pi i$ Ind $(\gamma)$.

Compute Ind $(\gamma)$ when $\gamma(t)=e^{i n t}, a=0, b=2 \pi$.

Explain why Ind $(\gamma)$ is of ten called the winding number of $\gamma$ around 0 .

24. Let $\gamma$ be as in Exercise 23, and assume in addition that the range of $\gamma$ does not intersect the negative real axis. Prove that Ind $(\gamma)=0$. Hint: For $0 \leq c<\infty$, Ind $(\gamma+c)$ is a continuous integer-valued function of $c$. Also, Ind $(\gamma+c) \rightarrow 0$ as $c \rightarrow \infty$.
25. Suppose $\gamma_{1}$ and $\gamma_{2}$ are curves as in Exercise 23, and

$$
\left|\gamma_{1}(t)-\gamma_{2}(t)\right|<\left|\gamma_{1}(t)\right| \quad(a \leq t \leq b) .
$$

Prove that Ind $\left(\gamma_{1}\right)=$ Ind $\left(\gamma_{2}\right)$.

Hint: Put $\gamma=\gamma_{2} / \gamma_{1}$. Then $|1-\gamma|<1$, hence Ind $(\gamma)=0$, by Exercise 24 . Also,

$$
\frac{\gamma^{\prime}}{\gamma}=\frac{\gamma_{2}^{\prime}}{\gamma_{2}}-\frac{\gamma_{1}^{\prime}}{\gamma_{1}}
$$

26. Let $\gamma$ be a closed curve in the complex plane (not necessarily differentiable) with parameter interval $[0,2 \pi]$, such that $\gamma(t) \neq 0$ for every $t \in[0,2 \pi]$.

Choose $\delta>0$ so that $|\gamma(t)|>\delta$ for all $t \in[0,2 \pi]$. If $P_{1}$ and $P_{2}$ are trigonometric polynomials such that $\left|P_{j}(t)-\gamma(t)\right|<\delta / 4$ for all $t \in[0,2 \pi]$ (their existence is assured by Theorem 8.15$)$, prove that

$$
\operatorname{Ind}\left(P_{1}\right)=\operatorname{Ind}\left(P_{2}\right)
$$

by applying Exercise 25 .

Define this common value to be Ind $(\gamma)$.

Prove that the statements of Exercises 24 and 25 hold without any differentiability assumption.

27. Let $f$ be a continuous complex function defined in the complex plane. Suppose there is a positive integer $n$ and a complex number $c \neq 0$ such that

$$
\lim _{|z| \rightarrow \infty} z^{-n} f(z)=c
$$

Prove that $f(z)=0$ for at least one complex number $z$.

Note that this is a generalization of Theorem 8.8.

Hint: Assume $f(z) \neq 0$ for all $z$, define

$$
\gamma_{r}(t)=f\left(r e^{i t}\right)
$$

for $0 \leq r<\infty, 0 \leq t \leq 2 \pi$, and prove the following statements about the curves $\gamma_{r}:$

(a) Ind $\left(\gamma_{0}\right)=0$.

(b) Ind $\left(\gamma_{r}\right)=n$ for all sufficiently large $r$.

(c) Ind $\left(\gamma_{r}\right)$ is a continuous function of $r$, on $[0, \infty)$.

[In $(b)$ and $(c)$, use the last part of Exercise 26.]

Show that $(a),(b)$, and $(c)$ are contradictory, since $n>0$.

28. Let $D$ be the closed unit disc in the complex plane. (Thus $z \in D$ if and only if $|z| \leq 1$.) Let $g$ be a continuous mapping of $D$ into the unit circle $T$. (Thus, $|g(z)|=1$ for every $z \in \bar{D}$.)

Prove that $g(z)=-z$ for at least one $z \in T$.

Hint: For $0 \leq r \leq 1,0 \leq t \leq 2 \pi$, put

$$
\gamma_{r}(t)=g\left(r e^{t t}\right),
$$

and put $\psi(t)=e^{-t t} \gamma_{1}(t)$. If $g(z) \neq-z$ for every $z \in T$, then $\psi(t) \neq-1$ for every $t \in[0,2 \pi]$. Hence Ind $(\psi)=0$, by Exercises 24 and 26. It follows that Ind $\left(\gamma_{1}\right)=1$. But Ind $\left(\gamma_{0}\right)=0$. Derive a contradiction, as in Exercise 27.

29. Prove that every continuous mapping $f$ of $\bar{D}$ into $D$ has a fixed point in $D$. (This is the 2-dimensional case of Brouwer's fixed-point theorem.)

Hint: Assume $f(z) \neq z$ for every $z \in D$. Associate to each $z \in D$ the point $g(z) \in T$ which lies on the ray that starts at $f(z)$ and passes through $z$. Then $g$ maps $D$ into $T, g(z)=z$ if $z \in T$, and $g$ is continuous, because

$$
g(z)=z-s(z)[f(z)-z]
$$

where $s(z)$ is the unique nonnegative root of a certain quadratic equation whose coefficients are continuous functions of $f$ and $z$. Apply Exercise 28 .

30. Use Stirling's formula to prove that

$$
\lim _{x \rightarrow \infty} \frac{\Gamma(x+c)}{x^{c} \Gamma(x)}=1
$$

for every real constant $c$.

31. In the proof of Theorem 7.26 it was shown that

$$
\int_{-1}^{1}\left(1-x^{2}\right)^{n} d x \geq \frac{4}{3 \sqrt{n}}
$$

for $n=1,2,3, \ldots$. Use Theorem 8.20 and Exercise 30 to show the more precise result

$$
\lim _{n \rightarrow \infty} \sqrt{n} \int_{-1}^{1}\left(1-x^{2}\right)^{n} d x=\sqrt{\pi}
$$

## FUNCTIONS OF SEVERAL VARIABLES

## LINEAR TRANSFORMATIONS

We begin this chapter with a discussion of sets of vectors in euclidean $n$-space $R^{n}$. The algebraic facts presented here extend without change to finite-dimensional vector spaces over any field of scalars. However, for our purposes it is quite sufficient to stay within the familiar framework provided by the euclidean spaces.

### 9.1 Definitions

(a) A nonempty set $X \subset R^{n}$ is a vector space if $\mathbf{x}+\mathbf{y} \in X$ and $c \mathbf{x} \in X$ for all $\mathbf{x} \in X, \mathbf{y} \in X$, and for all scalars $c$.

(b) If $\mathbf{x}_{1}, \ldots, \mathbf{x}_{k} \in R^{n}$ and $c_{1}, \ldots, c_{k}$ are scalars, the vector

$$
c_{1} \mathbf{x}_{1}+\cdots+c_{k} \mathbf{x}_{k}
$$

is called a linear combination of $\mathbf{x}_{1}, \ldots, \mathbf{x}_{k}$. If $S \subset R^{n}$ and if $E$ is the set of all linear combinations of elements of $S$, we say that $S$ spans $E$, or that $E$ is the span of $S$.

Observe that every span is a vector space.
(c) A set consisting of vectors $\mathbf{x}_{1}, \ldots, \mathbf{x}_{k}$ (we shall use the notation $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{k}\right\}$ for such a set) is said to be independent if the relation $c_{1} \mathbf{x}_{1}+\cdots+c_{k} \mathbf{x}_{k}=\mathbf{0}$ implies that $c_{1}=\cdots=c_{k}=0$. Otherwise $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{k}\right\}$ is said to be dependent.

Observe that no independent set contains the null vector.

(d) If a vector space $X$ contains an independent set of $r$ vectors but contains no independent set of $r+1$ vectors, we say that $X$ has dimension $r$, and write: $\operatorname{dim} X=r$.

The set consisting of $\mathbf{0}$ alone is a vector space; its dimension is 0 .

(e) An independent subset of a vector space $X$ which spans $X$ is called a basis of $X$.

Observe that if $B=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{r}\right\}$ is a basis of $X$, then every $\mathbf{x} \in X$ has a unique representation of the form $\mathbf{x}=\Sigma c_{j} \mathbf{x}_{j}$. Such a representation exists since $B$ spans $X$, and it is unique since $B$ is independent. The numbers $c_{1}, \ldots, c_{r}$ are called the coordinates of $\mathbf{x}$ with respect to the basis $B$.

The most familiar example of a basis is the set $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$, where $\mathbf{e}_{j}$ is the vector in $R^{n}$ whose $j$ th coordinate is 1 and whose other coordinates are all 0 . If $\mathbf{x} \in R^{n}, \mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)$, then $\mathbf{x}=\Sigma x_{j} \mathbf{e}_{j}$. We shall call

$$
\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}
$$

the standard basis of $R^{n}$.

9.2 Theorem Let $r$ be a positive integer. If a vector space $X$ is spanned by a set of $r$ vectors, then $\operatorname{dim} X \leq r$.

Proof If this is false, there is a vector space $X$ which contains an independent set $Q=\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r+1}\right\}$ and which is spanned by a set $S_{0}$ consisting of $r$ vectors.

Suppose $0 \leq i<r$, and suppose a set $S_{i}$ has been constructed which spans $X$ and which consists of all $\mathbf{y}_{j}$ with $1 \leq j \leq i$ plus a certain collection of $r-i$ members of $S_{0}$, say $\mathbf{x}_{1}, \ldots, \mathbf{x}_{r-i}$. (In other words, $S_{i}$ is obtained from $S_{0}$ by replacing $i$ of its elements by members of $Q$, without altering the span.) Since $S_{i}$ spans $X, \mathbf{y}_{i+1}$ is in the span of $S_{i}$; hence there are scalars $a_{1}, \ldots, a_{i+1}, b_{1}, \ldots, b_{r-i}$, with $a_{i+1}=1$, such that

$$
\sum_{j=1}^{i+1} a_{j} \mathbf{y}_{j}+\sum_{k=1}^{r-i} b_{k} \mathbf{x}_{k}=\mathbf{0}
$$

If all $b_{k}$ 's were 0 , the independence of $Q$ would force all $a_{j}$ 's to be 0 , a contradiction. It follows that some $\mathbf{x}_{k} \in S_{i}$ is a linear combination of the other members of $T_{i}=S_{i} \cup\left\{\mathbf{y}_{i+1}\right\}$. Remove this $\mathbf{x}_{k}$ from $T_{i}$ and call the remaining set $S_{i+1}$. Then $S_{i+1}$ spans the same set as $T_{i}$, namely $X$, so that $S_{i+1}$ has the properties postulated for $S_{i}$ with $i+1$ in place of $i$.

Starting with $S_{0}$, we thus construct sets $S_{1}, \ldots, S_{r}$. The last of these consists of $\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}$, and our construction shows that it spans $X$. But $Q$ is independent; hence $\mathbf{y}_{r+1}$ is not in the span of $S_{r}$. This contradiction establishes the theorem.

Corollary $\operatorname{dim} R^{n}=n$.

Proof Since $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ spans $R^{n}$, the theorem shows that $\operatorname{dim} R^{n} \leq n$. Since $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ is independent, $\operatorname{dim} R^{n} \geq n$.

9.3 Theorem Suppose $X$ is a vector space, and $\operatorname{dim} X=n$.

(a) A set $E$ of $n$ vectors in $X$ spans $X$ if and only if $E$ is independent.

(b) $X$ has a basis, and every basis consists of $n$ vectors.

(c) If $1 \leq r \leq n$ and $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$ is an independent set in $X$, then $X$ has $a$ basis containing $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$.

Proof Suppose $E=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$. Since $\operatorname{dim} X=n$, the set $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{y}\right\}$ is dependent, for every $\mathbf{y} \in X$. If $E$ is independent, it follows that $\mathbf{y}$ is in the span of $E$; hence $E$ spans $X$. Conversely, if $E$ is dependent, one of its members can be removed without changing the span of $E$. Hence $E$ cannot span $X$, by Theorem 9.2. This proves $(a)$.

Since $\operatorname{dim} X=n, X$ contains an independent set of $n$ vectors, and (a) shows that every such set is a basis of $X ;(b)$ now follows from $9.1(d)$ and 9.2.

To prove $(c)$, let $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ be a basis of $X$. The set

$$
S=\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}, \mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}
$$

spans $X$ and is dependent, since it contains more than $n$ vectors. The argument used in the proof of Theorem 9.2 shows that one of the $\mathbf{x}_{i}{ }^{\prime}$ 's is a linear combination of the other members of $S$. If we remove this $\mathbf{x}_{i}$ from $S$, the remaining set still spans $X$. This process can be repeated $r$ times and leads to a basis of $X$ which contains $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$, by $(a)$.

9.4 Definitions A mapping $A$ of a vector space $X$ into a vector space $Y$ is said to be a linear transformation if

$$
A\left(\mathbf{x}_{1}+\mathbf{x}_{2}\right)=A \mathbf{x}_{1}+A \mathbf{x}_{2}, \quad A(c \mathbf{x})=c A \mathbf{x}
$$

for all $\mathbf{x}, \mathbf{x}_{1}, \mathbf{x}_{2} \in X$ and all scalars $c$. Note that one often writes $A \mathbf{x}$ instead of $A(\mathbf{x})$ if $A$ is linear.

Observe that $A 0=0$ if $A$ is linear. Observe also that a linear transformation $A$ of $X$ into $Y$ is completely determined by its action on any basis: If
$\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ is a basis of $X$, then every $\mathbf{x} \in X$ has a unique representation of the form

$$
\mathbf{x}=\sum_{i=1}^{n} c_{i} \mathbf{x}_{i}
$$

and the linearity of $A$ allows us to compute $A \mathbf{x}$ from the vectors $A \mathbf{x}_{1}, \ldots, A \mathbf{x}_{n}$ and the coordinates $c_{1}, \ldots, c_{n}$ by the formula

$$
A \mathbf{x}=\sum_{i=1}^{n} c_{i} A \mathbf{x}_{i}
$$

Linear transformations of $X$ into $X$ are often called linear operators on $X$. If $A$ is a linear operator on $X$ which (i) is one-to-one and (ii) maps $X$ onto $X$, we say that $A$ is invertible. In this case we can define an operator $A^{-1}$ on $X$ by requiring that $A^{-1}(A \mathbf{x})=\mathbf{x}$ for all $\mathbf{x} \in X$. It is trivial to verify that we then also have $A\left(A^{-1} \mathbf{x}\right)=\mathbf{x}$, for all $\mathbf{x} \in X$, and that $A^{-1}$ is linear.

An important fact about linear operators on finite-dimensional vector spaces is that each of the above conditions (i) and (ii) implies the other:

9.5 Theorem $A$ linear operator $A$ on a finite-dimensional vector space $X$ is one-to-one if and only if the range of $A$ is all of $X$.

Proof Let $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ be a basis of $X$. The linearity of $A$ shows that its range $\mathscr{R}(A)$ is the span of the set $Q=\left\{A \mathbf{x}_{1}, \ldots, A \mathbf{x}_{n}\right\}$. We therefore infer from Theorem 9.3 $(a)$ that $\mathscr{R}(A)=X$ if and only if $Q$ is independent. We have to prove that this happens if and only if $A$ is one-to-one.

Suppose $A$ is one-to-one and $\Sigma c_{i} A \mathbf{x}_{i}=\mathbf{0}$. Then $A\left(\Sigma c_{i} \mathbf{x}_{i}\right)=\mathbf{0}$, hence $\Sigma c_{i} \mathbf{x}_{i}=\mathbf{0}$, hence $c_{1}=\cdots=c_{n}=0$, and we conclude that $Q$ is independent. Conversely, suppose $Q$ is independent and $A\left(\Sigma c_{i} \mathbf{x}_{i}\right)=\mathbf{0}$. Then $\Sigma c_{i} A \mathbf{x}_{i}=\mathbf{0}$, hence $c_{1}=\cdots=c_{n}=0$, and we conclude: $A \mathbf{x}=\mathbf{0}$ only if $\mathbf{x}=\mathbf{0}$. If now $A \mathbf{x}=A \mathbf{y}$, then $A(\mathbf{x}-\mathbf{y})=\boldsymbol{A} \mathbf{x}-\boldsymbol{A} \mathbf{y}=\mathbf{0}$, so that $\mathbf{x}-\mathbf{y}=\mathbf{0}$, and this says that $A$ is one-to-one.

### 9.6 Definitions

(a) Let $L(X, Y)$ be the set of all linear transformations of the vector space $X$ into the vector space $Y$. Instead of $L(X, X)$, we shall simply write $L(X)$. If $A_{1}, A_{2} \in L(X, Y)$ and if $c_{1}, c_{2}$ are scalars, define $c_{1} A_{1}+c_{2} A_{2}$ by

$$
\left(c_{1} A_{1}+c_{2} A_{2}\right) \mathbf{x}=c_{1} A_{1} \mathbf{x}+c_{2} A_{2} \mathbf{x} \quad(\mathbf{x} \in X)
$$

It is then clear that $c_{1} A_{1}+c_{2} A_{2} \in L(X, Y)$.

(b) If $X, Y, Z$ are vector spaces, and if $A \in L(X, Y)$ and $B \in L(Y, Z)$, we define their product $B A$ to be the composition of $A$ and $B$ :

$$
(B A) \mathbf{x}=B(A \mathbf{x}) \quad(\mathbf{x} \in X)
$$

Then $B A \in L(X, Z)$.

Note that $B A$ need not be the same as $A B$, even if $X=Y=Z$.

(c) For $A \in L\left(R^{n}, R^{m}\right)$, define the norm $\|A\|$ of $A$ to be the sup of all numbers $|A \mathbf{x}|$, where $\mathbf{x}$ ranges over all vectors in $R^{n}$ with $|\mathbf{x}| \leq 1$. Observe that the inequality

$$
|A \mathbf{x}| \leq\|A\||\mathbf{x}|
$$

holds for all $\mathbf{x} \in R^{n}$. Also, if $\lambda$ is such that $|A \mathbf{x}| \leq \lambda|\mathbf{x}|$ for all $\mathbf{x} \in R^{n}$, then $\|A\| \leq \lambda$.

### 9.7 Theorem

(a) If $A \in L\left(R^{n}, R^{m}\right)$, then $\|A\|<\infty$ and $A$ is a uniformly continuous mapping of $R^{n}$ into $R^{m}$.

(b) If $A, B \in L\left(R^{n}, R^{m}\right)$ and c is a scalar, then

$$
\|A+B\| \leq\|A\|+\|B\|, \quad\|c A\|=|c|\|A\| .
$$

With the distance between $A$ and $B$ defined as $\|A-B\|, L\left(R^{n}, R^{m}\right)$ is a metric space.

(c) If $A \in L\left(R^{n}, R^{m}\right)$ and $B \in L\left(R^{m}, R^{k}\right)$, then

$$
\|B A\| \leq\|B\|\|A\| .
$$

## Proof

(a) Let $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ be the standard basis in $R^{n}$ and suppose $\mathbf{x}=\Sigma c_{i} \mathbf{e}_{i}$, $|\mathbf{x}| \leq 1$, so that $\left|c_{i}\right| \leq 1$ for $i=1, \ldots, n$. Then

$$
|A \mathbf{x}|=\left|\sum c_{i} A \mathbf{e}_{i}\right| \leq \sum\left|c_{i}\right|\left|A \mathbf{e}_{i}\right| \leq \sum\left|A \mathbf{e}_{i}\right|
$$

so that

$$
\|A\| \leq \sum_{i=1}^{n}\left|A \mathbf{e}_{i}\right|<\infty
$$

Since $|A \mathbf{x}-A \mathbf{y}| \leq\|A\||\mathbf{x}-\mathbf{y}|$ if $\mathbf{x}, \mathbf{y} \in R^{n}$, we see that $A$ is uniformly continuous.

(b) The inequality in (b) follows from

$$
|(A+B) \mathbf{x}|=|A \mathbf{x}+B \mathbf{x}| \leq|A \mathbf{x}|+|B \mathbf{x}| \leq(\|A\|+\|B\|)|\mathbf{x}|
$$

The second part of $(b)$ is proved in the same manner. If

$$
A, B, C \in L\left(R^{n}, R^{m}\right)
$$

we have the triangle inequality

$$
\|A-C\|=\|(A-B)+(B-C)\| \leq\|A-B\|+\|B-C\|,
$$

and it is easily verified that $\|A-B\|$ has the other properties of a metric (Definition 2.15).

(c) Finally, (c) follows from

$$
|(B A) \mathbf{x}|=|B(A \mathbf{x})| \leq\|B\||A \mathbf{x}| \leq\|B\|\|A\||\mathbf{x}| .
$$

Since we now have metrics in the spaces $L\left(R^{n}, R^{m}\right)$, the concepts of open set, continuity, etc., make sense for these spaces. Our next theorem utilizes these concepts.

9.8 Theorem Let $\Omega$ be the set of all invertible linear operators on $R^{n}$.

(a) If $A \in \Omega, B \in L\left(R^{n}\right)$, and

$$
\|B-A\| \cdot\left\|A^{-1}\right\|<1
$$

then $B \in \Omega$.

(b) $\Omega$ is an open subset of $L\left(R^{n}\right)$, and the mapping $A \rightarrow A^{-1}$ is continuous on $\Omega$.

(This mapping is also obviously a $1-1$ mapping of $\Omega$ onto $\Omega$, which is its own inverse.)

## Proof

(a) Put $\left\|A^{-1}\right\|=1 / \alpha$, put $\|B-A\|=\beta$. Then $\beta<\alpha$. For every $\mathbf{x} \in R^{n}$,

$$
\begin{aligned}
\alpha|\mathbf{x}| & =\alpha\left|A^{-1} A \mathbf{x}\right| \leq \alpha\left\|A^{-1}\right\| \cdot|A \mathbf{x}| \\
& =|A \mathbf{x}| \leq|(A-B) \mathbf{x}|+|B \mathbf{x}| \leq \beta|\mathbf{x}|+|B \mathbf{x}|,
\end{aligned}
$$

so that

$$
(\alpha-\beta)|\mathbf{x}| \leq|B \mathbf{x}| \quad\left(\mathbf{x} \in R^{n}\right)
$$

Since $\alpha-\beta>0$, (1) shows that $B \mathbf{x} \neq 0$ if $\mathbf{x} \neq 0$. Hence $B$ is $1-1$. By Theorem 9.5, $B \in \Omega$. This holds for all $B$ with $\|B-A\|<\alpha$. Thus we have $(a)$ and the fact that $\Omega$ is open.

(b) Next, replace $\mathbf{x}$ by $B^{-1} \mathbf{y}$ in (1). The resulting inequality

$$
(\alpha-\beta)\left|B^{-1} \mathbf{y}\right| \leq\left|B B^{-1} \mathbf{y}\right|=|\mathbf{y}| \quad\left(\mathbf{y} \in R^{n}\right)
$$

shows that $\left\|B^{-1}\right\| \leq(\alpha-\beta)^{-1}$. The identity

$$
B^{-1}-A^{-1}=B^{-1}(A-B) A^{-1}
$$

combined with Theorem 9.7(c), implies therefore that

$$
\left\|B^{-1}-A^{-1}\right\| \leq\left\|B^{-1}\right\|\|A-B\|\left\|A^{-1}\right\| \leq \frac{\beta}{\alpha(\alpha-\beta)}
$$

This establishes the continuity assertion made in $(b)$, since $\beta \rightarrow 0$ as $B \rightarrow A$.

9.9 Matrices Suppose $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ and $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{m}\right\}$ are bases of vector spaces $X$ and $Y$, respectively. Then every $A \in L(X, Y)$ determines a set of numbers $a_{i j}$ such that

$$
A \mathbf{x}_{j}=\sum_{i=1}^{m} a_{i j} \mathbf{y}_{i} \quad(1 \leq j \leq n)
$$

It is convenient to visualize these numbers in a rectangular array of $m$ rows and $n$ columns, called an $m$ by $n$ matrix:

$$
[A]=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\cdots \cdots & \cdots \cdots & \cdots & \cdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m n}
\end{array}\right]
$$

Observe that the coordinates $a_{\imath j}$ of the vector $A \mathbf{x}_{j}$ (with respect to the basis $\left.\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{m}\right\}\right)$ appear in the $j$ th column of $[A]$. The vectors $A \mathbf{x}_{j}$ are therefore sometimes called the column vectors of $[A]$. With this terminology, the range of $A$ is spanned by the column vectors of $[A]$.

If $\mathbf{x}=\Sigma c_{j} \mathbf{x}_{j}$, the linearity of $A$, combined with (3), shows that

$$
A \mathbf{x}=\sum_{i=1}^{m}\left(\sum_{j=1}^{n} a_{i j} c_{j}\right) \mathbf{y}_{i}
$$

Thus the coordinates of $A \mathbf{x}$ are $\Sigma_{j} a_{i j} c_{j}$. Note that in (3) the summation ranges over the first subscript of $a_{i j}$, but that we sum over the second subscript when computing coordinates.

Suppose next that an $m$ by $n$ matrix is given, with real entries $a_{i j}$. If $A$ is then defined by (4), it is clear that $A \in L(X, Y)$ and that $[A]$ is the given matrix. Thus there is a natural 1-1 correspondence between $L(X, Y)$ and the set of all real $m$ by $n$ matrices. We emphasize, though, that $[A]$ depends not only on $A$ but also on the choice of bases in $X$ and $Y$. The same $A$ may give rise to many different matrices if we change bases, and vice versa. We shall not pursue this observation any further, since we shall usually work with fixed bases. (Some remarks on this may be found in Sec. 9.37.)

If $Z$ is a third vector space, with basis $\left\{\mathbf{z}_{1}, \ldots, \mathbf{z}_{\boldsymbol{p}}\right\}$, if $\boldsymbol{A}$ is given by (3), and if

$$
B \mathbf{y}_{i}=\sum_{k} b_{k i} \mathbf{z}_{k}, \quad(B A) \mathbf{x}_{j}=\sum_{k} c_{k j} \mathbf{z}_{k}
$$

then $A \in L(X, Y), B \in L(Y, Z), B A \in L(X, Z)$, and since

$$
\begin{aligned}
B\left(A \mathbf{x}_{j}\right) & =B \sum_{i} a_{i j} \mathbf{y}_{i}=\sum_{i} a_{i j} B \mathbf{y}_{i} \\
& =\sum a_{i j} \sum_{k} b_{k i} \mathbf{z}_{k}=\sum_{k}\left(\sum_{i} b_{k i} a_{i j}\right) \mathbf{z}_{k},
\end{aligned}
$$

the independence of $\left\{\mathbf{z}_{1}, \ldots, \mathbf{z}_{p}\right\}$ implies that

$$
c_{k j}=\sum_{i} b_{k i} a_{i j} \quad(1 \leq k \leq p, 1 \leq j \leq n)
$$

This shows how to compute the $p$ by $n$ matrix $[B A]$ from $[B]$ and $[A]$. If we define the product $[B][A]$ to be $[B A]$, then (5) describes the usual rule of matrix multiplication.

Finally, suppose $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ and $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{m}\right\}$ are standard bases of $R^{n}$ and $R^{m}$, and $A$ is given by (4). The Schwarz inequality shows that

$$
|A \mathbf{x}|^{2}=\sum_{i}\left(\sum_{j} a_{i j} c_{j}\right)^{2} \leq \sum_{i}\left(\sum_{j} a_{i j}^{2} \cdot \sum_{j} c_{j}^{2}\right)=\sum_{i, j} a_{i j}^{2}|\mathbf{x}|^{2} .
$$

Thus

$$
\|A\| \leq\left\{\sum_{i, j} a_{i j}^{2}\right\}^{1 / 2}
$$

If we apply (6) to $B-A$ in place of $A$, where $A, B \in L\left(R^{n}, R^{m}\right)$, we see that if the matrix elements $a_{i j}$ are continuous functions of a parameter, then the same is true of $A$. More precisely:

If $S$ is a metric space, if $a_{11}, \ldots, a_{m n}$ are real continuous functions on $S$, and if, for each $p \in S, A_{p}$ is the linear transformation of $R^{n}$ into $R^{m}$ whose matrix has entries $a_{i j}(p)$, then the mapping $p \rightarrow A_{p}$ is a continuous mapping of $S$ into $L\left(R^{n}, R^{m}\right)$.

## DIFFERENTIATION

9.10 Preliminaries In order to arrive at a definition of the derivative of a function whose domain is $R^{n}$ (or an open subset of $R^{n}$ ), let us take another look at the familiar case $n=1$, and let us see how to interpret the derivative in that case in a way which will naturally extend to $n>1$.

If $f$ is a real function with domain $(a, b) \subset R^{1}$ and if $x \in(a, b)$, then $f^{\prime}(x)$ is usually defined to be the real number

$$
\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}
$$

provided, of course, that this limit exists. Thus

$$
f(x+h)-f(x)=f^{\prime}(x) h+r(h)
$$

where the "remainder" $r(h)$ is small, in the sense that

$$
\lim _{h \rightarrow 0} \frac{r(h)}{h}=0
$$

Note that (8) expresses the difference $f(x+h)-f(x)$ as the sum of the linear function that takes $h$ to $f^{\prime}(x) h$, plus a small remainder.

We can therefore regard the derivative of $f$ at $x$, not as a real number, but as the linear operator on $R^{1}$ that takes $h$ to $f^{\prime}(x) h$.

[Observe that every real number $\alpha$ gives rise to a linear operator on $R^{1}$; the operator in question is simply multiplication by $\alpha$. Conversely, every linear function that carries $R^{1}$ to $R^{1}$ is multiplication by some real number. It is this natural 1-1 correspondence between $R^{1}$ and $L\left(R^{1}\right)$ which motivates the preceding statements.]

Let us next consider a function $f$ that maps $(a, b) \subset R^{1}$ into $R^{m}$. In that case, $f^{\prime}(x)$ was defined to be that vector $\mathbf{y} \in R^{m}$ (if there is one) for which

$$
\lim _{h \rightarrow 0}\left\{\frac{f(x+h)-\mathbf{f}(x)}{h}-\mathbf{y}\right\}=\mathbf{0} \text {. }
$$

We can again rewrite this in the form

$$
\mathbf{f}(x+h)-\mathbf{f}(x)=h \mathbf{y}+\mathbf{r}(h)
$$

where $\mathbf{r}(h) / h \rightarrow \mathbf{0}$ as $h \rightarrow 0$. The main term on the right side of (11) is again a linear function of $h$. Every $y \in R^{m}$ induces a linear transformation of $R^{1}$ into $R^{m}$, by associating to each $h \in R^{1}$ the vector $h y \in R^{m}$. This identification of $R^{m}$ with $L\left(R^{1}, R^{m}\right)$ allows us to regard $f^{\prime}(x)$ as a member of $L\left(R^{1}, R^{m}\right)$.

Thus, if $\mathrm{f}$ is a differentiable mapping of $(a, b) \subset R^{1}$ into $R^{m}$, and if $x \in(a, b)$, then $f^{\prime}(x)$ is the linear transformation of $R^{1}$ into $R^{m}$ that satisfies

$$
\lim _{h \rightarrow 0} \frac{\mathbf{f}(x+h)-\mathbf{f}(x)-\mathbf{f}^{\prime}(x) h}{h}=\mathbf{0},
$$

or, equivalently,

$$
\lim _{h \rightarrow 0} \frac{\left|\mathbf{f}(x+h)-\mathbf{f}(x)-\mathbf{f}^{\prime}(x) h\right|}{|h|}=0
$$

We are now ready for the case $n>1$.

9.11 Definition Suppose $E$ is an open set in $R^{n}, \mathbf{f}$ maps $E$ into $R^{m}$, and $\mathrm{x} \in E$. If there exists a linear transformation $A$ of $R^{n}$ into $R^{m}$ such that

$$
\lim _{\mathbf{h} \rightarrow 0} \frac{|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A \mathbf{h}|}{|\mathbf{h}|}=0
$$

then we say that $\mathbf{f}$ is differentiable at $\mathbf{x}$, and we write

$$
\mathbf{f}^{\prime}(\mathbf{x})=A \text {. }
$$

If $\mathbf{f}$ is differentiable at every $\mathbf{x} \in E$, we say that $\mathbf{f}$ is differentiable in $E$.

It is of course understood in (14) that $\mathbf{h} \in R^{n}$. If $|\mathbf{h}|$ is small enough, then $\mathbf{x}+\mathbf{h} \in E$, since $E$ is open. Thus $\mathbf{f}(\mathbf{x}+\mathbf{h})$ is defined, $\mathbf{f}(\mathbf{x}+\mathbf{h}) \in R^{m}$, and since $A \in L\left(R^{n}, R^{m}\right), A \mathbf{h} \in R^{m}$. Thus

$$
\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A \mathbf{h} \in R^{m}
$$

The norm in the numerator of (14) is that of $R^{m}$. In the denominator we have the $R^{n}$-norm of $\mathbf{h}$.

There is an obvious uniqueness problem which has to be settled before we go any further.

9.12 Theorem Suppose $E$ and $\mathbf{f}$ are as in Definition 9.11, $\mathrm{x} \in E$, and (14) holds with $A=A_{1}$ and with $A=A_{2}$. Then $A_{1}=A_{2}$.

Proof If $B=A_{1}-A_{2}$, the inequality

$$
|B \mathbf{h}| \leq\left|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A_{1} \mathbf{h}\right|+\left|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A_{2} \mathbf{h}\right|
$$

shows that $|B \mathbf{h}| /|\mathbf{h}| \rightarrow 0$ as $\mathbf{h} \rightarrow \mathbf{0}$. For fixed $\mathbf{h} \neq \mathbf{0}$, it follows that

$$
\frac{\mid B(t \mathbf{h})}{|t \mathbf{h}|} \rightarrow 0 \quad \text { as } \quad t \rightarrow 0
$$

The linearity of $B$ shows that the left side of (16) is independent of $t$. Thus $B \mathbf{h}=0$ for every $\mathbf{h} \in R^{n}$. Hence $B=0$.

### 9.13 Remarks

(a) The relation (14) can be rewritten in the form

$$
\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})=\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}+\mathbf{r}(\mathbf{h})
$$

where the remainder $\mathbf{r}(\mathbf{h})$ satisfies

$$
\lim _{\mathbf{h} \rightarrow \mathbf{0}} \frac{|\mathbf{r}(\mathbf{h})|}{|\mathbf{h}|}=0
$$

We may interpret (17), as in Sec. 9.10, by saying that for fixed $\mathbf{x}$ and small $\mathbf{h}$, the left side of (17) is approximately equal to $\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}$, that is, to the value of a linear transformation applied to $\mathbf{h}$.

(b) Suppose $\mathbf{f}$ and $E$ are as in Definition 9.11, and $\mathbf{f}$ is differentiable in $E$. For every $\mathbf{x} \in E, \mathbf{f}^{\prime}(\mathbf{x})$ is then a function, namely, a linear transformation of $R^{n}$ into $R^{m}$. But $\mathbf{f}^{\prime}$ is also a function: $\mathbf{f}^{\prime}$ maps $E$ into $L\left(R^{n}, R^{m}\right)$.

(c) A glance at (17) shows that $\mathbf{f}$ is continuous at any point at which $\mathbf{f}$ is differentiable.

(d) The derivative defined by (14) or (17) is often called the differential of $\mathbf{f}$ at $\mathbf{x}$, or the total derivative of $\mathbf{f}$ at $\mathbf{x}$, to distinguish it from the partial derivatives that will occur later.

9.14 Example We have defined derivatives of functions carrying $R^{n}$ to $R^{m}$ to be linear transformations of $R^{n}$ into $R^{m}$. What is the derivative of such a linear transformation? The answer is very simple.

If $A \in L\left(R^{n}, R^{m}\right)$ and if $\mathbf{x} \in R^{n}$, then

$$
A^{\prime}(\mathbf{x})=A \text {. }
$$

Note that $\mathbf{x}$ appears on the left side of (19), but not on the right. Both sides of (19) are members of $L\left(R^{n}, R^{m}\right)$, whereas $A \mathbf{x} \in R^{m}$.

The proof of (19) is a triviality, since

$$
A(\mathbf{x}+\mathbf{h})-A \mathbf{x}=A \mathbf{h}
$$

by the linearity of $A$. With $\mathbf{f}(\mathbf{x})=A \mathbf{x}$, the numerator in (14) is thus 0 for every $\mathbf{h} \in R^{n}$. In (17), $\mathbf{r}(\mathbf{h})=\mathbf{0}$.

We now extend the chain rule (Theorem 5.5) to the present situation.

9.15 Theorem Suppose $E$ is an open set in $R^{n}, f$ maps $E$ into $R^{m}, f$ is differentiable at $\mathbf{x}_{0} \in E, \mathbf{g}$ maps an open set containing $\mathbf{f}(E)$ into $R^{k}$, and $\mathbf{g}$ is differentiable at $\mathbf{f}\left(\mathbf{x}_{0}\right)$. Then the mapping $\mathbf{F}$ of $E$ into $R^{k}$ defined by

$$
\mathbf{F}(\mathbf{x})=\mathbf{g}(\mathbf{f}(\mathbf{x}))
$$

is differentiable at $\mathbf{x}_{0}$, and

$$
F^{\prime}\left(x_{0}\right)=\mathbf{g}^{\prime}\left(\mathbf{f}\left(x_{0}\right)\right) f^{\prime}\left(x_{0}\right)
$$

On the right side of (21), we have the product of two linear transformations, as defined in Sec. 9.6.

Proof Put $\mathbf{y}_{0}=\mathbf{f}\left(\mathbf{x}_{0}\right), A=\mathbf{f}^{\prime}\left(\mathbf{x}_{0}\right), B=\mathbf{g}^{\prime}\left(\mathbf{y}_{0}\right)$, and define

$$
\begin{aligned}
& \mathbf{u}(\mathbf{h})=\mathbf{f}\left(\mathbf{x}_{0}+\mathbf{h}\right)-\mathbf{f}\left(\mathbf{x}_{0}\right)-A \mathbf{h} \\
& \mathbf{v}(\mathbf{k})=\mathbf{g}\left(\mathbf{y}_{0}+\mathbf{k}\right)-\mathbf{g}\left(\mathbf{y}_{0}\right)-B \mathbf{k}
\end{aligned}
$$

for all $\mathbf{h} \in R^{n}$ and $\mathbf{k} \in R^{m}$ for which $\mathbf{f}\left(\mathbf{x}_{0}+\mathbf{h}\right)$ and $\mathbf{g}\left(\mathbf{y}_{0}+\mathbf{k}\right)$ are defined. Then

$$
|\mathbf{u}(\mathbf{h})|=\varepsilon(\mathbf{h})|\mathbf{h}|, \quad|\mathbf{v}(\mathbf{k})|=\eta(\mathbf{k})|\mathbf{k}|
$$

where $\varepsilon(\mathbf{h}) \rightarrow 0$ as $\mathbf{h} \rightarrow 0$ and $\eta(\mathbf{k}) \rightarrow 0$ as $\mathbf{k} \rightarrow \mathbf{0}$.

Given $\mathbf{h}$, put $\mathbf{k}=\mathbf{f}\left(\mathbf{x}_{0}+\mathbf{h}\right)-\mathbf{f}\left(\mathbf{x}_{0}\right)$. Then

and

$$
|\mathbf{k}|=|A \mathbf{h}+\mathbf{u}(\mathbf{h})| \leq[\|A\|+\varepsilon(\mathbf{h})]|\mathbf{h}|,
$$

$$
\begin{aligned}
\mathbf{F}\left(\mathbf{x}_{0}+\mathbf{h}\right)-\mathbf{F}\left(\mathbf{x}_{0}\right)-B A \mathbf{h} & =\mathbf{g}\left(\mathbf{y}_{0}+\mathbf{k}\right)-\mathbf{g}\left(\mathbf{y}_{0}\right)-B A \mathbf{h} \\
& =B(\mathbf{k}-A \mathbf{h})+\mathbf{v}(\mathbf{k}) \\
& =B \mathbf{u}(\mathbf{h})+\mathbf{v}(\mathbf{k}) .
\end{aligned}
$$

Hence (22) and (23) imply, for $\mathbf{h} \neq \mathbf{0}$, that

$$
\frac{\left|\mathbf{F}\left(\mathbf{x}_{0}+\mathbf{h}\right)-\mathbf{F}\left(\mathbf{x}_{0}\right)-B A \mathbf{h}\right|}{|\mathbf{h}|} \leq\|B\| \varepsilon(\mathbf{h})+[\|A\|+\varepsilon(\mathbf{h})] \eta(\mathbf{k}) .
$$

Let $\mathbf{h} \rightarrow \mathbf{0}$. Then $\varepsilon(\mathbf{h}) \rightarrow 0$. Also, $\mathbf{k} \rightarrow \mathbf{0}$, by (23), so that $\eta(\mathbf{k}) \rightarrow 0$. It follows that $\mathbf{F}^{\prime}\left(\mathbf{x}_{0}\right)=B A$, which is what (21) asserts.

9.16 Partial derivatives We again consider a function $f$ that maps an open set $E \subset R^{n}$ into $R^{m}$. Let $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ and $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{m}\right\}$ be the standard bases of $R^{n}$ and $R^{m}$. The components of $\mathrm{f}$ are the real functions $f_{1}, \ldots, f_{m}$ defined by

$$
\mathbf{f}(\mathbf{x})=\sum_{i=1}^{m} f_{i}(\mathbf{x}) \mathbf{u}_{i} \quad(\mathbf{x} \in E)
$$

or, equivalently, by $f_{i}(\mathbf{x})=\mathbf{f}(\mathbf{x}) \cdot \mathbf{u}_{i}, 1 \leq i \leq m$.

For $\mathbf{x} \in E, 1 \leq i \leq m, 1 \leq j \leq n$, we define

$$
\left(D_{j} f_{i}\right)(\mathbf{x})=\lim _{t \rightarrow 0} \frac{f_{i}\left(\mathbf{x}+t \mathbf{e}_{j}\right)-f_{i}(\mathbf{x})}{t}
$$

provided the limit exists. Writing $f_{i}\left(x_{1}, \ldots, x_{n}\right)$ in place of $f_{i}(x)$, we see that $D_{j} f_{i}$ is the derivative of $f_{i}$ with respect to $x_{j}$, keeping the other variables fixed. The notation

$$
\frac{\partial f_{i}}{\partial x_{j}}
$$

is therefore often used in place of $D_{j} f_{i}$, and $D_{j} f_{i}$ is called a partial derivative.

In many cases where the existence of a derivative is sufficient when dealing with functions of one variable, continuity or at least boundedness of the partial derivatives is needed for functions of several variables. For example, the functions $f$ and $g$ described in Exercise 7, Chap. 4, are not continuous, although their partial derivatives exist at every point of $R^{2}$. Even for continuous functions. the existence of all partial derivatives does not imply differentiability in the sense of Definition 9.11; see Exercises 6 and 14, and Theorem 9.21.

However, if $\mathbf{f}$ is known to be differentiable at a point $\mathbf{x}$, then its partial derivatives exist at $\mathbf{x}$, and they determine the linear transformation $f^{\prime}(\mathbf{x})$ completely:

9.17 Theorem Suppose $\mathrm{f}$ maps an open set $E \subset R^{n}$ into $R^{m}$, and $\mathrm{f}$ is differentiable at a point $\mathbf{x} \in E$. Then the partial derivatives $\left(D_{j} f_{i}\right)(\mathbf{x})$ exist, and

$$
\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{e}_{j}=\sum_{i=1}^{m}\left(D_{j} f_{i}\right)(\mathbf{x}) \mathbf{u}_{i} \quad(1 \leq j \leq n)
$$

Here, as in Sec. 9.16, $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ and $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{m}\right\}$ are the standard bases of $R^{n}$ and $R^{m}$.

Proof Fix $j$. Since $\mathbf{f}$ is differentiable at $\mathbf{x}$,

$$
\mathbf{f}\left(\mathbf{x}+t \mathbf{e}_{j}\right)-\mathbf{f}(\mathbf{x})=\mathbf{f}^{\prime}(\mathbf{x})\left(t \mathbf{e}_{j}\right)+\mathbf{r}\left(t \mathbf{e}_{j}\right)
$$

where $\left|\mathbf{r}\left(t \mathbf{e}_{j}\right)\right| / t \rightarrow 0$ as $t \rightarrow 0$. The linearity of $\mathbf{f}^{\prime}(\mathbf{x})$ shows therefore that

$$
\lim _{t \rightarrow 0} \frac{\mathbf{f}\left(\mathbf{x}+t \mathbf{e}_{j}\right)-\mathbf{f}(\mathbf{x})}{t}=\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{e}_{j}
$$

If we now represent $f$ in terms of its components, as in (24), then (28) becomes

$$
\lim _{t \rightarrow 0} \sum_{i=1}^{m} \frac{f_{i}\left(\mathbf{x}+t \mathbf{e}_{j}\right)-f_{i}(\mathbf{x})}{t} \mathbf{u}_{i}=\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{e}_{j}
$$

It follows that each quotient in this sum has a limit, as $t \rightarrow 0$ (see Theorem 4.10), so that each $\left(D_{j} f_{i}\right)(\mathbf{x})$ exists, and then (27) follows from (29).

Here are some consequences of Theorem 9.17:

Let $\left[\mathbf{f}^{\prime}(\mathbf{x})\right]$ be the matrix that represents $\mathbf{f}^{\prime}(\mathbf{x})$ with respect to our standard bases, as in Sec. 9.9.

Then $\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{e}_{j}$ is the $j$ th column vector of $\left[\mathbf{f}^{\prime}(\mathbf{x})\right]$, and (27) shows therefore that the number $\left(D_{j} f_{i}\right)(\mathbf{x})$ occupies the spot in the $i$ th row and $j$ th column of $\left[\mathbf{f}^{\prime}(\mathbf{x})\right]$. Thus

$$
\left[\mathbf{f}^{\prime}(\mathbf{x})\right]=\left[\begin{array}{lll}
\left(D_{1} f_{1}\right)(\mathbf{x}) & \cdots & \left(D_{n} f_{1}\right)(\mathbf{x}) \\
\cdots \cdots \cdots \cdots \cdots & \cdots & \cdots \\
\left(D_{1} f_{m}\right)(\mathbf{x}) & \cdots & \left(D_{n} f_{m}\right)(\mathbf{x})
\end{array}\right]
$$

If $\mathbf{h}=\Sigma h_{j} \mathbf{e}_{j}$ is any vector in $R^{n}$, then (27) implies that

$$
\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}=\sum_{i=1}^{m}\left\{\sum_{j=1}^{n}\left(D_{j} f_{i}\right)(\mathbf{x}) h_{j}\right\} \mathbf{u}_{i}
$$

9.18 Example Let $\gamma$ be a differentiable mapping of the segment $(a, b) \subset R^{1}$ into an open set $E \subset R^{n}$, in other words, $\gamma$ is a differentiable curve in $E$. Let $f$ be a real-valued differentiable function with domain $E$. Thus $f$ is a differentiable mapping of $E$ into $R^{1}$. Define

$$
g(t)=f(\gamma(t)) \quad(a<t<b) .
$$

The chain rule asserts then that

$$
g^{\prime}(t)=f^{\prime}(\gamma(t)) \gamma^{\prime}(t) \quad(a<t<b)
$$

Since $\gamma^{\prime}(t) \in L\left(R^{1}, R^{n}\right)$ and $f^{\prime}(\gamma(t)) \in L\left(R^{n}, R^{1}\right)$, (32) defines $g^{\prime}(t)$ as a linear operator on $R^{1}$. This agrees with the fact that $g$ maps $(a, b)$ into $R^{1}$. However, $g^{\prime}(t)$ can also be regarded as a real number. (This was discussed in Sec. 9.10.) This number can be computed in terms of the partial derivatives of $f$ and the derivatives of the components of $\gamma$, as we shall now see.

With respect to the standard basis $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ of $R^{n},\left[\gamma^{\prime}(t)\right]$ is the $n$ by 1 matrix (a "column matrix") which has $\gamma_{i}^{\prime}(t)$ in the $i$ th row, where $\gamma_{1}, \ldots, \gamma_{n}$ are the components of $\gamma$. For every $\mathbf{x} \in E,\left[f^{\prime}(\mathbf{x})\right]$ is the 1 by $n$ matrix (a "row matrix") which has $\left(D_{j} f\right)(\mathbf{x})$ in the $j$ th column. Hence $\left[g^{\prime}(t)\right]$ is the 1 by 1 matrix whose only entry is the real number

$$
g^{\prime}(t)=\sum_{i=1}^{n}\left(D_{i} f\right)(\gamma(t)) \gamma_{i}^{\prime}(t)
$$

This is a frequently encountered special case of the chain rule. It can be rephrased in the following manner.

Associate with each $\mathbf{x} \in E$ a vector, the so-called "grarient" of $f$ at $\mathbf{x}$, defined by

$$
(\nabla f)(\mathbf{x})=\sum_{i=1}^{n}\left(D_{i} f\right)(\mathbf{x}) \mathbf{e}_{i}
$$

Since

$$
\gamma^{\prime}(t)=\sum_{i=1}^{n} \gamma_{i}^{\prime}(t) \mathbf{e}_{i}
$$

(33) can be written in the form

$$
g^{\prime}(t)=(\nabla f)(\gamma(t)) \cdot \gamma^{\prime}(t)
$$

the scalar product of the vectors $(\nabla f)(\gamma(t))$ and $\gamma^{\prime}(t)$.

Let us now fix an $\mathbf{x} \in E$, let $\mathbf{u} \in R^{n}$ be a unit vector (that is, $|\mathbf{u}|=1$ ), and specialize $\gamma$ so that

$$
\gamma(t)=\mathbf{x}+t \mathbf{u} \quad(-\infty<t<\infty) .
$$

Then $\gamma^{\prime}(t)=\mathbf{u}$ for every $t$. Hence (36) shows that

$$
g^{\prime}(0)=(\nabla f)(\mathbf{x}) \cdot \mathbf{u} .
$$

On the other hand, (37) shows that

$$
g(t)-g(0)=f(\mathbf{x}+t \mathbf{u})-f(\mathbf{x})
$$

Hence (38) gives

$$
\lim _{t \rightarrow 0} \frac{f(\mathbf{x}+t \mathbf{u})-f(\mathbf{x})}{t}=(\nabla f)(\mathbf{x}) \cdot \mathbf{u} .
$$

The limit in (39) is usually called the directional derivative of $f$ at $\mathbf{x}$, in the direction of the unit vector $\mathbf{u}$, and may be denoted by $\left(D_{\mathbf{u}} f\right)(\mathbf{x})$.

If $f$ and $\mathbf{x}$ are fixed, but $\mathbf{u}$ varies, then (39) shows that $\left(D_{\mathbf{u}} f\right)(\mathbf{x})$ attains its maximum when $\mathbf{u}$ is a positive scalar multiple of $(\nabla f)(\mathbf{x})$. [The case $(\nabla f)(\mathbf{x})=0$ should be excluded here.]

If $\mathbf{u}=\Sigma u_{i} \mathbf{e}_{i}$, then (39) shows that $\left(D_{\mathbf{v}} f\right)(\mathbf{x})$ can be expressed in terms of the partial derivatives of $f$ at $\mathbf{x}$ by the formula

$$
\left(D_{\mathrm{n}} f\right)(\mathbf{x})=\sum_{i=1}^{n}\left(D_{i} f\right)(\mathbf{x}) u_{i}
$$

Some of these ideas will play a role in the following theorem.

9.19 Theorem Suppose $\mathrm{f}$ maps a convex open set $E \subset R^{n}$ into $R^{m}, \mathrm{f}$ is differentiable in $E$, and there is a real number $M$ such that

$$
\left\|\mathbf{f}^{\prime}(\mathbf{x})\right\| \leq M
$$

for every $\mathbf{x} \in E$. Then

$$
|\mathbf{f}(\mathbf{b})-\mathbf{f}(\mathbf{a})| \leq M|\mathbf{b}-\mathbf{a}|
$$

for all $\mathbf{a} \in E, \mathbf{b} \in E$.

Proof Fix $\mathbf{a} \in E, \mathbf{b} \in E$. Define

$$
\gamma(t)=(1-t) \mathbf{a}+t \mathbf{b}
$$

for all $t \in R^{1}$ such that $\gamma(t) \in E$. Since $E$ is convex, $\gamma(t) \in E$ if $0 \leq t \leq 1$. Put

$$
\mathbf{g}(t)=\mathbf{f}(\gamma(t))
$$

Then

$$
\mathbf{g}^{\prime}(t)=\mathbf{f}^{\prime}(\gamma(t)) \gamma^{\prime}(t)=\mathbf{f}^{\prime}(\gamma(t))(\mathbf{b}-\mathbf{a})
$$

so that

$$
\left|\mathbf{g}^{\prime}(t)\right| \leq\left\|\mathbf{f}^{\prime}(\gamma(t))\right\||\mathbf{b}-\mathbf{a}| \leq M|\mathbf{b}-\mathbf{a}|
$$

for all $t \in[0,1]$. By Theorem 5.19,

$$
|\mathbf{g}(1)-\mathbf{g}(0)| \leq M|\mathbf{b}-\mathbf{a}|
$$

But $\mathbf{g}(0)=\mathbf{f}(\mathbf{a})$ and $\mathbf{g}(1)=\mathbf{f}(\mathbf{b})$. This completes the proof.

Corollary If, in addition, $\mathbf{f}^{\prime}(\mathbf{x})=\mathbf{0}$ for all $\mathbf{x} \in E$, then $\mathbf{f}$ is constant.

Proof To prove this, note that the hypotheses of the theorem hold now with $M=0$.

9.20 Definition A differentiable mapping $\mathrm{f}$ of an open set $E \subset R^{n}$ into $R^{m}$ is said to be continuously differentiable in $E$ if $\mathbf{f}^{\prime}$ is a continuous mapping of $E$ into $L\left(R^{n}, R^{m}\right)$.

More explicitly, it is required that to every $\mathbf{x} \in E$ and to every $\varepsilon>0$ corresponds a $\delta>0$ such that

$$
\left\|\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right\|<\varepsilon
$$

if $\mathbf{y} \in E$ and $|\mathbf{x}-\mathbf{y}|<\delta$.

If this is so, we also say that $\mathrm{f}$ is a $\mathscr{C}^{\prime}$-mapping, or that $\mathbf{f} \in \mathscr{C}^{\prime}(E)$.

9.21 Theorem Suppose $\mathrm{f}$ maps an open set $E \subset R^{n}$ into $R^{m}$. Then $\mathrm{f} \in \mathscr{C}^{\prime}(E)$ if and only if the partial derivatives $D_{j} f_{i}$ exist and are continuous on $E$ for $1 \leq i \leq m$, $1 \leq j \leq n$.

Proof Assume first that $\mathbf{f} \in \mathscr{C}^{\prime}(E)$. By (27),

$$
\left(D_{j} f_{i}\right)(\mathbf{x})=\left(\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{e}_{j}\right) \cdot \mathbf{u}_{i}
$$

for all $i, j$, and for all $\mathrm{x} \in E$. Hence

$$
\left(D_{j} f_{i}\right)(\mathbf{y})-\left(D_{j} f_{i}\right)(\mathbf{x})=\left\{\left[\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right] \mathbf{e}_{j}\right\} \cdot \mathbf{u}_{i}
$$

and since $\left|\mathbf{u}_{i}\right|=\left|\mathbf{e}_{j}\right|=1$, it follows that

$$
\begin{aligned}
\left|\left(D_{j} f_{i}\right)(\mathbf{y})-\left(D_{j} f_{i}\right)(\mathbf{x})\right| & \leq\left|\left[\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right] \mathbf{e}_{j}\right| \\
& \leq\left\|\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right\| .
\end{aligned}
$$

Hence $D_{j} f_{i}$ is continuous.

For the converse, it suffices to consider the case $m=1$. (Why?) Fix $\mathbf{x} \in E$ and $\varepsilon>0$. Since $E$ is open, there is an open ball $S \subset E$, with center at $\mathbf{x}$ and radius $r$, and the continuity of the functions $D_{j} f$ shows that $r$ can be chosen so that

$$
\left|\left(D_{j} f\right)(\mathbf{y})-\left(D_{j} f\right)(\mathbf{x})\right|<\frac{\varepsilon}{n} \quad(\mathbf{y} \in S, 1 \leq j \leq n) .
$$

Suppose $\mathbf{h}=\Sigma h_{j} \mathbf{e}_{j},|\mathbf{h}|<r$, put $\mathbf{v}_{0}=\mathbf{0}$, and $\mathbf{v}_{k}=h_{1} \mathbf{e}_{1}+\cdots+h_{k} \mathbf{e}_{k}$, for $1 \leq k \leq n$. Then

$$
f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})=\sum_{j=1}^{n}\left[f\left(\mathbf{x}+\mathbf{v}_{j}\right)-f\left(\mathbf{x}+\mathbf{v}_{j-1}\right)\right]
$$

Since $\left|\mathbf{v}_{\boldsymbol{k}}\right|<r$ for $1 \leq k \leq n$ and since $S$ is convex, the segments with end points $\mathbf{x}+\mathbf{v}_{j-1}$ and $\mathbf{x}+\mathbf{v}_{j}$ lie in $S$. Since $\mathbf{v}_{j}=\mathbf{v}_{j-1}+h_{j} \mathbf{e}_{j}$, the mean value theorem (5.10) shows that the $j$ th summand in (42) is equal to

$$
h_{j}\left(D_{j} f\right)\left(\mathbf{x}+\mathbf{v}_{j-1}+\theta_{j} h_{j} \mathbf{e}_{j}\right)
$$

for some $\theta_{j} \in(0,1)$, and this differs from $h_{j}\left(D_{j} f\right)(\mathbf{x})$ by less than $\left|h_{j}\right| \varepsilon / n$, using (41). By (42), it follows that

$$
\left|f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})-\sum_{j=1}^{n} h_{j}\left(D_{j} f\right)(\mathbf{x})\right| \leq \frac{1}{n} \sum_{j=1}^{n}\left|h_{j}\right| \varepsilon \leq|\mathbf{h}| \varepsilon
$$

for all $\mathbf{h}$ such that $|\mathbf{h}|<r$.

This says that $f$ is differentiable at $\mathbf{x}$ and that $f^{\prime}(\mathbf{x})$ is the linear function which assigns the number $\Sigma h_{j}\left(D_{j} f\right)(\mathbf{x})$ to the vector $\mathbf{h}=\Sigma h_{j} \mathbf{e}_{j}$. The matrix $\left[f^{\prime}(\mathbf{x})\right]$ consists of the row $\left(D_{1} f\right)(\mathbf{x}), \ldots,\left(D_{n} f\right)(\mathbf{x})$; and since $D_{1} f, \ldots, D_{n} f$ are continuous functions on $E$, the concluding remarks of Sec. 9.9 show that $f \in \mathscr{C}^{\prime}(E)$.

## THE CONTRACTION PRINCIPLE

We now interrupt our discussion of differentiation to insert a fixed point theorem that is valid in arbitrary complete metric spaces. It will be used in the proof of the inverse function theorem.

9.22 Definition Let $X$ be a metric space, with metric $d$. If $\varphi$ maps $X$ into $X$ and if there is a number $c<1$ such that

$$
d(\varphi(x), \varphi(y)) \leq c d(x, y)
$$

for all $x, y \in X$, then $\varphi$ is said to be a contraction of $X$ into $X$.

9.23 Theorem If $X$ is a complete metric space, and if $\varphi$ is a contraction of $X$ into $X$, then there exists one and only one $x \in X$ such that $\varphi(x)=x$.

In other words, $\varphi$ has a unique fixed point. The uniqueness is a triviality, for if $\varphi(x)=x$ and $\varphi(y)=y$, then (43) gives $d(x, y) \leq c d(x, y)$, which can only happen when $d(x, y)=0$.

The existence of a fixed point of $\varphi$ is the essential part of the theorem. The proof actually furnishes a constructive method for locating the fixed point.

Proof Pick $x_{0} \in X$ arbitrarily, and define $\left\{x_{n}\right\}$ recursively, by setting

$$
x_{n+1}=\varphi\left(x_{n}\right) \quad(n=0,1,2, \ldots)
$$

Choose $c<1$ so that (43) holds. For $n \geq 1$ we then have

$$
d\left(x_{n+1}, x_{n}\right)=d\left(\varphi\left(x_{n}\right), \varphi\left(x_{n-1}\right)\right) \leq c d\left(x_{n}, x_{n-1}\right)
$$

Hence induction gives

$$
d\left(x_{n+1}, x_{n}\right) \leq c^{n} d\left(x_{1}, x_{0}\right) \quad(n=0,1,2, \ldots)
$$

If $n<m$, it follows that

$$
\begin{aligned}
d\left(x_{n}, x_{m}\right) & \leq \sum_{i=n+1}^{m} d\left(x_{i}, x_{i-1}\right) \\
& \leq\left(c^{n}+c^{n+1}+\cdots+c^{m-1}\right) d\left(x_{1}, x_{0}\right) \\
& \leq\left[(1-c)^{-1} d\left(x_{1}, x_{0}\right)\right] c^{n} .
\end{aligned}
$$

Thus $\left\{x_{n}\right\}$ is a Cauchy sequence. Since $X$ is complete, $\lim x_{n}=x$ for some $x \in X$.

Since $\varphi$ is a contraction, $\varphi$ is continuous (in fact, uniformly continuous) on $X$. Hence

$$
\varphi(x)=\lim _{n \rightarrow \infty} \varphi\left(x_{n}\right)=\lim _{n \rightarrow \infty} x_{n+1}=x
$$

## THE INVERSE FUNCTION THEOREM

The inverse function theorem states, roughly speaking, that a continuously differentiable mapping $f$ is invertible in a neighborhood of any point $x$ at which the linear transformation $\mathbf{f}^{\prime}(\mathbf{x})$ is invertible:

9.24 Theorem Suppose $\mathbf{f}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}, \mathbf{f}^{\prime}(\mathbf{a})$ is invertible for some $\mathbf{a} \in E$, and $\mathbf{b}=\mathbf{f}(\mathbf{a})$. Then

(a) there exist open sets $U$ and $V$ in $R^{n}$ such that $\mathbf{a} \in U, \mathbf{b} \in V, \mathbf{f}$ is one-toone on $U$, and $\mathbf{f}(U)=V$;

(b) if $\mathbf{g}$ is the inverse of $\mathbf{f}$ [which exists, by $(a)$ ], defined in $V$ by

$$
\mathbf{g}(\mathbf{f}(\mathbf{x}))=\mathbf{x} \quad(\mathbf{x} \in U)
$$

then $\mathbf{g} \in \mathscr{C}^{\prime}(V)$.

Writing the equation $\mathbf{y}=\mathbf{f}(\mathbf{x})$ in component form, we arrive at the following interpretation of the conclusion of the theorem: The system of $n$ equations

$$
y_{i}=f_{1}\left(x_{1}, \ldots, x_{n}\right) \quad(1 \leq i \leq n)
$$

can be solved for $x_{1}, \ldots, x_{n}$ in terms of $y_{1}, \ldots, y_{n}$, if we restrict $\mathbf{x}$ and $\mathbf{y}$ to small enough neighborhoods of $\mathbf{a}$ and $\mathbf{b}$; the solutions are unique and continuously differentiable.

Proof

(a) Put $\mathbf{f}^{\prime}(\mathbf{a})=A$, and choose $\lambda$ so that

$$
2 \lambda\left\|A^{-1}\right\|=1 \text {. }
$$

Since $\mathbf{f}^{\prime}$ is continuous at a, there is an open ball $U \subset E$, with center at a, such that

We associate to each $\mathrm{y} \in R^{n}$ a function $\varphi$, defined by

$$
\varphi(\mathbf{x})=\mathbf{x}+A^{-1}(\mathbf{y}-\mathbf{f}(\mathbf{x})) \quad(\mathbf{x} \in E)
$$

Note that $\mathbf{f}(\mathbf{x})=\mathbf{y}$ if and only if $\mathbf{x}$ is a fixed point of $\varphi$.

Since $\varphi^{\prime}(\mathbf{x})=I-A^{-1} \mathbf{f}^{\prime}(\mathbf{x})=A^{-1}\left(A-\mathbf{f}^{\prime}(\mathbf{x})\right)$, (46) and (47) imply

that

$$
\left\|\varphi^{\prime}(\mathbf{x})\right\|<\frac{1}{2} \quad(\mathbf{x} \in U) .
$$

Hence

$$
\left|\varphi\left(\mathbf{x}_{1}\right)-\varphi\left(\mathbf{x}_{2}\right)\right| \leq \frac{1}{2}\left|\mathbf{x}_{1}-\mathbf{x}_{2}\right| \quad\left(\mathbf{x}_{1}, \mathbf{x}_{2} \in U\right)
$$

by Theorem 9.19. It follows that $\varphi$ has at most one fixed point in $U$, so that $\mathbf{f}(\mathbf{x})=\mathbf{y}$ for at most one $\mathbf{x} \in U$.

Thus $\mathbf{f}$ is $1-1$ in $U$.

Next, put $V=\mathbf{f}(U)$, and pick $\mathbf{y}_{0} \in V$. Then $\mathbf{y}_{0}=\mathbf{f}\left(\mathbf{x}_{0}\right)$ for some $\mathbf{x}_{0} \in U$. Let $B$ be an open ball with center at $\mathbf{x}_{0}$ and radius $r>0$, so small that its closure $\bar{B}$ lies in $U$. We will show that $\mathbf{y} \in V$ whenever $\left|\mathbf{y}-\mathbf{y}_{0}\right|<\lambda r$. This proves, of course, that $V$ is open.

Fix $\mathbf{y},\left|\mathbf{y}-\mathbf{y}_{0}\right|<\lambda r$. With $\varphi$ as in (48),

$$
\left|\varphi\left(\mathbf{x}_{0}\right)-\mathbf{x}_{0}\right|=\left|A^{-1}\left(\mathbf{y}-\mathbf{y}_{0}\right)\right|<\left\|A^{-1}\right\| \lambda r=\frac{r}{2} .
$$

If $\mathbf{x} \in \bar{B}$, it therefore follows from (50) that

$$
\begin{aligned}
\left|\varphi(\mathbf{x})-\mathbf{x}_{0}\right| & \leq\left|\varphi(\mathbf{x})-\varphi\left(\mathbf{x}_{0}\right)\right|+\left|\varphi\left(\mathbf{x}_{0}\right)-\mathbf{x}_{0}\right| \\
& <\frac{1}{2}\left|\mathbf{x}-\mathbf{x}_{0}\right|+\frac{r}{2} \leq r
\end{aligned}
$$

hence $\varphi(\mathbf{x}) \in B$. Note that (50) holds if $\mathbf{x}_{1} \in \bar{B}, \mathbf{x}_{2} \in \bar{B}$.

Thus $\varphi$ is a contraction of $\bar{B}$ into $\bar{B}$. Being a closed subset of $R^{n}$, $\bar{B}$ is complete. Theorem 9.23 implies therefore that $\varphi$ has a fixed point $\mathbf{x} \in \bar{B}$. For this $\mathbf{x}, f(\mathbf{x})=\mathbf{y}$. Thus $\mathbf{y} \in \mathbf{f}(\bar{B}) \subset \mathbf{f}(U)=V$.

This proves part $(a)$ of the theorem.

(b) Pick $\mathbf{y} \in V, \mathbf{y}+\mathbf{k} \in V$. Then there exist $\mathbf{x} \in U, \mathbf{x}+\mathbf{h} \in U$, so that $\mathbf{y}=\mathbf{f}(\mathbf{x}), \mathbf{y}+\mathbf{k}=\mathbf{f}(\mathbf{x}+\mathbf{h})$. With $\varphi$ as in (48),

$$
\varphi(\mathbf{x}+\mathbf{h})-\varphi(\mathbf{x})=\mathbf{h}+A^{-1}[\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{x}+\mathbf{h})]=\mathbf{h}-A^{-1} \mathbf{k}
$$

By (50), $\left|\mathbf{h}-A^{-1} \mathbf{k}\right| \leq \frac{1}{2}|\mathbf{h}|$. Hence $\left|A^{-1} \mathbf{k}\right| \geq \frac{1}{2}|\mathbf{h}|$, and

$$
|\mathbf{h}| \leq 2\left\|A^{-1}\right\||\mathbf{k}|=\lambda^{-1}|\mathbf{k}|
$$

By (46), (47), and Theorem $9.8, \mathbf{f}^{\prime}(\mathbf{x})$ has an inverse, say $T$. Since

$$
\mathbf{g}(\mathbf{y}+\mathbf{k})-\mathbf{g}(\mathbf{y})-T \mathbf{k}=\mathbf{h}-T \mathbf{k}=-T\left[\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}\right]
$$

(51) implies

$$
\frac{|\mathbf{g}(\mathbf{y}+\mathbf{k})-\mathbf{g}(\mathbf{y})-T \mathbf{k}|}{|\mathbf{k}|} \leq \frac{\|T\|}{\lambda} \cdot \frac{\left|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}\right|}{|\mathbf{h}|} .
$$

As $\mathbf{k} \rightarrow \mathbf{0}$, (51) shows that $\mathbf{h} \rightarrow \mathbf{0}$. The right side of the last inequality thus tends to 0 . Hence the same is true of the left. We have thus proved that $\mathbf{g}^{\prime}(\mathbf{y})=T$. But $T$ was chosen to be the inverse of $\mathbf{f}^{\prime}(\mathbf{x})=\mathbf{f}^{\prime}(\mathbf{g}(\mathbf{y}))$. Thus

$$
\mathbf{g}^{\prime}(\mathbf{y})=\left\{\mathbf{f}^{\prime}(\mathbf{g}(\mathbf{y}))\right\}^{-1} \quad(\mathbf{y} \in V)
$$

Finally, note that $\mathbf{g}$ is a continuous mapping of $V$ onto $U$ (since $\mathbf{g}$ is differentiable), that $\mathbf{f}^{\prime}$ is a continuous mapping of $U$ into the set $\Omega$ of all invertible elements of $L\left(R^{n}\right)$, and that inversion is a continuous mapping of $\Omega$ onto $\Omega$, by Theorem 9.8. If we combine these facts with (52), we see that $\mathbf{g} \in \mathscr{C}^{\prime}(V)$.

This completes the proof.

Remark. The full force of the assumption that $f \in \mathscr{C}^{\prime}(E)$ was only used in the last paragraph of the preceding proof. Everything else, down to Eq. (52), was derived from the existence of $\mathbf{f}^{\prime}(\mathbf{x})$ for $\mathbf{x} \in E$, the invertibility of $\mathbf{f}^{\prime}(\mathbf{a})$, and the continuity of $\mathbf{f}^{\prime}$ at just the point $\mathbf{a}$. In this connection, we refer to the article by A. Nijenhuis in Amer. Math. Monthly, vol. 81, 1974, pp. 969-980.

The following is an immediate consequence of part $(a)$ of the inverse function theorem.

9.25 Theorem If $\mathbf{f}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}$ and if $\mathbf{f}^{\prime}(\mathbf{x})$ is invertible for every $\mathbf{x} \in E$, then $\mathbf{f}(W)$ is an open subset of $R^{n}$ for every open set $W \subset E$.

In other words, $\mathrm{f}$ is an open mapping of $E$ into $R^{n}$.

The hypotheses made in this theorem ensure that each point $\mathbf{x} \in E$ has a neighborhood in which $f$ is $1-1$. This may be expressed by saying that $f$ is locally one-to-one in $E$. But $\mathbf{f}$ need not be 1-1 in $E$ under these circumstances. For an example, see Exercise 17.

## THE IMPLICIT FUNCTION THEOREM

If $f$ is a continuously differentiable real function in the plane, then the equation $f(x, y)=0$ can be solved for $y$ in terms of $x$ in a neighborhood of any point
$(a, b)$ at which $f(a, b)=0$ and $\partial f / \partial y \neq 0$. Likewise, one can solve for $x$ in terms of $y$ near $(a, b)$ if $\partial f / \partial x \neq 0$ at $(a, b)$. For a simple example which illustrates the need for assuming $\partial f / \partial y \neq 0$, consider $f(x, y)=x^{2}+y^{2}-1$.

The preceding very informal statement is the simplest case (the case $m=n=1$ of Theorem 9.28) of the so-called "implicit function theorem." Its proof makesstronguse of the fact that continuously differentiable transformations behave locally very much like their derivatives. Accordingly, we first prove Theorem 9.27, the linear version of Theorem 9.28.

9.26 Notation If $\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right) \in R^{n}$ and $\mathbf{y}=\left(y_{1}, \ldots, y_{m}\right) \in R^{m}$, let us write $(\mathbf{x}, \mathbf{y})$ for the point (or vector)

$$
\left(x_{1}, \ldots, x_{n}, y_{1}, \ldots, y_{m}\right) \in R^{n+m}
$$

In what follows, the first entry in (x,y) or in a similar symbol will always be a vector in $R^{n}$, the second will be a vector in $R^{m}$.

Every $A \in L\left(R^{n+m}, R^{n}\right)$ can be split into two linear transformations $A_{x}$ and $A_{y}$, defined by

$$
A_{x} \mathbf{h}=A(\mathbf{h}, \mathbf{0}), \quad A_{y} \mathbf{k}=A(\mathbf{0}, \mathbf{k})
$$

for any $\mathrm{h} \in R^{n}, \mathrm{k} \in R^{m}$. Then $A_{x} \in L\left(R^{n}\right), A_{y} \in L\left(R^{m}, R^{n}\right)$, and

$$
A(\mathbf{h}, \mathbf{k})=A_{x} \mathbf{h}+A_{y} \mathbf{k}
$$

The linear version of the implicit function theorem is now almost obvious.

9.27 Theorem If $A \in L\left(R^{n+m}, R^{n}\right)$ and if $A_{x}$ is invertible, then there corresponds to every $\mathbf{k} \in R^{m}$ a unique $\mathbf{h} \in R^{n}$ such that $A(\mathbf{h}, \mathbf{k})=\mathbf{0}$.

This $\mathbf{h}$ can be computed from $\mathbf{k}$ by the formula

$$
\mathbf{h}=-\left(A_{x}\right)^{-1} A_{y} \mathbf{k}
$$

Proof By (54), $A(\mathbf{h}, \mathbf{k})=0$ if and only if

$$
A_{x} \mathbf{h}+A_{y} \mathbf{k}=\mathbf{0}
$$

which is the same as (55) when $A_{x}$ is invertible.

The conclusion of Theorem 9.27 is, in other words, that the equation $A(\mathbf{h}, \mathbf{k})=\mathbf{0}$ can be solved (uniquely) for $\mathbf{h}$ if $\mathbf{k}$ is given, and that the solution $\mathbf{h}$ is a linear function of $\mathbf{k}$. Those who have some acquaintance with linear algebra will recognize this as a very familiar statement about systems of linear equations.

9.28 Theorem Let $\mathbf{f}$ be a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n+m}$ into $R^{n}$, such that $\mathbf{f}(\mathbf{a}, \mathbf{b})=\mathbf{0}$ for some point $(\mathbf{a}, \mathbf{b}) \in E$.

Put $A=\mathbf{f}^{\prime}(\mathbf{a}, \mathbf{b})$ and assume that $A_{x}$ is invertible.

Then there exist open sets $U \subset R^{n+m}$ and $W \subset R^{m}$, with $(\mathbf{a}, \mathbf{b}) \in U$ and $\mathbf{b} \in W$, having the following property:

To every $\mathbf{y} \in W$ corresponds a unique $\mathbf{x}$ such that

$$
(\mathbf{x}, \mathbf{y}) \in U \quad \text { and } \quad \mathbf{f}(\mathbf{x}, \mathbf{y})=0
$$

If this $\mathbf{x}$ is defined to be $\mathbf{g}(\mathbf{y})$, then $\mathbf{g}$ is a $\mathscr{C}^{\prime}$-mapping of $W$ into $R^{n}, \mathbf{g}(\mathbf{b})=\mathbf{a}$,

$$
\mathbf{f}(\mathbf{g}(\mathbf{y}), \mathbf{y})=\mathbf{0} \quad(\mathbf{y} \in W)
$$

and

$$
\mathbf{g}^{\prime}(\mathbf{b})=-\left(A_{x}\right)^{-1} A_{y} \text {. }
$$

The function $g$ is "implicitly" defined by (57). Hence the name of the theorem.

The equation $\mathbf{f}(\mathbf{x}, \mathbf{y})=\mathbf{0}$ can be written as a system of $n$ equations in $n+m$ variables:

$$
\begin{aligned}
& f_{1}\left(x_{1}, \ldots, x_{n}, y_{1}, \ldots, y_{m}\right)=0 \\
& \left.\ldots \ldots, \ldots, \ldots, \ldots, x_{n}, y_{1}, \ldots, y_{m}\right)=0 \\
& f_{n}\left(x_{1}, \ldots, x_{n}\right.
\end{aligned}
$$

The assumption that $A_{x}$ is invertible means that the $n$ by $n$ matrix

$$
\left[\begin{array}{ccc}
D_{1} f_{1} & \cdots & D_{n} f_{1} \\
\cdots \cdots \cdots & \cdots & \cdots \\
D_{1} f_{n} & \cdots & D_{n} f_{n}
\end{array}\right]
$$

evaluated at $(\mathbf{a}, \mathbf{b})$ defines an invertible linear operator in $R^{n}$; in other words, its column vectors should be independent, or, equivalently, its determinant should be $\neq 0$. (See Theorem 9.36.) If, furthermore, (59) holds when $\mathbf{x}=\mathbf{a}$ and $\mathbf{y}=\mathbf{b}$, then the conclusion of the theorem is that (59) can be solved for $x_{1}, \ldots, x_{n}$ in terms of $y_{1}, \ldots, y_{m}$, for every $\mathbf{y}$ near $\mathbf{b}$, and that these solutions are continuously differentiable functions of $\mathbf{y}$.

Proof Define $\mathbf{F}$ by

$$
\mathbf{F}(\mathbf{x}, \mathbf{y})=(\mathbf{f}(\mathbf{x}, \mathbf{y}), \mathbf{y}) \quad((\mathbf{x}, \mathbf{y}) \in E)
$$

Then $\mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of $E$ into $R^{n+m}$. We claim that $\mathbf{F}^{\prime}(\mathbf{a}, \mathbf{b})$ is an invertible element of $L\left(R^{n+m}\right)$ :

Since $\mathbf{f}(\mathbf{a}, \mathbf{b})=\mathbf{0}$, we have

$$
\mathbf{f}(\mathbf{a}+\mathbf{h}, \mathbf{b}+\mathbf{k})=A(\mathbf{h}, \mathbf{k})+\mathbf{r}(\mathbf{h}, \mathbf{k})
$$

where $\mathbf{r}$ is the remainder that occurs in the definition of $\mathbf{f}^{\prime}(\mathbf{a}, \mathbf{b})$. Since

$$
\begin{aligned}
\mathbf{F}(\mathbf{a}+\mathbf{h}, \mathbf{b}+\mathbf{k})-\mathbf{F}(\mathbf{a}, \mathbf{b}) & =(\mathbf{f}(\mathbf{a}+\mathbf{h}, \mathbf{b}+\mathbf{k}), \mathbf{k}) \\
& =(\mathbf{A}(\mathbf{h}, \mathbf{k}), \mathbf{k})+(\mathbf{r}(\mathbf{h}, \mathbf{k}), \mathbf{0})
\end{aligned}
$$

it follows that $\mathbf{F}^{\prime}(\mathbf{a}, \mathbf{b})$ is the linear operator on $R^{n+m}$ that maps $(\mathbf{h}, \mathbf{k})$ to $(A(\mathbf{h}, \mathbf{k}), \mathbf{k})$. If this image vector is $\mathbf{0}$, then $A(\mathbf{h}, \mathbf{k})=\mathbf{0}$ and $\mathbf{k}=\mathbf{0}$, hence $A(\mathbf{h}, \mathbf{0})=\mathbf{0}$, and Theorem 9.27 implies that $\mathbf{h}=\mathbf{0}$. It follows that $\mathbf{F}^{\prime}(\mathbf{a}, \mathbf{b})$ is $1-1$; hence it is invertible (Theorem 9.5).

The inverse function theorem can therefore be applied to $\mathbf{F}$. It shows that there exist open sets $U$ and $V$ in $R^{n+m}$, with $(\mathbf{a}, \mathbf{b}) \in U,(\mathbf{0}, \mathbf{b}) \in V$, such that $\mathbf{F}$ is a $1-1$ mapping of $U$ onto $V$. $\mathbf{b} \in W$.

We let $W$ be the set of all $\mathbf{y} \in R^{m}$ such that $(0, \mathbf{y}) \in V$. Note that

It is clear that $W$ is open since $V$ is open.

If $\mathbf{y} \in W$, then $(\mathbf{0}, \mathbf{y})=\mathbf{F}(\mathbf{x}, \mathbf{y})$ for some $(\mathbf{x}, \mathbf{y}) \in U$. By $(60), \mathbf{f}(\mathbf{x}, \mathbf{y})=\mathbf{0}$ for this $\mathbf{x}$.

Suppose, with the same $\mathbf{y}$, that $\left(\mathbf{x}^{\prime}, \mathbf{y}\right) \in U$ and $\mathbf{f}\left(\mathbf{x}^{\prime}, \mathbf{y}\right)=\mathbf{0}$. Then

$$
\mathbf{F}\left(\mathbf{x}^{\prime}, \mathbf{y}\right)=\left(\mathbf{f}\left(\mathbf{x}^{\prime}, \mathbf{y}\right), \mathbf{y}\right)=(\mathbf{f}(\mathbf{x}, \mathbf{y}), \mathbf{y})=\mathbf{F}(\mathbf{x}, \mathbf{y})
$$

Since $\mathbf{F}$ is $1-1$ in $U$, it follows that $\mathbf{x}^{\prime}=\mathbf{x}$.

This proves the first part of the theorem.

For the second part, define $\mathbf{g}(\mathbf{y})$, for $\mathbf{y} \in W$, so that $(\mathbf{g}(\mathbf{y}), \mathbf{y}) \in U$ and (57) holds. Then

$$
\mathbf{F}(\mathbf{g}(\mathbf{y}), \mathbf{y})=(\mathbf{0}, \mathbf{y}) \quad(\mathbf{y} \in W)
$$

If $\mathbf{G}$ is the mapping of $V$ onto $U$ that inverts $\mathbf{F}$, then $\mathbf{G} \in \mathscr{C}^{\prime}$, by the inverse function theorem, and (61) gives

$$
(\mathbf{g}(\mathbf{y}), \mathbf{y})=\mathbf{G}(\mathbf{0}, \mathbf{y}) \quad(\mathbf{y} \in W)
$$

Since $\mathbf{G} \in \mathscr{C}^{\prime}$, (62) shows that $\mathbf{g} \in \mathscr{C}^{\prime}$.

Finally, to compute $\mathbf{g}^{\prime}(\mathbf{b})$, put $(\mathbf{g}(\mathbf{y}), \mathbf{y})=\Phi(\mathbf{y})$. Then

$$
\Phi^{\prime}(\mathbf{y}) \mathbf{k}=\left(\mathbf{g}^{\prime}(\mathbf{y}) \mathbf{k}, \mathbf{k}\right) \quad\left(\mathbf{y} \in W, \mathbf{k} \in R^{m}\right) .
$$

By $(57), \mathbf{f}(\Phi(\mathbf{y}))=\mathbf{0}$ in $W$. The chain rule shows therefore that

$$
\mathbf{f}^{\prime}(\Phi(\mathbf{y})) \Phi^{\prime}(\mathbf{y})=0
$$

When $\mathbf{y}=\mathbf{b}$, then $\Phi(\mathbf{y})=(\mathbf{a}, \mathbf{b})$, and $\mathbf{f}^{\prime}(\Phi(\mathbf{y}))=A$. Thus

$$
A \Phi^{\prime}(\mathbf{b})=0 .
$$

It now follows from (64), (63), and (54), that

$$
A_{x} \mathbf{g}^{\prime}(\mathbf{b}) \mathbf{k}+A_{y} \mathbf{k}=A\left(\mathbf{g}^{\prime}(\mathbf{b}) \mathbf{k}, \mathbf{k}\right)=A \Phi^{\prime}(\mathbf{b}) \mathbf{k}=\mathbf{0}
$$

for every $\mathbf{k} \in R^{m}$. Thus

$$
A_{x} \mathbf{g}^{\prime}(\mathbf{b})+A_{y}=0
$$

This is equivalent to (58), and completes the proof.

Note. In terms of the components of $\mathbf{f}$ and $g,(65)$ becomes

$$
\sum_{j=1}^{n}\left(D_{j} f_{i}\right)(\mathbf{a}, \mathbf{b})\left(D_{k} g_{j}\right)(\mathbf{b})=-\left(D_{n+k} f_{i}\right)(\mathbf{a}, \mathbf{b})
$$

or

$$
\sum_{j=1}^{n}\left(\frac{\partial f_{i}}{\partial x_{j}}\right)\left(\frac{\partial g_{j}}{\partial y_{k}}\right)=-\left(\frac{\partial f_{i}}{\partial y_{k}}\right)
$$

where $1 \leq i \leq n, 1 \leq k \leq m$.

For each $k$, this is a system of $n$ linear equations in which the derivatives $\partial g_{j} / \partial y_{k}(1 \leq j \leq n)$ are the unknowns.

9.29 Example Take $n=2, m=3$, and consider the mapping $\mathbf{f}=\left(f_{1}, f_{2}\right)$ of $R^{5}$ into $R^{2}$ given by

$$
\begin{aligned}
& f_{1}\left(x_{1}, x_{2}, y_{1}, y_{2}, y_{3}\right)=2 e^{x_{1}}+x_{2} y_{1}-4 y_{2}+3 \\
& f_{2}\left(x_{1}, x_{2}, y_{1}, y_{2}, y_{3}\right)=x_{2} \cos x_{1}-6 x_{1}+2 y_{1}-y_{3} .
\end{aligned}
$$

If $\mathbf{a}=(0,1)$ and $\mathbf{b}=(3,2,7)$, then $\mathbf{f}(\mathbf{a}, \mathbf{b})=0$.

With respect to the standard bases, the matrix of the transformation $A=\mathbf{f}^{\prime}(\mathbf{a}, \mathbf{b})$ is

$$
[A]=\left[\begin{array}{rrrrr}
2 & 3 & 1 & -4 & 0 \\
-6 & 1 & 2 & 0 & -1
\end{array}\right]
$$

Hence

$$
\left[A_{x}\right]=\left[\begin{array}{rr}
2 & 3 \\
-6 & 1
\end{array}\right], \quad\left[A_{y}\right]=\left[\begin{array}{rrr}
1 & -4 & 0 \\
2 & 0 & -1
\end{array}\right]
$$

We see that the column vectors of $\left[A_{x}\right]$ are independent. Hence $A_{x}$ is invertible and the implicit function theorem asserts the existence of a $\mathscr{C}^{\prime}$-mapping $\mathbf{g}$, defined in a neighborhood of $(3,2,7)$, such that $\mathbf{g}(3,2,7)=(0,1)$ and $\mathbf{f}(\mathbf{g}(\mathbf{y}), \mathbf{y})=\mathbf{0}$. We can use (58) to compute $g^{\prime}(3,2,7)$ : Since

$$
\left[\left(A_{x}\right)^{-1}\right]=\left[A_{x}\right]^{-1}=\frac{1}{20}\left[\begin{array}{rr}
1 & -3 \\
6 & 2
\end{array}\right]
$$

(58) gives

$$
\left[\mathbf{g}^{\prime}(3,2,7)\right]=-\frac{1}{20}\left[\begin{array}{rr}
1 & -3 \\
6 & 2
\end{array}\right]\left[\begin{array}{rrr}
1 & -4 & 0 \\
2 & 0 & -1
\end{array}\right]=\left[\begin{array}{rrr}
\frac{1}{4} & \frac{1}{5} & -\frac{3}{20} \\
-\frac{1}{2} & \frac{6}{5} & \frac{1}{10}
\end{array}\right]
$$

In terms of partial derivatives, the conclusion is that

$$
\begin{array}{lll}
D_{1} g_{1}=\frac{1}{4}, & D_{2} g_{1}=\frac{1}{5} & D_{3} g_{1}=-\frac{3}{20} \\
D_{1} g_{2}=-\frac{1}{2} & D_{2} g_{2}=\frac{6}{5} & D_{3} g_{2}=\frac{1}{10}
\end{array}
$$

at the point $(3,2,7)$.

## THE RANK THEOREM

Although this theorem is not as important as the inverse function theorem or the implicit function theorem, we include it as another interesting illustration of the general principle that the local behavior of a continuously differentiable mapping $\mathbf{F}$ near a point $\mathbf{x}$ is similar to that of the linear transformation $\mathbf{F}^{\prime}(\mathbf{x})$. Before stating it, we need a few more facts about linear transformations.

9.30 Definitions Suppose $X$ and $Y$ are vector spaces, and $A \in L(X, Y)$, as in Definition 9.6. The null space of $A, \mathcal{N}(A)$, is the set of all $\mathbf{x} \in X$ at which $A \mathbf{x}=\mathbf{0}$. It is clear that $\mathcal{N}(A)$ is a vector space in $X$.

Likewise, the range of $A, \mathscr{R}(A)$, is a vector space in $Y$.

The rank of $A$ is defined to be the dimension of $\mathscr{R}(A)$.

For example, the invertible elements of $L\left(R^{n}\right)$ are precisely those whose rank is $n$. This follows from Theorem 9.5.

If $A \in L(X, Y)$ and $A$ has rank 0 , then $A \mathbf{x}=\mathbf{0}$ for all $x \in A$, hence. $\mathcal{N}(A)=X$. In this connection, see Exercise 25.

9.31 Projections Let $X$ be a vector space. An operator $P \in L(X)$ is said to be a projection in $X$ if $P^{2}=P$.

More explicitly, the requirement is that $P(P \mathbf{x})=P \mathbf{x}$ for every $\mathbf{x} \in X$. In other words, $P$ fixes every vector in its range $\mathscr{R}(P)$.

Here are some elementary properties of projections:

(a) If $P$ is a projection in $X$, then every $\mathbf{x} \in X$ has a unique representation of the form

$$
\mathbf{x}=\mathbf{x}_{1}+\mathbf{x}_{2}
$$

where $\mathbf{x}_{1} \in \mathscr{R}(P), \mathbf{x}_{2} \in \mathscr{N}(P)$.

To obtain the representation, put $\mathbf{x}_{1}=P \mathbf{x}, \mathbf{x}_{2}=\mathbf{x}-\mathbf{x}_{1}$. Then $P \mathrm{x}_{2}=P \mathrm{x}-P \mathbf{x}_{1}=P \mathrm{x}-P^{2} \mathbf{x}=0$. As regards the uniqueness, apply $P$ to the equation $\mathbf{x}=\mathbf{x}_{1}+\mathbf{x}_{2}$. Since $\mathbf{x}_{1} \in \mathscr{R}(P), P \mathbf{x}_{1}=\mathbf{x}_{1} ;$ since $P \mathbf{x}_{2}=0$, it follows that $\mathbf{x}_{1}=P \mathbf{x}$.

(b) If $X$ is a finite-dimensional vector space and if $X_{1}$ is a vector space in $X$, then there is a projection $P$ in $X$ with $\mathscr{R}(P)=X_{1}$.

If $X_{1}$ contains only $\mathbf{0}$, this is trivial: put $P \mathbf{x}=\mathbf{0}$ for all $\mathbf{x} \in X$.

Assume $\operatorname{dim} X_{1}=k>0$. By Theorem 9.3, $X$ has then a basis $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}\right\}$ such that $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{k}\right\}$ is a basis of $X_{1}$. Define

$$
P\left(c_{1} \mathbf{u}_{1}+\cdots+c_{n} \mathbf{u}_{n}\right)=c_{1} \mathbf{u}_{1}+\cdots+c_{k} \mathbf{u}_{k}
$$

for arbitrary scalars $c_{1}, \ldots, c_{n}$.

Then $P \mathbf{x}=\mathbf{x}$ for every $\mathbf{x} \in X_{1}$, and $X_{1}=\mathscr{R}(P)$.

Note that $\left\{\mathbf{u}_{k+1}, \ldots, \mathbf{u}_{n}\right\}$ is a basis of $\mathcal{N}(P)$. Note also that there are infinitely many projections in $X$, with range $X_{1}$, if $0<\operatorname{dim} X_{1}<\operatorname{dim} X$.

9.32 Theorem Suppose $m, n, r$ are nonnegative integers, $m \geq r, n \geq r, \mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{m}$, and $\mathbf{F}^{\prime}(\mathbf{x})$ has rank $r$ for every $\mathbf{x} \in E$.

Fix $\mathbf{a} \in E$, put $A=\mathbf{F}^{\prime}(\mathbf{a})$, let $Y_{1}$ be the range of $A$, and let $P$ be a projection in $R^{m}$ whose range is $Y_{1}$. Let $Y_{2}$ be the null space of $P$.

Then there are open sets $U$ and $V$ in $R^{n}$, with $\mathbf{a} \in U, U \subset E$, and there is a 1-1 $\mathscr{C}^{\prime}$-mapping $\mathbf{H}$ of $V$ onto $U$ (whose inverse is also of class $\mathscr{C}^{\prime}$ ) such that

$$
\mathbf{F}(\mathbf{H}(\mathbf{x}))=A \mathbf{x}+\varphi(A \mathbf{x}) \quad(\mathbf{x} \in V)
$$

where $\varphi$ is a $\mathscr{C}^{\prime}$-mapping of the open set $A(V) \subset Y_{1}$ into $Y_{2}$.

After the proof we shall give a more geometric description of the information that (66) contains.

Proof If $r=0$, Theorem 9.19 shows that $\mathbf{F}(\mathbf{x})$ is constant in a neighborhood $U$ of a, and (66) holds trivially, with $V=U, \mathbf{H}(\mathbf{x})=\mathbf{x}, \varphi(\mathbf{0})=\mathbf{F}(\mathbf{a})$.

From now on we assume $r>0$. Since $\operatorname{dim} Y_{1}=r, Y_{1}$ has a basis $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$. Choose $\mathbf{z}_{i} \in R^{n}$ so that $A \mathbf{z}_{i}=\mathbf{y}_{i}(1 \leq i \leq r)$, and define a linear mapping $S$ of $Y_{1}$ into $R^{n}$ by setting

$$
S\left(c_{1} \mathbf{y}_{1}+\cdots+c_{r} \mathbf{y}_{r}\right)=c_{1} \mathbf{z}_{1}+\cdots+c_{r} \mathbf{z}_{r}
$$

for all scalars $c_{1}, \ldots, c_{r}$.

Then $A S \mathbf{y}_{i}=A \mathbf{z}_{i}=\mathbf{y}_{i}$ for $1 \leq i \leq r$. Thus

$$
A S \mathbf{y}=\mathbf{y} \quad\left(\mathbf{y} \in Y_{1}\right)
$$

Define a mapping $\mathbf{G}$ of $E$ into $R^{n}$ by setting

$$
\mathbf{G}(\mathbf{x})=\mathbf{x}+S P[\mathbf{F}(\mathbf{x})-A \mathbf{x}] \quad(\mathbf{x} \in E) .
$$

Since $\mathbf{F}^{\prime}(\mathbf{a})=A$, differentiation of (69) shows that $\mathbf{G}^{\prime}(\mathbf{a})=I$, the identity operator on $R^{n}$. By the inverse function theorem, there are open sets $U$ and $V$ in $R^{n}$, with $\mathbf{a} \in U$, such that $\mathbf{G}$ is a 1-1 mapping of $U$ onto $V$ whose inverse $\mathbf{H}$ is also of class $\mathscr{C}^{\prime}$. Moreover, by shrinking $U$ and $V$, if necessary, we can arrange it so that $V$ is convex and $\mathbf{H}^{\prime}(\mathbf{x})$ is invertible for every $\mathbf{x} \in V$.

Note that $A S P A=A$, since $P A=A$ and (68) holds. Therefore (69) gives

$$
\psi(\mathbf{x})=\mathbf{F}(\mathbf{H}(\mathbf{x}))-A \mathbf{x} \quad(\mathbf{x} \in V) .
$$

Since $P A=A,(71)$ implies that $P \psi(\mathbf{x})=\mathbf{0}$ for all $\mathbf{x} \in V$. Thus $\psi$ is a $\mathscr{C}^{\prime}$-mapping of $V$ into $Y_{2}$.

Since $V$ is open, it is clear that $A(V)$ is an open subset of its range $\mathscr{R}(A)=Y_{1}$.

To complete the proof, i.e., to go from (72) to (66), we have to show that there is a $\mathscr{C}^{\prime}$-mapping $\varphi$ of $A(V)$ into $Y_{2}$ which satisfies

$$
\varphi(A \mathbf{x})=\psi(\mathbf{x}) \quad(\mathbf{x} \in V)
$$

As a step toward (73), we will first prove that

$$
\psi\left(\mathbf{x}_{1}\right)=\psi\left(\mathbf{x}_{2}\right)
$$

if $\mathbf{x}_{1} \in V, \mathbf{x}_{2} \in V, A \mathbf{x}_{1}=A \mathbf{x}_{2}$.

Put $\Phi(\mathbf{x})=\mathbf{F}(\mathbf{H}(\mathbf{x}))$, for $\mathbf{x} \in V$. Since $\mathbf{H}^{\prime}(\mathbf{x})$ has rank $n$ for every $\mathbf{x} \in V$, and $\mathbf{F}^{\prime}(\mathbf{x})$ has rank $r$ for every $\mathbf{x} \in U$, it follows that By (71),

$$
\operatorname{rank} \Phi^{\prime}(\mathbf{x})=\operatorname{rank} \mathbf{F}^{\prime}(\mathbf{H}(\mathbf{x})) \mathbf{H}^{\prime}(\mathbf{x})=r \quad(\mathbf{x} \in V)
$$

Fix $\mathbf{x} \in V$. Let $M$ be the range of $\Phi^{\prime}(\mathbf{x})$. Then $M \subset R^{m}, \operatorname{dim} M=r$.

$$
P \Phi^{\prime}(\mathbf{x})=A .
$$

Thus $P$ maps $M$ onto $\mathscr{R}(A)=Y_{1}$. Since $M$ and $Y_{1}$ have the same dimension, it follows that $P$ (restricted to $M$ ) is 1-1.

Suppose now that $A \mathbf{h}=\mathbf{0}$. Then $P \Phi^{\prime}(\mathbf{x}) \mathbf{h}=\mathbf{0}$, by (76). But $\Phi^{\prime}(\mathbf{x}) \mathbf{h} \in M$, and $P$ is $1-1$ on $M$. Hence $\Phi^{\prime}(\mathbf{x}) \mathbf{h}=\mathbf{0}$. A look at (72) shows now that we have proved the following:

If $\mathbf{x} \in V$ and $A \mathbf{h}=\mathbf{0}$, then $\psi^{\prime}(\mathbf{x}) \mathbf{h}=\mathbf{0}$.

We can now prove (74). Suppose $\mathbf{x}_{1} \in V, \mathbf{x}_{2} \in V, A \mathbf{x}_{1}=A \mathbf{x}_{2}$. Put $\mathbf{h}=\mathbf{x}_{2}-\mathbf{x}_{1}$ and define

$$
\mathbf{g}(t)=\psi\left(\mathbf{x}_{1}+t \mathbf{h}\right) \quad(0 \leq t \leq 1) .
$$

The convexity of $V$ shows that $\mathbf{x}_{1}+t \mathbf{h} \in V$ for these $t$. Hence

$$
\mathbf{g}^{\prime}(t)=\psi^{\prime}\left(\mathbf{x}_{1}+t \mathbf{h}\right) \mathbf{h}=\mathbf{0} \quad(0 \leq t \leq 1)
$$

so that $\mathbf{g}(1)=\mathbf{g}(0)$. But $\mathbf{g}(1)=\psi\left(\mathbf{x}_{2}\right)$ and $\mathbf{g}(0)=\psi\left(\mathbf{x}_{1}\right)$. This proves (74).

By (74), $\psi(\mathbf{x})$ depends only on $A \mathbf{x}$, for $\mathbf{x} \in V$. Hence (73) defines $\varphi$ unambiguously in $A(V)$. It only remains to be proved that $\varphi \in \mathscr{C}^{\prime}$.

Fix $\mathbf{y}_{0} \in A(V)$, fix $\mathbf{x}_{0} \in V$ so that $A \mathbf{x}_{0}=\mathbf{y}_{0}$. Since $V$ is open, $\mathbf{y}_{0}$ has a neighborhood $W$ in $Y_{1}$ such that the vector

$$
\mathbf{x}=\mathbf{x}_{0}+S\left(\mathbf{y}-\mathbf{y}_{0}\right)
$$

lies in $V$ for all $\mathbf{y} \in W$. By (68),

$$
A \mathbf{x}=A \mathbf{x}_{0}+\mathbf{y}-\mathbf{y}_{0}=\mathbf{y}
$$

Thus (73) and (79) give

$$
\varphi(\mathbf{y})=\psi\left(\mathbf{x}_{0}-S \mathbf{y}_{0}+S \mathbf{y}\right) \quad(\mathbf{y} \in W)
$$

This formula shows that $\varphi \in \mathscr{C}^{\prime}$ in $W$, hence in $A(V)$, since $\mathrm{y}_{0}$ was chosen arbitrarily in $A(V)$.

The proof is now complete.

Here is what the theorem tells us about the geometry of the mapping $\mathbf{F}$.

If $\mathbf{y} \in \mathbf{F}(U)$ then $\mathbf{y}=\mathbf{F}(\mathbf{H}(\mathbf{x})$ ) for some $\mathbf{x} \in V$, and (66) shows that $P \mathbf{y}=A \mathbf{x}$. Therefore

$$
\mathbf{y}=P \mathbf{y}+\varphi(P \mathbf{y}) \quad(\mathbf{y} \in \mathbf{F}(U))
$$

This shows that $\mathbf{y}$ is determined by its projection $P \mathbf{y}$, and that $P$, restricted to $\mathbf{F}(U)$, is a 1-1 mapping of $\mathbf{F}(U)$ onto $A(V)$. Thus $\mathbf{F}(U)$ is an " $r$-dimensional surface" with precisely one point "over" each point of $A(V)$. We may also regard $\mathbf{F}(U)$ as the graph of $\varphi$.

If $\Phi(\mathbf{x})=\mathbf{F}(\mathbf{H}(\mathbf{x}))$, as in the proof, then (66) shows that the level sets of $\Phi$ (these are the sets on which $\Phi$ attains a given value) are precisely the level sets of $A$ in $V$. These are "flat" since they are intersections with $V$ of translates of the vector space $\mathcal{N}(A)$. Note that $\operatorname{dim} \mathcal{N}(A)=n-r$ (Exercise 25).

The level sets of $\mathbf{F}$ in $U$ are the images under $\mathbf{H}$ of the flat level sets of $\Phi$ in $V$. They are thus " $(n-r)$-dimensional surfaces" in $U$.

## DETERMINANTS

Determinants are numbers associated to square matrices, and hence to the operators represented by such matrices. They are 0 if and only if the corresponding operator fails to be invertible. They can therefore be used to decide whether the hypotheses of some of the preceding theorems are satisfied. They will play an even more important role in Chap. 10.

9.33 Definition If $\left(j_{1}, \ldots, j_{n}\right)$ is an ordered $n$-tuple of integers, define

$$
s\left(j_{1}, \ldots, j_{n}\right)=\prod_{p<q} \operatorname{sgn}\left(j_{q}-j_{p}\right)
$$

where $\operatorname{sgn} x=1$ if $x>0, \operatorname{sgn} x=-1$ if $x<0, \operatorname{sgn} x=0$ if $x=0$. Then $s\left(j_{1}, \ldots, j_{n}\right)=1,-1$, or 0 , and it changes sign if any two of the $j$ 's are interchanged.

Let $[A]$ be the matrix of a linear operator $A$ on $R^{n}$, relative to the standard basis $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$, with entries $a(i, j)$ in the $i$ th row and $j$ th column. The determinant of $[A]$ is defined to be the number

$$
\operatorname{det}[A]=\sum s\left(j_{1}, \ldots, j_{n}\right) a\left(1, j_{1}\right) a\left(2, j_{2}\right) \cdots a\left(n, j_{n}\right)
$$

The sum in (83) extends over all ordered $n$-tuples of integers $\left(j_{1}, \ldots, j_{n}\right)$ with $1 \leq j_{r} \leq n$.

The column vectors $\mathbf{x}_{j}$ of $[A]$ are

$$
\mathbf{x}_{j}=\sum_{i=1}^{n} a(i, j) \mathbf{e}_{i} \quad(1 \leq j \leq n)
$$

It will be convenient to think of $\operatorname{det}[A]$ as a function of the column vectors of $[A]$. If we write

$$
\operatorname{det}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\operatorname{det}[A]
$$

det is now a real function on the set of all ordered $n$-tuples of vectors in $R^{n}$.

### 9.34 Theorem

(a) If I is the identity operator on $R^{n}$, then

$$
\operatorname{det}[I]=\operatorname{det}\left(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right)=1
$$

(b) det is a linear function of each of the column vectors $\mathbf{x}_{j}$, if the others are held fixed.

(c) If $[A]_{1}$ is obtained from $[A]$ by interchanging two columns, then $\operatorname{det}[A]_{1}=-\operatorname{det}[A]$.

(d) If $[A]$ has two equal columns, then $\operatorname{det}[A]=0$.

Proof If $A=I$, then $a(i, i)=1$ and $a(i, j)=0$ for $i \neq j$. Hence

$$
\operatorname{det}[I]=s(1,2, \ldots, n)=1 \text {, }
$$

which proves $(a)$. By $(82), s\left(j_{1}, \ldots, j_{n}\right)=0$ if any two of the $j$ 's are equal. Each of the remaining $n$ ! products in (83) contains exactly one factor from each column. This proves (b). Part $(c)$ is an immediate consequence of the fact that $s\left(j_{1}, \ldots, j_{n}\right)$ changes sign if any two of the $j$ 's are interchanged, and $(d)$ is a corollary of $(c)$.

9.35 Theorem If $[A]$ and $[B]$ are $n$ by $n$ matrices, then

$$
\operatorname{det}([B][A])=\operatorname{det}[B] \operatorname{det}[A] .
$$

Proof If $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$ are the columns of $[A]$, define

$$
\Delta_{B}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\Delta_{B}[A]=\operatorname{det}([B][A])
$$

The columns of $[B][A]$ are the vectors $B \mathbf{x}_{1}, \ldots, B \mathbf{x}_{n}$. Thus

$$
\Delta_{B}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\operatorname{det}\left(B \mathbf{x}_{1}, \ldots, B \mathbf{x}_{n}\right)
$$

By (86) and Theorem 9.34, $\Delta_{B}$ also has properties $9.34(b)$ to $(d)$. By $(b)$ and (84),

$$
\Delta_{B}[A]=\Delta_{B}\left(\sum_{i} a(i, 1) \mathbf{e}_{i}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\right)=\sum_{i} a(i, 1) \Delta_{B}\left(\mathbf{e}_{i}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\right)
$$

Repeating this process with $\mathbf{x}_{2}, \ldots, \mathbf{x}_{n}$, we obtain

$$
\Delta_{B}[A]=\sum a\left(i_{1}, 1\right) a\left(i_{2}, 2\right) \cdots a\left(i_{n}, n\right) \Delta_{B}\left(\mathbf{e}_{i_{1}}, \ldots, \mathbf{e}_{i_{n}}\right)
$$

the sum being extended over all ordered $n$-tuples $\left(i_{1}, \ldots, i_{n}\right)$ with $1 \leq i_{r} \leq n$. By $(c)$ and $(d)$,

$$
\Delta_{B}\left(\mathbf{e}_{i_{1}}, \ldots, \mathbf{e}_{i_{n}}\right)=t\left(i_{1}, \ldots, i_{n}\right) \Delta_{B}\left(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right)
$$

where $t=1,0$, or -1 , and since $[B][I]=[B],(85)$ shows that

$$
\Delta_{B}\left(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right)=\operatorname{det}[B]
$$

Substituting (89) and (88) into (87), we obtain

$$
\operatorname{det}([B][A])=\left\{\sum a\left(i_{1}, 1\right) \cdots a\left(i_{n}, n\right) t\left(i_{1}, \ldots, i_{n}\right)\right\} \operatorname{det}[B]
$$

for all $n$ by $n$ matrices $[A]$ and $[B]$. Taking $B=I$, we see that the above sum in braces is det $[A]$. This proves the theorem.

9.36 Theorem A linear operator $A$ on $R^{n}$ is invertible if and only if $\operatorname{det}[A] \neq 0$.

Proof If $A$ is invertible, Theorem 9.35 shows that

$$
\operatorname{det}[A] \operatorname{det}\left[A^{-1}\right]=\operatorname{det}\left[A A^{-1}\right]=\operatorname{det}[I]=1,
$$

so that $\operatorname{det}[A] \neq 0$.

If $A$ is not invertible, the columns $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$ of $[A]$ are dependent (Theorem 9.5); hence there is one, say, $\mathbf{x}_{k}$, such that

$$
\mathbf{x}_{k}+\sum_{j \neq k} c_{j} \mathbf{x}_{j}=0
$$

for certain scalars $c_{j}$. By $9.34(b)$ and $(d), \mathbf{x}_{k}$ can be replaced by $\mathbf{x}_{k}+c_{j} \mathbf{x}_{j}$ without altering the determinant, if $j \neq k$. Repeating, we see that $\mathbf{x}_{k}$ can
be replaced by the left side of ( 90 ), i.e., by 0 , without altering the determinant. But a matrix which has $\mathbf{0}$ for one column has determinant 0 . Hence $\operatorname{det}[A]=0$.

9.37 Remark Suppose $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ and $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}\right\}$ are bases in $R^{n}$. Every linear operator $A$ on $R^{n}$ determines matrices $[A]$ and $[A]_{U}$, with entries $a_{i j}$ and $\alpha_{i j}$, given by

$$
A \mathbf{e}_{j}=\sum_{i} a_{i j} \mathbf{e}_{i}, \quad A \mathbf{u}_{j}=\sum_{i} \alpha_{i j} \mathbf{u}_{i} .
$$

If $\mathbf{u}_{j}=B \mathbf{e}_{j}=\Sigma b_{i j} \mathbf{e}_{i}$, then $A \mathbf{u}_{j}$ is equal to

$$
\sum_{k} \alpha_{k j} B \mathbf{e}_{k}=\sum_{k} \alpha_{k j} \sum_{i} b_{i k} \mathbf{e}_{i}=\sum_{i}\left(\sum_{k} b_{i k} \alpha_{k j}\right) \mathbf{e}_{i}
$$

and also to

$$
A B \mathbf{e}_{j}=A \sum_{k} b_{k j} \mathbf{e}_{k}=\sum_{i}\left(\sum_{k} a_{i k} b_{k j}\right) \mathbf{e}_{i} .
$$

Thus $\Sigma b_{i k} \alpha_{k j}=\Sigma a_{i k} b_{k j}$, or

$$
[B][A]_{U}=[A][B]
$$

Since $B$ is invertible, $\operatorname{det}[B] \neq 0$. Hence (91), combined with Theorem 9.35, shows that

$$
\operatorname{det}[A]_{U}=\operatorname{det}[A]
$$

The determinant of the matrix of a linear operator does therefore not depend on the basis which is used to construct the matrix. It is thus meaningful to speak of the determinant of a linear operator, without having any basis in mind.

9.38 Jacobians If $f$ maps an open set $E \subset R^{n}$ into $R^{n}$, and if $f$ is differentiable at a point $\mathbf{x} \in E$, the determinant of the linear operator $\mathbf{f}^{\prime}(\mathbf{x})$ is called the Jacobian of $\mathbf{f}$ at $\mathbf{x}$. In symbols,

$$
J_{\mathbf{f}}(\mathbf{x})=\operatorname{det} \mathbf{f}^{\prime}(\mathbf{x})
$$

We shall also use the notation

$$
\frac{\partial\left(y_{1}, \ldots, y_{n}\right)}{\partial\left(x_{1}, \ldots, x_{n}\right)}
$$

for $J_{\mathbf{f}}(\mathbf{x})$, if $\left(y_{1}, \ldots, y_{n}\right)=\mathbf{f}\left(x_{1}, \ldots, x_{n}\right)$.

In terms of Jacobians, the crucial hypothesis in the inverse function theorem is that $J_{f}(a) \neq 0$ (compare Theorem 9.36). If the implicit function theorem is stated in terms of the functions (59), the assumption made there on $A$ amounts to

$$
\frac{\partial\left(f_{1}, \ldots, f_{n}\right)}{\partial\left(x_{1}, \ldots, x_{n}\right)} \neq 0
$$

## DERIVATIVES OF HIGHER ORDER

9.39 Definition Suppose $f$ is a real function defined in an open set $E \subset R^{n}$, with partial derivatives $D_{1} f, \ldots, D_{n} f$. If the functions $D_{j} f$ are themselves differentiable, then the second-order partial derivatives of $f$ are defined by

$$
D_{i j} f=D_{i} D_{j} f \quad(i, j=1, \ldots, n)
$$

If all these functions $D_{i j} f$ are continuous in $E$, we say that $f$ is of class $\mathscr{C}^{\prime \prime}$ in $E$, or that $f \in \mathscr{C}^{\prime \prime}(E)$.

A mapping $\mathbf{f}$ of $E$ into $R^{m}$ is said to be of class $\mathscr{C}^{\prime \prime}$ if each component of $\mathbf{f}$ is of class $\mathscr{C}^{\prime \prime}$.

It can happen that $D_{i j} f \neq D_{j i} f$ at some point, although both derivatives exist (see Exercise 27). However, we shall see below that $D_{i j} f=D_{j i} f$ whenever these derivatives are continuous.

For simplicity (and without loss of generality) we state our next two theorems for real functions of two variables. The first one is a mean value theorem.

9.40 Theorem Suppose $f$ is defined in an open set $E \subset R^{2}$, and $D_{1} f$ and $D_{21} f$ exist at every point of $E$. Suppose $Q \subset E$ is a closed rectangle with sides parallel to the coordinate axes, having $(a, b)$ and $(a+h, b+k)$ as opposite vertices $(h \neq 0, k \neq 0)$. Put

$$
\Delta(f, Q)=f(a+h, b+k)-f(a+h, b)-f(a, b+k)+f(a, b)
$$

Then there is a point $(x, y)$ in the interior of $Q$ such that

$$
\Delta(f, Q)=h k\left(D_{21} f\right)(x, y)
$$

Note the analogy between (95) and Theorem 5.10; the area of $Q$ is $h k$.

Proof Put $u(t)=f(t, b+k)-f(t, b)$. Two applications of Theorem 5.10 show that there is an $x$ between $a$ and $a+h$, and that there is a $y$ between $b$ and $b+k$, such that

$$
\begin{aligned}
\Delta(f, Q) & =u(a+h)-u(a) \\
& =h u^{\prime}(x) \\
& =h\left[\left(D_{1} f\right)(x, b+k)-\left(D_{1} f\right)(x, b)\right] \\
& =h k\left(D_{21} f\right)(x, y) .
\end{aligned}
$$

9.41 Theorem Suppose $f$ is defined in an open set $E \subset R^{2}$, suppose that $D_{1} f$, $D_{21} f$, and $D_{2} f$ exist at every point of $E$, and $D_{21} f$ is continuous at some point $(a, b) \in E$.

Then $D_{12} f$ exists at $(a, b)$ and

$$
\left(D_{12} f\right)(a, b)=\left(D_{21} f\right)(a, b)
$$

Corollary $\quad D_{21} f=D_{12} f$ if $f \in \mathscr{C}^{\prime \prime}(E)$.

Proof Put $A=\left(D_{21} f\right)(a, b)$. Choose $\varepsilon>0$. If $Q$ is a rectangle as in Theorem 9.40, and if $h$ and $k$ are sufficiently small, we have

$$
\left|A-\left(D_{21} f\right)(x, y)\right|<\varepsilon
$$

for all $(x, y) \in Q$. Thus

$$
\left|\frac{\Delta(f, Q)}{h k}-A\right|<\varepsilon
$$

by (95). Fix $h$, and let $k \rightarrow 0$. Since $D_{2} f$ exists in $E$, the last inequality implies that

$$
\left|\frac{\left(D_{2} f\right)(a+h, b)-\left(D_{2} f\right)(a, b)}{h}-A\right| \leq \varepsilon \text {. }
$$

Since $\varepsilon$ was arbitrary, and since (97) holds for all sufficiently small $h \neq 0$, it follows that $\left(D_{12} f\right)(a, b)=A$. This gives (96).

## DIFFERENTIATION OF INTEGRALS

Suppose $\varphi$ is a function of two variables which can be integrated with respect to one and which can be differentiated with respect to the other. Under what conditions will the result be the same if these two limit processes are carried out in the opposite order? To state the question more precisely: Under what conditions on $\varphi$ can one prove that the equation

$$
\frac{d}{d t} \int_{a}^{b} \varphi(x, t) d x=\int_{a}^{b} \frac{\partial \varphi}{\partial t}(x, t) d x
$$

is true? (A counter example is furnished by Exercise 28.)

It will be convenient to use the notation

$$
\varphi^{t}(x)=\varphi(x, t) .
$$

Thus $\varphi^{t}$ is, for each $t$, a function of one variable.

### 9.42 Theorem Suppose

(a) $\varphi(x, t)$ is defined for $a \leq x \leq b, c \leq t \leq d$;

(b) $\alpha$ is an increasing function on $[a, b]$;
(c) $\varphi^{t} \in \mathscr{R}(\alpha)$ for every $t \in[c, d]$;

(d) $c<s<d$, and to every $\varepsilon>0$ corresponds a $\delta>0$ such that

$$
\left|\left(D_{2} \varphi\right)(x, t)-\left(D_{2} \varphi\right)(x, s)\right|<\varepsilon
$$

for all $x \in[a, b]$ and for all $t \in(s-\delta, s+\delta)$.

Define

$$
f(t)=\int_{a}^{b} \varphi(x, t) d \alpha(x) \quad(c \leq t \leq d)
$$

Then $\left(D_{2} \varphi\right)^{s} \in \mathscr{R}(\alpha), f^{\prime}(s)$ exists, and

$$
f^{\prime}(s)=\int_{a}^{b}\left(D_{2} \varphi\right)(x, s) d \alpha(x)
$$

Note that $(c)$ simply asserts the existence of the integrals (100) for all $t \in[c, d]$. Note also that $(d)$ certainly holds whenever $D_{2} \varphi$ is continuous on the rectangle on which $\varphi$ is defined.

Proof Consider the difference quotients

$$
\psi(x, t)=\frac{\varphi(x, t)-\varphi(x, s)}{t-s}
$$

for $0<|t-s|<\delta$. By Theorem 5.10 there corresponds to each $(x, t)$ a number $u$ between $s$ and $t$ such that

$$
\psi(x, t)=\left(D_{2} \varphi\right)(x, u) .
$$

Hence $(d)$ implies that

$$
\left|\psi(x, t)-\left(D_{2} \varphi\right)(x, s)\right|<\varepsilon \quad(a \leq x \leq b, \quad 0<|t-s|<\delta) .
$$

Note that

$$
\frac{f(t)-f(s)}{t-s}=\int_{a}^{b} \psi(x, t) d \alpha(x)
$$

By (102), $\psi^{t} \rightarrow\left(D_{2} \varphi\right)^{s}$, uniformly on $[a, b]$, as $t \rightarrow s$. Since each $\psi^{t} \in \mathscr{R}(\alpha)$, the desired conclusion follows from (103) and Theorem 7.16.

9.43 Example One can of course prove analogues of Theorem 9.42 with $(-\infty, \infty)$ in place of $[a, b]$. Instead of doing this, let us simply look at an example. Define

$$
f(t)=\int_{-\infty}^{\infty} e^{-x^{2}} \cos (x t) d x
$$

and

$$
g(t)=-\int_{-\infty}^{\infty} x e^{-x^{2}} \sin (x t) d x
$$

for $-\infty<t<\infty$. Both integrals exist (they converge absolutely) since the absolute values of the integrands are at most $\exp \left(-x^{2}\right)$ and $|x| \exp \left(-x^{2}\right)$, respectively.

Note that $g$ is obtained from $f$ by differentiating the integrand with respect to $t$. We claim that $f$ is differentiable and that

$$
f^{\prime}(t)=g(t) \quad(-\infty<t<\infty)
$$

To prove this, let us first examine the difference quotients of the cosine: if $\beta>0$, then

$$
\frac{\cos (\alpha+\beta)-\cos \alpha}{\beta}+\sin \alpha=\frac{1}{\beta} \int_{\alpha}^{\alpha+\beta}(\sin \alpha-\sin t) d t \text {. }
$$

Since $|\sin \alpha-\sin t| \leq|t-\alpha|$, the right side of (107) is at most $\beta / 2$ in absolute value; the case $\beta<0$ is handled similarly. Thus

$$
\left|\frac{\cos (\alpha+\beta)-\cos \alpha}{\beta}+\sin \alpha\right| \leq|\beta|
$$

for all $\beta$ (if the left side is interpreted to be 0 when $\beta=0$ ).

Now fix $t$, and fix $h \neq 0$. Apply (108) with $\alpha=x t, \beta=x h$; it follows from (104) and (105) that

$$
\left|\frac{f(t+h)-f(t)}{h}-g(t)\right| \leq|h| \int_{-\infty}^{\infty} x^{2} e^{-x^{2}} d x
$$

When $h \rightarrow 0$, we thus obtain (106).

Let us go a step further: An integration by parts, applied to (104), shows that

$$
f(t)=2 \int_{-\infty}^{\infty} x e^{-x^{2}} \frac{\sin (x t)}{t} d x
$$

Thus $t f(t)=-2 g(t)$, and (106) implies now that $f$ satisfies the differential equation

$$
2 f^{\prime}(t)+t f(t)=0
$$

If we solve this differential equation and use the fact that $f(0)=\sqrt{\pi}$ (see Sec. 8.21), we find that

$$
f(t)=\sqrt{\pi} \exp \left(-\frac{t^{2}}{4}\right)
$$

The integral (104) is thus explicitly determined.

## EXERCISES

1. If $S$ is a nonempty subset of a vector space $X$, prove (as asserted in Sec. 9.1) that the span of $S$ is a vector space.
2. Prove (as asserted in Sec. 9.6) that $B A$ is linear if $A$ and $B$ are linear transformations. Prove also that $A^{-1}$ is linear and invertible.
3. Assume $A \in L(X, Y)$ and $A \mathbf{x}=\mathbf{0}$ only when $\mathbf{x}=\mathbf{0}$. Prove that $A$ is then 1-1.
4. Prove (as asserted in Sec. 9.30) that null spaces and ranges of linear transformations are vector spaces.
5. Prove that to every $A \in L\left(R^{n}, R^{1}\right)$ corresponds a unique $\mathbf{y} \in R^{n}$ such that $A \mathbf{x}=\mathbf{x} \cdot \mathbf{y}$. Prove also that $\|A\|=|\mathbf{y}|$.

Hint: Under certain conditions, equality holds in the Schwarz inequality.

6. If $f(0,0)=0$ and

$$
f(x, y)=\frac{x y}{x^{2}+y^{2}} \quad \text { if }(x, y) \neq(0,0)
$$

prove that $\left(D_{1} f\right)(x, y)$ and $\left(D_{2} f\right)(x, y)$ exist at every point of $R^{2}$, although $f$ is not continuous at $(0,0)$.

7. Suppose that $f$ is a real-valued function defined in an open set $E \subset R^{n}$, and that the partial derivatives $D_{1} f, \ldots, D_{n} f$ are bounded in $E$. Prove that $f$ is continuous in $E$.

Hint: Proceed as in the proof of Theorem 9.21.

8. Suppose that $f$ is a differentiable real function in an open set $E \subset R^{n}$, and that $f$ has a local maximum at a point $\mathbf{x} \in E$. Prove that $f^{\prime}(\mathbf{x})=0$.
9. If $\mathrm{f}$ is a differentiable mapping of a connected open set $E \subset R^{n}$ into $R^{m}$, and if $\mathbf{f}^{\prime}(\mathbf{x})=0$ for every $\mathbf{x} \in E$, prove that $\mathbf{f}$ is constant in $E$.
10. If $f$ is a real function defined in a convex open set $E \subset R^{n}$, such that $\left(D_{1} f\right)(\mathbf{x})=0$ for every $\mathbf{x} \in E$, prove that $f(\mathbf{x})$ depends only on $x_{2}, \ldots, x_{n}$.

Show that the convexity of $E$ can be replaced by a weaker condition, but that some condition is required. For example, if $n=2$ and $E$ is shaped like a horseshoe, the statement may be false.

11. If $f$ and $g$ are differentiable real functions in $R^{n}$, prove that

$$
\nabla(f g)=f \nabla g+g \nabla f
$$

and that $\nabla(1 / f)=-f^{-2} \nabla f$ wherever $f \neq 0$.

12. Fix two real numbers $a$ and $b, 0<a<b$. Define a mapping $\mathrm{f}=\left(f_{1}, f_{2}, f_{3}\right)$ of $R^{2}$ into $R^{3}$ by

$$
\begin{aligned}
& f_{1}(s, t)=(b+a \cos s) \cos t \\
& f_{2}(s, t)=(b+a \cos s) \sin t \\
& f_{3}(s, t)=a \sin s
\end{aligned}
$$

Describe the range $K$ of $\mathrm{f}$. (It is a certain compact subset of $R^{3}$.)

(a) Show that there are exactly 4 points $\mathrm{p} \in K$ such that

$$
\left(\nabla f_{1}\right)\left(\mathbf{f}^{-1}(\mathbf{p})\right)=\mathbf{0}
$$

Find these points.

(b) Determine the set of all $\mathbf{q} \in K$ such that

$$
\left(\nabla f_{3}\right)\left(\mathbf{f}^{-1}(\mathbf{q})\right)=\mathbf{0}
$$

(c) Show that one of the points $\mathbf{p}$ found in part (a) corresponds to a local maximum of $f_{1}$, one corresponds to a local minimum, and that the other two are neither (they are so-called "saddle points").

Which of the points $\mathbf{q}$ found in part $(b)$ correspond to maxima or minima?

(d) Let $\lambda$ be an irrational real number, and define $\mathbf{g}(t)=\mathbf{f}(t, \lambda t)$. Prove that $\mathbf{g}$ is a

1-1 mapping of $R^{1}$ onto a dense subset of $K$. Prove that

$$
\left|\mathbf{g}^{\prime}(t)\right|^{2}=a^{2}+\lambda^{2}(b+a \cos t)^{2} .
$$

13. Suppose $\mathrm{f}$ is a differentiable mapping of $R^{1}$ into $R^{3}$ such that $|\mathrm{f}(t)|=1$ for every $t$.

Prove that $\mathbf{f}^{\prime}(t) \cdot \mathbf{f}(t)=0$.

Interpret this result geometrically.

14. Define $f(0,0)=0$ and

$$
f(x, y)=\frac{x^{3}}{x^{2}+y^{2}} \quad \text { if }(x, y) \neq(0,0)
$$

(a) Prove that $D_{1} f$ and $D_{2} f$ are bounded functions in $R^{2}$. (Hence $f$ is continuous.)

(b) Let $\mathrm{u}$ be any unit vector in $R^{2}$. Show that the directional derivative $\left(D_{\mathrm{u}} f\right)(0,0)$ exists, and that its absolute value is at most 1.

(c) Let $\gamma$ be a differentiable mapping of $R^{1}$ into $R^{2}$ (in other words, $\gamma$ is a differentiable curve in $\left.R^{2}\right)$, with $\gamma(0)=(0,0)$ and $\left|\gamma^{\prime}(0)\right|>0$. Put $g(t)=f(\gamma(t))$ and prove that $g$ is differentiable for every $t \in R^{1}$.

If $\gamma \in \mathscr{C}^{\prime}$, prove that $g \in \mathscr{C}^{\prime}$.

(d) In spite of this, prove that $f$ is not differentiable at $(0,0)$.

Hint: Formula (40) fails.

15. Define $f(0,0)=0$, and put

$$
f(x, y)=x^{2}+y^{2}-2 x^{2} y-\frac{4 x^{6} y^{2}}{\left(x^{4}+y^{2}\right)^{2}}
$$

if $(x, y) \neq(0,0)$.

(a) Prove, for all $(x, y) \in R^{2}$, that

$$
4 x^{4} y^{2} \leq\left(x^{4}+y^{2}\right)^{2} \text {. }
$$

Conclude that $f$ is continuous.
(b) For $0 \leq \theta \leq 2 \pi,-\infty<t<\infty$, define

$$
g_{\theta}(t)=f(t \cos \theta, t \sin \theta) .
$$

Show that $g_{\theta}(0)=0, g_{\theta}^{\prime}(0)=0, g_{\theta}^{\prime \prime}(0)=2$. Each $g_{\theta}$ has therefore a strict local minimum at $t=0$.

In other words, the restriction of $f$ to each line through $(0,0)$ has a strict local minimum at $(0,0)$.

(c) Show that $(0,0)$ is nevertheless not a local minimum for $f$, since $f\left(x, x^{2}\right)=-x^{4}$.

16. Show that the continuity of $\mathbf{f}^{\prime}$ at the point $a$ is needed in the inverse function theorem, even in the case $n=1$ : If

$$
f(t)=t+2 t^{2} \sin \left(\frac{1}{t}\right)
$$

for $t \neq 0$, and $f(0)=0$, then $f^{\prime}(0)=1, f^{\prime}$ is bounded in $(-1,1)$, but $f$ is not one-to-one in any neighborhood of 0 .

17. Let $\mathbf{f}=\left(f_{1}, f_{2}\right)$ be the mapping of $R^{2}$ into $R^{2}$ given by

$$
f_{1}(x, y)=e^{x} \cos y, \quad f_{2}(x, y)=e^{x} \sin y .
$$

(a) What is the range of $f$ ?

(b) Show that the Jacobian of $f$ is not zero at any point of $R^{2}$. Thus every point of $R^{2}$ has a neighborhood in which $f$ is one-to-one. Nevertheless, $f$ is not one-toone on $R^{2}$.

(c) Put $\mathbf{a}=(0, \pi / 3), \mathbf{b}=f(\mathbf{a})$, let $\mathbf{g}$ be the continuous inverse of $\mathbf{f}$, defined in a neighborhood of $\mathbf{b}$, such that $\mathbf{g}(\mathbf{b})=\mathbf{a}$. Find an explicit formula for $\mathbf{g}$, compute $\mathbf{f}^{\prime}(\mathbf{a})$ and $\mathbf{g}^{\prime}(\mathbf{b})$, and verify the formula (52).

(d) What are the images under $f$ of lines parallel to the coordinate axes?

18. Answer analogous questions for the mapping defined by

$$
u=x^{2}-y^{2}, \quad v=2 x y .
$$

19. Show that the system of equations

$$
\begin{array}{r}
3 x+y-z+u^{2}=0 \\
x-y+2 z+u=0 \\
2 x+2 y-3 z+2 u=0
\end{array}
$$

can be solved for $x, y, u$ in terms of $z$; for $x, z, u$ in terms of $y$; for $y, z, u$ in terms of $x$; but not for $x, y, z$ in terms of $u$.

20. Take $n=m=1$ in the implicit function theorem, and interpret the theorem (as well as its proof) graphically.
21. Define $f$ in $R^{2}$ by

$$
f(x, y)=2 x^{3}-3 x^{2}+2 y^{3}+3 y^{2} .
$$

(a) Find the four points in $R^{2}$ at which the gradient of $f$ is zero. Show that $f$ has exactly one local maximum and one local minimum in $R^{2}$.
(b) Let $S$ be the set of all $(x, y) \in R^{2}$ at which $f(x, y)=0$. Find those points of $S$ that have no neighborhoods in which the equation $f(x, y)=0$ can be solved for $y$ in terms of $x$ (or for $x$ in terms of $y$ ). Describe $S$ as precisely as you can.

22. Give a similar discussion for

$$
f(x, y)=2 x^{3}+6 x y^{2}-3 x^{2}+3 y^{2} .
$$

23. Define $f$ in $R^{3}$ by

$$
f\left(x, y_{1}, y_{2}\right)=x^{2} y_{1}+e^{x}+y_{2} \text {. }
$$

Show that $f(0,1,-1)=0,\left(D_{1} f\right)(0,1,-1) \neq 0$, and that there exists therefore a differentiable function $g$ in some neighborhood of $(1,-1)$ in $R^{2}$, such that $g(1,-1)=0$ and

$$
f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right)=0
$$

Find $\left(D_{1} g\right)(1,-1)$ and $\left(D_{2} g\right)(1,-1)$.

24. For $(x, y) \neq(0,0)$, define $\mathbf{f}=\left(f_{1}, f_{2}\right)$ by

$$
f_{1}(x, y)=\frac{x^{2}-y^{2}}{x^{2}+y^{2}}, \quad f_{2}(x, y)=\frac{x y}{x^{2}+y^{2}}
$$

Compute the rank of $\mathrm{f}^{\prime}(x, y)$, and find the range of $\mathbf{f}$.

25. Suppose $A \in L\left(R^{n}, R^{m}\right)$, let $r$ be the rank of $A$.

(a) Define $S$ as in the proof of Theorem 9.32. Show that $S A$ is a projection in $R^{n}$ whose null space is $\mathscr{N}(A)$ and whose range is $\mathscr{R}(S)$. Hint: By (68), SASA $=S A$. (b) Use (a) to show that

$$
\operatorname{dim} \mathscr{N}(A)+\operatorname{dim} \mathscr{R}(A)=n
$$

26. Show that the existence (and even the continuity) of $D_{12} f$ does not imply the existence of $D_{1} f$. For example, let $f(x, y)=g(x)$, where $g$ is nowhere differentiable.
27. Put $f(0,0)=0$, and

$$
f(x, y)=\frac{x y\left(x^{2}-y^{2}\right)}{x^{2}+y^{2}}
$$

if $(x, y) \neq(0,0)$. Prove that

(a) $f, D_{1} f, D_{2} f$ are continuous in $R^{2}$;

(b) $D_{12} f$ and $D_{21} f$ exist at every point of $R^{2}$, and are continuous except at $(0,0)$;

(c) $\left(D_{12} f\right)(0,0)=1$, and $\left(D_{21} f\right)(0,0)=-1$.

28. For $t \geq 0$, put

$$
\varphi(x, t)= \begin{cases}x & (0 \leq x \leq \sqrt{t}) \\ -x+2 \sqrt{t} & (\sqrt{t} \leq x \leq 2 \sqrt{t}) \\ 0 & \text { (otherwise) }\end{cases}
$$

and put $\varphi(x, t)=-\varphi(x,|t|)$ if $t<0$.

Show that $\varphi$ is continuous on $R^{2}$, and

$$
\left(D_{2} \varphi\right)(x, 0)=0
$$

for all $x$. Define

$$
f(t)=\int_{-1}^{1} \varphi(x, t) d x .
$$

Show that $f(t)=t$ if $|t|<t$. Hence

$$
f^{\prime}(0) \neq \int_{-1}^{1}\left(D_{2} \varphi\right)(x, 0) d x
$$

29. Let $E$ be an open set in $R^{n}$. The classes $\mathscr{C}^{\prime}(E)$ and $\mathscr{C}^{\prime \prime}(E)$ are defined in the text. By induction, $\mathscr{C}^{(k)}(E)$ can be defined as follows, for all positive integers $k$ : To say that $f \in \mathscr{C}^{(k)}(E)$ means that the partial derivatives $D_{1} f, \ldots, D_{n} f$ belong to $\mathscr{C}^{(k-1)}(E)$.

Assume $f \in \mathscr{C}^{(k)}(E)$. and show (by repeated application of Theorem 9.41) that the $k$ th-order derivative

$$
D_{l_{1} l_{2} \ldots l_{k}} f=D_{l_{1}} D_{l_{2}} \ldots D_{l_{k}} f
$$

is unchanged if the subscripts $i_{1}, \ldots, i_{k}$ are permuted.

For instance, if $n \geq 3$, then

$$
D_{1213} f=D_{3112} f
$$

for every $f \in \mathscr{C}^{(4)}$.

30. Let $f \in \mathscr{C}^{(m)}(E)$, where $E$ is an open subset of $R^{n}$. Fix $\mathbf{a} \in E$, and suppose $\mathbf{x} \in R^{n}$ is so close to 0 that the points

$$
\mathbf{p}(t)=\mathbf{a}+t \mathbf{x}
$$

lie in $E$ whenever $0 \leq t \leq 1$. Define

$$
h(t)=f(\mathrm{p}(t))
$$

for all $t \in R^{1}$ for which $\mathrm{p}(t) \in E$.

(a) For $1 \leq k \leq m$, show (by repeated application of the chain rule) that

$$
h^{(k)}(t)=\sum\left(D_{l_{1}} \ldots{l_{k}} f\right)(\mathrm{p}(t)) x_{l_{1} \ldots} x_{l_{k}} .
$$

The sum extends over all ordered $k$-tuples $\left(i_{1}, \ldots, i_{k}\right)$ in which each $i_{j}$ is one of the integers $1, \ldots, n$.

(b) By Taylor's theorem (5.15),

$$
h(1)=\sum_{k=0}^{m-1} \frac{h^{(k)}(0)}{k !}+\frac{h^{(m)}(t)}{m !}
$$

for some $t \in(0,1)$. Use this to prove Taylor's theorem in $n$ variables by showing that the formula

$$
f(\mathbf{a}+\mathbf{x})=\sum_{k=0}^{m-1} \frac{1}{k !} \sum\left(D_{i 1} \ldots \iota_{k} f\right)(\mathbf{a}) x_{l_{1}} \cdots x_{l_{k}}+r(\mathbf{x})
$$

represents $f(a+x)$ as the sum of its so-called "Taylor polynomial of degree $m-1$, , plus a remainder that satisfies

$$
\lim _{\mathbf{x} \rightarrow 0} \frac{r(\mathbf{x})}{|\mathbf{x}|^{m-1}}=0
$$

Each of the inner sums extends over all ordered $k$-tuples $\left(i_{1}, \ldots, i_{k}\right)$, as in part $(a)$; as usual, the zero-order derivative of $f$ is simply $f$, so that the constant term of the Taylor polynomial of $f$ at $\mathbf{a}$ is $f(\mathbf{a})$.

(c) Exercise 29 shows that repetition occurs in the Taylor polynomial as written in part (b). For instance, $D_{113}$ occurs three times, as $D_{113}, D_{131}, D_{311}$. The sum of the corresponding three terms can be written in the form

$$
3\left(D_{1}^{2} D_{3} f\right)(\mathbf{a}) x_{1}^{2} x_{3} .
$$

Prove (by calculating how often each derivative occurs) that the Taylor polynomial in $(b)$ can be written in the form

$$
\sum \frac{\left(D_{1}^{s_{1}} \cdots D_{n}^{s_{n}} f\right)(\mathrm{a})}{s_{1} ! \cdots s_{n} !} x_{1}^{s_{1}} \cdots x_{n}^{s_{n}}
$$

Here the summation extends over all ordered $n$-tuples $\left(s_{1}, \ldots, s_{n}\right)$ such that each $s_{1}$ is a nonnegative integer, and $s_{1}+\cdots+s_{n} \leq m-1$.

31. Suppose $f \in \mathscr{C}^{(3)}$ in some neighborhood of a point $\mathbf{a} \in R^{2}$, the gradient of $f$ is 0 at a, but not all second-order derivatives of $f$ are 0 at a. Show how one can then determine from the Taylor polynomial of $f$ at a (of degree 2 ) whether $f$ has a local maximum, or a local minimum, or neither, at the point a.

Extend this to $R^{n}$ in place of $R^{2}$.

## INTEGRATION OF DIFFERENTIAL FORMS

Integration can be studied on many levels. In Chap. 6, the theory was developed for reasonably well-behaved functions on subintervals of the real line. In Chap. 11 we shall encounter a very highly developed theory of integration that can be applied to much larger classes of functions, whose domains are more or less arbitrary sets, not necessarily subsets of $R^{n}$. The present chapter is devoted to those aspects of integration theory that are closely related to the geometry of euclidean spaces, such as the change of variables formula, line integrals, and the machinery of differential forms that is used in the statement and proof of the $n$-dimensional analogue of the fundamental theorem of calculus, namely Stokes' theorem.

## INTEGRATION

10.1 Definition Suppose $I^{k}$ is a $k$-cell in $R^{k}$, consisting of all

such that

$$
\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right)
$$

$$
a_{i} \leq x_{i} \leq b_{i} \quad(i=1, \ldots, k)
$$

$I^{j}$ is the $j$-cell in $R^{j}$ defined by the first $j$ inequalities (1), and $f$ is a real continuous function on $I^{k}$.

Put $f=f_{k}$, and define $f_{k-1}$ on $I^{k-1}$ by

$$
f_{k-1}\left(x_{1}, \ldots, x_{k-1}\right)=\int_{a_{k}}^{b_{k}} f_{k}\left(x_{1}, \ldots, x_{k-1}, x_{k}\right) d x_{k}
$$

The uniform continuity of $f_{k}$ on $I^{k}$ shows that $f_{k-1}$ is continuous on $I^{k-1}$. Hence we can repeat this process and obtain functions $f_{j}$, continuous on $I^{j}$, such that $f_{j-1}$ is the integral of $f_{j}$, with respect to $x_{j}$, over $\left[a_{j}, b_{j}\right]$. After $k$ steps we arrive at a number $f_{0}$, which we call the integral of $f$ over $I^{k}$; we write it in the form

$$
\int_{I^{k}} f(\mathbf{x}) d \mathbf{x} \quad \text { or } \quad \int_{I^{k}} f
$$

A priori, this definition of the integral depends on the order in which the $k$ integrations are carried out. However, this dependence is only apparent. To prove this, let us introduce the temporary notation $L(f)$ for the integral (2) and $L^{\prime}(f)$ for the result obtained by carrying out the $k$ integrations in some other order.

10.2 Theorem For every $f \in \mathscr{C}\left(I^{k}\right), L(f)=L^{\prime}(f)$.

Proof If $h(\mathbf{x})=h_{1}\left(x_{1}\right) \cdots h_{k}\left(x_{k}\right)$, where $h_{j} \in \mathscr{C}\left(\left[a_{j}, b_{j}\right]\right)$, then

$$
L(h)=\prod_{i=1}^{k} \int_{a_{i}}^{b_{i}} h_{i}\left(x_{i}\right) d x_{i}=L^{\prime}(h)
$$

If $\mathscr{A}$ is the set of all finite sums of such functions $h$, it follows that $L(g)=$ $L^{\prime}(g)$ for all $g \in \mathscr{A}$. Also, $\mathscr{A}$ is an algebra of functions on $I^{k}$ to which the Stone-Weierstrass theorem applies.

Put $V=\prod_{1}^{k}\left(b_{i}-a_{i}\right)$. If $f \in \mathscr{C}\left(I^{k}\right)$ and $\varepsilon>0$, there exists $g \in \mathscr{A}$ such that $\|f-g\|<\varepsilon / V$, where $\|f\|$ is defined as $\max |f(\mathbf{x})|\left(\mathbf{x} \in I^{k}\right)$. Then $|L(f-g)|<\varepsilon,\left|L^{\prime}(f-g)\right|<\varepsilon$, and since

$$
L(f)-L^{\prime}(f)=L(f-g)+L^{\prime}(g-f)
$$

we conclude that $\left|L(f)-L^{\prime}(f)\right|<2 \varepsilon$.

In this connection, Exercise 2 is relevant.

10.3 Definition The support of a (real or complex) function $f$ on $R^{k}$ is the closure of the set of all points $\mathbf{x} \in R^{k}$ at which $f(\mathbf{x}) \neq 0$. If $f$ is a continuous
function with compact support, let $I^{k}$ be any $k$-cell which contains the support of $f$, and define

$$
\int_{R^{k}} f=\int_{I^{k}} f .
$$

The integral so defined is evidently independent of the choice of $I^{k}$, provided only that $I^{k}$ contains the support of $f$.

It is now tempting to extend the definition of the integral over $R^{k}$ to functions which are limits (in some sense) of continuous functions with compact support. We do not want to discuss the conditions under which this can be done; the proper setting for this question is the Lebesgue integral. We shall merely describe one very simple example which will be used in the proof of Stokes' theorem.

10.4 Example Let $Q^{k}$ be the $k$-simplex which consists of all points $\mathbf{x}=$ $\left(x_{1}, \ldots, x_{k}\right)$ in $R^{k}$ for which $x_{1}+\cdots+x_{k} \leq 1$ and $x_{i} \geq 0$ foi $i=1, \ldots, k$. If $k=3$, for example, $Q^{k}$ is a tetrahedron, with vertices at $\mathbf{0}, \mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}$. If $f \in \mathscr{C}\left(Q^{k}\right)$, extend $f$ to a function on $I^{k}$ by setting $f(\mathbf{x})=0$ off $Q^{k}$, and define

$$
\int_{Q^{k}} f=\int_{I^{k}} f
$$

Here $I^{k}$ is the "unit cube" defined by

$$
0 \leq x_{i} \leq 1 \quad(1 \leq i \leq k)
$$

Since $f$ may be discontinuous on $I^{k}$, the existence of the integral on the right of (4) needs proof. We also wish to show that this integral is independent of the order in which the $k$ single integrations are carried out.

To do this, suppose $0<\delta<1$, put

$$
\varphi(t)= \begin{cases}1 & (t \leq 1-\delta) \\ \frac{(1-t)}{\delta} & (1-\delta<t \leq 1) \\ 0 & (1<t),\end{cases}
$$

and define

$$
F(\mathbf{x})=\varphi\left(x_{1}+\cdots+x_{k}\right) f(\mathbf{x}) \quad\left(\mathbf{x} \in I^{k}\right)
$$

Then $F \in \mathscr{C}\left(I^{k}\right)$.

Put $\mathbf{y}=\left(x_{1}, \ldots, x_{k-1}\right), \mathbf{x}=\left(\mathbf{y}, x_{k}\right)$. For each $\mathbf{y} \in I^{k-1}$, the set of all $x_{k}$ such that $F\left(y, x_{k}\right) \neq f\left(y ; x_{k}\right)$ is either empty or is a segment whose length does not exceed $\delta$. Since $0 \leq \varphi \leq 1$, it follows that

$$
\left|F_{k-1}(\mathbf{y})-f_{k-1}(\mathrm{y})\right| \leq \delta\|f\| \quad\left(\mathrm{y} \in I^{k-1}\right)
$$

where $\|f\|$ has the same meaning as in the proof of Theorem 10.2, and $F_{k-1}$, $f_{k-1}$ are as in Definition 10.1.

As $\delta \rightarrow 0$, (7) exhibits $f_{k-1}$ as a uniform limit of a sequence of continuous functions. Thus $f_{k-1} \in \mathscr{C}\left(I^{k-1}\right)$, and the further integrations present no problem. This proves the existence of the integral (4). Moreover, (7) shows that

$$
\left|\int_{I^{k}} F(\mathbf{x}) d \mathbf{x}-\int_{I^{k}} f(\mathbf{x}) d \mathbf{x}\right| \leq \delta\|f\|
$$

Note that (8) is true, regardless of the order in which the $k$ single integrations are carried out. Since $F \in \mathscr{C}\left(I^{k}\right), \int F$ is unaffected by any change in this order. Hence (8) shows that the same is true of $\int f$.

This completes the proof.

Our next goal is the change of variables formula stated in Theorem 10.9. To facilitate its proof, we first discuss so-called primitive mappings, and partitions of unity. Primitive mappings will enable us to get a clearer picture of the local action of a $\mathscr{C}^{\prime}$-mapping with invertible derivative, and partitions of unity are a very useful device that makes it possible to use local information in a global setting.

## PRIMITIVE MAPPINGS

10.5 Definition If $\mathbf{G}$ maps an open set $E \subset R^{n}$ into $R^{n}$, and if there is an integer $m$ and a real function $g$ with domain $E$ such that

$$
\mathbf{G}(\mathbf{x})=\sum_{i \neq m} x_{i} \mathbf{e}_{i}+g(\mathbf{x}) \mathbf{e}_{m} \quad(\mathbf{x} \in E)
$$

then we call $\mathbf{G}$ primitive. A primitive mapping is thus one that changes at most one coordinate. Note that (9) can also be written in the form

$$
\mathbf{G}(\mathbf{x})=\mathbf{x}+\left[g(\mathbf{x})-x_{m}\right] \mathbf{e}_{m} .
$$

If $g$ is differentiable at some point $\mathbf{a} \in E$, so is $\mathbf{G}$. The matrix $\left[\alpha_{i j}\right]$ of the operator $\mathbf{G}^{\prime}(\mathbf{a})$ has

$$
\left(D_{1} g\right)(\mathbf{a}), \ldots,\left(D_{m} g\right)(\mathbf{a}), \ldots,\left(D_{n} g\right)(\mathbf{a})
$$

as its $m$ th row. For $j \neq m$, we have $\alpha_{j j}=1$ and $\alpha_{i j}=0$ if $i \neq j$. The Jacobian of $\mathbf{G}$ at $\mathbf{a}$ is thus given by

$$
J_{\mathbf{G}}(\mathbf{a})=\operatorname{det}\left[\mathbf{G}^{\prime}(\mathbf{a})\right]=\left(D_{m} g\right)(\mathbf{a}),
$$

and we see (by Theorem 9.36) that $\mathbf{G}^{\prime}(\mathbf{a})$ is invertible if and only if $\left(D_{m} g\right)(\mathbf{a}) \neq 0$.

10.6 Definition A linear operator $B$ on $R^{n}$ that interchanges some pair of members of the standard basis and leaves the others fixed will be called a flip. For example, the flip $B$ on $R^{4}$ that interchanges $\mathbf{e}_{2}$ and $\mathbf{e}_{4}$ has the form

$$
B\left(x_{1} \mathbf{e}_{1}+x_{2} \mathbf{e}_{2}+x_{3} \mathbf{e}_{3}+x_{4} \mathbf{e}_{4}\right)=x_{1} \mathbf{e}_{1}+x_{2} \mathbf{e}_{4}+x_{3} \mathbf{e}_{3}+x_{4} \mathbf{e}_{2}
$$

or, equivalently,

$$
B\left(x_{1} \mathbf{e}_{1}+x_{2} \mathbf{e}_{2}+x_{3} \mathbf{e}_{3}+x_{4} \mathbf{e}_{4}\right)=x_{1} \mathbf{e}_{1}+x_{4} \mathbf{e}_{2}+x_{3} \mathbf{e}_{3}+x_{2} \mathbf{e}_{4}
$$

Hence $B$ can also be thought of as interchanging two of the coordinates, rather than two basis vectors.

In the proof that follows, we shall use the projections $P_{0}, \ldots, P_{n}$ in $R^{n}$, defined by $P_{0} \mathbf{x}=\mathbf{0}$ and

$$
P_{m} \mathbf{x}=x_{1} \mathbf{e}_{1}+\cdots+x_{m} \mathbf{e}_{m}
$$

for $1 \leq m \leq n$. Thus $P_{m}$ is the projection whose range and null space are spanned by $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{m}\right\}$ and $\left\{\mathbf{e}_{m+1}, \ldots, \mathbf{e}_{n}\right\}$, respectively.

10.7 Theorem Suppose $\mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}, 0 \in E$, $\mathbf{F}(\mathbf{0})=\mathbf{0}$, and $\mathbf{F}^{\prime}(\mathbf{0})$ is invertible.

Then there is a neighborhood of 0 in $R^{n}$ in which a representation

$$
\mathbf{F}(\mathbf{x})=B_{1} \cdots B_{n-1} \mathbf{G}_{n} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x})
$$

is valid.

In (16), each $\mathbf{G}_{i}$ is a primitive $\mathscr{C}^{\prime}$-mapping in some neighborhood of $\mathbf{0}$; $\mathbf{G}_{i}(\mathbf{0})=\mathbf{0}, \mathbf{G}_{i}^{\prime}(\mathbf{0})$ is invertible, and each $B_{i}$ is either a flip or the identity operator. and flips.

Briefly, (16) represents $\mathbf{F}$ locally as a composition of primitive mappings

Proof Put $\mathbf{F}=\mathbf{F}_{1}$. Assume $1 \leq m \leq n-1$, and make the following induction hypothesis (which evidently holds for $m=1$ ):

$V_{m}$ is a neighborhood of $\mathbf{0}, \mathbf{F}_{m} \in \mathscr{C}^{\prime}\left(V_{m}\right), \mathbf{F}_{m}(\mathbf{0})=\mathbf{0}, \mathbf{F}_{m}^{\prime}(\mathbf{0})$ is invertible, and

$$
P_{m-1} \mathbf{F}_{m}(\mathbf{x})=P_{m-1} \mathbf{x} \quad\left(\mathbf{x} \in V_{m}\right)
$$

By (17), we have

$$
\mathbf{F}_{m}(\mathbf{x})=P_{m-1} \mathbf{x}+\sum_{i=m}^{n} \alpha_{i}(\mathbf{x}) \mathbf{e}_{i}
$$

where $\alpha_{m}, \ldots, \alpha_{n}$ are real $\mathscr{C}^{\prime}$-functions in $V_{m}$. Hence

$$
\mathbf{F}_{m}^{\prime}(\mathbf{0}) \mathbf{e}_{m}=\sum_{i=m}^{n}\left(D_{m} \alpha_{l}\right)(\mathbf{0}) \mathbf{e}_{i}
$$

Since $\mathbf{F}_{m}^{\prime}(\mathbf{0})$ is invertible, the left side of (19) is not $\mathbf{0}$, and therefore there is a $k$ such that $m \leq k \leq n$ and $\left(D_{m} \alpha_{k}\right)(0) \neq 0$.

Let $B_{m}$ be the flip that interchanges $m$ and this $k$ (if $k=m, B_{m}$ is the identity) and define

$$
\mathbf{G}_{m}(\mathbf{x})=\mathbf{x}+\left[\alpha_{k}(\mathbf{x})-x_{m}\right] \mathbf{e}_{m} \quad\left(\mathbf{x} \in V_{m}\right)
$$

Then $\mathbf{G}_{m} \in \mathscr{C}^{\prime}\left(V_{m}\right), \mathbf{G}_{m}$ is primitive, and $\mathbf{G}_{m}^{\prime}(\mathbf{0})$ is invertible, since $\left(D_{m} \alpha_{k}\right)(\mathbf{0}) \neq 0$.

The inverse function theorem shows therefore that there is an open set $U_{m}$, with $\mathbf{0} \in U_{m} \subset V_{m}$, such that $\mathbf{G}_{m}$ is a 1-1 mapping of $U_{m}$ onto a neighborhood $V_{m+1}$ of $\mathbf{0}$, in which $\mathbf{G}_{m}^{-1}$ is continuously differentiable. Define $\mathbf{F}_{m+1}$ by

$$
\mathbf{F}_{m+1}(\mathbf{y})=B_{m} \mathbf{F}_{m} \circ \mathbf{G}_{m}^{-1}(\mathbf{y}) \quad\left(\mathbf{y} \in V_{m+1}\right)
$$

Then $\mathbf{F}_{m+1} \in \mathscr{C}^{\prime}\left(V_{m+1}\right), \mathbf{F}_{m+1}(\mathbf{0})=\mathbf{0}$, and $\mathbf{F}_{m+1}^{\prime}(\mathbf{0})$ is invertible (by the chain rule). Also, for $\mathbf{x} \in U_{m}$,

$$
\begin{aligned}
P_{m} \mathbf{F}_{m+1}\left(\mathbf{G}_{m}(\mathbf{x})\right) & =P_{m} B_{m} \mathbf{F}_{m}(\mathbf{x}) \\
& =P_{m}\left[P_{m-1} \mathbf{x}+\alpha_{k}(\mathbf{x}) \mathbf{e}_{m}+\cdots\right] \\
& =P_{m-1} \mathbf{x}+\alpha_{k}(\mathbf{x}) \mathbf{e}_{m} \\
& =P_{m} \mathbf{G}_{m}(\mathbf{x})
\end{aligned}
$$

so that

$$
P_{m} \mathbf{F}_{m+1}(\mathbf{y})=P_{m} \mathbf{y} \quad\left(\mathbf{y} \in V_{m+1}\right)
$$

Our induction hypothesis holds therefore with $m+1$ in place of $m$.

[In (22), we first used (21), then (18) and the definition of $B_{m}$, then the definition of $P_{m}$, and finally (20).]

Since $B_{m} B_{m}=I$, (21), with $\mathbf{y}=\mathbf{G}_{m}(\mathbf{x})$, is equivalent to

$$
\mathbf{F}_{m}(\mathbf{x})=B_{m} \mathbf{F}_{m+1}\left(\mathbf{G}_{m}(\mathbf{x})\right) \quad\left(\mathbf{x} \in U_{m}\right)
$$

If we apply this with $m=1, \ldots, n-1$, we successively obtain

$$
\begin{aligned}
\mathbf{F}=\mathbf{F}_{1} & =B_{1} \mathbf{F}_{2} \circ \mathbf{G}_{1} \\
& =B_{1} B_{2} \mathbf{F}_{3} \circ \mathbf{G}_{2} \circ \mathbf{G}_{1}=\cdots \\
& =B_{1} \cdots B_{n-1} \mathbf{F}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_{1}
\end{aligned}
$$

in some neighborhood of $\mathbf{0}$. By (17), $\mathbf{F}_{n}$ is primitive. This completes the proof.

## PARTITIONS OF UNITY

10.8 Theorem Suppose $K$ is a compact subset of $R^{n}$, and $\left\{\mathrm{V}_{\alpha}\right\}$ is an open cover of $K$. Then there exist functions $\psi_{1}, \ldots, \psi_{s} \in \mathscr{C}\left(R^{n}\right)$ such that

(a) $0 \leq \psi_{i} \leq 1$ for $1 \leq i \leq s$;

(b) each $\psi_{i}$ has its support in some $V_{\alpha}$, and

(c) $\psi_{1}(\mathbf{x})+\cdots+\psi_{s}(\mathbf{x})=1$ for every $\mathbf{x} \in K$.

Because of $(c),\left\{\psi_{i}\right\}$ is called a partition of unity, and $(b)$ is sometimes expressed by saying that $\left\{\psi_{i}\right\}$ is subordinate to the cover $\left\{V_{\alpha}\right\}$.

Corollary If $f \in \mathscr{C}\left(R^{n}\right)$ and the support of $f$ lies in $K$, then

$$
f=\sum_{i=1}^{s} \psi_{i} f
$$

Each $\psi_{i} f$ has its support in some $V_{\alpha}$.

The point of (25) is that it furnishes a representation of $f$ as a sum of continuous functions $\psi_{i} f$ with "small" supports.

Proof Associate with each $\mathbf{x} \in K$ an index $\alpha(\mathbf{x})$ so that $\mathbf{x} \in V_{\alpha(\mathbf{x})}$. Then there are open balls $B(\mathbf{x})$ and $W(\mathbf{x})$, centered at $\mathbf{x}$, with

$$
\overline{B(\mathbf{x})} \subset W(\mathbf{x}) \subset \overline{W(\mathbf{x})} \subset V_{\alpha(\mathbf{x})}
$$

Since $K$ is compact, there are points $\mathbf{x}_{1}, \ldots, \mathbf{x}_{s}$ in $K$ such that

$$
K \subset B\left(\mathbf{x}_{1}\right) \cup \cdots \cup B\left(\mathbf{x}_{s}\right) .
$$

By (26), there are functions $\varphi_{1}, \ldots, \varphi_{s} \in \mathscr{C}\left(R^{n}\right)$, such that $\varphi_{i}(\mathbf{x})=1$ on $B\left(\mathbf{x}_{i}\right), \varphi_{i}(\mathbf{x})=0$ outside $W\left(\mathbf{x}_{i}\right)$, and $0 \leq \varphi_{i}(\mathbf{x}) \leq 1$ on $R^{n}$. Define $\psi_{1}=\varphi_{1}$ and

for $i=1, \ldots, s-1$.

Properties $(a)$ and $(b)$ are clear. The relation

$$
\psi_{1}+\cdots+\psi_{i}=1-\left(1-\varphi_{1}\right) \cdots\left(1-\varphi_{i}\right)
$$

is trivial for $i=1$. If (29) holds for some $i<s$, addition of (28) and (29) yields (29) with $i+1$ in place of $i$. It follows that

$$
\sum_{i=1}^{s} \psi_{i}(\mathbf{x})=1-\prod_{i=1}^{s}\left[1-\varphi_{i}(\mathbf{x})\right] \quad\left(\mathbf{x} \in R^{n}\right)
$$

If $\mathbf{x} \in K$, then $\mathbf{x} \in B\left(\mathbf{x}_{i}\right)$ for some $i$, hence $\varphi_{i}(\mathbf{x})=1$, and the product in (30) is 0 . This proves (c).

## CHANGE OF VARIABLES

We can now describe the effect of a change of variables on a multiple integral. For simplicity, we confine ourselves here to continuous functions with compact support, although this is too restrictive for many applications. This is illustrated by Exercises 9 to 13 .

10.9 Theorem Suppose $T$ is a 1-1 $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{k}$ into $R^{k}$ such that $J_{T}(\mathbf{x}) \neq 0$ for all $\mathbf{x} \in E$. If $f$ is a continuous function on $R^{k}$ whose support is compact and lies in $T(E)$, then

$$
\int_{R^{k}} f(\mathbf{y}) d \mathbf{y}=\int_{R^{k}} f(T(\mathbf{x}))\left|J_{T}(\mathbf{x})\right| d \mathbf{x} .
$$

We recall that $J_{T}$ is the Jacobian of $T$. The assumption $J_{T}(\mathbf{x}) \neq 0$ implies, by the inverse function theorem, that $T^{-1}$ is continuous on $T(E)$, and this ensures that the integrand on the right of (31) has compact support in $E$ (Theorem 4.14).

The appearance of the absolute value of $J_{T}(\mathbf{x})$ in (31) may call for a comment. Take the case $k=1$, and suppose $T$ is a 1-1 $\mathscr{C}^{\prime}$-mapping of $R^{1}$ onto $R^{1}$. Then $J_{T}(x)=T^{\prime}(x)$; and if $T$ is increasing, we have

$$
\int_{R^{1}} f(y) d y=\int_{R^{1}} f(T(x)) T^{\prime}(x) d x,
$$

by Theorems 6.19 and 6.17 , for all continuous $f$ with compact support. But if $T$ decreases, then $T^{\prime}(x)<0$; and if $f$ is positive in the interior of its support, the left side of (32) is positive and the right side is negative. A correct equation is obtained if $T^{\prime}$ is replaced by $\left|T^{\prime}\right|$ in (32).

The point is that the integrals we are now considering are integrals of functions over subsets of $R^{k}$, and we associate no direction or orientation with these subsets. We shall adopt a different point of view when we come to integration of differential forms over surfaces.

Proof It follows from the remarks just made that (31) is true if $T$ is a primitive $\mathscr{C}$ '-mapping (see Definition 10.5), and Theorem 10.2 shows that (31) is true if $T$ is a linear mapping which merely interchanges two coordinates. then

If the theorem is true for transformations $P, Q$, and if $S(\mathbf{x})=P(Q(\mathbf{x}))$,

$$
\begin{aligned}
\int f(\mathbf{z}) d \mathbf{z} & =\int f(P(\mathbf{y}))\left|J_{P}(\mathbf{y})\right| d \mathbf{y} \\
& =\int f(P(Q(\mathbf{x})))\left|J_{P}(Q(\mathbf{x}))\right|\left|J_{Q}(\mathbf{x})\right| d \mathbf{x} \\
& =\int f(S(\mathbf{x}))\left|J_{S}(\mathbf{x})\right| d \mathbf{x},
\end{aligned}
$$

since

$$
\begin{aligned}
J_{P}(Q(\mathbf{x})) J_{Q}(\mathbf{x}) & =\operatorname{det} P^{\prime}(Q(\mathbf{x})) \operatorname{det} Q^{\prime}(\mathbf{x}) \\
& =\operatorname{det} P^{\prime}(Q(\mathbf{x})) Q^{\prime}(\mathbf{x})=\operatorname{det} S^{\prime}(\mathbf{x})=J_{S}(\mathbf{x})
\end{aligned}
$$

by the multiplication theorem for determinants and the chain.rule. Thus the theorem is also true for $S$.

Each point $\mathbf{a} \in E$ has a neighborhood $U \subset E$ in which

$$
T(\mathbf{x})=T(\mathbf{a})+B_{1} \cdots B_{k-1} \mathbf{G}_{k} \circ \mathbf{G}_{k-1} \circ \cdots \circ \mathbf{G}_{\mathbf{1}}(\mathbf{x}-\mathbf{a})
$$

where $\mathbf{G}_{i}$ and $B_{i}$ are as in Theorem 10.7. Setting $V=T(U)$, it follows that (31) holds if the support of $f$ lies in $V$. Thus:

Each point $\mathbf{y} \in T(E)$ lies in an open set $V_{y} \subset T(E)$ such that (31) holds for all continuous functions whose support lies in $V_{\mathbf{y}}$.

Now let $f$ be a continuous function with compact support $K \subset T(E)$. Since $\left\{V_{\mathbf{y}}\right\}$ covers $K$, the Corollary to Theorem 10.8 she'vs that $f=\Sigma \psi_{i} f$, where each $\psi_{i}$ is continuous, and each $\psi_{i}$ has its support in some $\mathbf{V}_{\mathbf{y}}$. Thus (31) holds for each $\psi_{i} f$, and hence also for their sum $f$.

## DIFFERENTIAL FORMS

We shall now develop some of the machinery that is needed for the $n$-dimensional version of the fundamental theorem of calculus which is usually called Stokes' theorem. The original form of Stokes' theorem arose in applications of vector analysis to electromagnetism and was stated in terms of the curl of a vector field. Green's theorem and the divergence theorem are other special cases. These topics are briefly discussed at the end of the chapter.

It is a curious feature of Stokes' theorem that the only thing that is difficult about it is the elaborate structure of definitions that are needed for its statement. These definitions concern differential forms, their derivatives, boundaries, and orientation. Once these concepts are understood, the statement of the theorem is very brief and succinct, and its proof presents little difficulty.

Up to now we have considered derivatives of functions of several variables only for functions defined in open sets. This was done to avoid difficulties that can occur at boundary points. It will now be convenient, however, to discuss differentiable functions on compact sets. We therefore adopt the following convention:

To say that $\mathbf{f}$ is a $\mathscr{C}^{\prime}$-mapping (or a $\mathscr{C}^{\prime \prime}$-mapping) of a compact set $D \subset R^{k}$ into $R^{n}$ means that there is a $\mathscr{C}^{\prime}$-mapping (or a $\mathscr{C}^{\prime \prime}$-mapping) $\mathbf{g}$ of an open set $W \subset R^{k}$ into $R^{n}$ such that $D \subset W$ and such that $\mathbf{g}(\mathbf{x})=\mathbf{f}(\mathbf{x})$ for all $\mathbf{x} \in D$.

10.10 Definition Suppose $E$ is an open set in $R^{n}$. A $k$-surface in $E$ is a $\mathscr{C}^{\prime}$ mapping $\Phi$ from a compact set $D \subset R^{k}$ into $E$.

$D$ is called the parameter domain of $\Phi$. Points of $D$ will be denoted by $\mathbf{u}=\left(u_{1}, \ldots, u_{k}\right)$.

We shall confine ourselves to the simple situation in which $D$ is either a $k$-cell or the $k$-simplex $Q^{k}$ described in Example 10.4. The reason for this is that we shall have to integrate over $D$, and we have not yet discussed integration over more complicated subsets of $R^{k}$. It will be seen that this restriction on $D$ (which will be tacitly made from now on) entails no significant loss of generality in the resulting theory of differential forms.

We stress that $k$-surfaces in $E$ are defined to be mappings into $E$, not subsets of $E$. This agrees with our earlier definition of curves (Definition 6.26). In fact, 1-surfaces are precisely the same as continuously differentiable curves.

10.11 Definition Suppose $E$ is an open set in $R^{n}$. A differential form of order $k \geq 1$ in $E$ (briefly, a $k$-form in $E$ ) is a function $\omega$, symbolically represented by the sum

$$
\omega=\sum a_{i_{1}} \cdots i_{k}(\mathbf{x}) d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}}
$$

(the indices $i_{1}, \ldots, i_{k}$ range independently from 1 to $n$ ), which assigns to each $k$-surface $\Phi$ in $E$ a number $\omega(\Phi)=\int_{\Phi} \omega$, according to the rule

$$
\int_{\Phi} \omega=\int_{D} \sum a_{i_{1}} \cdots_{i_{k}}(\Phi(\mathbf{u})) \frac{\partial\left(x_{i_{1}}, \ldots, x_{i_{k}}\right)}{\partial\left(u_{1}, \ldots, u_{k}\right)} d \mathbf{u}
$$

where $D$ is the parameter domain of $\Phi$.

The functions $a_{i_{1}} \cdots_{i_{k}}$ are assumed to be real and continuous in $E$. If $\phi_{1}, \ldots, \phi_{n}$ are the components of $\Phi$, the Jacobian in (35) is the one determined by the mapping

$$
\left(u_{1}, \ldots, u_{k}\right) \rightarrow\left(\phi_{i_{1}}(\mathbf{u}), \ldots, \phi_{i_{k}}(\mathrm{u})\right)
$$

Note that the right side of (35) is an integral over $D$, as defined in Definition 10.1 (or Example 10.4) and that (35) is the definition of the symbol $\int_{\Phi} \omega$.

A $k$-form $\omega$ is said to be of class $\mathscr{C}^{\prime}$ or $\mathscr{C}^{\prime \prime}$ if the functions $a_{i_{1}} \cdots i_{k}$ in (34) are all of class $\mathscr{C}^{\prime}$ or $\mathscr{C}^{\prime \prime}$.

A 0 -form in $E$ is defined to be a continuous function in $E$.

### 10.12 Examples

(a) Let $\gamma$ be a 1 -surface (a curve of class $\mathscr{C}^{\prime}$ ) in $R^{3}$, with parameter domain $[0,1]$.

Write $(x, y, z)$ in place of $\left(x_{1}, x_{2}, x_{3}\right)$, and put

$$
\omega=x d y+y d x
$$

Then

$$
\int_{\gamma} \omega=\int_{0}^{1}\left[\gamma_{1}(t) \gamma_{2}^{\prime}(t)+\gamma_{2}(t) \gamma_{1}^{\prime}(t)\right] d t=\gamma_{1}(1) \gamma_{2}(1)-\gamma_{1}(0) \gamma_{2}(0)
$$

Note that in this example $\int_{\gamma} \omega$ depends only on the initial point $\gamma(0)$ and on the end point $\gamma(1)$ of $\gamma$. In particular, $\int_{\gamma} \omega=0$ for every closed curve $\gamma$. (As we shall see later, this is true for every 1-form $\omega$ which is exact.)

Integrals of 1-forms are often called line integrals.

(b) Fix $a>0, b>0$, and define

$$
\gamma(t)=(a \cos t, b \sin t) \quad(0 \leq t \leq 2 \pi)
$$

so that $\gamma$ is a closed curve in $R^{2}$. (Its range is an ellipse.) Then

$$
\int_{\gamma} x d y=\int_{0}^{2 \pi} a b \cos ^{2} t d t=\pi a b
$$

whereas

$$
\int_{\gamma} y d x=-\int_{0}^{2 \pi} a b \sin ^{2} t d t=-\pi a b
$$

Note that $\int_{\gamma} x d y$ is the area of the region bounded by $\gamma$. This is a special case of Green's theorem.

(c) Let $D$ be the 3-cell defined by

$$
0 \leq r \leq 1, \quad 0 \leq \theta \leq \pi, \quad 0 \leq \varphi \leq 2 \pi
$$

Define $\Phi(r, \theta, \varphi)=(x, y, z)$, where

$$
\begin{aligned}
& x=r \sin \theta \cos \varphi \\
& y=r \sin \theta \sin \varphi \\
& z=r \cos \theta
\end{aligned}
$$

Then

$$
J_{\Phi}(r, \theta, \varphi)=\frac{\partial(x, y, z)}{\partial(r, \theta, \varphi)}=r^{2} \sin \theta
$$

Hence

$$
\int_{\Phi} d x \wedge d y \wedge d z=\int_{D} J_{\Phi}=\frac{4 \pi}{3}
$$

Note that $\Phi$ maps $D$ onto the closed unit ball of $R^{3}$, that the mapping is 1-1 in the interior of $D$ (but certain boundary points are identified by $\Phi)$, and that the integral (36) is equal to the volume of $\Phi(D)$.

10.13 Elementary properties Let $\omega, \omega_{1}, \omega_{2}$ be $k$-forms in $E$. We write $\omega_{1}=\omega_{2}$ if and only if $\omega_{1}(\Phi)=\omega_{2}(\Phi)$ for every $k$-surface $\Phi$ in $E$. In particular, $\omega=0$ means that $\omega(\Phi)=0$ for every $k$-surface $\Phi$ in $E$. If $c$ is a real number, then $c \omega$ is the $k$-form defined by

$$
\int_{\Phi} c \omega=c \int_{\Phi} \omega
$$

and $\omega=\omega_{1}+\omega_{2}$ means that

$$
\int_{\Phi} \omega=\int_{\Phi} \omega_{1}+\int_{\Phi} \omega_{2}
$$

for every $k$-surface $\Phi$ in $E$. As a special case of (37), note that $-\omega$ is defined so that

$$
\int_{\Phi}(-\omega)=-\int_{\Phi} d \omega .
$$

Consider a $k$-form

$$
\omega=a(\mathbf{x}) d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}}
$$

and let $\bar{\omega}$ be the $k$-form obtained by interchanging some pair of subscripts in (40). If (35) and (39) are combined with the fact that a determinant changes sign if two of its rows are interchanged, we see that

$$
\bar{\omega}=-\omega .
$$

As a special case of this, note that the anticommutative relation

$$
d x_{i} \wedge d x_{j}=-d x_{j} \wedge d x_{i}
$$

holds for all $i$ and $j$. In particular,

$$
d x_{i} \wedge d x_{i}=0 \quad(i=1, \ldots, n)
$$

More generally, let us return to (40), and assume that $i_{r}=i_{s}$ for some $r \neq s$. If these two subscripts are interchanged, then $\bar{\omega}=\omega$, hence $\omega=0$, by (41).

In other words, if $\omega$ is given by (40), then $\omega=0$ unless the subscripts $i_{1}, \ldots, i_{k}$ are all distinct.

If $\omega$ is as in (34), the summands with repeated subscripts can therefore be omitted without changing $\omega$.

It follows that 0 is the only $k$-form in any open subset of $R^{n}$, if $k>n$.

The anticommutativity expressed by (42) is the reason for the inordinate amount of attention that has to be paid to minus signs when studying differential forms.

10.14 Basic $k$-forms If $i_{1}, \ldots, i_{k}$ are integers such that $1 \leq i_{1}<i_{2}<\cdots$ $<i_{k} \leq n$, and if $I$ is the ordered $k$-tuple $\left\{i_{1}, \ldots, i_{k}\right\}$, then we call $I$ an increasing $k$-index, and we use the brief notation

$$
d x_{I}=d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}}
$$

These forms $d x_{I}$ are the so-called basic $k$-forms in $R^{n}$.

It is not hard to verify that there are precisely $n ! / k !(n-k)$ ! basic $k$-forms in $R^{n}$; we shall make no use of this, however.

Much more important is the fact that every $k$-form can be represented in terms of basic $k$-forms. To see this, note that every $k$-tuple $\left\{j_{1}, \ldots, j_{k}\right\}$ of distinct integers can be converted to an increasing $k$-index $J$ by a finite number of interchanges of pairs; each of these amounts to a multiplication by -1 , as we saw in Sec. 10.13; hence

$$
d x_{j_{1}} \wedge \cdots \wedge d x_{j_{k}}=\varepsilon\left(j_{1}, \ldots, j_{k}\right) d x_{J}
$$

where $\varepsilon\left(j_{1}, \ldots, j_{k}\right)$ is 1 or -1 , depending on the number of interchanges that are needed. In fact, it is easy to see that

$$
\varepsilon\left(j_{1}, \ldots, j_{k}\right)=s\left(j_{1}, \ldots, j_{k}\right)
$$

where $s$ is as in Definition 9.33.

For example,

$$
d x_{1} \wedge d x_{5} \wedge d x_{3} \wedge d x_{2}=-d x_{1} \wedge d x_{2} \wedge d x_{3} \wedge d x_{5}
$$

and

$$
d x_{4} \wedge d x_{2} \wedge d x_{3}=d x_{2} \wedge d x_{3} \wedge d x_{4} \text {. }
$$

If every $k$-tuple in (34) is converted to an increasing $k$-index, then we obtain the so-called standard presentation of $\omega$ :

$$
\omega=\sum_{I} b_{I}(\mathbf{x}) d x_{I}
$$

The summation in (47) extends over all increasing $k$-indices $I$. [Of course, every increasing $k$-index arises from many (from $k$ !, to be precise) $k$-tuples. Each $b_{I}$ in (47) may thus be a sum of several of the coefficients that occur in (34).]

For example,

$$
x_{1} d x_{2} \wedge d x_{1}-x_{2} d x_{3} \wedge d x_{2}+x_{3} d x_{2} \wedge d x_{3}+d x_{1} \wedge d x_{2}
$$

is a 2 -form in $R^{3}$ whose standard presentation is

$$
\left(1-x_{1}\right) d x_{1} \wedge d x_{2}+\left(x_{2}+x_{3}\right) d x_{2} \wedge d x_{3} .
$$

The following uniqueness theorem is one of the main reasons for the introduction of the standard presentation of a $k$-form.

### 10.15 Theorem Suppose

$$
\omega=\sum_{I} b_{I}(\mathbf{x}) d x_{I}
$$

is the standard presentation of a $k$-form $\omega$ in an open set $E \subset R^{n}$. If $\omega=0$ in $E$, then $b_{I}(\mathbf{x})=0$ for every increasing $k$-index $I$ and for every $\mathbf{x} \in E$.

Note that the analogous statement would be false for sums such as (34), since, for example,

$$
d x_{1} \wedge d x_{2}+d x_{2} \wedge d x_{1}=0
$$

Proof Assume, to reach a contradiction, that $b_{J}(\mathbf{v})>0$ for some $\mathbf{v} \in E$ and for some increasing $k$-index $J=\left\{j_{1}, \ldots, j_{k}\right\}$. Since $b_{J}$ is continuous, there exists $h>0$ such that $b_{J}(\mathbf{x})>0$ for all $\mathbf{x} \in R^{n}$ whose coordinates satisfy $\left|x_{i}-v_{i}\right| \leq h$. Let $D$ be the $k$-cell in $R^{k}$ such that $\mathbf{u} \in D$ if and only if $\left|u_{r}\right| \leq h$ for $r=1, \ldots, k$. Define

$$
\Phi(\mathbf{u})=\mathbf{v}+\sum_{r=1}^{k} u_{r} \mathbf{e}_{j_{r}} \quad(\mathbf{u} \in D)
$$

Then $\Phi$ is a $k$-surface in $E$, with parameter domain $D$, and $b_{J}(\Phi(\mathbf{u}))>0$ for every $\mathbf{u} \in D$.

We claim that

$$
\int_{\Phi} \omega=\int_{D} b_{J}(\Phi(\mathbf{u})) d \mathbf{u}
$$

Since the right side of $(50)$ is positive, it follows that $\omega(\Phi) \neq 0$. Hence (50) gives our contradiction.

To prove (50), apply (35) to the presentation (48). More specifically, compute the Jacobians that occur in (35). By (49),

$$
\frac{\partial\left(x_{j_{1}}, \ldots, x_{j_{k}}\right)}{\partial\left(u_{1}, \ldots, u_{k}\right)}=1
$$

For any other increasing $k$-index $I \neq J$, the Jacobian is 0 , since it is the determinant of a matrix with at least one row of zeros.

10.16 Products of basic $\boldsymbol{k}$-forms Suppose

$$
I=\left\{i_{1}, \ldots, i_{p}\right\}, \quad J=\left\{j_{1}, \ldots, j_{q}\right\}
$$

where $1 \leq i_{1}<\cdots<i_{p} \leq n$ and $1 \leq j_{1}<\cdots<j_{q} \leq n$. The product of the corresponding basic forms $d x_{I}$ and $d x_{J}$ in $R^{n}$ is a $(p+q)$-form in $R^{n}$, denoted by the symbol $d x_{I} \wedge d x_{J}$, and defined by

$$
d x_{I} \wedge d x_{J}=d x_{i_{1}} \wedge \cdots \wedge d x_{i_{p}} \wedge d x_{j_{1}} \wedge \cdots \wedge d x_{j_{q}}
$$

If $I$ and $J$ have an element in common, then the discussion in Sec. 10.13 shows that $d x_{I} \wedge d x_{J}=0$.

If $I$ and $J$ have no element in common, let us write $[I, J]$ for the increasing $(p+q)$-index which is obtained by arranging the members of $I \cup J$ in increasing order. Then $d x_{[I, J]}$ is a basic $(p+q)$-form. We claim that

$$
d x_{I} \wedge d x_{J}=(-1)^{\alpha} d x_{[I, J]}
$$

where $\alpha$ is the number of differences $j_{t}-i_{s}$ that are negative. (The number of positive differences is thus $p q-\alpha$.)

To prove (53), perform the following operations on the numbers

$$
i_{1}, \ldots, i_{p} ; j_{1}, \ldots, j_{q}
$$

Move $i_{p}$ to the right, step by step, until its right neighbor is larger than $i_{p}$. The number of steps is the number of subscripts $t$ such that $i_{p}<j_{t}$. (Note that 0 steps are a distinct possibility.) Then do the same for $i_{p-1}, \ldots, i_{1}$. The total number of steps taken is $\alpha$. The final arrangement reached is $[I, J]$. Each step, when applied to the right side of (52), multiplies $d x_{I} \wedge d x_{J}$ by -1 . Hence (53) holds.

Note that the right side of (53) is the standard presentation of $d x_{I} \wedge d x_{J}$. Next, let $K=\left(k_{1}, \ldots, k_{r}\right)$ be an increasing $r$-index in $\{1, \ldots, n\}$. We shall use (53) to prove that

$$
\left(d x_{I} \wedge d x_{J}\right) \wedge d x_{K}=d x_{I} \wedge\left(d x_{J} \wedge d x_{K}\right)
$$

If any two of the sets $I, J, K$ have an element in common, then each side of (55) is 0 , hence they are equal.

So let us assume that $I, J, K$ are pairwise disjoint. Let $[I, J, K]$ denote the increasing $(p+q+r)$-index obtained from their union. Associate $\beta$ with the ordered pair $(J, K)$ and $\gamma$ with the ordered pair $(I, K)$ in the way that $\alpha$ was associated with $(I, J)$ in (53). The left side of $(55)$ is then

$$
(-1)^{\alpha} d x_{[I, J]} \wedge d x_{K}=(-1)^{\alpha}(-1)^{\beta+\gamma} d x_{[I, J, K]}
$$

by two applications of (53), and the right side of (55) is

$$
(-1)^{\beta} d x_{I} \wedge d x_{[J, K]}=(-1)^{\beta}(-1)^{\alpha+\gamma} d x_{[I, J, K]}
$$

Hence (55) is correct.

10.17 Multiplication Suppose $\omega$ and $\lambda$ are $p$ - and $q$-forms, respectively, in some open set $E \subset R^{n}$, with standard presentations

$$
\omega=\sum_{I} b_{I}(\mathbf{x}) d x_{I}, \quad \lambda=\sum_{J} c_{J}(\mathbf{x}) d x_{J}
$$

where $I$ and $J$ range over all increasing $p$-indices and over all increasing $q$-indices taken from the set $\{1, \ldots, n\}$.

Their product, denoted by the symbol $\omega \wedge \lambda$, is defined to be

$$
\omega \wedge \lambda=\sum_{I, J} b_{I}(\mathbf{x}) c_{J}(\mathbf{x}) d x_{I} \wedge d x_{J}
$$

In this sum, $I$ and $J$ range independently over their possible values, and $d x_{I} \wedge d x_{J}$ is as in Sec. 10.16. Thus $\omega \wedge \lambda$ is a $(p+q)$-form in $E$. tive laws

It is quite easy to see (we leave the details as an exercise) that the distribu-

$$
\left(\omega_{1}+\omega_{2}\right) \wedge \lambda=\left(\omega_{1} \wedge \lambda\right)+\left(\omega_{2} \wedge \lambda\right)
$$

and

$$
\omega \wedge\left(\lambda_{1}+\lambda_{2}\right)=\left(\omega \wedge \lambda_{1}\right)+\left(\omega \wedge \lambda_{2}\right)
$$

hold, with respect to the addition defined in Sec. 10.13. If these distributive laws are combined with (55), we obtain the associative law

$$
(\omega \wedge \lambda) \wedge \sigma=\omega \wedge(\lambda \wedge \sigma)
$$

for arbitrary forms $\omega, \lambda, \sigma$ in $E$.

In this discussion it was tacitly assumed that $p \geq 1$ and $q \geq 1$. The product of a 0 -form $f$ with the $p$-form $\omega$ given by (56) is simply defined to be the $p$-form

$$
f \omega=\omega f=\sum_{I} f(\mathbf{x}) b_{I}(\mathbf{x}) d x_{I}
$$

It is customary to write $f \omega$, rather than $f \wedge \omega$, when $f$ is a 0 -form.

10.18 Differentiation We shall now define a differentiation operator $d$ which associates a $(k+1)$-form $d \omega$ to each $k$-form $\omega$ of class $\mathscr{C}^{\prime}$ in some open set $E \subset R^{n}$.

A 0 -form of class $\mathscr{C}^{\prime}$ in $E$ is just a real function $f \in \mathscr{C}^{\prime}(E)$, and we define

$$
d f=\sum_{i=1}^{n}\left(D_{i} f\right)(\mathbf{x}) d x_{i}
$$

If $\omega=\boldsymbol{\Sigma} b_{I}(\mathbf{x}) d x_{I}$ is the standard presentation of a $k$-form $\omega$, and $b_{I} \in \mathscr{C}^{\prime}(E)$ for each increasing $k$-index $I$, then we define

$$
d \omega=\sum_{I}\left(d b_{I}\right) \wedge d x_{I}
$$

10.19 Example Suppose $E$ is open in $R^{n}, f \in \mathscr{C}^{\prime}(E)$, and $\gamma$ is a continuously differentiable curve in $E$, with domain $[0,1]$. By (59) and (35),

$$
\int_{\gamma} d f=\int_{0}^{1} \sum_{i=1}^{n}\left(D_{i} f\right)(\gamma(t)) \gamma_{i}^{\prime}(t) d t
$$

By the chain rule, the last integrand is $(f \circ \gamma)^{\prime}(t)$. Hence

$$
\int_{\gamma} d f=f(\gamma(1))-f(\gamma(0))
$$

and we see that $\int_{\gamma} d f$ is the same for all $\gamma$ with the same initial point and the same end point, as in $(a)$ of Example 10.12.

Comparison with Example 10.12(b) shows therefore that the 1-form $x d y$ is not the derivative of any 0 -form $f$. This could also be deduced from part $(b)$ of the following theorem, since

$$
d(x d y)=d x \wedge d y \neq 0
$$

### 10.20 Theorem

(a) If $\omega$ and $\lambda$ are $k$ - and $m$-forms, respectively, of class $\mathscr{C}^{\prime}$ in $E$, then

$$
d(\omega \wedge \lambda)=(d \omega) \wedge \lambda+(-1)^{k} \omega \wedge d \lambda
$$

(b) If $\omega$ is of class $\mathscr{C}^{\prime \prime}$ in $E$, then $d^{2} \omega=0$.

Here $d^{2} \omega$ means, of course, $d(d \omega)$.

Proof Because of (57) and (60), (a) follows if (63) is proved for the special case

$$
\omega=f d x_{I}, \quad \lambda=g d x_{J}
$$

where $f, g \in \mathscr{C}^{\prime}(E), d x_{I}$ is a basic $k$-form, and $d x_{J}$ is a basic $m$-form. [If $k$ or $m$ or both are 0 , simply omit $d x_{I}$ or $d x_{J}$ in (64); the proof that follows is unaffected by this.] Then

$$
\omega \wedge \lambda=f g d x_{I} \wedge d x_{J} .
$$

Let us assume that $I$ and $J$ have no element in common. [In the other case each of the three terms in (63) is 0.] Then, using (53),

$$
d(\omega \wedge \lambda)=d\left(f g d x_{I} \wedge d x_{J}\right)=(-1)^{\alpha} d\left(f g d x_{[I, J]}\right)
$$

By (59), $d(f g)=f d g+g d f$. Hence (60) gives

$$
\begin{aligned}
d(\omega \wedge \lambda) & =(-1)^{\alpha}(f d g+g d f) \wedge d x_{[I, J]} \\
& =(g d f+f d g) \wedge d x_{I} \wedge d x_{J}
\end{aligned}
$$

Since $d g$ is a 1 -form and $d x_{I}$ is a $k$-form, we have

$$
d g \wedge d x_{I}=(-1)^{k} d x_{I} \wedge d g
$$

by (42). Hence

$$
\begin{aligned}
d(\omega \wedge \lambda) & =\left(d f \wedge d x_{I}\right) \wedge\left(g d x_{J}\right)+(-1)^{k}\left(f d x_{I}\right) \wedge\left(d g \wedge d x_{J}\right) \\
& =(d \omega) \wedge \lambda+(-1)^{k} \omega \wedge d \lambda
\end{aligned}
$$

which proves $(a)$.

Note that the associative law (58) was used freely.

Let us prove $(b)$ first for a 0 -form $f \in \mathscr{C}^{\prime \prime}$ :

$$
\begin{aligned}
d^{2} f & =d\left(\sum_{j=1}^{n}\left(D_{j} f\right)(\mathbf{x}) d x_{j}\right) \\
& =\sum_{j=1}^{n} d\left(D_{j} f\right) \wedge d x_{j} \\
& =\sum_{i, j=1}^{n}\left(D_{i j} f\right)(\mathbf{x}) d x_{i} \wedge d x_{j} .
\end{aligned}
$$

Since $D_{i j} f=D_{j i} f$ (Theorem 9.41) and $d x_{i} \wedge d x_{j}=-d x_{j} \wedge d x_{i}$, we see that $d^{2} f=0$.

If $\omega=f d x_{I}$, as in (64), then $d \omega=(d f) \wedge d x_{I}$. By (60), $d\left(d x_{I}\right)=0$. Hence (63) shows that

$$
d^{2} \omega=\left(d^{2} f\right) \wedge d x_{I}=0
$$

10.21 Change of variables Suppose $E$ is an open set in $R^{n}, T$ is a $\mathscr{C}^{\prime}$-mapping of $E$ into an open set $V \subset R^{m}$, and $\omega$ is a $k$-form in $V$, whose standard presentation is

$$
\omega=\sum_{I} b_{I}(\mathbf{y}) d y_{I}
$$

(We use $\mathbf{y}$ for points of $V, \mathbf{x}$ for points of $E$.)

Let $t_{1}, \ldots, t_{m}$ be the components of $T$ : If

$$
\mathbf{y}=\left(y_{1}, \ldots, y_{m}\right)=T(\mathbf{x})
$$

then $y_{i}=t_{i}(\mathbf{x})$. As in (59),

$$
d t_{i}=\sum_{j=1}^{n}\left(D_{j} t_{i}\right)(\mathbf{x}) d x_{j} \quad(1 \leq i \leq m)
$$

Thus each $d t_{i}$ is a 1 -form in $E$.

The mapping $T$ transforms $\omega$ into a $k$-form $\omega_{T}$ in $E$, whose definition is

$$
\omega_{T}=\sum_{I} b_{\jmath}(T(\mathbf{x})) d t_{i_{l}} \wedge \cdots \wedge d t_{i_{k}}
$$

In each summand of (67), $I=\left\{i_{1}, \ldots, i_{k}\right\}$ is an increasing $k$-index.

Our next theorem shows that addition, multiplication, and differentiation of forms are defined in such a way that they commute with changes of variables.

10.22 Theorem With $E$ and $T$ as in Sec. 10.21, let $\omega$ and $\lambda$ be $k$ - and $m$-forms in $V$, respectively. Then

(a) $(\omega+\lambda)_{T}=\omega_{T}+\lambda_{T}$ if $k=m$

(b) $(\omega \wedge \lambda)_{T}=\omega_{T} \wedge \lambda_{T}$

(c) $d\left(\omega_{T}\right)=(d \omega)_{T}$ if $\omega$ is of class $\mathscr{C}^{\prime}$ and $T$ is of class $\mathscr{C}^{\prime \prime}$.

Proof Part $(a)$ follows immediately from the definitions. Part $(b)$ is almost as obvious, once we realize that

$$
\left(d y_{i_{1}} \wedge \cdots \wedge d y_{i_{r}}\right)_{T}=d t_{i_{1}} \wedge \cdots \wedge d t_{i_{r}}
$$

regardless of whether $\left\{i_{1}, \ldots, i_{r}\right\}$ is increasing or not; (68) holds because the same number of minus signs are needed on each side of (68) to produce increasing rearrangements.

We turn to the proof of $(c)$. If $f$ is a 0 -form of class $\mathscr{C}^{\prime}$ in $V$, then

$$
f_{T}(\mathbf{x})=f(T(\mathbf{x})), \quad d f=\sum_{i}\left(D_{i} f\right)(\mathbf{y}) d y_{i}
$$

By the chain rule, it follows that

$$
\begin{aligned}
d\left(f_{T}\right) & =\sum_{j}\left(D_{j} f_{T}\right)(\mathbf{x}) d x_{j} \\
& =\sum_{j} \sum_{i}\left(D_{i} f\right)(T(\mathbf{x}))\left(D_{j} t_{i}\right)(\mathbf{x}) d x_{j} \\
& =\sum_{i}\left(D_{i} f\right)(T(\mathbf{x})) d t_{i} \\
& =(d f)_{T} .
\end{aligned}
$$

If $d y_{I}=d y_{i_{1}} \wedge \cdots \wedge d y_{i_{k}}$, then $\left(d y_{I}\right)_{T}=d t_{i_{1}} \wedge \cdots \wedge d t_{i_{k}}$, and Theorem 10.20 shows that

$$
d\left(\left(d y_{1}\right)_{T}\right)=0
$$

(This is where the assumption $T \in \mathscr{C}^{\prime \prime}$ is used.)

Assume now that $\omega=f d y_{I}$. Then

$$
\omega_{T}=f_{T}(\mathbf{x})\left(d y_{I}\right)_{T}
$$

and the preceding calculations lead to

$$
\begin{aligned}
d\left(\omega_{T}\right) & =d\left(f_{T}\right) \wedge\left(d y_{I}\right)_{T}=(d f)_{T} \wedge\left(d y_{I}\right)_{T} \\
& =\left((d f) \wedge d y_{T}\right)_{T}=(d \omega)_{T}
\end{aligned}
$$

The first equality holds by (63) and (70), the second by (69), the third by part $(b)$, and the last by the definition of $d \omega$.

The general case of $(c)$ follows from the special case just proved, if we apply $(a)$. This completes the proof.

Our next objective is Theorem 10.25. This will follow directly from two other important transformation properties of differential forms, which we state first.

10.23 Theorem Suppose $T$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m}, S$ is a $\mathscr{C}^{\prime}$-mapping of $V$ into an open set $W \subset R^{p}$, and $\omega$ is a $k$-form in $W$, so that $\omega_{S}$ is a $k$-form in $V$ and both $\left(\omega_{S}\right)_{T}$ and $\omega_{S T}$ are $k$-forms in $E$, where $S T$ is defined by $(S T)(\mathbf{x})=S(T(\mathbf{x}))$. Then

$$
\left(\omega_{S}\right)_{T}=\omega_{S T}
$$

Proof If $\omega$ and $\lambda$ are forms in $W$, Theorem 10.22 shows that

$$
\left((\omega \wedge \lambda)_{S}\right)_{T}=\left(\omega_{S} \wedge \lambda_{S}\right)_{T}=\left(\omega_{S}\right)_{T} \wedge\left(\lambda_{S}\right)_{T}
$$

and

$$
(\omega \wedge \lambda)_{S T}=\omega_{S T} \wedge \lambda_{S T}
$$

Thus if (71) holds for $\omega$ and for $\lambda$, it follows that (71) also holds for $\omega \wedge \lambda$. Since every form can be built up from 0 -forms and 1 -forms by addition and multiplication, and since (71) is trivial for 0 -forms, it is enough to prove (71) in the case $\omega=d z_{q}, q=1, \ldots, p$. (We denote the points of $E, V, W$ by $\mathbf{x}, \mathbf{y}, \mathbf{z}$, respectively.)

Let $t_{1}, \ldots, t_{m}$ be the components of $T$, let $s_{1}, \ldots, s_{p}$ be the components of $S$, and let $r_{1}, \ldots, r_{p}$ be the components of $S T$. If $\omega=d z_{q}$, then

$$
\omega_{s}=d s_{q}=\sum_{j}\left(D_{j} s_{q}\right)(\mathbf{y}) d y_{j}
$$

so that the chain rule implies

$$
\begin{aligned}
\left(\omega_{S}\right)_{T} & =\sum_{j}\left(D_{j} s_{q}\right)(T(\mathbf{x})) d t_{j} \\
& =\sum_{j}\left(D_{j} s_{q}\right)(T(\mathbf{x})) \sum_{i}\left(D_{i} t_{j}\right)(\mathbf{x}) d x_{i} \\
& =\sum_{i}\left(D_{i} r_{q}\right)(\mathbf{x}) d x_{i}=d r_{q}=\omega_{S T} .
\end{aligned}
$$

10.24 Theorem Suppose $\omega$ is a $k$-form in an open set $E \subset R^{n}, \Phi$ is a $k$-surface in $E$, with parameter domain $D \subset R^{k}$, and $\Delta$ is the $k$-surface in $R^{k}$, with parameter domain $D$, defined by $\Delta(\mathbf{u})=\mathbf{u}(\mathbf{u} \in D)$. Then

$$
\int_{\Phi} \omega=\int_{\Delta} \omega_{\Phi} .
$$

Proof We need only consider the case

$$
\omega=a(\mathbf{x}) d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}} .
$$

If $\phi_{1}, \ldots, \phi_{n}$ are the components of $\Phi$, then

$$
\omega_{\Phi}=a(\Phi(\mathbf{u})) d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}} .
$$

The theorem will follow if we can show that

$$
d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}}=J(\mathbf{u}) d u_{1} \wedge \cdots \wedge d u_{k}
$$

where

$$
J(\mathbf{u})=\frac{\partial\left(x_{i_{1}}, \ldots, x_{i_{k}}\right)}{\partial\left(u_{1}, \ldots, u_{k}\right)}
$$

since (72) implies

$$
\begin{aligned}
\int_{\Phi} \omega & =\int_{D} a(\Phi(\mathbf{u})) J(\mathbf{u}) d \mathbf{u} \\
& =\int_{\Delta} a(\Phi(\mathbf{u})) J(\mathbf{u}) d u_{1} \wedge \cdots \wedge d u_{k}=\int_{\Delta} \omega_{\Phi} .
\end{aligned}
$$

Let $[A]$ be the $k$ by $k$ matrix with entries

$$
\alpha(p, q)=\left(D_{q} \phi_{i_{p}}\right)(\mathbf{u}) \quad(p, q=1, \ldots, k)
$$

Then

$$
d \phi_{i_{p}}=\sum_{q} \alpha(p, q) d u_{q}
$$

so that

$$
d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}}=\sum \alpha\left(1, q_{1}\right) \cdots \alpha\left(k, q_{k}\right) d u_{q_{1}} \wedge \cdots \wedge d u_{q_{k}}
$$

In this last sum, $q_{1}, \ldots, q_{k}$ range independently over $1, \ldots, k$. The anticommutative relation (42) implies that

$$
d u_{q_{1}} \wedge \cdots \wedge d u_{q_{k}}=s\left(q_{1}, \ldots, q_{k}\right) d u_{1} \wedge \cdots \wedge d u_{k}
$$

where $s$ is as in Definition 9.33; applying this definition, we see that

$$
d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}}=\operatorname{det}[A] d u_{1} \wedge \cdots \wedge d u_{k}
$$

and since $J(\mathbf{u})=\operatorname{det}[A],(72)$ is proved.

The final result of this section combines the two preceding theorems.

10.25 Theorem Suppose $T$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m}, \Phi$ is a $k$-surface in $E$, and $\omega$ is a $k$-form in $V$. Then

$$
\int_{T \Phi} \omega=\int_{\Phi} \omega_{T}
$$

Proof Let $D$ be the parameter domain of $\Phi$ (hence also of $T \Phi$ ) and define $\Delta$ as in Theorem 10.24.

Then

$$
\int_{T \Phi} \omega=\int_{\Delta} \omega_{T \Phi}=\int_{\Delta}\left(\omega_{T}\right)_{\Phi}=\int_{\Phi} \omega_{T}
$$

The first of these equalities is Theorem 10.24, applied to $T \Phi$ in place of $\Phi$. The second follows from Theorem 10.23. The third is Theorem 10.24, with $\omega_{T}$ in place of $\omega$.

## SIMPLEXES AND CHAINS

10.26 Affine simplexes A mapping $\mathrm{f}$ that carries a vector space $X$ into a vector space $Y$ is said to be affine if $\mathbf{f}-\mathbf{f}(\mathbf{0})$ is linear. In other words, the requirement is that

$$
\mathbf{f}(\mathbf{x})=\mathbf{f}(\mathbf{0})+A \mathbf{x}
$$

for some $A \in L(X, Y)$.

An affine mapping of $R^{k}$ into $R^{n}$ is thus determined if we know $\mathbf{f}(\mathbf{0})$ and $\mathbf{f}\left(\mathbf{e}_{i}\right)$ for $1 \leq i \leq k$; as usual, $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right\}$ is the standard basis of $R^{k}$.

We define the standard simplex $Q^{k}$ to be the set of all $\mathbf{u} \in R^{k}$ of the form

$$
\mathbf{u}=\sum_{i=1}^{k} \alpha_{i} \mathbf{e}_{i}
$$

such that $\alpha_{i} \geq 0$ for $i=1, \ldots, k$ and $\Sigma \alpha_{i} \leq 1$.

Assume now that $\mathbf{p}_{0}, \mathbf{p}_{1}, \ldots, \mathbf{p}_{k}$ are points of $R^{n}$. The oriented affine $k$-simplex

$$
\sigma=\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \ldots, \mathbf{p}_{k}\right]
$$

is defined to be the $k$-surface in $R^{n}$ with parameter domain $Q^{k}$ which is given by the affine mapping

$$
\sigma\left(\alpha_{1} \mathbf{e}_{1}+\cdots+\alpha_{k} \mathbf{e}_{k}\right)=\mathbf{p}_{0}+\sum_{i=1}^{k} \alpha_{i}\left(\mathbf{p}_{i}-\mathbf{p}_{0}\right)
$$

Note that $\sigma$ is characterized by

$$
\sigma(\mathbf{0})=\mathbf{p}_{0}, \quad \sigma\left(\mathbf{e}_{i}\right)=\mathbf{p}_{i} \quad(\text { for } 1 \leq i \leq k)
$$

and that

$$
\sigma(\mathbf{u})=\mathbf{p}_{0}+A \mathbf{u} \quad\left(\mathbf{u} \in Q^{k}\right)
$$

where $A \in L\left(R^{k}, R^{n}\right)$ and $A \mathbf{e}_{i}=\mathbf{p}_{i}-\mathbf{p}_{0}$ for $1 \leq i \leq k$.

We call $\sigma$ oriented to emphasize that the ordering of the vertices $\mathbf{p}_{0}, \ldots, \mathbf{p}_{k}$ is taken into account. If

$$
\bar{\sigma}=\left[p_{i_{0}}, p_{i_{1}}, \ldots, p_{i_{k}}\right]
$$

where $\left\{i_{0}, i_{1}, \ldots, i_{k}\right\}$ is a permutation of the ordered set $\{0,1, \ldots, k\}$, we adopt the notation

$$
\bar{\sigma}=s\left(i_{0}, i_{1}, \ldots, i_{k}\right) \sigma
$$

where $s$ is the function defined in Definition 9.33. Thus $\bar{\sigma}= \pm \sigma$, depending on whether $s=1$ or $s=-1$. Strictly speaking, having adopted (75) and (76) as the definition of $\sigma$, we should not write $\bar{\sigma}=\sigma$ unless $i_{0}=0, \ldots, i_{k}=k$, even if $s\left(i_{0}, \ldots, i_{k}\right)=1$; what we have here is an equivalence relation, not an equality. However, for our purposes the notation is justified by Theorem 10.27.

If $\bar{\sigma}=\varepsilon \sigma$ (using the above convention) and if $\varepsilon=1$, we say that $\bar{\sigma}$ and $\sigma$ have the same orientation; if $\varepsilon=-1, \bar{\sigma}$ and $\sigma$ are said to have opposite orientations. Note that we have not defined what we mean by the "orientation of a simplex." What we have defined is a relation between pairs of simplexes having the same set of vertices, the relation being that of "having the same orientation."

There is, however, one situation where the orientation of a simplex can be defined in a natural way. This happens when $n=k$ and when the vectors $\mathbf{p}_{i}-\mathbf{p}_{0}(1 \leq i \leq k)$ are independent. In that case, the linear transformation $A$ that appears in (78) is invertible, and its determinant (which is the same as the Jacobian of $\sigma$ ) is not 0 . Then $\sigma$ is said to be positively (or negatively) oriented if $\operatorname{det} A$ is positive (or negative). In particular, the simplex $\left[\mathbf{0}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right]$ in $R^{k}$, given by the identity mapping, has positive orientation.

So far we have assumed that $k \geq 1$. An oriented 0-simplex is defined to be a point with a sign attached. We write $\sigma=+\mathbf{p}_{0}$ or $\sigma=-\mathbf{p}_{0}$. If $\sigma=\varepsilon \mathbf{p}_{0}$ $(\varepsilon= \pm 1)$ and if $f$ is a 0 -form (i.e., a real function), we define

$$
\int_{\sigma} f=\varepsilon f\left(p_{0}\right)
$$

10.27 Theorem If $\sigma$ is an oriented rectilinear $k$-simplex in an open set $E \subset R^{n}$ and if $\bar{\sigma}=\varepsilon \sigma$ then

$$
\int_{\bar{\sigma}} \omega=\varepsilon \int_{\sigma} \omega
$$

for every $k$-form $\omega$ in $E$.

Proof For $k=0$, (81) follows from the preceding definition. So we assume $k \geq 1$ and assume that $\sigma$ is given by (75).

Suppose $1 \leq j \leq k$, and suppose $\bar{\sigma}$ is obtained from $\sigma$ by interchanging $\mathbf{p}_{0}$ and $\mathbf{p}_{j}$. Then $\varepsilon=-1$, and

$$
\bar{\sigma}(\mathbf{u})=\mathbf{p}_{j}+B \mathbf{u} \quad\left(\mathbf{u} \in Q^{k}\right)
$$

where $B$ is the linear mapping of $R^{k}$ into $R^{n}$ defined by $B \mathbf{e}_{j}=\mathbf{p}_{0}-\mathbf{p}_{j}$, $B \mathbf{e}_{i}=\mathbf{p}_{i}-\mathbf{p}_{j}$ if $i \neq j$. If we write $A \mathbf{e}_{i}=\mathbf{x}_{i}(1 \leq i \leq k)$, where $A$ is given by (78), the column vectors of $B$ (that is, the vectors $B \mathbf{e}_{i}$ ) are

$$
\mathbf{x}_{1}-\mathbf{x}_{j}, \ldots, \mathbf{x}_{j-1}-\mathbf{x}_{j},-\mathbf{x}_{j}, \mathbf{x}_{j+1}-\mathbf{x}_{j}, \ldots, \mathbf{x}_{k}-\mathbf{x}_{j}
$$

If we subtract the $j$ th column from each of the others, none of the determinants in (35) are affected, and we obtain columns $\mathbf{x}_{1}, \ldots, \mathbf{x}_{j-1},-\mathbf{x}_{j}$, $\mathbf{x}_{j+1}, \ldots, \mathbf{x}_{k}$. These differ from those of $A$ only in the sign of the $j$ th column. Hence (81) holds for this case.

Suppose next that $0<i<j \leq k$ and that $\bar{\sigma}$ is obtained from $\sigma$ by interchanging $\mathbf{p}_{i}$ and $\mathbf{p}_{j}$. Then $\bar{\sigma}(\mathbf{u})=\mathbf{p}_{0}+C \mathbf{u}$, where $C$ has the same columns as $A$, except that the $i$ th and $j$ th columns have been interchanged. This again implies that (81) holds, since $\varepsilon=-1$.

The general case follows, since every permutation of $\{0,1, \ldots, k\}$ is a composition of the special cases we have just dealt with.

10.28 Affine chains An affine $k$-chain $\Gamma$ in an open set $E \subset R^{n}$ is a collection of finitely many oriented affine $k$-simplexes $\sigma_{1}, \ldots, \sigma_{r}$ in $E$. These need not be distinct; a simplex may thus occur in $\Gamma$ with a certain multiplicity.

If $\Gamma$ is as above, and if $\omega$ is a $k$-form in $E$, we define

$$
\int_{\Gamma} \omega=\sum_{i=1}^{r} \int_{\sigma_{i}} \omega .
$$

We may view a $k$-surface $\Phi$ in $E$ as a function whose domain is the collection of all $k$-forms in $E$ and which assigns the number $\int_{\Phi} \omega$ to $\omega$. Since realvalued functions can be added (as in Definition 4.3), this suggests the use of the notation

$$
\Gamma=\sigma_{1}+\cdots+\sigma_{r}
$$

or, more compactly,

$$
\Gamma=\sum_{i=1}^{r} \sigma_{i}
$$

to state the fact that (82) holds for every $k$-form $\omega$ in $E$.

To avoid misunderstanding, we point out explicitly that the notations introduced by (83) and (80) have to be handled with care. The point is that every oriented affine $k$-simplex $\sigma$ in $R^{n}$ is a function in two ways, with different domains and different ranges, and that therefore two entirely different operations
of addition are possible. Originally, $\sigma$ was defined as an $R^{n}$-valued function with domain $Q^{k}$; accordingly, $\sigma_{1}+\sigma_{2}$ could be interpreted to be the function $\sigma$ that assigns the vector $\sigma_{1}(\mathbf{u})+\sigma_{2}(\mathbf{u})$ to every $\mathbf{u} \in Q^{k}$; note that $\sigma$ is then again an oriented affine $k$-simplex in $R^{n}$ ! This is not what is meant by (83).

For example, if $\sigma_{2}=-\sigma_{1}$ as in (80) (that is to say, if $\sigma_{1}$ and $\sigma_{2}$ have the same set of vertices but are oppositely oriented) and if $\Gamma=\sigma_{1}+\sigma_{2}$, then $\int_{\Gamma} \omega=0$ for all $\omega$, and we may express this by writing $\Gamma=0$ or $\sigma_{1}+\sigma_{2}=0$. This does not mean that $\sigma_{1}(\mathbf{u})+\sigma_{2}(\mathbf{u})$ is the null vector of $R^{n}$.

10.29 Boundaries For $k \geq 1$, the boundary of the oriented affine $k$-simplex

$$
\sigma=\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \ldots, \mathbf{p}_{k}\right]
$$

is defined to be the affine $(k-1)$-chain

$$
\partial \sigma=\sum_{j=0}^{k}(-1)^{j}\left[\mathbf{p}_{0}, \ldots, \mathbf{p}_{j-1}, \mathbf{p}_{j+1}, \ldots, \mathbf{p}_{k}\right]
$$

For example, if $\sigma=\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \mathbf{p}_{2}\right]$, then

$$
\partial \sigma=\left[\mathbf{p}_{1}, \mathbf{p}_{2}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{2}\right]+\left[\mathbf{p}_{0}, \mathbf{p}_{1}\right]=\left[\mathbf{p}_{0}, \mathbf{p}_{1}\right]+\left[\mathbf{p}_{1}, \mathbf{p}_{2}\right]+\left[\mathbf{p}_{2}, \mathbf{p}_{0}\right]
$$

which coincides with the usual notion of the oriented boundary of a triangle.

For $1 \leq j \leq k$, observe that the simplex $\sigma_{j}=\left[\mathbf{p}_{0}, \ldots, \mathbf{p}_{j-1}, \mathbf{p}_{j+1}, \ldots, \mathbf{p}_{k}\right]$ which occurs in (85) has $Q^{k-1}$ as its parameter domain and that it is defined by

$$
\sigma_{j}(\mathbf{u})=\mathbf{p}_{0}+B \mathbf{u} \quad\left(\mathbf{u} \in Q^{k-1}\right)
$$

where $B$ is the linear mapping from $R^{k-1}$ to $R^{n}$ determined by

$$
\begin{aligned}
& B \mathbf{e}_{i}=\mathbf{p}_{i}-\mathbf{p}_{0} \quad(\text { if } \quad 1 \leq i \leq j-1) \\
& B \mathbf{e}_{i}=\mathbf{p}_{i+1}-\mathbf{p}_{0} \quad(\text { if } \quad j \leq i \leq k-1) .
\end{aligned}
$$

The simplex

$$
\sigma_{0}=\left[\mathbf{p}_{1}, \mathbf{p}_{2}, \ldots, \mathbf{p}_{k}\right]
$$

which also occurs in (85), is given by the mapping

$$
\sigma_{0}(\mathbf{u})=\mathbf{p}_{1}+B \mathbf{u}
$$

where $B \mathbf{e}_{i}=\mathbf{p}_{i+1}-\mathbf{p}_{1}$ for $1 \leq i \leq k-1$.

10.30 Differentiable simplexes and chains Let $T$ be a $\mathscr{C}^{\prime \prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m} ; T$ need not be one-to-one. If $\sigma$ is an oriented affine $k$-simplex in $E$, then the composite mapping $\Phi=T \circ \sigma$ (which we shall sometimes write in the simpler form $T \sigma$ ) is a $k$-surface in $V$, with parameter domain $Q^{k}$. We call $\Phi$ an oriented $k$-simplex of class $\mathscr{C}^{\prime \prime}$.

A finite collection $\Psi$ of oriented $k$-simplexes $\Phi_{1}, \ldots, \Phi_{r}$ of class $\mathscr{C}^{\prime \prime}$ in $V$ is called a $k$-chain of class $\mathscr{C}^{\prime \prime}$ in $V$. If $\omega$ is a $k$-form in $V$, we define

$$
\int_{\Psi} \omega=\sum_{i=1}^{r} \int_{\Phi_{i}} \omega
$$

and use the corresponding notation $\Psi=\Sigma \Phi_{i}$.

If $\Gamma=\Sigma \sigma_{i}$ is an affine chain and if $\Phi_{i}=T \circ \sigma_{i}$, we also write $\Psi=T \circ \Gamma$, or

$$
T\left(\sum \sigma_{i}\right)=\sum T \sigma_{i}
$$

The boundary $\partial \Phi$ of the oriented $k$-simplex $\Phi=T \circ \sigma$ is defined to be the $(k-1)$ chain

$$
\partial \Phi=T(\partial \sigma)
$$

In justification of (89), observe that if $T$ is affine, then $\Phi=T \circ \sigma$ is an oriented affine $k$-simplex, in which case (89) is not a matter of definition, but is seen to be a consequence of (85). Thus (89) generalizes this special case.

It is immediate that $\partial \Phi$ is of class $\mathscr{C}^{\prime \prime}$ if this is true of $\Phi$.

Finally, we define the boundary $\partial \Psi$ of the $k$-chain $\Psi=\Sigma \Phi_{i}$ to be the $(k-1)$ chain

$$
\partial \Psi=\sum \partial \Phi_{i}
$$

10.31 Positively oriented boundaries So far we have associated boundaries to chains, not to subsets of $R^{n}$. This notion of boundary is exactly the one that is most suitable for the statement and proof of Stokes' theorem. However, in applications, especially in $R^{2}$ or $R^{3}$, it is customary and convenient to talk about "oriented boundaries" of certain sets as well. We shall now describe this briefly.

Let $Q^{n}$ be the standard simplex in $R^{n}$, let $\sigma_{0}$ be the identity mapping with domain $Q^{n}$. As we saw in Sec. 10.26, $\sigma_{0}$ may be regarded as a positively oriented $n$-simplex in $R^{n}$. Its boundary $\partial \sigma_{0}$ is an affine $(n-1)$-chain. This chain is called the positively oriented boundary of the set $Q^{n}$.

For example, the positively oriented boundary of $Q^{3}$ is

$$
\left[\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}\right]-\left[0, \mathbf{e}_{2}, \mathbf{e}_{3}\right]+\left[0, \mathbf{e}_{1}, \mathbf{e}_{3}\right]-\left[0, \mathbf{e}_{1}, \mathbf{e}_{2}\right]
$$

Now let $T$ be a 1-1 mapping of $Q^{n}$ into $R^{n}$, of class $\mathscr{C}^{\prime \prime}$, whose Jacobian is positive (at least in the interior of $Q^{n}$ ). Let $E=T\left(Q^{n}\right)$. By the inverse function theorem, $E$ is the closure of an open subset of $R^{n}$. We define the positively oriented boundary of the set $E$ to be the $(n-1)$-chain

$$
\partial T=T\left(\partial \sigma_{0}\right)
$$

and we may denote this $(n-1)$-chain by $\partial E$.

An obvious question occurs here: If $E=T_{1}\left(Q^{n}\right)=T_{2}\left(Q^{n}\right)$, and if both $T_{1}$ and $T_{2}$ have positive Jacobians, is it true that $\partial T_{1}=\partial T_{2}$ ? That is to say, does the equality

$$
\int_{\partial T_{1}} \omega=\int_{\partial T_{2}} \omega
$$

hold for every $(n-1)$-form $\omega$ ? The answer is yes, but we shall omit the proof. (To see an example, compare the end of this section with Exercise 17.)

One can go further. Let

$$
\Omega=\mathrm{E}_{1} \cup \cdots \cup E_{r},
$$

where $E_{i}=T_{i}\left(Q^{n}\right)$, each $T_{i}$ has the properties that $T$ had above, and the interiors of the sets $E_{i}$ are pairwise disjoint. Then the $(n-1)$-chain

$$
\partial T_{1}+\cdots+\partial T_{r}=\partial \Omega
$$

is called the positively oriented boundary of $\Omega$.

For example, the unit square $I^{2}$ in $R^{2}$ is the union of $\sigma_{1}\left(Q^{2}\right)$ and $\sigma_{2}\left(Q^{2}\right)$, where

$$
\sigma_{1}(\mathbf{u})=\mathbf{u}, \quad \sigma_{2}(\mathbf{u})=\mathbf{e}_{1}+\mathbf{e}_{2}-\mathbf{u} .
$$

Both $\sigma_{1}$ and $\sigma_{2}$ have Jacobian $1>0$. Since

$$
\sigma_{1}=\left[\mathbf{0}, \mathbf{e}_{1}, \mathbf{e}_{2}\right], \quad \sigma_{2}=\left[\mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{2}, \mathbf{e}_{1}\right]
$$

we have

$$
\begin{aligned}
& \partial \sigma_{1}=\left[\mathbf{e}_{1}, \mathbf{e}_{2}\right]-\left[\mathbf{0}, \mathbf{e}_{2}\right]+\left[\mathbf{0}, \mathbf{e}_{1}\right] \\
& \partial \sigma_{2}=\left[\mathbf{e}_{2}, \mathbf{e}_{1}\right]-\left[\mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{1}\right]+\left[\mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{2}\right]
\end{aligned}
$$

The sum of these two boundaries is

$$
\partial I^{2}=\left[0, \mathbf{e}_{1}\right]+\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]+\left[\mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{2}\right]+\left[\mathbf{e}_{2}, 0\right]
$$

the positively oriented boundary of $I^{2}$. Note that $\left[\mathbf{e}_{1}, \mathbf{e}_{2}\right]$ canceled $\left[\mathbf{e}_{2}, \mathbf{e}_{1}\right]$.

If $\Phi$ is a 2 -surface in $R^{m}$, with parameter domain $I^{2}$, then $\Phi$ (regarded as a function on 2-forms) is the same as the 2-chain

$$
\Phi \circ \sigma_{1}+\Phi \circ \sigma_{2} \text {. }
$$

Thus

$$
\begin{aligned}
\partial \Phi & =\partial\left(\Phi \circ \sigma_{1}\right)+\partial\left(\Phi \circ \sigma_{2}\right) \\
& =\Phi\left(\partial \sigma_{1}\right)+\Phi\left(\partial \sigma_{2}\right)=\Phi\left(\partial I^{2}\right) .
\end{aligned}
$$

In other words, if the parameter domain of $\Phi$ is the square $I^{2}$, we need not refer back to the simplex $Q^{2}$, but can obtain $\partial \Phi$ directly from $\partial I^{2}$.

Other examples may be found in Exercises 17 to 19.

10.32 Example For $0 \leq u \leq \pi, 0 \leq v \leq 2 \pi$, define

$$
\Sigma(u, v)=(\sin u \cos v, \sin u \sin v, \cos u)
$$

Then $\Sigma$ is a 2 -surface in $R^{3}$, whose parameter domain is a rectangle $D \subset R^{2}$, and whose range is the unit sphere in $R^{3}$. Its boundary is

$$
\partial \Sigma=\Sigma(\partial D)=\gamma_{1}+\gamma_{2}+\gamma_{3}+\gamma_{4}
$$

where

$$
\begin{aligned}
& \gamma_{1}(u)=\Sigma(u, 0)=(\sin u, 0, \cos u), \\
& \gamma_{2}(v)=\Sigma(\pi, v)=(0,0,-1) \\
& \gamma_{3}(u)=\Sigma(\pi-u, 2 \pi)=(\sin u, 0,-\cos u) \\
& \gamma_{4}(v)=\Sigma(0,2 \pi-v)=(0,0,1)
\end{aligned}
$$

with $[0, \pi]$ and $[0,2 \pi]$ as parameter intervals for $u$ and $v$, respectively.

Since $\gamma_{2}$ and $\gamma_{4}$ are constant, their derivatives are 0 , hence the integral of any 1-form over $\gamma_{2}$ or $\gamma_{4}$ is 0. [See Example 1.12(a).]

Since $\gamma_{3}(u)=\gamma_{1}(\pi-u)$, direct application of (35) shows that

$$
\int_{\gamma_{3}} \omega=-\int_{\gamma_{1}}^{v} \omega
$$

for every 1-form $\omega$. Thus $\int_{\partial \Sigma} \omega=0$, and we conclude that $\partial \Sigma=0$.

(In geographic terminology, $\partial \Sigma$ starts at the north pole $N$, runs to the south pole $S$ along a meridian, pauses at $S$, returns to $N$ along the same meridian, and finally pauses at $N$. The two passages along the meridian are in opposite directions. The corresponding two line integrals therefore cancel each other. In Exercise 32 there is also one curve which occurs twice in the boundary, but without cancellation.)

## STOKES' THEOREM

10.33 Theorem If $\Psi$ is a $k$-chain of class $\mathscr{C}^{\prime \prime}$ in an open set $V \subset R^{m}$ and if $\omega$ is a $(k-1)$-form of class $\mathscr{C}^{\prime}$ in $V$, then

$$
\int_{\Psi} d \omega=\int_{\partial \Psi} \omega .
$$

The case $k=m=1$ is nothing but the fundamental theorem of calculus (with an additional differentiability assumption). The case $k=m=2$ is Green's theorem, and $k=m=3$ gives the so-called "divergence theorem" of Gauss. The case $k=2, m=3$ is the one originally discovered by Stokes. (Spivak's
book describes some of the historical background.) These special cases will be discussed further at the end of the present chapter.

Proof It is enough to prove that

$$
\int_{\Phi} d \omega=\int_{\partial \Phi} \omega
$$

for every oriented $k$-simplex $\Phi$ of class $\mathscr{C}^{\prime \prime}$ in $V$. For if (92) is proved and if $\Psi=\Sigma \Phi_{i}$, then (87) and (89) imply (91).

Fix such a $\Phi$ and put

$$
\sigma=\left[\mathbf{0}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right]
$$

Thus $\sigma$ is the oriented affine $k$-simplex with parameter domain $Q^{k}$ which is defined by the identity mapping. Since $\Phi$ is also defined on $Q^{k}$ (see Definition 10.30) and $\Phi \in \mathscr{C}^{\prime \prime}$, there is an open set $E \subset R^{k}$ which contains $Q^{k}$, and there is a $\mathscr{C}^{\prime \prime}$-mapping $T$ of $E$ into $V$ such that $\Phi=T \circ \sigma$. By Theorems 10.25 and $10.22(c)$, the left side of (92) is equal to

$$
\int_{T \sigma} d \omega=\int_{\sigma}(d \omega)_{T}=\int_{\sigma} d\left(\omega_{T}\right)
$$

Another application of Theorem 10.25 shows, by (89), that the right side of $(92)$ is

$$
\int_{\partial(T \sigma)} \omega=\int_{T(\partial \sigma)} \omega=\int_{\partial \sigma} \omega_{T}
$$

Since $\omega_{T}$ is a $(k-1)$-form in $E$, we see that in order to prove (92) we merely have to show that

$$
\int_{\sigma}^{0} d \lambda=\int_{\partial \sigma} \lambda
$$

for the special simplex (93) and for every $(k-1)$-form $\lambda$ of class $\mathscr{C}^{\prime}$ in $E$.

If $k=1$, the definition of an oriented 0 -simplex shows that (94) merely asserts that

$$
\int_{0}^{1} f^{\prime}(u) d u=f(1)-f(0)
$$

for every continuously differentiable function $f$ on $[0,1]$, which is true by the fundamental theorem of calculus.

From now on we assume that $k>1$, fix an integer $r(1 \leq r \leq k)$, and choose $f \in \mathscr{C}^{\prime}(E)$. It is then enough to prove (94) for the case

$$
\lambda=f(\mathbf{x}) d x_{1} \wedge \cdots \wedge d x_{r-1} \wedge d x_{r+1} \wedge \cdots \wedge d x_{k}
$$

since every $(k-1)$-form is a sum of these special ones, for $r=1, \ldots, k$.

By (85), the boundary of the simplex (93) is

$$
\partial \sigma=\left[\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right]+\sum_{i=1}^{k}(-1)^{i} \tau_{i}
$$

where

$$
\tau_{i}=\left[0, \mathbf{e}_{1}, \ldots, \mathbf{e}_{i-1}, \mathbf{e}_{i+1}, \ldots, \mathbf{e}_{k}\right]
$$

for $i=1, \ldots, k$. Put

$$
\tau_{0}=\left[\mathbf{e}_{r}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{r-1}, \mathbf{e}_{r+1}, \ldots, \mathbf{e}_{k}\right]
$$

Note that $\tau_{0}$ is obtained from $\left[\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right]$ by $r-1$ successive interchanges of $e_{r}$ and its left neighbors. Thus

$$
\partial \sigma=(-1)^{r-1} \tau_{0}+\sum_{i=1}^{k}(-1)^{i} \tau_{i}
$$

Each $\tau_{i}$ has $Q^{k-1}$ as parameter domain.

If $\mathbf{x}=\tau_{0}(\mathbf{u})$ and $\mathbf{u} \in Q^{k-1}$, then

$$
x_{j}= \begin{cases}u_{j} & (1 \leq j<r) \\ 1-\left(u_{1}+\cdots+u_{k-1}\right) & (j=r), \\ u_{j-1} & (r<j \leq k)\end{cases}
$$

If $1 \leq i \leq k, \mathbf{u} \in Q^{k-1}$, and $\mathbf{x}=\tau_{i}(\mathbf{u})$, then

$$
x_{j}= \begin{cases}u_{j} & (1 \leq j<i) \\ 0 & (j=i) \\ u_{j-1} & (i<j \leq k)\end{cases}
$$

For $0 \leq i \leq k$, let $J_{i}$ be the Jacobian of the mapping

$$
\left(u_{1}, \ldots, u_{k-1}\right) \rightarrow\left(x_{1}, \ldots, x_{r-1}, x_{r+1}, \ldots, x_{k}\right)
$$

induced by $\tau_{i}$. When $i=0$ and when $i=r$, (98) and (99) show that (100) is the identity mapping. Thus $J_{0}=1, J_{r}=1$. For other $i$, the fact that $x_{i}=0$ in (99) shows that $J_{i}$ has a row of zeros, hence $J_{i}=0$. Thus

$$
\int_{\tau_{i}} \lambda=0 \quad(i \neq 0, i \neq r)
$$

by (35) and (96). Consequently, (97) gives

$$
\begin{aligned}
\int_{\partial \sigma} \lambda & =(-1)^{r-1} \int_{\tau_{0}} \lambda+(-1)^{r} \int_{\tau_{r}} \lambda \\
& =(-1)^{r-1} \int_{0}\left[f\left(\tau_{0}(\mathbf{u})\right)-f\left(\tau_{r}(\mathbf{u})\right)\right] d \mathbf{u} .
\end{aligned}
$$

On the other hand,

$$
\begin{aligned}
d \lambda & =\left(D_{r} f\right)(\mathbf{x}) d x_{r} \wedge d x_{1} \wedge \cdots \wedge d x_{r-1} \wedge d x_{r+1} \wedge \cdots \wedge d x_{k} \\
& =(-1)^{r-1}\left(D_{r} f\right)(\mathbf{x}) d x_{1} \wedge \cdots \wedge d x_{k}
\end{aligned}
$$

so that

$$
\int_{0}^{0} d \lambda=(-1)^{r-1} \int_{Q^{k}}\left(D_{r} f\right)(\mathbf{x}) d \mathbf{x}
$$

We evaluate (103) by first integrating with respect to $x_{r}$, over the interval

$$
\left[0,1-\left(x_{1}+\cdots+x_{r-1}+x_{r+1}+\cdots+x_{k}\right)\right]
$$

put $\left(x_{1}, \ldots, x_{r-1}, x_{r+1}, \ldots, x_{k}\right)=\left(u_{1}, \ldots, u_{k-1}\right)$, and see with the aid of (98) that the integral over $Q^{k}$ in (103) is equal to the integral over $Q^{k-1}$ in (102). Thus (94) holds, and the proof is complete.

## CLOSED FORMS AND EXACT FORMS

10.34 Definition Let $\omega$ be a $k$-form in an open set $E \subset R^{n}$. If there is a $(k-1)$ form $\lambda$ in $E$ such that $\omega=d \lambda$, then $\omega$ is said to be exact in $E$.

If $\omega$ is of class $\mathscr{C}^{\prime}$ and $d \omega=0$, then $\omega$ is said to be closed.

Theorem $10.20(b)$ shows that every exact form of class $\mathscr{C}^{\prime}$ is closed.

ln certain sets $E$, for example in convex ones, the converse is true; this is the content of Theorem 10.39 (usually known as Poincar√©s lemma) and Theorem 10.40. However, Examples 10.36 and 10.37 will exhibit closed forms that are not exact.

### 10.35 Remarks

(a) Whether a given $k$-form $\omega$ is or is not closed can be verified by simply differentiating the coefficients in the standard presentation of $\omega$. For example, a 1-form

$$
\omega=\sum_{i=1}^{n} f_{i}(\mathbf{x}) d x_{i}
$$

with $f_{i} \in \mathscr{C}^{\prime}(E)$ for some open set $E \subset R^{n}$, is closed if and only if the equations

$$
\left(D_{j} f_{i}\right)(\mathbf{x})=\left(D_{i} f_{j}\right)(\mathbf{x})
$$

hold for all $i, j$ in $\{1, \ldots, n\}$ and for all $\mathbf{x} \in E$.

Note that (105) is a "pointwise" condition; it does not involve any global properties that depend on the shape of $E$.

On the other hand, to show that $\omega$ is exact in $E$, one has to prove the existence of a form $\lambda$, defined in $E$, such that $d \lambda=\omega$. This amounts to solving a system of partial differential equations, not just locally, but in all of $E$. For example, to show that (104) is exact in a set $E$, one has to find a function (or 0 -form) $g \in \mathscr{C}^{\prime}(E)$ such that

$$
\left(D_{i} g\right)(\mathbf{x})=f_{i}(\mathbf{x}) \quad(\mathbf{x} \in E, 1 \leq i \leq n)
$$

Of course, (105) is a necessary condition for the solvability of (106).

(b) Let $\omega$ be an exact $k$-form in $E$. Then there is a $(k-1)$-form $\lambda$ in $E$ with $d \lambda=\omega$, and Stokes' theorem asserts that

$$
\int_{\Psi} \omega=\int_{\Psi} d \lambda=\int_{\partial \Psi} \lambda
$$

for every $k$-chain $\Psi$ of class $\mathscr{C}^{\prime \prime}$ in $E$.

If $\Psi_{1}$ and $\Psi_{2}$ are such chains, and if they have the same boundaries, it follows that

$$
\int_{\Psi_{1}} \omega=\int_{\Psi_{2}} \omega
$$

In particular, the integral of an exact $k$-form in $E$ is 0 over every $k$-chain in $E$ whose boundary is 0 .

As an important special case of this, note that integrals of exact 1-forms in $E$ are 0 over closed (differentiable) curves in $E$.

(c) Let $\omega$ be a closed $k$-form in $E$. Then $d \omega=0$, and Stokes' theorem asserts that

$$
\int_{\partial \Psi} \omega=\int_{\Psi} d \omega=0
$$

for every $(k+1)$-chain $\Psi$ of class $\mathscr{C}^{\prime \prime}$ in $E$.

In other words, integrals of closed $k$-forms in $E$ are 0 over $k$-chains that are boundaries of $(k+1)$-chains in $E$.

(d) Let $\Psi$ be a $(k+1)$-chain in $E$ and let $\lambda$ be a $(k-1)$-form in $E$, both of class $\mathscr{C}^{\prime \prime}$. Since $d^{2} \lambda=0$, two applications of Stokes' theorem show that

$$
\int_{\partial \partial \Psi} \lambda=\int_{\partial \Psi} d \lambda=\int_{\Psi} d^{2} \lambda=0 .
$$

We conclude that $\partial^{2} \Psi=0$. In other words, the boundary of a boundary is 0 .

See Exercise 16 for a more direct proof of this.

10.36 Example Let $E=R^{2}-\{0\}$, the plane with the origin removed. The 1 -form

$$
\eta=\frac{x d y-y d x}{x^{2}+y^{2}}
$$

is closed in $R^{2}-\{0\}$. This is easily verified by differentiation. Fix $r>0$, and define

$$
\gamma(t)=(r \cos t, r \sin t) \quad(0 \leq t \leq 2 \pi) .
$$

Then $\gamma$ is a curve (an "oriented 1-simplex") in $R^{2}-\{0\}$. Since $\gamma(0)=\gamma(2 \pi)$, we have

$$
\partial \gamma=0
$$

Direct computation shows that

$$
\int_{\gamma}^{*} \eta=2 \pi \neq 0
$$

The discussion in Remarks $10.35(b)$ and $(c)$ shows that we can draw two conclusions from (113):

First, $\eta$ is not exact in $R^{2}-\{0\}$, for otherwise (112) would force the integral (113) to be 0 .

Secondly, $\gamma$ is not the boundary of any 2-chain in $R^{2}-\{0\}$ (of class $\mathscr{C}^{\prime \prime}$ ), for otherwise the fact that $\eta$ is closed would force the integral (113) to be 0 .

10.37 Example Let $E=R^{3}-\{0\}$, 3-space with the origin removed. Define

$$
\zeta=\frac{x d y \wedge d z+y d z \wedge d x+z d x \wedge d y}{\left(x^{2}+y^{2}+z^{2}\right)^{3 / 2}}
$$

where we have written $(x, y, z)$ in place of $\left(x_{1}, x_{2}, x_{3}\right)$. Differentiation shows that $d \zeta=0$, so that $\zeta$ is a closed 2 -form in $R^{3}-\{0\}$.

Let $\Sigma$ be the 2-chain in $R^{3}-\{0\}$ that was constructed in Example 10.32; recall that $\Sigma$ is a parametrization of the unit sphere in $R^{3}$. Using the rectangle $D$ of Example 10.32 as parameter domain, it is easy to compute that

$$
\int_{\Sigma} \zeta=\int_{D} \sin u d u d v=4 \pi \neq 0
$$

As in the preceding example, we can now conclude that $\zeta$ is not exact in $R^{3}-\{0\}$ (since $\partial \boldsymbol{\Sigma}=0$, as was shown in Example 10.32) and that the sphere $\boldsymbol{\Sigma}$ is not the boundary of any 3-chain in $R^{3}-\{0\}$ (of class $\mathscr{C}^{\prime \prime}$ ), although $\partial \Sigma=0$.

The following result will be used in the proof of Theorem 10.39.

10.38 Theorem Suppose $E$ is a convex open set in $R^{n}, f \in \mathscr{C}^{\prime}(E), p$ is an integer, $1 \leq p \leq n$, and

$$
\left(D_{j} f\right)(\mathbf{x})=0 \quad(p<j \leq n, \mathbf{x} \in E)
$$

Then there exists an $F \in \mathscr{C}^{\prime}(E)$ such that

$$
\left(D_{p} F\right)(\mathbf{x})=f(\mathbf{x}), \quad\left(D_{j} F\right)(\mathbf{x})=0 \quad(p<j \leq n, \mathbf{x} \in E)
$$

Proof Write $\mathbf{x}=\left(\mathbf{x}^{\prime}, x_{p}, \mathbf{x}^{\prime \prime}\right)$, where

$$
\mathbf{x}^{\prime}=\left(x_{1}, \ldots, x_{p-1}\right), \mathbf{x}^{\prime \prime}=\left(x_{p+1}, \ldots, x_{n}\right)
$$

(When $p=1, \mathbf{x}^{\prime}$ is absent; when $p=n, \mathbf{x}^{\prime \prime}$ is absent.) Let $V$ be the set of all $\left(\mathbf{x}^{\prime}, x_{p}\right) \in R^{p}$ such that $\left(\mathbf{x}^{\prime}, x_{p}, \mathbf{x}^{\prime \prime}\right) \in E$ for some $\mathbf{x}^{\prime \prime}$. Being a projection of $E, V$ is a convex open set in $R^{p}$. Since $E$ is convex and (116) holds, $f(\mathbf{x})$ does not depend on $\mathbf{x}^{\prime \prime}$. Hence there is a function $\varphi$, with domain $V$, such that

for all $\mathbf{x} \in E$.

$$
f(\mathbf{x})=\varphi\left(\mathbf{x}^{\prime}, x_{p}\right)
$$

If $p=1, V$ is a segment in $R^{1}$ (possibly unbounded). Pick $c \in V$ and define

$$
F(\mathbf{x})=\int_{c}^{x_{1}} \varphi(t) d t \quad(\mathbf{x} \in E)
$$

If $p>1$, let $U$ be the set of all $\mathbf{x}^{\prime} \in R^{p-1}$ such that $\left(\mathbf{x}^{\prime}, x_{p}\right) \in V$ for some $x_{p}$. Then $U$ is a convex open set in $R^{p-1}$, and there is a function $\alpha \in \mathscr{C}^{\prime}(U)$ such that $\left(\mathbf{x}^{\prime}, \alpha\left(\mathbf{x}^{\prime}\right)\right) \in V$ for every $\mathbf{x}^{\prime} \in U$; in other words, the graph of $\alpha$ lies in $V$ (Exercise 29). Define

$$
F(\mathbf{x})=\int_{a\left(\mathbf{x}^{\prime}\right)}^{x_{p}} \varphi\left(\mathbf{x}^{\prime}, t\right) d t \quad(\mathbf{x} \in E)
$$

In either case, $F$ satisfies (117).

(Note: Recall the usual convention that $\int_{a}^{b}$ means $-\int_{b}^{a}$ if $b<a$.)

10.39 Theorem If $E \subset R^{n}$ is convex and open, if $k \geq 1$, if $\omega$ is a $k$-form of class $\mathscr{C}^{\prime}$ in $E$, and if $d \omega=0$, then there is $a(k-1)$-form $\lambda$ in $E$ such that $\omega=d \lambda$.

Briefly, closed forms are exact in convex sets.

Proof For $p=1, \ldots, n$, let $Y_{p}$ denote the set of all $k$-forms $\omega$, of class $\mathscr{C}^{\prime}$ in $E$, whose standard presentation

$$
\omega=\sum_{I} f_{I}(\mathbf{x}) d x_{I}
$$

does not involve $d x_{p+1}, \ldots, d x_{n}$. In other words, $I \subset\{1, \ldots, p\}$ if $f_{I}(\mathbf{x}) \neq 0$ for some $\mathbf{x} \in E$.

We shall proceed by induction on $p$.

Assume first that $\omega \in Y_{1}$. Then $\omega=f(\mathbf{x}) d x_{1}$. Since $d \omega=0$, $\left(D_{j} f\right)(\mathbf{x})=0$ for $1<j \leq n, \mathbf{x} \in E$. By Theorem 10.38 there is an $F \in \mathscr{C}^{\prime}(E)$ such that $D_{1} F=f$ and $D_{j} F=0$ for $1<j \leq n$. Thus

$$
d F=\left(D_{1} F\right)(\mathbf{x}) d x_{1}=f(\mathbf{x}) d x_{1}=\omega .
$$

Now we take $p>1$ and make the following induction hypothesis: Every closed $k$-form that belongs to $Y_{p-1}$ is exact in $E$.

Choose $\omega \in Y_{p}$ so that $d \omega=0$. By (118),

$$
\sum_{I} \sum_{j=1}^{n}\left(D_{j} f_{I}\right)(\mathbf{x}) d x_{j} \wedge d x_{I}=d \omega=0
$$

Consider a fixed $j$, with $p<j \leq n$. Each $I$ that occurs in (118) lies in $\{1, \ldots, p\}$. If $I_{1}, I_{2}$ are two of these $k$-indices, and if $I_{1} \neq I_{2}$, then the $(k+1)$-indices $\left(I_{1}, j\right),\left(I_{2}, j\right)$ are distinct. Thus there is no cancellation, and we conclude from (119) that every coefficient in (i18) satisfies

$$
\left(D_{j} f_{I}\right)(\mathbf{x})=0 \quad(\mathbf{x} \in E, p<j \leq n) .
$$

We now gather those terms in (118) that contain $d x_{p}$ and rewrite $\omega$ in the form

$$
\omega=\alpha+\sum_{I_{0}} f_{I}(\mathbf{x}) d x_{I_{0}} \wedge d x_{p}
$$

where $\alpha \in Y_{p-1}$, each $I_{0}$ is an increasing $(k-1)$-index in $\{1, \ldots, p-1\}$, and $I=\left(I_{0}, p\right)$. By $(120)$, Theorem 10.38 furnishes functions $F_{I} \in \mathscr{C}^{\prime}(E)$ such that

$$
D_{p} F_{I}=f_{I}, \quad D_{j} F_{I}=0 \quad(p<j \leq n) .
$$

Put

$$
\beta=\sum_{I_{0}} F_{I}(\mathbf{x}) d x_{I_{0}}
$$

and define $\gamma=\omega-(-1)^{k-1} d \beta$. Since $\beta$ is a $(k-1)$-form, it follows that

$$
\begin{aligned}
\gamma & =\omega-\sum_{I_{0}} \sum_{j=1}^{p}\left(D_{j} F_{I}\right)(\mathbf{x}) d x_{I_{0}} \wedge d x_{j} \\
& =\alpha-\sum_{I_{0}} \sum_{j=1}^{p-1}\left(D_{j} F_{I}\right)(\mathbf{x}) d x_{I_{0}} \wedge d x_{j},
\end{aligned}
$$

which is clearly in $Y_{p-1}$. Since $d \omega=0$ and $d^{2} \beta=0$, we have $d \gamma=0$. Our induction hypothesis shows therefore that $\gamma=d \mu$ for some $(k-1)$-form $\mu$ in $E$. If $\lambda=\mu+(-1)^{k-1} \beta$, we conclude that $\omega=d \lambda$.

By induction, this completes the proof.

10.40 Theorem Fix $k, 1 \leq k \leq n$. Let $E \subset R^{n}$ be an open set in which every closed $k$-form is exact. Let $T$ be a 1-1 $\mathscr{C}^{\prime \prime}$-mapping of $E$ onto an open set $U \subset R^{n}$ whose inverse $S$ is also of class $\mathscr{C}^{\prime \prime}$.

Then every closed $k$-form in $U$ is exact in $U$.

Note that every convex open set $E$ satisfies the present hypothesis, by Theorem 10.39. The relation between $E$ and $U$ may be expressed by saying that they are $\mathscr{C} "$-equivalent.

Thus every closed form is exact in any set which is $\mathscr{C}^{\prime \prime}$-equivalent to a convex open set.

Proof Let $\omega$ be a $k$-form in $U$, with $d \omega=0$. By Theorem $10.22(c)$, $\omega_{T}$ is a $k$-form in $E$ for which $d\left(\omega_{T}\right)=0$. Hence $\omega_{T}=d \lambda$ for some $(k-1)$-form $\lambda$ in $E$. By Theorem 10.23, and another application of Theorem 10.22(c),

$$
\omega=\left(\omega_{T}\right)_{S}=(d \lambda)_{S}=d\left(\lambda_{S}\right)
$$

Since $\lambda_{S}$ is a $(k-1)$-form in $U, \omega$ is exact in $U$.

10.41 Remark In applications, cells (see Definition 2.17) are often more convenient parameter domains than simplexes. If our whole development had been based on cells rather than simplexes, the computation that occurs in the proof of Stokes' theorem would be even simpler. (It is done that way in Spivak's book.) The reason for preferring simplexes is that the definition of the boundary of an oriented simplex seems easier and more natural than is the case for a cell. (See Exercise 19.) Also, the partitioning of sets into simplexes (called "triangulation") plays an important role in topology, and there are strong connections between certain aspects of topology, on the one hand, and differential forms, on the other. These are hinted at in Sec. 10.35. The book by Singer and Thorpe contains a good introduction to this topic.

Since every cell can be triangulated, we may regard it as a chain. For dimension 2, this was done in Example 10.32; for dimension 3, see Exercise 18.

Poincare's lemma (Theorem 10.39) can be proved in several ways. See, for example, page 94 in Spivak's book, or page 280 in Fleming's. Two simple proofs for certain special cases are indicated in Exercises 24 and 27.

## VECTOR ANALYSIS

We conclude this chapter with a few applications of the preceding material to theorems concerning vector analysis in $R^{3}$. These are special cases of theorems about differential forms, but are usually stated in different terminology. We are thus faced with the job of translating from one language to another.

10.42 Vector fields Let $\mathbf{F}=F_{1} \mathbf{e}_{1}+F_{2} \mathbf{e}_{2}+F_{3} \mathbf{e}_{3}$ be a continuous mapping of an open set $E \subset R^{3}$ into $R^{3}$. Since $\mathbf{F}$ associates a vector to each point of $E, \mathbf{F}$ is sometimes called a vector field, especially in physics. With every such $\mathbf{F}$ is associated a 1-form

$$
\lambda_{\mathrm{F}}=F_{1} d x+F_{2} d y+F_{3} d z
$$

and a 2 -form

$$
\omega_{\mathbf{F}}=F_{1} d y \wedge d z+F_{2} d z \wedge d x+F_{3} d x \wedge d y
$$

Here, and in the rest of this chapter, we use the customary notation $(x, y, z)$ in place of $\left(x_{1}, x_{2}, x_{3}\right)$.

It is clear, conversely, that every 1-form $\lambda$ in $E$ is $\lambda_{\mathbf{F}}$ for some vector field $\mathbf{F}$ in $E$, and that every 2-form $\omega$ is $\omega_{\mathbf{F}}$ for some $\mathbf{F}$. In $R^{3}$, the study of 1 -forms and 2 -forms is thus coextensive with the study of vector fields.

If $u \in \mathscr{C}^{\prime}(E)$ is a real function, then its gradient

$$
\nabla u=\left(D_{1} u\right) \mathbf{e}_{1}+\left(D_{2} u\right) \mathbf{e}_{2}+\left(D_{3} u\right) \mathbf{e}_{3}
$$

is an example of a vector field in $E$.

Suppose now that $\mathbf{F}$ is a vector field in $E$, of class $\mathscr{C}^{\prime}$. Its $\operatorname{curl} \nabla \times \mathbf{F}$ is the vector field defined in $E$ by

$$
\nabla \times \mathbf{F}=\left(D_{2} F_{3}-D_{3} F_{2}\right) \mathbf{e}_{1}+\left(D_{3} F_{1}-D_{1} F_{3}\right) \mathbf{e}_{2}+\left(D_{1} F_{2}-D_{2} F_{1}\right) \mathbf{e}_{3}
$$

and its divergence is the real function $\nabla \cdot \mathbf{F}$ defined in $E$ by

$$
\nabla \cdot \mathbf{F}=D_{1} F_{1}+D_{2} F_{2}+D_{3} F_{3} .
$$

These quantities have various physical interpretations. We refer to the book by $\mathrm{O}$. D. Kellogg for more details.

Here are some relations between gradients, curls, and divergences.

10.43 Theorem Suppose $E$ is an open set in $R^{3}, u \in \mathscr{C}^{\prime \prime}(E)$, and $\mathbf{G}$ is a vector field in $E$, of class $C^{\prime \prime}$.

(a) If $\mathbf{F}=\nabla u$, then $\nabla \times \mathbf{F}=\mathbf{0}$.

(b) If $\mathbf{F}=\nabla \times \mathbf{G}$, then $\boldsymbol{\nabla} \cdot \mathbf{F}=0$.

Furthermore, if $E$ is $\mathscr{C}^{\prime \prime}$-equivalent to a convex set, then (a) and (b) have converses, in which we assume that $\mathbf{F}$ is a vector field in $E$, of class $\mathscr{C}^{\prime}$ :

$\left(a^{\prime}\right)$ If $\nabla \times \mathbf{F}=\mathbf{0}$, then $\mathbf{F}=\nabla u$ for some $u \in \mathscr{C}^{\prime \prime}(E)$.

$\left(b^{\prime}\right)$ If $\nabla \cdot \mathbf{F}=0$, then $\mathbf{F}=\nabla \times \mathbf{G}$ for some vector field $\mathbf{G}$ in $E$, of class $\mathscr{C}^{\prime \prime}$

Proof If we compare the definitions of $\nabla u, \nabla \times \mathbf{F}$, and $\nabla \cdot \mathbf{F}$ with the differential forms $\lambda_{F}$ and $\omega_{F}$ given by (124) and (125), we obtain the following four statements:

$$
\begin{aligned}
& \mathbf{F}=\nabla u \quad \text { if and only if } \lambda_{\mathbf{F}}=d u \text {. } \\
& \nabla \times \mathbf{F}=\mathbf{0} \quad \text { if and only if } d \lambda_{\mathbf{F}}=0 \text {. } \\
& \mathbf{F}=\nabla \times \mathbf{G} \quad \text { if and only if } \quad \omega_{\mathbf{F}}=d \lambda_{\mathbf{G}} . \\
& \nabla \cdot \mathbf{F}=0 \quad \text { if and only if } d \omega_{\mathbf{F}}=0
\end{aligned}
$$

Now if $\mathbf{F}=\nabla u$, then $\lambda_{\mathbf{F}}=d u$, hence $d \lambda_{\mathbf{F}}=d^{2} u=0$ (Theorem 10.20), which means that $\nabla \times \mathbf{F}=\mathbf{0}$. Thus $(a)$ is proved.

As regards $\left(a^{\prime}\right)$, the hypothesis amounts to saying that $d \lambda_{\mathrm{F}}=0$ in $E$. By Theorem $10.40, \lambda_{F}=d u$ for some 0 -form $u$. Hence $\mathbf{F}=\nabla u$.

The proofs of $(b)$ and $\left(b^{\prime}\right)$ follow exactly the same pattern.

10.44 Volume elements The $k$-form

$$
d x_{1} \wedge \cdots \wedge d x_{k}
$$

is called the volume element in $R^{k}$. It is of ten denoted by $d V$ (or by $d V_{k}$ if it seems desirable to indicate the dimension explicitly), and the notation

$$
\int_{\Phi} f(\mathbf{x}) d x_{1} \wedge \cdots \wedge d x_{k}=\int_{\Phi} f d V
$$

is used when $\Phi$ is a positively oriented $k$-surface in $R^{k}$ and $f$ is a continuous function on the range of $\Phi$.

The reason for using this terminology is very simple: If $D$ is a parameter domain in $R^{k}$, and if $\Phi$ is a 1-1 $\mathscr{C}^{\prime}$-mapping of $D$ into $R^{k}$, with positive Jacobian $J_{\Phi}$, then the left side of (126) is

$$
\int_{D} f(\Phi(\mathbf{u})) J_{\Phi}(\mathbf{u}) d \mathbf{u}=\int_{\Phi(D)} f(\mathbf{x}) d \mathbf{x}
$$

by (35) and Theorem 10.9.

In particular, when $f=1,(126)$ defines the volume of $\Phi$. We already saw a special case of this in (36).

The usual notation for $d V_{2}$ is $d A$.

10.45 Green's theorem Suppose $E$ is an open set in $R^{2}, \alpha \in \mathscr{C}^{\prime}(E), \beta \in \mathscr{C}^{\prime}(E)$, and $\Omega$ is a closed subset of $E$, with positively oriented boundary $\partial \Omega$, as described in Sec. 10.31. Then

$$
\int_{\partial \Omega}(\alpha d x+\beta d y)=\int_{\Omega}\left(\frac{\partial \beta}{\partial x}-\frac{\partial \alpha}{\partial y}\right) d A \text {. }
$$

Proof Put $\lambda=\alpha d x+\beta d y$. Then

$$
\begin{aligned}
d \lambda & =\left(D_{2} \alpha\right) d y \wedge d x+\left(D_{1} \beta\right) d x \wedge d y \\
& =\left(D_{1} \beta-D_{2} \alpha\right) d A
\end{aligned}
$$

and (127) is the same as

$$
\int_{\partial \Omega} \lambda=\int_{\Omega} d \lambda
$$

which is true by Theorem 10.33.

With $\alpha(x, y)=-y$ and $\beta(x, y)=x$, (127) becomes

$$
\frac{1}{2} \int_{\partial \Omega}(x d y-y d x)=A(\Omega)
$$

the area of $\Omega$.

With $\alpha=0, \beta=x$, a similar formula is obtained. Example $10.12(b)$ contains a special case of this.

10.46 Area elements in $R^{3}$ Let $\Phi$ be a 2-surface in $R^{3}$, of class $\mathscr{C}^{\prime}$, with parameter domain $D \subset R^{2}$. Associate with each point $(u, v) \in D$ the vector

$$
\mathbf{N}(u, v)=\frac{\partial(y, z)}{\partial(u, v)} \mathbf{e}_{1}+\frac{\partial(z, x)}{\partial(u, v)} \mathbf{e}_{2}+\frac{\partial(x, y)}{\partial(u, v)} \mathbf{e}_{3} \text {. }
$$

The Jacobians in (129) correspond to the equation

$$
(x, y, z)=\Phi(u, v)
$$

If $f$ is a continuous function on $\Phi(D)$, the area integral of $f$ over $\Phi$ is defined to be

$$
\int_{\Phi} f d A=\int_{D} f(\Phi(u, v))|\mathbf{N}(u, v)| d u d v
$$

In particular, when $f=1$ we obtain the area of $\Phi$, namely,

$$
A(\Phi)=\int_{D}|\mathbf{N}(u, v)| d u d v
$$

The following discussion will show that (131) and its special case (132) are reasonable definitions. It will also describe the geometric features of the vector $\mathbf{N}$.

Write $\Phi=\varphi_{1} \mathbf{e}_{1}+\varphi_{2} \mathbf{e}_{2}+\varphi_{3} \mathbf{e}_{3}$, fix a point $\mathbf{p}_{0}=\left(u_{0}, v_{0}\right) \in D$, put $\mathbf{N}=\mathbf{N}\left(\mathbf{p}_{0}\right)$, put

$$
\alpha_{i}=\left(D_{1} \varphi_{i}\right)\left(\mathbf{p}_{0}\right), \quad \beta_{i}=\left(D_{2} \varphi_{i}\right)\left(\mathrm{p}_{0}\right) \quad(i=1,2,3)
$$

and let $T \in L\left(R^{2}, R^{3}\right)$ be the linear transformation given by

$$
T(u, v)=\sum_{i=1}^{3}\left(\alpha_{i} u+\beta_{i} v\right) \mathbf{e}_{i}
$$

Note that $T=\Phi^{\prime}\left(\mathbf{p}_{0}\right)$, in accordance with Definition 9.11.

Let us now assume that the rank of $T$ is 2 . (If it is 1 or 0 , then $\mathbf{N}=\mathbf{0}$, and the tangent plane mentioned below degenerates to a line or to a point.) The range of the affine mapping

$$
(u, v) \rightarrow \Phi\left(\mathrm{p}_{0}\right)+T(u, v)
$$

is then a plane $\Pi$, called the tangent plane to $\Phi$ at $\mathbf{p}_{0}$. [One would like to call $\Pi$ the tangent plane at $\Phi\left(\mathbf{p}_{0}\right)$, rather than at $\mathbf{p}_{0}$; if $\Phi$ is not one-to-one, this runs into difficulties.]

If we use (133) in (129), we obtain

$$
\mathbf{N}=\left(\alpha_{2} \beta_{3}-\alpha_{3} \beta_{2}\right) \mathbf{e}_{1}+\left(\alpha_{3} \beta_{1}-\alpha_{1} \beta_{3}\right) \mathbf{e}_{2}+\left(\alpha_{1} \beta_{2}-\alpha_{2} \beta_{1}\right) \mathbf{e}_{3},
$$

and (134) shows that

$$
T \mathbf{e}_{1}=\sum_{i=1}^{3} \alpha_{i} \mathbf{e}_{i}, \quad T \mathbf{e}_{2}=\sum_{i=1}^{3} \beta_{i} \mathbf{e}_{i}
$$

A straightforward computation now leads to

$$
\mathbf{N} \cdot\left(T \mathrm{e}_{1}\right)=0=\mathbf{N} \cdot\left(T \mathrm{e}_{2}\right)
$$

Hence $\mathbf{N}$ is perpendicular to $\Pi$. It is therefore called the normal to $\Phi$ at $\mathbf{p}_{0}$.

A second property of $\mathbf{N}$, also verified by a direct computation based on (135) and (136), is that the determinant of the linear transformation of $R^{3}$ that takes $\left\{\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}\right\}$ to $\left\{\mathbf{T e}_{1}, T \mathbf{e}_{2}, \mathbf{N}\right\}$ is $|\mathbf{N}|^{2}>0$ (Exercise 30). The 3-simplex

$$
\left[0, T \mathbf{e}_{1}, T \mathbf{e}_{2}, \mathbf{N}\right]
$$

is thus positively oriented.

The third property of $\mathbf{N}$ that we shall use is a consequence of the first two: The above-mentioned determinant, whose value is $|\mathbf{N}|^{2}$, is the volume of the parallelepiped with edges $\left[0, T \mathbf{e}_{1}\right],\left[0, T \mathbf{e}_{2}\right],[0, \mathbf{N}]$. By $(137),[0, \mathbf{N}]$ is perpendicular to the other two edges. The area of the parallelogram with vertices

$$
0, T \mathbf{e}_{1}, T \mathbf{e}_{2}, T\left(\mathrm{e}_{1}+\mathrm{e}_{2}\right)
$$

is therefore $|\mathbf{N}|$.

This parallelogram is the image under $T$ of the unit square in $R^{2}$. If $E$ is any rectangle in $R^{2}$, it follows (by the linearity of $T$ ) that the area of the parallelogram $T(E)$ is

$$
A(T(E))=|\mathbf{N}| A(E)=\int_{E}\left|\mathbf{N}\left(u_{0}, v_{0}\right)\right| d u d v
$$

We conclude that (132) is correct when $\Phi$ is affine. To justify the definition (132) in the general case, divide $D$ into small rectangles, pick a point $\left(u_{0}, v_{0}\right)$ in each, and replace $\Phi$ in each rectangle by the corresponding tangent plane. The sum of the areas of the resulting parallelograms, obtained via (140), is then an approximation to $A(\Phi)$. Finally, one can justify (131) from (132) by approximating $f$ by step functions.

10.47 Example Let $0<a<b$ be fixed. Let $K$ be the 3-cell determined by

$$
0 \leq t \leq a, \quad 0 \leq u \leq 2 \pi, \quad 0 \leq v \leq 2 \pi .
$$

The equations

$$
\begin{aligned}
& x=t \cos u \\
& y=(b+t \sin u) \cos v \\
& z=(b+t \sin u) \sin v
\end{aligned}
$$

describe a mapping $\Psi$ of $R^{3}$ into $R^{3}$ which is 1-1 in the interior of $K$, such that $\Psi(K)$ is a solid torus. Its Jacobian is

$$
J_{\Psi}=\frac{\partial(x, y, z)}{\partial(t, u, v)}=t(b+t \sin u)
$$

which is positive on $K$, except on the face $t=0$. If we integrate $J_{\Psi}$ over $K$, we obtain

$$
\operatorname{vol}(\Psi(K))=2 \pi^{2} a^{2} b
$$

as the volume of our solid torus.

Now consider the 2-chain $\Phi=\partial \Psi$. (See Exercise 19.) $\Psi$ maps the faces $u=0$ and $u=2 \pi$ of $K$ onto the same cylindrical strip, but with opposite orientations. $\Psi$ maps the faces $v=0$ and $v=2 \pi$ onto the same circular disc, but with opposite orientations. $\Psi$ maps the face $t=0$ onto a circle, which contributes 0 to the 2-chain $\partial \Psi$. (The relevant Jacobians are 0 .) Thus $\Phi$ is simply the 2-surface obtained by setting $t=a$ in (141), with parameter domain $D$ the square defined by $0 \leq u \leq 2 \pi, 0 \leq v \leq 2 \pi$.

According to (129) and (141), the normal to $\Phi$ at $(u, v) \in D$ is thus the vector

$$
\mathbf{N}(u, v)=a(b+a \sin u) \mathbf{n}(u, v)
$$

where

$$
\mathbf{n}(u, v)=(\cos u) \mathbf{e}_{1}+(\sin u \cos v) \mathbf{e}_{2}+(\sin u \sin v) \mathbf{e}_{3} .
$$

Since $|\mathbf{n}(u, v)|=1$, we have $|\mathbf{N}(u, v)|=a(b+a \sin u)$, and if we integrate this over $D,(131)$ gives

$$
A(\Phi)=4 \pi^{2} a b
$$

as the surface area of our torus.

If we think of $\mathbf{N}=\mathbf{N}(u, v)$ as a directed line segment, pointing from $\Phi(u, v)$ to $\Phi(u, v)+\mathbf{N}(u, v)$, then $\mathbf{N}$ points outward, that is to say, away from $\Psi(K)$. This is so because $\mathrm{J}_{\Psi}>0$ when $t=a$.

For example, take $u=v=\pi / 2, t=a$. This gives the largest value of $z$ on $\Psi(K)$, and $\mathbf{N}=a(b+a) \mathbf{e}_{3}$ points "upward" for this choice of $(u, v)$.

10.48 Integrals of 1-forms in $R^{3}$ Let $\gamma$ be a $\mathscr{C}^{\prime}$-curve in an open set $E \subset R^{3}$, with parameter interval $[0,1]$, let $\mathbf{F}$ be a vector field in $E$, as in Sec. 10.42 , and define $\lambda_{F}$ by (124). The integral of $\lambda_{F}$ over $\gamma$ can be rewritten in a certain way which we now describe.

For any $u \in[0,1]$,

$$
\gamma^{\prime}(u)=\gamma_{1}^{\prime}(u) \mathbf{e}_{1}+\gamma_{2}^{\prime}(u) \mathbf{e}_{2}+\gamma_{3}^{\prime}(u) \mathbf{e}_{3}
$$

is called the tangent vector to $\gamma$ at $u$. We define $\mathbf{t}=\mathbf{t}(u)$ to be the unit vector in the direction of $\gamma^{\prime}(u)$. Thus

$$
\gamma^{\prime}(u)=\left|\gamma^{\prime}(u)\right| \mathbf{t}(u)
$$

[If $\gamma^{\prime}(u)=\mathbf{0}$ for some $u$, put $\mathbf{t}(u)=\mathbf{e}_{1}$; any other choice would do just as well.] By (35),

$$
\begin{aligned}
\int_{\gamma} \lambda_{\mathbf{F}} & =\sum_{i=1}^{3} \int_{0}^{1} F_{i}(\gamma(u)) \gamma_{i}^{\prime}(u) d u \\
& =\int_{0}^{1} \mathbf{F}(\gamma(u)) \cdot \gamma^{\prime}(u) d u \\
& =\int_{0}^{1} \mathbf{F}(\gamma(u)) \cdot \mathbf{t}(u)\left|\gamma^{\prime}(u)\right| d u .
\end{aligned}
$$

Theorem 6.27 makes it reasonable to call $\left|\gamma^{\prime}(u)\right| d u$ the element of arc length along $\gamma$. A customary notation for it is $d s$, and (142) is rewritten in the form

$$
\int_{\gamma} \lambda_{\mathbf{F}}=\int_{\gamma}(\mathbf{F} \cdot \mathbf{t}) d s
$$

Since $\mathbf{t}$ is a unit tangent vector to $\gamma, \mathbf{F} \cdot \mathbf{t}$ is called the tangential component of $\mathbf{F}$ along $\gamma$.

The right side of (143) should be regarded as just an abbreviation for the last integral in (142). The point is that $\mathbf{F}$ is defined on the range of $\gamma$, but $\mathbf{t}$ is defined on $[0,1]$; thus $\mathbf{F} \cdot \mathbf{t}$ has to be properly interpreted. Of course, when $\gamma$ is one-to-one, then $\mathbf{t}(u)$ can be replaced by $\mathbf{t}(\gamma(u))$, and this difficulty disappears.

10.49 Integrals of 2-forms in $R^{3} \quad$ Let $\Phi$ be a 2-surface in an open set $E \subset R^{3}$, of class $\mathscr{C}^{\prime}$, with parameter domain $D \subset R^{2}$. Let $\mathbf{F}$ be a vector field in $E$, and define $\omega_{F}$ by (125). As in the preceding section, we shall obtain a different representation of the integral of $\omega_{\mathbf{F}}$ over $\Phi$.

By (35) and (129),

$$
\begin{aligned}
\int_{\Phi} \omega_{\mathbf{F}} & =\int_{\Phi}\left(F_{1} d y \wedge d z+F_{2} d z \wedge d x+F_{3} d x \wedge d y\right) \\
& =\int_{D}\left\{\left(F_{1} \circ \Phi\right) \frac{\partial(y, z)}{\partial(u, v)}+\left(F_{2} \circ \Phi\right) \frac{\partial(z, x)}{\partial(u, v)}+\left(F_{3} \circ \Phi\right) \frac{\partial(x, y)}{\partial(u, v)}\right\} d u d v \\
& =\int_{D} \mathbf{F}(\Phi(u, v)) \cdot \mathbf{N}(u, v) d u d v
\end{aligned}
$$

Now let $\mathbf{n}=\mathbf{n}(u, v)$ be the unit vector in the direction of $\mathbf{N}(u, v)$. [If $\mathbf{N}(u, v)=\mathbf{0}$ for some $(u, v) \in D$, take $\mathbf{n}(u, v)=\mathbf{e}_{1}$.] Then $\mathbf{N}=|\mathbf{N}| \mathbf{n}$, and therefore the last integral becomes

$$
\int_{D} \mathbf{F}(\Phi(u, v)) \cdot \mathbf{n}(u, v)|\mathbf{N}(u, v)| d u d v
$$

By (131), we can finally write this in the form

$$
\int_{\Phi} \omega_{\mathbf{F}}=\int_{\Phi}(\mathbf{F} \cdot \mathbf{n}) d A
$$

With regard to the meaning of $\mathbf{F} \cdot \mathbf{n}$, the remark made at the end of Sec. 10.48 applies here as well.

We can now state the original form of Stokes' theorem.

10.50 Stokes' formula If $\mathbf{F}$ is a vector field of class $\mathscr{C}^{\prime}$ in an open set $E \subset R^{3}$, and if $\Phi$ is a 2-surface of class $\mathscr{C}^{\prime \prime}$ in $E$, then

$$
\int_{\Phi}(\nabla \times \mathbf{F}) \cdot \mathbf{n} d A=\int_{\partial \Phi}(\mathbf{F} \cdot \mathbf{t}) d s .
$$

Proof Put $\mathbf{H}=\nabla \times \mathbf{F}$. Then, as in the proof of Theorem 10.43, we have

$$
\omega_{\mathbf{H}}=d \lambda_{\mathbf{F}}
$$

Hence

$$
\begin{aligned}
\int_{\Phi}(\nabla \times \mathbf{F}) \cdot \mathbf{n} d A & =\int_{\Phi}(\mathbf{H} \cdot \mathbf{n}) d A=\int_{\Phi} \omega_{\mathbf{H}} \\
& =\int_{\Phi} d \lambda_{\mathbf{F}}=\int_{\partial \Phi} \lambda_{\mathbf{F}}=\int_{\partial \Phi}(\mathbf{F} \cdot \mathbf{t}) d s .
\end{aligned}
$$

Here we used the definition of $\mathbf{H}$, then (144) with $\mathbf{H}$ in place of $F$, then (146), then-the main step-Theorem 10.33, and finally (143), extended in the obvious way from curves to 1-chains.

10.51 The divergence theorem If $\mathbf{F}$ is a vector field of class $\mathscr{C}^{\prime}$ in an open set $E \subset R^{3}$, and if $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial \Omega$ (as described in Sec. 10.31) then

$$
\int_{\Omega}(\nabla \cdot \mathbf{F}) d V=\int_{\partial \boldsymbol{\Omega}}(\mathbf{F} \cdot \mathbf{n}) d A .
$$

Proof By (125),

$$
d \omega_{\mathbf{F}}=(\nabla \cdot \mathbf{F}) d x \wedge d y \wedge d z=(\nabla \cdot \mathbf{F}) d V
$$

Hence

$$
\int_{\Omega}(\nabla \cdot \mathbf{F}) d V=\int_{\Omega} d \omega_{\mathbf{F}}=\int_{\partial \Omega} \omega_{\mathbf{F}}=\int_{\partial \Omega}(\mathbf{F} \cdot \mathbf{n}) d A
$$

by Theorem 10.33, applied to the 2-form $\omega_{F}$, and (144).

## EXERCISES

1. Let $H$ be a compact convex set in $R^{k}$, with nonempty interior. Let $f \in \mathscr{C}(H)$, put $f(x)=0$ in the complement of $H$, and define $\int_{H} f$ as in Definition 10.3.

Prove that $\int_{H} f$ is independent of the order in which the $k$ integrations are carried out.

Hint: Approximate $f$ by functions that are continuous on $R^{k}$ and whose supports are in $H$, as was done in Example 10.4.

2. For $i=1,2,3, \ldots$, let $\varphi_{l} \in \mathscr{C}\left(R^{1}\right)$ have support in $\left(2^{-t}, 2^{1-l}\right)$, such that $\int \varphi_{l}=1$. Put

$$
f(x, y)=\sum_{l=1}^{\infty}\left[\varphi_{l}(x)-\varphi_{l+1}(x)\right] \varphi_{l}(y)
$$

Then $f$ has compact support in $R^{2}, f$ is continuous except at $(0,0)$, and

$$
\int d y \int f(x, y) d x=0 \quad \text { but } \quad \int d x \int f(x, y) d y=1 \text {. }
$$

Observe that $f$ is unbounded in every neighborhood of $(0,0)$.

3. (a) If $F$ is as in Theorem 10.7, put $\mathbf{A}=\mathbf{F}^{\prime}(0), \mathbf{F}_{1}(\mathbf{x})=\mathbf{A}^{-1} \mathbf{F}(\mathbf{x})$. Then $\mathbf{F}_{1}^{\prime}(\mathbf{0})=I$. Show that

$$
\mathbf{F}_{1}(\mathbf{x})=\mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x})
$$

in some neighborhood of $\mathbf{0}$, for certain primitive mappings $\mathbf{G}_{1}, \ldots, \mathbf{G}_{n}$. This gives another version of Theorem 10.7:

$$
\mathbf{F}(\mathbf{x})=\mathbf{F}^{\prime}(\mathbf{0}) \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x}) .
$$

(b) Prove that the mapping $(x, y) \rightarrow(y, x)$ of $R^{2}$ onto $R^{2}$ is not the composition of any two primitive mappings, in any neighborhood of the origin. (This shows that the flips $B_{1}$ cannot be omitted from the statement of Theorem 10.7.)

4. For $(x, y) \in R^{2}$, define

$$
\mathbf{F}(x, y)=\left(e^{x} \cos y-1, e^{x} \sin y\right) .
$$

Prove that $\mathbf{F}=\mathbf{G}_{\mathbf{2}} \circ \mathbf{G}_{1}$, where

$$
\begin{aligned}
& \mathbf{G}_{1}(x, y)=\left(e^{x} \cos y-1, y\right) \\
& \mathbf{G}_{2}(u, v)=(u,(1+u) \tan v)
\end{aligned}
$$

are primitive in some neighborhood of $(0,0)$.

Compute the Jacobians of $\mathbf{G}_{1}, \mathbf{G}_{2}, \mathbf{F}$ at $(0,0)$. Define

$$
\mathbf{H}_{2}(x, y)=\left(x, e^{x} \sin y\right)
$$

and find

$$
\mathbf{H}_{1}(u, v)=(h(u, v), v)
$$

so that $\mathbf{F}=\mathbf{H}_{1} \circ \mathbf{H}_{2}$ is some neighborhood of $(0,0)$.

5. Formulate and prove an analogue of Theorem 10.8, in which $K$ is a compact subset of an arbitrary metric space. (Replace the functions $\varphi_{l}$ that occur in the proof of Theorem 10.8 by functions of the type constructed in Exercise 22 of Chap. 4.)
6. Strengthen the conclusion of Theorem 10.8 by showing that the functions $\psi_{l}$ can be made differentiable, and even infinitely differentiable. (Use Exercise 1 of Chap. 8 in the construction of the auxiliary functions $\varphi_{t}$.)
7. (a) Show that the simplex $Q^{k}$ is the smallest convex subset of $R^{k}$ that contains $\mathbf{0}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{k}$.

(b) Show that affine mappings take convex sets to convex sets.

8. Let $H$ be the parallelogram in $R^{2}$ whose vertices are $(1,1),(3,2),(4,5),(2,4)$. Find the affine map $T$ which sends $(0,0)$ to $(1,1),(1,0)$ to $(3,2),(0,1)$ to $(2,4)$. Show that $J_{T}=5$. Use $T$ to convert the integral

$$
\alpha=\int_{H} e^{x-y} d x d y
$$

to an integral over $I^{2}$ and thus compute $\alpha$.

9. Define $(x, y)=T(r, \theta)$ on the rectangle

$$
0 \leq r \leq a, \quad 0 \leq \theta \leq 2 \pi
$$

by the equations

$$
x=r \cos \theta, \quad y=r \sin \theta
$$

Show that $T$ maps this rectangle onto the closed disc $D$ with center at $(0,0)$ and radius $a$, that $T$ is one-to-one in the interior of the rectangle, and that $J_{T}(r, \theta)=r$. If $f \in \mathscr{C}(D)$, prove the formula for integration in polar coordinates:

$$
\int_{D} f(x, y) d x d y=\int_{0}^{a} \int_{0}^{2 \pi} f(T(r, \theta)) r d r d \theta
$$

Hint: Let $D_{0}$ be the interior of $D$, minus the interval from $(0,0)$ to $(0, a)$. As it stands, Theorem 10.9 applies to continuous functions $f$ whose support lies in $D_{0}$. To remove this restriction, proceed as in Example 10.4.

10. Let $a \rightarrow \infty$ in Exercise 9 and prove that

$$
\int_{R^{2}} f(x, y) d x d y=\int_{0}^{\infty} \int_{0}^{2 \pi} f(T(r, \theta)) r d r d \theta
$$

for continuous functions $f$ that decrease sufficiently rapidly as $|x|+|y| \rightarrow \infty$. (Find a more precise formulation.) Apply this to

$$
f(x, y)=\exp \left(-x^{2}-y^{2}\right)
$$

to derive formula (101) of Chap. 8 .

11. Define $(u, v)=T(s, t)$ on the strip

$$
0<s<\infty, \quad 0<t<1
$$

by setting $u=s-s t, v=s t$. Show that $T$ is a 1-1 mapping of the strip onto the positive quadrant $Q$ in $R^{2}$. Show that $J_{T}(s, t)=s$.

For $x>0, y>0$, integrate

$$
u^{x-1} e^{-u} v^{y-1} e^{-v}
$$

over $Q$, use Theorem 10.9 to convert the integral to one over the strip, and derive formula (96) of Chap. 8 in this way.

(For this application, Theorem 10.9 has to be extended so as to cover certain improper integrals. Provide this extension.)

12. Let $I^{k}$ be the set of all $\mathbf{u}=\left(u_{1}, \ldots, u_{k}\right) \in R^{k}$ with $0 \leq u_{l} \leq 1$ for all $i$; let $Q^{k}$ be the set of all $\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right) \in R^{k}$ with $x_{l} \geq 0, \Sigma x_{l} \leq 1$. ( $I^{k}$ is the unit cube; $Q^{k}$ is the standard simplex in $R^{k}$.) Define $\mathbf{x}=T(\mathbf{u})$ by

$$
\begin{aligned}
& x_{1}=u_{1} \\
& x_{2}=\left(1-u_{1}\right) u_{2} \\
& \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\
& x_{k}=\left(1-u_{1}\right) \cdots\left(1-u_{k-1}\right) u_{k} .
\end{aligned}
$$

Show that

$$
\sum_{i=1}^{k} x_{i}=1-\prod_{i=1}^{k}\left(1-u_{i}\right)
$$

Show that $T$ maps $I^{k}$ onto $Q^{k}$, that $T$ is $1-1$ in the interior of $I^{k}$, and that its inverse $S$ is defined in the interior of $Q^{k}$ by $u_{1}=x_{1}$ and

$$
u_{l}=\frac{x_{l}}{1-x_{1}-\cdots-x_{l-1}}
$$

for $i=2, \ldots, k$. Show that

$$
J_{T}(\mathrm{u})=\left(1-u_{1}\right)^{k-1}\left(1-u_{2}\right)^{k-2} \cdots\left(1-u_{k-1}\right)
$$

and

$$
J_{s}(\mathbf{x})=\left[\left(1-x_{1}\right)\left(1-x_{1}-x_{2}\right) \cdots\left(1-x_{1}-\cdots-x_{k-1}\right)\right]^{-1}
$$

13. Let $r_{1}, \ldots, r_{k}$ be nonnegative integers, and prove that

$$
\int_{\mathbf{Q}^{k}} x_{1}^{r_{1}} \cdots x_{k}^{r_{k}} d x=\frac{r_{1} ! \cdots r_{k} !}{\left(k+r_{1}+\cdots+r_{k}\right) !}
$$

Hint: Use Exercise 12, Theorems 10.9 and 8.20.

Note that the special case $r_{1}=\cdots=r_{k}=0$ shows that the volume of $Q^{k}$ is $1 / k$ !.

14. Prove formula (46).
15. If $\omega$ and $\lambda$ are $k$ - and $m$-forms, respectively, prove that

$$
\omega \wedge \lambda=(-1)^{k m} \lambda \wedge \omega .
$$

16. If $k \geq 2$ and $\sigma=\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \ldots, \mathbf{p}_{k}\right]$ is an oriented affine $k$-simplex, prove that $\partial^{2} \sigma=0$, directly from the definition of the boundary operator $\partial$. Deduce from this that $\partial^{2} \Psi=0$ for every chain $\Psi$.

Hint: For orientation, do it first for $k=2, k=3$. In general, if $i<j$, let $\sigma_{i \jmath}$ be the $(k-2)$-simplex obtained by deleting $\mathbf{p}_{\imath}$ and $\mathbf{p}_{\jmath}$ from $\sigma$. Show that each $\sigma_{\imath \jmath}$ occurs twice in $\partial^{2} \sigma$, with opposite sign.

17. Put $J^{2}=\tau_{1}+\tau_{2}$, where

$$
\tau_{1}=\left[0, \mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right], \quad \tau_{2}=-\left[0, \mathbf{e}_{2}, \mathbf{e}_{2}+\mathbf{e}_{1}\right]
$$

Explain why it is reasonable to call $J^{2}$ the positively oriented unit square in $R^{2}$. Show that $\partial J^{2}$ is the sum of 4 oriented affine 1-simplexes. Find these. What is $\partial\left(\tau_{1}-\tau_{2}\right)$ ?

18. Consider the oriented affine 3-simplex

$$
\sigma_{1}=\left[0, \mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]
$$

in $R^{3}$. Show that $\sigma_{1}$ (regarded as a linear transformation) has determinant 1. Thus $\sigma_{1}$ is positively oriented.

Let $\sigma_{2}, \ldots, \sigma_{6}$ be five other oriented 3-simplexes, obtained as follows: There are five permutations $\left(i_{1}, i_{2}, i_{3}\right)$ of $(1,2,3)$, distinct from $(1,2,3)$. Associate with each $\left(i_{1}, i_{2}, i_{3}\right)$ the simplex

$$
s\left(i_{1}, i_{2}, i_{3}\right)\left[0, \mathbf{e}_{t_{1}}, \mathbf{e}_{t_{1}}+\mathbf{e}_{t_{2}}, \mathbf{e}_{t_{1}}+\mathbf{e}_{t_{2}}+\mathbf{e}_{t_{3}}\right]
$$

where $s$ is the sign that occurs in the definition of the determinant. (This is how $\tau_{2}$ was obtained from $\tau_{1}$ in Exercise 17.)

Show that $\sigma_{2}, \ldots, \sigma_{6}$ are positively oriented.

Put $J^{3}=\sigma_{1}+\cdots+\sigma_{6}$. Then $J^{3}$ may be called the positively oriented unit cube in $R^{3}$.

Show that $\partial J^{3}$ is the sum of 12 oriented affine 2 -simplexes. (These 12 triangles cover the surface of the unit cube $I^{3}$.)

Show that $\mathbf{x}=\left(x_{1}, x_{2}, x_{3}\right)$ is in the range of $\sigma_{1}$ if and only if $0 \leq x_{3} \leq x_{2}$ $\leq x_{1} \leq 1$.

Show that the ranges of $\sigma_{1}, \ldots, \sigma_{6}$ have disjoint interiors, and that their union covers $I^{3}$. (Compare with Exercise 13; note that $3 !=6$.)

19. Let $J^{2}$ and $J^{3}$ be as in Exercise 17 and 18. Define

$$
\begin{array}{ll}
B_{01}(u, v)=(0, u, v), & B_{11}(u, v)=(1, u, v), \\
B_{02}(u, v)=(u, 0, v), & B_{12}(u, v)=(u, 1, v), \\
B_{03}(u, v)=(u, v, 0), & B_{13}(u, v)=(u, v, 1) .
\end{array}
$$

These are affine, and map $R^{2}$ into $R^{3}$.

Put $\beta_{r l}=B_{r l}\left(J^{2}\right)$, for $r=0,1, i=1,2,3$. Each $\beta_{r l}$ is an affine-oriented 2-chain. (See Sec. 10.30.) Verify that

$$
\partial J^{3}=\sum_{t=1}^{3}(-1)^{t}\left(\beta_{0 l}-\beta_{1 \imath}\right),
$$

in agreement with Exercise 18.

20. State conditions under which the formula

$$
\int_{\oplus} f d \omega=\int_{s \Phi} f \omega-\int_{\oplus}(d f) \wedge \omega
$$

is valid, and show that it generalizes the formula for integration by parts.

Hint: $d(f \omega)=(d f) \wedge \omega+f d \omega$.

21. As in Example 10.36, consider the 1 -form

$$
\eta=\frac{x d y-y d x}{x^{2}+y^{2}}
$$

in $R^{2}-\{0\}$.

(a) Carry out the computation that leads to formula (113), and prove that $d \eta=0$.

(b) Let $\gamma(t)=(r \cos t, r \sin t)$, for some $r>0$, and let $\Gamma$ be a $\mathscr{C}^{\prime \prime}$-curve in $R^{2}-\{0\}$,
with parameter interval $[0,2 \pi]$, with $\Gamma(0)=\Gamma(2 \pi)$, such that the intervals $[\gamma(t)$, $\Gamma(t)]$ do not contain 0 for any $t \in[0,2 \pi]$. Prove that

$$
\int_{\Gamma} \eta=2 \pi .
$$

Hint: For $0 \leq t \leq 2 \pi, 0 \leq u \leq 1$, define

$$
\Phi(t, u)=(1-u) \Gamma(t)+u \gamma(t)
$$

Then $\Phi$ is a 2 -surface in $R^{2}-\{0\}$ whose parameter domain is the indicated rectangle. Because of cancellations (as in Example 10.32),

$$
\partial \Phi=\Gamma-\gamma .
$$

Use Stokes' theorem to deduce that

$$
\int_{\Gamma} \eta=\int_{\nu} \eta
$$

because $d \eta=0$.

(c) Take $\Gamma(t)=(a \cos t, b \sin t)$ where $a>0, b>0$ are fixed. Use part (b) to show that

$$
\int_{0}^{2 \pi} \frac{a b}{a^{2} \cos ^{2} t+b^{2} \sin ^{2} t} d t=2 \pi
$$

(d) Show that

$$
\eta=d\left(\arctan \frac{y}{x}\right)
$$

in any convex open set in which $x \neq 0$, and that

$$
\eta=d\left(-\arctan \frac{x}{y}\right)
$$

in any convex open set in which $y \neq 0$.

Explain why this justifies the notation $\eta=d \theta$, in spite of the fact that $\eta$ is not exact in $R^{2}-\{0\}$.

(e) Show that $(b)$ can be derived from $(d)$.

( $f$ ) If $\Gamma$ is any closed $\mathscr{C}^{\prime}$-curve in $R^{2}-\{0\}$, prove that

$$
\frac{1}{2 \pi} \int_{\Gamma} \eta=\operatorname{Ind}(\Gamma) .
$$

(See Exercise 23 of Chap. 8 for the definition of the index of a curve.)

22. As in Example 10.37, define $\zeta$ in $R^{3}-\{0\}$ by

$$
\zeta=\frac{x d y \wedge d z+y d z \wedge d x+z d x \wedge d y}{r^{3}}
$$

where $r=\left(x^{2}+y^{2}+z^{2}\right)^{1 / 2}$, let $D$ be the rectangle given by $0 \leq u \leq \pi, 0 \leq v \leq 2 \pi$, and let $\Sigma$ be the 2 -surface in $R^{3}$, with parameter domain $D$, given by

$$
x=\sin u \cos v, \quad y=\sin u \sin v, \quad z=\cos u
$$

(a) Prove that $d \zeta=0$ in $R^{3}-\{0\}$.

(b) Let $S$ denote the restriction of $\Sigma$ to a parameter domain $E \subset D$. Prove that

$$
\int_{S} \zeta=\int_{E} \sin u d u d v=A(S)
$$

where $A$ denotes area, as in Sec. 10.43. Note that this contains (115) as a special case.

(c) Suppose $g, h_{1}, h_{2}, h_{3}$, are $\mathscr{C}^{\prime \prime}$-functions on $[0,1], g>0$. Let $(x, y, z)=\Phi(s, t)$ define a 2-surface $\Phi$, with parameter domain $I^{2}$, by

$$
x=g(t) h_{1}(s), \quad y=g(t) h_{2}(s), \quad z=g(t) h_{3}(s)
$$

Prove that

$$
\int_{\bullet} \zeta=0
$$

directly from (35).

Note the shape of the range of $\Phi$ : For fixed $s, \Phi(s, t)$ runs over an interval on a line through 0 . The range of $\Phi$ thus lies in a "cone" with vertex at the origin.

(d) Let $E$ be a closed rectangle in $D$, with edges parallel to those of $D$. Suppose $f \in \mathscr{C}^{\prime \prime}(D), f>0$. Let $\Omega$ be the 2-surface with parameter domain $E$, defined by

$$
\Omega(u, v)=f(u, v) \Sigma(u, v)
$$

Define $S$ as in $(b)$ and prove that

$$
\int_{\mathbf{0}} \zeta=\int_{\mathbf{s}} \zeta=A(S)
$$

(Since $S$ is the "radial projection" of $\Omega$ into the unit sphere, this result makes it reasonable to call $\int_{\Omega} \zeta$ the "solid angle" subtended by the range of $\Omega$ at the origin.)

Hint: Consider the 3-surface $\Psi$ given by

$$
\Psi(t, u, v)=[1-t+t f(u, v)] \Sigma(u, v)
$$

where $(u, v) \in E, 0 \leq t \leq 1$. For fixed $v$, the mapping $(t, u) \rightarrow \Psi(t, u, v)$ is a 2-sur-
face $\Phi$ to which $(c)$ can be applied to show that $\int_{\oplus} \zeta=0$. The same thing holds when $u$ is fixed. By $(a)$ and Stokes' theorem,

$$
\int_{\partial \Psi} \zeta=\int_{\Psi} d \zeta=0
$$

(e) Put $\lambda=-(z / r) \eta$, where

$$
\eta=\frac{x d y-y d x}{x^{2}+y^{2}}
$$

as in Exercise 21. Then $\lambda$ is a 1-form in the open set $V \subset R^{3}$ in which $x^{2}+y^{2}>0$. Show that $\zeta$ is exact in $V$ by showing that

$$
\zeta=d \lambda
$$

$(f)$ Derive $(d)$ from $(e)$, without using $(c)$.

Hint: To begin with, assume $0<u<\pi$ on $E$. By $(e)$,

$$
\int_{\Omega} \zeta=\int_{o \Omega} \lambda \quad \text { and } \quad \int_{s} \zeta=\int_{o s} \lambda
$$

Show that the two integrals of $\lambda$ are equal, by using part $(d)$ of Exercise 21, and by noting that $z / r$ is the same at $\Sigma(u, v)$ as at $\Omega(u, v)$.

(g) Is $\zeta$ exact in the complement of every line through the origin?

23. Fix $n$. Define $r_{k}=\left(x_{1}^{2}+\cdots+x_{k}^{2}\right)^{1 / 2}$ for $1 \leq k \leq n$, let $E_{k}$ be the set of all $\mathbf{x} \in R^{n}$ at which $r_{k}>0$, and let $\omega_{k}$ be the $(k-1)$-form defined in $E_{k}$ by

$$
\omega_{k}=\left(r_{k}\right)^{-k} \sum_{l=1}^{k}(-1)^{l-1} x_{l} d x_{1} \wedge \cdots \wedge d x_{l-1} \wedge d x_{l+1} \wedge \cdots \wedge d x_{k}
$$

Note that $\omega_{2}=\eta, \omega_{3}=\zeta$, in the terminology of Exercises 21 and 22. Note also that

$$
E_{1} \subset E_{2} \subset \cdots \subset E_{n}=R^{n}-\{0\} .
$$

(a) Prove that $d \omega_{k}=0$ in $E_{k}$.

(b) For $k=2, \ldots, n$, prove that $\omega_{k}$ is exact in $E_{k-1}$, by showing that

$$
\omega_{k}=d\left(f_{k} \omega_{k-1}\right)=\left(d f_{k}\right) \wedge \omega_{k-1},
$$

where $f_{k}(\mathbf{x})=(-1)^{k} g_{k}\left(x_{k} / r_{k}\right)$ and

$$
g_{k}(t)=\int_{-1}^{t}\left(1-s^{2}\right)^{(k-3) / 2} d s \quad(-1<t<1)
$$

Hint: $f_{k}$ satisfies the differential equations

and

$$
\mathbf{x} \cdot\left(\nabla f_{k}\right)(\mathbf{x})=0
$$

$$
\left(D_{k} f_{k}\right)(\mathbf{x})=\frac{(-1)^{k}\left(r_{k-1}\right)^{k-1}}{\left(r_{k}\right)^{k}}
$$

(c) Is $\omega_{n}$ exact in $E_{n}$ ?

$(d)$ Note that $(b)$ is a generalization of part $(e)$ of Exercise 22. Try to extend some of the other assertions of Exercises 21 and 22 to $\omega_{n}$, for arbitrary $n$.

24. Let $\omega=\Sigma a_{l}(\mathbf{x}) d x_{l}$ be a 1-form of class ' $C$ " in a convex open set $E \subset R^{n}$. Assume $d \omega=0$ and prove that $\omega$ is exact in $E$, by completing the following outline:

Fix $\mathbf{p} \in E$. Define

$$
f(\mathbf{x})=\int_{[\mathbf{p}, \mathbf{x}]} \omega \quad(\mathbf{x} \in E) .
$$

Apply Stokes' theorem to affine-oriented 2-simplexes $[\mathbf{p}, \mathbf{x}, \mathbf{y}]$ in $E$. Deduce that

$$
f(\mathbf{y})-f(\mathbf{x})=\sum_{l=1}^{n}\left(y_{l}-x_{i}\right) \int_{0}^{1} a_{l}((1-t) \mathbf{x}+t \mathbf{y}) d t
$$

for $\mathbf{x} \in E, \mathbf{y} \in E$. Hence $\left(D_{l} f\right)(\mathbf{x})=a_{l}(\mathbf{x})$.

25. Assume that $\omega$ is a 1 -form in an open set $E \subset R^{n}$ such that

$$
\int_{y} \omega=0
$$

for every closed curve $\gamma$ in $E$, of class $\mathscr{C}^{\prime}$. Prove that $\omega$ is exact in $E$, by imitating part of the argument sketched in Exercise 24.

26. Assume $\omega$ is a 1 -form in $R^{3}-\{0\}$, of class $\mathscr{C}^{\prime}$ and $d \omega=0$. Prove that $\omega$ is exact in $R^{3}-\{0\}$.

Hint: Every closed continuously differentiable curve in $R^{3}-\{0\}$ is the boundary of a 2-surface in $\boldsymbol{R}^{3}-\{0\}$. Apply Stokes' theorem and Exercise 25.

27. Let $E$ be an open 3-cell in $R^{3}$, with edges parallel to the coordinate axes. Suppose $(a, b, c) \in E, f_{l} \in \mathscr{C}^{\prime}(E)$ for $i=1,2,3$,

$$
\omega=f_{1} d y \wedge d z+f_{2} d z \wedge d x+f_{3} d x \wedge d y
$$

and assume that $d \omega=0$ in $E$. Define

$$
\lambda=g_{1} d x+g_{2} d y
$$

where

$$
\begin{aligned}
& g_{1}(x, y, z)=\int_{c}^{z} f_{2}(x, y, s) d s-\int_{b}^{y} f_{3}(x, t, c) d t \\
& g_{2}(x, y, z)=-\int_{c}^{z} f_{1}(x, y, s) d s,
\end{aligned}
$$

for $(x, y, z) \in E$. Prove that $d \lambda=\omega$ in $E$.

Evaluate these integrals when $\omega=\zeta$ and thus find the form $\lambda$ that occurs in part $(e)$ of Exercise 22.

28. Fix $b>a>0$, define

$$
\Phi(r, \theta)=(r \cos \theta, r \sin \theta)
$$

for $a \leq r \leq b, 0 \leq \theta \leq 2 \pi$. (The range of $\Phi$ is an annulus in $R^{2}$.) Put $\omega=x^{3} d y$, and compute both

$$
\int_{\infty} d \omega \text { and } \int_{\infty} \omega
$$

to verify that they are equal.

29. Prove the existence of a function $\alpha$ with the properties needed in the proof of Theorem 10.38, and prove that the resulting function $F$ is of class $\mathscr{C}^{\prime}$. (Both assertions become trivial if $E$ is an open cell or an open ball, since $\alpha$ can then be taken to be a constant. Refer to Theorem 9.42.)
30. If $\mathbf{N}$ is the vector given by (135), prove that

$$
\operatorname{det}\left[\begin{array}{lll}
\alpha_{1} & \beta_{1} & \alpha_{2} \beta_{3}-\alpha_{3} \beta_{2} \\
\alpha_{2} & \beta_{2} & \alpha_{3} \beta_{1}-\alpha_{1} \beta_{3} \\
\alpha_{3} & \beta_{3} & \alpha_{1} \beta_{2}-\alpha_{2} \beta_{1}
\end{array}\right]=|\mathbf{N}|^{2}
$$

Also, verify Eq. (137).

31. Let $E \subset R^{3}$ be open, suppose $g \in \mathscr{C}^{\prime \prime}(E), h \in \mathscr{C}^{\prime \prime}(E)$, and consider the vector field

$$
\mathbf{F}=g \nabla h .
$$

(a) Prove that

$$
\nabla \cdot F=g \nabla^{2} h+(\nabla g) \cdot(\nabla h)
$$

where $\nabla^{2} h=\nabla \cdot(\nabla h)=\Sigma \partial^{2} h / \partial x_{l}^{2}$ is the so-called "Laplacian" of $h$.

(b) If $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial \Omega$ (as in Theorem 10.51), prove that

$$
\int_{\Omega}\left[g \nabla^{2} h+(\nabla g) \cdot(\nabla h)\right] d V=\int_{o \Omega} g \frac{\partial h}{\partial n} d A
$$

where (as is customary) we have written $\partial h / \partial n$ in place of $(\nabla h) \cdot \mathbf{n}$. (Thus $\partial h / \partial n$ is the directional derivative of $h$ in the direction of the outward normal to $\partial \Omega$, the so-called normal derivative of $h$.) Interchange $g$ and $h$, subtract the resulting formula from the first one, to obtain

$$
\int_{\Omega}\left(g \nabla^{2} h-h \nabla^{2} g\right) d V=\int_{o \Omega}\left(g \frac{\partial h}{\partial n}-h \frac{d g}{\partial n}\right) d A
$$

These two formulas are usually called Green's identities.

(c) Assume that $h$ is harmonic in $E$; this means that $\nabla^{2} h=0$. Take $g=1$ and conclude that

$$
\int_{0 \Omega} \frac{\partial h}{\partial n} d A=0
$$

Take $g=h$, and conclude that $h=0$ in $\Omega$ if $h=0$ on $\partial \Omega$.

$(d)$ Show that Green's identities are also valid in $R^{2}$.

32. Fix $\delta, 0<\delta<1$. Let $D$ be the set of all $(\theta, t) \in R^{2}$ such that $0 \leq \theta \leq \pi,-\delta \leq t \leq \delta$. Let $\Phi$ be the 2 -surface in $R^{3}$, with parameter domain $D$, given by

$$
\begin{aligned}
& x=(1-t \sin \theta) \cos 2 \theta \\
& y=(1-t \sin \theta) \sin 2 \theta \\
& z=t \cos \theta
\end{aligned}
$$

where $(x, y, z)=\Phi(\theta, t)$. Note that $\Phi(\pi, t)=\Phi(0,-t)$, and that $\Phi$ is one-to-one on the rest of $D$.

The range $M=\Phi(D)$ of $\Phi$ is known as a M√∂bius band. It is the simplest example of a nonorientable surface.

Prove the various assertions made in the following description: Put $\mathbf{p}_{1}=(0,-\delta), \mathbf{p}_{2}=(\pi,-\delta), \mathbf{p}_{3}=(\pi, \delta), \mathbf{p}_{4}=(0, \delta), \mathbf{p}_{5}=\mathbf{p}_{1}$. Put $\gamma_{\mathbf{t}}=\left[\mathbf{p}_{i}, \mathbf{p}_{t+1}\right]$, $i=1, \ldots, 4$, and put $\Gamma_{t}=\Phi \circ \gamma_{t}$. Then

$$
\partial \Phi=\Gamma_{1}+\Gamma_{2}+\Gamma_{3}+\Gamma_{4} .
$$

Put $\mathbf{a}=(1,0,-\delta), \mathbf{b}=(1,0, \delta)$. Then

$$
\Phi\left(\mathbf{p}_{1}\right)=\Phi\left(\mathbf{p}_{3}\right)=\mathbf{a}, \quad \Phi\left(\mathbf{p}_{2}\right)=\Phi\left(\mathbf{p}_{4}\right)=\mathbf{b}
$$

and $\partial \Phi$ can be described as follows.

$\Gamma_{1}$ spirals up from $\mathbf{a}$ to $\mathbf{b}$; its projection into the $(x, y)$-plane has winding number +1 around the origin. (See Exercise 23, Chap. 8.)

$\Gamma_{2}=[\mathbf{b}, \mathbf{a}]$.

$\Gamma_{3}$ spirals up from $\mathbf{a}$ to $\mathbf{b}$; its projection into the $(x, y)$ plane has winding number -1 around the origin.

$\Gamma_{4}=[b, a]$.

Thus $\partial \Phi=\Gamma_{1}+\Gamma_{3}+2 \Gamma_{2}$.

If we go from $\mathbf{a}$ to $\mathbf{b}$ along $\Gamma_{1}$ and continue along the "edge" of $M$ until we return to a, the curve traced out is

$$
\Gamma=\Gamma_{1}-\Gamma_{3},
$$

which may also be represented on the parameter interval $[0,2 \pi]$ by the equations

$$
\begin{aligned}
& x=(1+\delta \sin \theta) \cos 2 \theta \\
& y=(1+\delta \sin \theta) \sin 2 \theta \\
& z=-\delta \cos \theta
\end{aligned}
$$

It should be emphasized that $\Gamma \neq \partial \Phi:$ Let $\eta$ be the 1-form discussed in Exercises 21 and 22. Since $d \eta=0$, Stokes' theorem shows that

$$
\int_{0 \infty} \eta=0 .
$$

But although $\Gamma$ is the "geometric" boundary of $M$, we have

$$
\int_{\Gamma} \eta=4 \pi .
$$

In order to avoid this possible source of confusion, Stokes' formula (Theorem 10.50 ) is frequently stated only for orientable surfaces $\Phi$.

## 11

## THE LEBESGUE THEORY

It is the purpose of this chapter to present the fundamental concepts of the Lebesgue theory of measure and integration and to prove some of the crucial theorems in a rather general setting, without obscuring the main lines of the development by a mass of comparatively trivial detail. Therefore proofs are only sketched in some cases, and some of the easier propositions are stated without proof. However, the reader who has become familiar with the techniques used in the preceding chapters will certainly find no difficulty in supplying the missing steps.

The theory of the Lebesgue integral can be developed in several distinct ways. Only one of these methods will be discussed here. For alternative procedures we refer to the more specialized treatises on integration listed in the Bibliography.

## SET FUNCTIONS

If $A$ and $B$ are any two sets, we write $A-B$ for the set of all elements $x$ such that $x \in A, x \notin B$. The notation $A-B$ does not imply that $B \subset A$. We denote the empty set by 0 , and say that $A$ and $B$ are disjoint if $A \cap B=0$.

11.1 Definition A family $\mathscr{R}$ of sets is called a ring if $A \in \mathscr{R}$ and $B \in \mathscr{R}$ implies

$$
A \cup B \in \mathscr{R}, \quad A-B \in \mathscr{R} .
$$

Since $A \cap B=A-(A-B)$, we also have $A \cap B \in \mathscr{R}$ if $\mathscr{R}$ is a ring.

A ring $\mathscr{R}$ is called a $\sigma$-ring if

$$
\bigcup_{n=1}^{\infty} A_{n} \in \mathscr{R}
$$

whenever $A_{n} \in \mathscr{R}(n=1,2,3, \ldots)$. Since

$$
\bigcap_{n=1}^{\infty} A_{n}=A_{1}-\bigcup_{n=1}^{\infty}\left(A_{1}-A_{n}\right)
$$

we also have

$$
\bigcap_{n=1}^{\infty} A_{n} \in \mathscr{R}
$$

if $\mathscr{R}$ is a $\sigma$-ring.

11.2 Definition We say that $\phi$ is a set function defined on $\mathscr{R}$ if $\phi$ assigns to every $A \in \mathscr{R}$ a number $\phi(A)$ of the extended real number system. $\phi$ is additive if $A \cap B=0$ implies

$$
\phi(A \cup B)=\phi(A)+\phi(B)
$$

and $\phi$ is countably additive if $A_{i} \cap A_{j}=0(i \neq j)$ implies

$$
\phi\left(\bigcup_{n=1}^{\infty} A_{n}\right)=\sum_{n=1}^{\infty} \phi\left(A_{n}\right)
$$

We shall always assume that the range of $\phi$ does not contain both $+\infty$ and $-\infty$; for if it did, the right side of (3) could become meaningless. Also, we exclude set functions whose only value is $+\infty$ or $-\infty$.

It is interesting to note that the left side of (4) is independent of the order in which the $A_{n}$ 's are arranged. Hence the rearrangement theorem shows that the right side of (4) converges absolutely if it converges at all; if it does not converge, the partial sums tend to $+\infty$, or to $-\infty$.

If $\phi$ is additive, the following properties are easily verified:

$$
\begin{aligned}
\phi(0) & =0 . \\
\phi\left(A_{1} \cup \cdots \cup A_{n}\right) & =\phi\left(A_{1}\right)+\cdots+\phi\left(A_{n}\right)
\end{aligned}
$$

if $A_{i} \cap A_{j}=0$ whenever $i \neq j$.

$$
\phi\left(A_{1} \cup A_{2}\right)+\phi\left(A_{1} \cap A_{2}\right)=\phi\left(A_{1}\right)+\phi\left(A_{2}\right) .
$$

If $\phi(A) \geq 0$ for all $A$, and $A_{1} \subset A_{2}$, then

$$
\phi\left(A_{1}\right) \leq \phi\left(A_{2}\right)
$$

Because of (8), nonnegative additive set functions are often called monotonic.

$$
\phi(A-B)=\phi(A)-\phi(B)
$$

if $B \subset A$, and $|(\phi B)|<+\infty$.

11.3 Theorem Suppose $\phi$ is countably additive on a ring $\mathscr{R}$. Suppose $A_{n} \in \mathscr{R}$ $(n=1,2,3, \ldots), A_{1} \subset A_{2} \subset A_{3} \subset \cdots, A \in \mathscr{R}$, and

$$
A=\bigcup_{n=1}^{\infty} A_{n}
$$

Then, as $n \rightarrow \infty$,

$$
\phi\left(A_{n}\right) \rightarrow \phi(A)
$$

Proof Put $B_{1}=A_{1}$, and

$$
B_{n}=A_{n}-A_{n-1} \quad(n=2,3, \ldots)
$$

Then $B_{i} \cap B_{j}=0$ for $i \neq j, A_{n}=B_{1} \cup \cdots \cup B_{n}$, and $A=\cup B_{n}$. Hence

$$
\phi\left(A_{n}\right)=\sum_{i=1}^{n} \phi\left(B_{i}\right)
$$

and

$$
\phi(A)=\sum_{i=1}^{\infty} \phi\left(B_{i}\right)
$$

## CONSTRUCTION OF THE LEBESGUE MEASURE

11.4 Definition Let $R^{p}$ denote $p$-dimensional euclidean space. By an interval in $R^{p}$ we mean the set of points $\mathbf{x}=\left(x_{1}, \ldots, x_{p}\right)$ such that

$$
a_{i} \leq x_{i} \leq b_{i} \quad(i=1, \ldots, p)
$$

or the set of points which is characterized by (10) with any or all of the $\leq$ signs replaced by $<$. The possibility that $a_{i}=b_{i}$ for any value of $i$ is not ruled out; in particular, the empty set is included among the intervals.

If $A$ is the union of a finite number of intervals, $A$ is said to be an elementary set.

If $I$ is an interval, we define

$$
m(I)=\prod_{i=1}^{p}\left(b_{i}-a_{i}\right)
$$

no matter whether equality is included or excluded in any of the inequalities (10). If $A=I_{1} \cup \cdots \cup I_{n}$, and if these intervals are pairwise disjoint, we set

$$
m(A)=m\left(I_{1}\right)+\cdots+m\left(I_{n}\right)
$$

We let $\mathscr{E}$ denote the family of all elementary subsets of $R^{p}$.

At this point, the following properties should be verified:

(12) $\mathscr{E}$ is a ring, but not a $\sigma$-ring.

(13) If $A \in \mathscr{E}$, then $A$ is the union of a finite number of disjoint intervals.

(14) If $A \in \mathscr{E}, m(A)$ is well defined by (11); that is, if two different decompositions of $A$ into disjoint intervals are used, each gives rise to the same value of $m(A)$.

(15) $m$ is additive on $\mathscr{E}$.

Note that if $p=1,2,3$, then $m$ is length, area, and volume, respectively.

11.5 Definition A nonnegative additive set function $\phi$ defined on $\mathscr{E}$ is said to be regular if the following is true: To every $A \in \mathscr{E}$ and to every $\varepsilon>0$ there exist sets $F \in \mathscr{E}, G \in \mathscr{E}$ such that $F$ is closed, $G$ is open, $F \subset A \subset G$, and

$$
\phi(G)-\varepsilon \leq \phi(A) \leq \phi(F)+\varepsilon
$$

### 11.6 Examples

(a) The set function $m$ is regular.

If $A$ is an interval, it is trivial that the requirements of Definition 11.5 are satisfied. The general case follows from (13).

(b) Take $R^{p}=R^{1}$, and let $\alpha$ be a monotonically increasing function, defined for all real $x$. Put

$$
\begin{aligned}
& \mu([a, b))=\alpha(b-)-\alpha(a-), \\
& \mu([a, b])=\alpha(b+)-\alpha(a-), \\
& \mu((a, b])=\alpha(b+)-\alpha(a+), \\
& \mu((a, b))=\alpha(b-)-\alpha(a+) .
\end{aligned}
$$

Here $[a, b)$ is the set $a \leq x<b$, etc. Because of the possible discontinuities of $\alpha$, these cases have to be distinguished. If $\mu$ is defined for
elementary sets as in (11), $\mu$ is regular on $\mathscr{E}$. The proof is just like that of $(a)$.

Our next objective is to show that every regular set function on $\mathscr{E}$ can be extended to a countably additive set function on a $\sigma$-ring which contains $\mathscr{E}$.

11.7 Definition Let $\mu$ be additive, regular, nonnegative, and finite on $\mathscr{E}$. Consider countable coverings of any set $E \subset R^{p}$ by open elementary sets $A_{n}$ :

$$
E \subset \bigcup_{n=1}^{\infty} A_{n}
$$

Define

$$
\mu^{*}(E)=\inf \sum_{n=1}^{\infty} \mu\left(A_{n}\right)
$$

the inf being taken over all countable coverings of $E$ by open elementary sets. $\mu^{*}(E)$ is called the outer measure of $E$, corresponding to $\mu$.

It is clear that $\mu^{*}(E) \geq 0$ for all $E$ and that

$$
\mu^{*}\left(E_{1}\right) \leq \mu^{*}\left(E_{2}\right)
$$

if $E_{1} \subset E_{2}$.

### 11.8 Theorem

(a) For every $A \in \mathscr{E}, \mu^{*}(A)=\mu(A)$.

(b) If $E=\bigcup_{1}^{\infty} E_{n}$, then

$$
\mu^{*}(E) \leq \sum_{n=1}^{\infty} \mu^{*}\left(E_{n}\right) .
$$

Note that $(a)$ asserts that $\mu^{*}$ is an extension of $\mu$ from $\mathscr{E}$ to the family of all subsets of $R^{p}$. The property (19) is called subadditivity.

Proof Choose $A \in \mathscr{E}$ and $\varepsilon>0$.

The regularity of $\mu$ shows that $A$ is contained in an open elementary set $G$ such that $\mu(G) \leq \mu(A)+\varepsilon$. Since $\mu^{*}(A) \leq \mu(G)$ and since $\varepsilon$ was arbitrary, we have

$$
\mu^{*}(A) \leq \mu(A)
$$

The definition of $\mu^{*}$ shows that there is a sequence $\left\{A_{n}\right\}$ of open elementary sets whose union contains $A$, such that

$$
\sum_{n=1}^{\infty} \mu\left(A_{n}\right) \leq \mu^{*}(A)+\varepsilon .
$$

The regularity of $\mu$ shows that $A$ contains a closed elementary set $F$ such that $\mu(F) \geq \mu(A)-\varepsilon$; and since $F$ is compact, we have

$$
F \subset A_{1} \cup \cdots \cup A_{N}
$$

for some $N$. Hence

$$
\mu(A) \leq \mu(F)+\varepsilon \leq \mu\left(A_{1} \cup \cdots \cup A_{N}\right)+\varepsilon \leq \sum_{1}^{N} \mu\left(A_{n}\right)+\varepsilon \leq \mu^{*}(A)+2 \varepsilon .
$$

In conjunction with (20), this proves $(a)$.

Next, suppose $E=\bigcup E_{n}$, and assume that $\mu^{*}\left(E_{n}\right)<+\infty$ for all $n$. Given $\varepsilon>0$, there are coverings $\left\{A_{n k}\right\}, k=1,2,3, \ldots$, of $E_{n}$ by open elementary sets such that

$$
\sum_{k=1}^{\infty} \mu\left(A_{n k}\right) \leq \mu^{*}\left(E_{n}\right)+2^{-n} \varepsilon
$$

Then

$$
\mu^{*}(E) \leq \sum_{n=1}^{\infty} \sum_{k=1}^{\infty} \mu\left(A_{n k}\right) \leq \sum_{n=1}^{\infty} \mu^{*}\left(E_{n}\right)+\varepsilon
$$

and (19) follows. In the excluded case, i.e., if $\mu^{*}\left(E_{n}\right)=+\infty$ for some $n$, (19) is of course trivial.

11.9 Definition For any $A \subset R^{p}, B \subset R^{p}$, we define

$$
\begin{aligned}
S(A, B) & =(A-B) \cup(B-A) \\
d(A, B) & =\mu^{*}(S(A, B))
\end{aligned}
$$

We write $A_{n} \rightarrow A$ if

$$
\lim _{n \rightarrow \infty} d\left(A, A_{n}\right)=0
$$

If there is a sequence $\left\{A_{n}\right\}$ of elementary sets such that $A_{n} \rightarrow A$, we say that $A$ is finitely $\mu$-measurable and write $A \in \mathfrak{M i}_{F}(\mu)$.

If $A$ is the union of a countable collection of finitely $\mu$-measurable sets, we say that $A$ is $\mu$-measurable and write $A \in \mathfrak{M}(\mu)$.

$S(A, B)$ is the so-called "symmetric difference" of $A$ and $B$. We shall see that $d(A, B)$ is essentially a distance function.

The following theorem will enable us to obtain the desired extension of $\mu$.

11.10 Theorem $\mathfrak{M}(\mu)$ is a $\sigma$-ring, and $\mu^{*}$ is countably additive on $\mathfrak{M}(\mu)$.

Before we turn to the proof of this theorem, we develop some of the properties of $S(A, B)$ and $d(A, B)$. We have

$$
\begin{gathered}
S(A, B)=S(B, A), \quad S(A, A)=0 \\
S(A, B) \subset S(A, C) \cup S(C, B)
\end{gathered}
$$

$$
\left.\begin{array}{l}
S\left(A_{1} \cup A_{2}, B_{1} \cup B_{2}\right) \\
S\left(A_{1} \cap A_{2}, B_{1} \cap B_{2}\right) \\
S\left(A_{1}-A_{2}, B_{1}-B_{2}\right)
\end{array}\right\} \subset S\left(A_{1}, B_{1}\right) \cup S\left(A_{2}, B_{2}\right) .
$$

(24) is clear, and (25) follows from

$$
(A-B) \subset(A-C) \cup(C-B), \quad(B-A) \subset(C-A) \cup(B-C)
$$

The first formula of (26) is obtained from

$$
\left(A_{1} \cup A_{2}\right)-\left(B_{1} \cup B_{2}\right) \subset\left(A_{1}-B_{1}\right) \cup\left(A_{2}-B_{2}\right)
$$

Next, writing $E^{c}$ for the complement of $E$, we have

$$
\begin{aligned}
S\left(A_{1} \cap A_{2}, B_{1} \cap B_{2}\right) & =S\left(A_{1}^{c} \cup A_{2}^{c}, B_{1}^{c} \cup B_{2}^{c}\right) \\
& \subset S\left(A_{1}^{c}, B_{1}^{c}\right) \cup S\left(A_{2}^{c}, B_{2}^{c}\right)=S\left(A_{1}, B_{1}\right) \cup S\left(A_{2}, B_{2}\right)
\end{aligned}
$$

and the last formula of (26) is obtained if we note that

$$
A_{1}-A_{2}=A_{1} \cap A_{2}^{c} .
$$

By (23), (19), and (18), these properties of $S(A, B)$ imply

$$
\begin{aligned}
& d(A, B)=d(B, A), \quad d(A, A)=0, \\
& d(A, B) \leq d(A, C)+d(C, B) \\
& d\left(A_{1} \cap A_{2}, B_{1} \cap B_{2}\right) \leq d\left(A_{1}, B_{1}\right)+d\left(A_{2}, B_{2}\right) .
\end{aligned}
$$

$$
\left.\begin{array}{l}
d\left(A_{1} \cup A_{2}, B_{1} \cup B_{2}\right) \\
d\left(A_{1} \cap A_{2}, B_{1} \cap B_{2}\right) \\
d\left(A_{1}-A_{2}, B_{1}-B_{2}\right)
\end{array}\right\}
$$

The relations (27) and (28) show that $d(A, B)$ satisfies the requirements of Definition 2.15, except that $d(A, B)=0$ does not imply $A=B$. For instance, if $\mu=m, A$ is countable, and $B$ is empty, we have

$$
d(A, B)=m^{*}(A)=0
$$

to see this, cover the $n$th point of $A$ by an interval $I_{n}$ such that

$$
m\left(I_{n}\right)<2^{-n} \varepsilon .
$$

But if we define two sets $A$ and $B$ to be equivalent, provided

$$
d(A, B)=0
$$

we divide the subsets of $R^{p}$ into equivalence classes, and $d(A, B)$ makes the set of these equivalence classes into a metric space. $\mathfrak{M}_{F}(\mu)$ is then obtained as the closure of $\mathscr{E}$. This interpretation is not essential for the proof, but it explains the underlying idea.

We need one more property of $d(A, B)$, namely,

$$
\left|\mu^{*}(A)-\mu^{*}(B)\right| \leq d(A, B)
$$

if at least one of $\mu^{*}(A), \mu^{*}(B)$ is finite. For suppose $0 \leq \mu^{*}(B) \leq \mu^{*}(A)$. Then (28) shows that

$$
d(A, 0) \leq d(A, B)+d(B, 0)
$$

that is,

$$
\mu^{*}(A) \leq d(A, B)+\mu^{*}(B)
$$

Since $\mu^{*}(B)$ is finite, it follows that

$$
\mu^{*}(A)-\mu^{*}(B) \leq d(A, B)
$$

Proof of Theorem 11.10 Suppose $A \in \mathfrak{M}_{F}(\mu), B \in \mathfrak{M}_{F}(\mu)$. Choose $\left\{A_{n}\right\}$, $\left\{B_{n}\right\}$ such that $A_{n} \in \mathscr{E} . B_{n} \in \mathscr{E}, A_{n} \rightarrow A, B_{n} \rightarrow B$. Then (29) and (30) show that

$$
\begin{gathered}
A_{n} \cup B_{n} \rightarrow A \cup B, \\
A_{n} \cap B_{n} \rightarrow A \cap B, \\
A_{n}-B_{n} \rightarrow A-B, \\
\mu^{*}\left(A_{n}\right) \rightarrow \mu^{*}(A),
\end{gathered}
$$

and $\mu^{*}(A)<+\infty$ since $d\left(A_{n}, A\right) \rightarrow 0$. By (31) and (33), $\mathfrak{M}_{F}(\mu)$ is a ring. By (7),

$$
\mu\left(A_{n}\right)+\mu\left(B_{n}\right)=\mu\left(A_{n} \cup B_{n}\right)+\mu\left(A_{n} \cap B_{n}\right) .
$$

Letting $n \rightarrow \infty$, we obtain, by (34) and Theorem 11.8(a),

$$
\mu^{*}(A)+\mu^{*}(B)=\mu^{*}(A \cup B)+\mu^{*}(A \cap B)
$$

If $A \cap B=0$, then $\mu^{*}(A \cap B)=0$.

It follows that $\mu^{*}$ is additive on $\mathfrak{M}_{F}(\mu)$.

Now let $A \in \mathfrak{M}(\mu)$. Then $A$ can be represented as the union of a countable collection of disjoint sets of $\mathfrak{M}_{F}(\mu)$. For if $A=\bigcup A_{n}^{\prime}$ with $A_{n}^{\prime} \in \mathfrak{M}_{\mathrm{F}}(\mu)$, write $A_{1}=A_{1}^{\prime}$, and

$$
A_{n}=\left(A_{1}^{\prime} \cup \cdots \cup A_{n}^{\prime}\right)-\left(A_{n}^{\prime} \cup \cdots \cup A_{n-1}^{\prime}\right) \quad(n=2,3,4, \ldots)
$$

Then

$$
A=\bigcup_{n=1}^{\infty} A_{n}
$$

is the required representation. By (19)

$$
\mu^{*}(A) \leq \sum_{n=1}^{\infty} \mu^{*}\left(A_{n}\right)
$$

On the other hand, $A \supset A_{1} \cup \cdots \cup A_{n}$; and by the additivity of $\mu^{*}$ on $\mathfrak{M}_{F}(\mu)$ we obtain

$$
\mu^{*}(A) \geq \mu^{*}\left(A_{1} \cup \cdots \cup A_{n}\right)=\mu^{*}\left(A_{1}\right)+\cdots+\mu^{*}\left(A_{n}\right)
$$

Equations (36) and (37) imply

$$
\mu^{*}(A)=\sum_{n=1}^{\infty} \mu^{*}\left(A_{n}\right)
$$

Suppose $\mu^{*}(A)$ is finite. Put $B_{n}=A_{1} \cup \cdots \cup A_{n}$. Then (38) shows that

$$
d\left(A, B_{n}\right)=\mu^{*}\left(\bigcup_{i=n+1}^{\infty} A_{i}\right)=\sum_{i=n+1}^{\infty} \mu^{*}\left(A_{i}\right) \rightarrow 0
$$

as $n \rightarrow \infty$. Hence $B_{n} \rightarrow A$; and since $B_{n} \in \mathfrak{M}_{F}(\mu)$, it is easily seen that $A \in \mathfrak{M}_{F}(\mu)$.

We have thus shown that $A \in \mathfrak{M}_{F}(\mu)$ if $A \in \mathfrak{M}(\mu)$ and $\mu^{*}(A)<+\infty$. It is now clear that $\mu^{*}$ is countably additive on $\mathfrak{M}(\mu)$. For if

$$
A=\bigcup A_{n},
$$

where $\left\{A_{n}\right\}$ is a sequence of disjoint sets of $\mathfrak{M}(\mu)$, we have shown that (38) holds if $\mu^{*}\left(A_{n}\right)<+\infty$ for every $n$, and in the other case (38) is trivial.

Finally, we have to show that $\mathfrak{M}(\mu)$ is a $\sigma$-ring. If $A_{n} \in \mathfrak{M}(\mu), n=1$, $2,3, \ldots$, it is clear that $\bigcup A_{n} \in \mathfrak{M}(\mu)$ (Theorem 2.12). Suppose $A \in \mathfrak{M}(\mu)$, $B \in \mathfrak{M}(\mu)$, and

$$
A=\bigcup_{n=1}^{\infty} A_{n}, \quad B=\bigcup_{n=1}^{\infty} B_{n}
$$

where $A_{n}, B_{n} \in \mathfrak{M}_{F}(\mu)$. Then the identity

$$
A_{n} \cap B=\bigcup_{i=1}^{\infty}\left(A_{n} \cap B_{i}\right)
$$

shows that $A_{n} \cap B \in \mathfrak{M}(\mu)$; and since

$$
\mu^{*}\left(A_{n} \cap B\right) \leq \mu^{*}\left(A_{n}\right)<+\infty,
$$

$A_{n} \cap B \in \mathfrak{M}_{F}(\mu)$. Hence $\quad A_{n}-B \in \mathfrak{M}_{F}(\mu)$, and $A-B \in \mathfrak{M}(\mu) \quad$ since $A-B=\bigcup_{n=1}^{\infty}\left(A_{n}-B\right)$.

We now replace $\mu^{*}(A)$ by $\mu(A)$ if $A \in \mathfrak{M}(\mu)$. Thus $\mu$, originally only defined on $\mathscr{E}$, is extended to a countably additive set function on the $\sigma$-ring $\mathfrak{M}(\mu)$. This extended set function is called a measure. The special case $\mu=m$ is called the Lebesgue measure on $R^{p}$.

### 11.11 Remarks

(a) If $A$ is open, then $A \in \mathfrak{M}(\mu)$. For every open set in $R^{p}$ is the union of a countable collection of open intervals. To see this, it is sufficient to construct a countable base whose members are open intervals.

By taking complements, it follows that every closed set is in $\mathfrak{M}(\mu)$.

(b) If $A \in \mathfrak{M}(\mu)$ and $\varepsilon>0$, there exist sets $F$ and $G$ such that

$$
F \subset A \subset G
$$

$F$ is closed, $G$ is open, and

$$
\mu(G-A)<\varepsilon, \quad \mu(A-F)<\varepsilon .
$$

The first inequality holds since $\mu^{*}$ was defined by means of coverings by open elementary sets. The second inequality then follows by taking complements.

(c) We say that $E$ is a Borel set if $E$ can be obtained by a countable number of operations, starting from open sets, each operation consisting in taking unions, intersections, or complements. The collection $\mathscr{B}$ of all Borel sets in $R^{p}$ is a $\sigma$-ring; in fact, it is the smallest $\sigma$-ring which contains all open sets. By Remark $(a), E \in \mathfrak{M}(\mu)$ if $E \in \mathscr{B}$.

(d) If $A \in \mathfrak{M}(\mu)$, there exist Borel sets $F$ and $G$ such that $F \subset A \subset G$, and

$$
\mu(G-A)=\mu(A-F)=0
$$

This follows from $(b)$ if we take $\varepsilon=1 / n$ and let $n \rightarrow \infty$.

Since $A=F \cup(A-F)$, we see that every $A \in \mathfrak{M}(\mu)$ is the union of a Borel set and a set of measure zero.

The Borel sets are $\mu$-measurable for every $\mu$. But the sets of measure zero [that is, the sets $E$ for which $\mu^{*}(E)=0$ ] may be different for different $\mu$ 's.

(e) For every $\mu$, the sets of measure zero form a $\sigma$-ring.

$(f)$ In case of the Lebesgue measure, every countable set has measure zero. But there are uncountable (in fact, perfect) sets of measure zero. The Cantor set may be taken as an example: Using the notation of Sec. 2.44 , it is easily seen that

$$
m\left(E_{n}\right)=\left(\frac{2}{3}\right)^{n} \quad(n=1,2,3, \ldots)
$$

and since $P=\bigcap E_{n}, P \subset E_{n}$ for every $n$, so that $m(P)=0$.

## MEASURE SPACES

11.12 Definition Suppose $X$ is a set, not necessarily a subset of a euclidean space, or indeed of any metric space. $X$ is said to be a measure space if there exists a $\sigma$-ring $\mathfrak{M}$ of subsets of $X$ (which are called measurable sets) and a nonnegative countably additive set function $\mu$ (which is called a measure), defined on $\mathfrak{P}$.

If, in addition, $X \in \mathfrak{M}$, then $X$ is said to be a measurable space.

For instance, we can take $X=R^{p}, \mathfrak{M}$ the collection of all Lebesguemeasurable subsets of $R^{p}$, and $\mu$ Lebesgue measure.

Or, let $X$ be the set of all positive integers, $\mathfrak{M}$ the collection of all subsets of $X$, and $\mu(E)$ the number of elements of $E$.

Another example is provided by probability theory, where events may be considered as sets, and the probability of the occurrence of events is an additive (or countably additive) set function.

In the following sections we shall always deal with measurable spaces. It should be emphasized that the integration theory which we shall soon discuss would not become simpler in any respect if we sacrificed the generality we have now attained and restricted ourselves to Lebesgue measure, say, on an interval of the real line. In fact, the essential features of the theory are brought out with much greater clarity in the more general situation, where it is seen that everything depends only on the countable additivity of $\mu$ on a $\sigma$-ring.

It will be convenient to introduce the notation

$$
\{x \mid P\}
$$

for the set of all elements $x$ which have the property $P$.

## MEASURABLE FUNCTIONS

11.13 Definition Let $f$ be a function defined on the measurable space $~ X X$, with values in the extended real number system. The function $f$ is said to be measurable if the set

$$
\{x \mid f(x)>a\}
$$

is measurable for every real $a$.

11.14 Example If $X=R^{p}$ and $\mathfrak{M}=\mathfrak{M}(\mu)$ as defined in Definition 11.9, every continuous $f$ is measurable, since then (42) is an open set.

11.15 Theorem Each of the following four conditions implies the other three: $\{x \mid f(x)>a\}$ is measurable for every real a. $\{x \mid f(x) \geq a\}$ is measurable for every real a. $\{x \mid f(x)<a\}$ is measurable for every real a. $\{x \mid f(x) \leq a\}$ is measurable for every real a.

Proof The relations

$$
\begin{aligned}
& \{x \mid f(x) \geq a\}=\bigcap_{n=1}^{\infty}\left\{x \mid f(x)>a-\frac{1}{n}\right\}, \\
& \{x \mid f(x)<a\}=X-\{x \mid f(x) \geq a\}, \\
& \{x \mid f(x) \leq a\}=\bigcap_{n=1}^{\infty}\left\{x \mid f(x)<a+\frac{1}{n}\right\}, \\
& \{x \mid f(x)>a\}=X-\{x \mid f(x) \leq a\}
\end{aligned}
$$

show successively that (43) implies (44), (44) implies (45), (45) implies (46), and (46) implies (43).

Hence any of these conditions may be used instead of (42) to define measurability.

11.16 Theorem If $f$ is measurable, then $|f|$ is measurable.

Proof

$$
\{x|| f(x) \mid<a\}=\{x \mid f(x)<a\} \cap\{x \mid f(x)>-a\} .
$$

11.17 Theorem Let $\left\{f_{n}\right\}$ be a sequence of measurable functions. For $x \in X$, put

$$
\begin{aligned}
& g(x)=\sup f_{n}(x) \quad(n=1,2,3, \ldots) \\
& h(x)=\limsup _{n \rightarrow \infty} f_{n}(x)
\end{aligned}
$$

Then $g$ and $h$ are measurable.

The same is of course true of the inf and lim inf.

Proof

$$
\begin{aligned}
\{x \mid g(x)>a\} & =\bigcup_{n=1}^{\infty}\left\{x \mid f_{n}(x)>a\right\} \\
h(x) & =\inf g_{m}(x)
\end{aligned}
$$

where $g_{m}(x)=\sup f_{n}(x)(n \geq m)$.

## Corollaries

(a) Iffand $g$ are measurable, then $\max (f, g)$ and $\min (f, g)$ are measurable. If

$$
f^{+}=\max (f, 0), \quad f^{-}=-\min (f, 0)
$$

it follows, in particular, that $f^{+}$and $f^{-}$are measurable.

(b) The limit of a convergent sequence of measurable functions is measurable.

11.18 Theorem Let $f$ and $g$ be measurable real-valued functions defined on $X$, let $F$ be real and continuous on $R^{2}$, and put

$$
h(x)=F(f(x), g(x)) \quad(x \in X)
$$

Then $h$ is measurable.

In particular, $f+g$ and $f g$ are measurable.

Proof Let

$$
G_{a}=\{(u, v) \mid F(u, v)>a\}
$$

Then $G_{a}$ is an open subset of $R^{2}$, and we can write

$$
G_{a}=\bigcup_{n=1}^{\infty} I_{n}
$$

where $\left\{I_{n}\right\}$ is a sequence of open intervals:

$$
I_{n}=\left\{(u, v) \mid a_{n}<u<b_{n}, c_{n}<v<d_{n}\right\} .
$$

Since

$$
\left\{x \mid a_{n}<f(x)<b_{n}\right\}=\left\{x \mid f(x)>a_{n}\right\} \cap\left\{x \mid f(x)<b_{n}\right\}
$$

is measurable, it follows that the set

$$
\left\{x \mid(f(x), g(x)) \in I_{n}\right\}=\left\{x \mid a_{n}<f(x)<b_{n}\right\} \cap\left\{x \mid c_{n}<g(x)<d_{n}\right\}
$$

is measurable. Hence the same is true of

$$
\begin{aligned}
\{x \mid h(x)>a\} & =\left\{x \mid(f(x), g(x)) \in G_{a}\right\} \\
& =\bigcup_{n=1}^{\infty}\left\{x \mid(f(x), g(x)) \in I_{n}\right\}
\end{aligned}
$$

Summing up, we may say that all ordinary operations of analysis, including limit operations, when applied to measurable functions, lead to measurable functions; in other words, all functions that are ordinarily met with are measurable.

That this is, however, only a rough statement is shown by the following example (based on Lebesgue measure, on the real line): If $h(x)=f(g(x))$, where
$f$ is measurable and $g$ is continuous, then $h$ is not necessarily measurable. (For the details, we refer to McShane, page 241.)

The reader may have noticed that measure has not been mentioned in our discussion of measurable functions. In fact, the class of measurable functions on $X$ depends only on the $\sigma$-ring $\mathfrak{M}$ (using the notation of Definition 11.12). For instance, we may speak of Borel-measurable functions on $R^{p}$, that is, of function $f$ for which

$$
\{x \mid f(x)>a\}
$$

is always a Borel set, without reference to any particular measure.

## SIMPLE FUNCTIONS

11.19 Definition Let $s$ be a real-valued function defined on $X$. If the range of $s$ is finite, we say that $s$ is a simple function.

Let $E \subset X$, and put

$$
K_{E}(x)= \begin{cases}1 & (x \in E), \\ 0 & (x \notin E) .\end{cases}
$$

$K_{E}$ is called the characteristic function of $E$.

Suppose the range of $s$ consists of the distinct numbers $c_{1}, \ldots, c_{n}$. Let

$$
E_{l}=\left\{x \mid s(x)=c_{i}\right\} \quad(i=1, \ldots, n)
$$

Then

$$
s=\sum_{n=1}^{n} c_{i} K_{E_{i}}
$$

that is, every simple function is a finite linear combination of characteristic functions. It is clear that $s$ is measurable if and only if the sets $E_{1}, \ldots, E_{n}$ are measurable.

It is of interest that every function can be approximated by simple functions:

11.20 Theorem Let $f$ be a real function on $X$. There exists a sequence $\left\{s_{n}\right\}$ of simple functions such that $s_{n}(x) \rightarrow f(x)$ as $n \rightarrow \infty$, for every $x \in X$. If $f$ is measurable, $\left\{s_{n}\right\}$ may be chosen to be a sequence of measurable functions. If $f \geq 0,\left\{s_{n}\right\}$ may be chosen to be a monotonically increasing sequence.

Proof If $f \geq 0$, define

$$
E_{n i}=\left\{x \mid \frac{i-1}{2^{n}} \leq f(x)<\frac{i}{2^{n}}\right\}, \quad F_{n}=\{x \mid f(x) \geq n\}
$$

for $n=1,2,3, \ldots, i=1,2, \ldots, n 2^{n}$. Put

$$
s_{n}=\sum_{i=1}^{n 2^{n}} \frac{i-1}{2^{n}} K_{E_{n t}}+n K_{F_{n}}
$$

In the general case, let $f=f^{+}-f^{-}$, and apply the preceding construction to $f^{+}$and to $f^{-}$.

It may be noted that the sequence $\left\{s_{n}\right\}$ given by $(50)$ converges uniformly to $f$ if $f$ is bounded.

## INTEGRATION

We shall define integration on a measurable space $X$, in which $\mathfrak{M}$ is the $\sigma$-ring of measurable sets, and $\mu$ is the measure. The reader who wishes to visualize a more concrete situation may think of $X$ as the real line, or an interval, and of $\mu$ as the Lebesgue measure $m$.

### 11.21 Definition Suppose

$$
s(x)=\sum_{i=1}^{n} c_{i} K_{E_{i}}(x) \quad\left(x \in X, c_{i}>0\right)
$$

is measurable, and suppose $E \in \mathfrak{M}$. We define

$$
I_{E}(s)=\sum_{i=1}^{n} c_{i} \mu\left(E \cap E_{i}\right)
$$

If $f$ is measurable and nonnegative, we define

$$
\int_{E} f d \mu=\sup I_{E}(s),
$$

where the sup is taken over all measurable simple functions $s$ such that $0 \leq s \leq f$.

The left member of (53) is called the Lebesgue integral of $f$, with respect to the measure $\mu$, over the set $E$. It should be noted that the integral may have the value $+\infty$.

It is easily verified that

$$
\int_{E} s d \mu=I_{E}(s)
$$

for every nonnegative simple measurable function $s$.

11.22 Definition Let $f$ be measurable, and consider the two integrals

$$
\int_{E} f^{+} d \mu, \quad \int_{E} f^{-} d \mu
$$

where $f^{+}$and $f^{-}$are defined as in (47).

If at least one of the integrals (55) is finite, we define

$$
\int_{E} f d \mu=\int_{E} f^{+} d \mu-\int_{E} f^{-} d \mu
$$

If both integrals in (55) are finite, then (56) is finite, and we say that $f$ is integrable (or summable) on $E$ in the Lebesgue sense, with respect to $\mu$; we write $f \in \mathscr{L}(\mu)$ on $E$. If $\mu=m$, the usual notation is: $f \in \mathscr{L}$ on $E$.

This terminology may be a little confusing: If (56) is $+\infty$ or $-\infty$, then the integral of $f$ over $E$ is defined, although $f$ is not integrable in the above sense of the word; $f$ is integrable on $E$ only if its integral over $E$ is finite.

We shall be mainly interested in integrable functions, although in some cases it is desirable to deal with the more general situation.

11.23 Remarks The following properties are evident:

(a) If $f$ is measurable and bounded on $E$, and if $\mu(E)<+\infty$, then $f \in \mathscr{L}(\mu)$ on $E$.

(b) If $a \leq f(x) \leq b$ for $x \in E$, and $\mu(E)<+\infty$, then

$$
a \mu(E) \leq \int_{E} f d \mu \leq b \mu(E)
$$

(c) If $f$ and $g \in \mathscr{L}(\mu)$ on $E$, and if $f(x) \leq g(x)$ for $x \in E$, then

$$
\int_{E} f d \mu \leq \int_{E} g d \mu .
$$

(d) If $f \in \mathscr{L}(\mu)$ on $E$, then $c f \in \mathscr{L}(\mu)$ on $E$, for every finite constant $c$, and

$$
\int_{E} c f d \mu=c \int_{E} f d \mu
$$

(e) If $\mu(E)=0$, and $f$ is measurable, then

$$
\int_{E} f d \mu=0
$$

(f) If $f \in \mathscr{L}(\mu)$ on $E, A \in \mathfrak{M}$, and $A \subset E$, then $f \in \mathscr{L}(\mu)$ on $A$.

### 11.24 Theorem

(a) Suppose $f$ is measurable and nonnegative on $X$. For $A \in \mathfrak{M}$, define

$$
\phi(A)=\int_{A} f d \mu
$$

Then $\phi$ is countably additive on $\mathfrak{M}$.

(b) The same conclusion holds if $f \in \mathscr{L}(\mu)$ on $X$.

Proof It is clear that $(b)$ follows from $(a)$ if we write $f=f^{+}-f^{-}$and apply (a) to $f^{+}$and to $f^{-}$.

To prove $(a)$, we have to show that

$$
\phi(A)=\sum_{n=1}^{\infty} \phi\left(A_{n}\right)
$$

if $A_{n} \in \mathfrak{M}(n=1,2,3, \ldots), A_{i} \cap A_{j}=0$ for $i \neq j$, and $A=\bigcup_{1}^{\infty} A_{n}$.

If $f$ is a characteristic function, then the countable additivity of $\phi$ is precisely the same as the countable additivity of $\mu$, since

$$
\int_{A} K_{E} d \mu=\mu(A \cap E)
$$

If $f$ is simple, then $f$ is of the form (51), and the conclusion again holds.

In the general case, we have, for every measurable simple function $s$ such that $0 \leq s \leq f$,

$$
\int_{A} s d \mu=\sum_{n=1}^{\infty} \int_{A_{n}} s d \mu \leq \sum_{n=1}^{\infty} \phi\left(A_{n}\right)
$$

Therefore, by (53),

$$
\phi(A) \leq \sum_{n=1}^{\infty} \phi\left(A_{n}\right)
$$

Now if $\phi\left(A_{n}\right)=+\infty$ for some $n,(58)$ is trivial, since $\phi(A) \geq \phi\left(A_{n}\right)$. Suppose $\phi\left(A_{n}\right)<+\infty$ for every $n$.

Given $\varepsilon>0$, we can choose a measurable function $s$ such that $0 \leq s \leq f$, and such that

$$
\int_{A_{1}} s d \mu \geq \int_{A_{1}} f d \mu-\varepsilon, \quad \int_{A_{2}} s d \mu \geq \int_{A_{2}} f d \mu-\varepsilon
$$

Hence

$$
\phi\left(A_{1} \cup A_{2}\right) \geq \int_{A_{1} \cup A_{2}} s d \mu=\int_{A_{1}} s d \mu+\int_{A_{2}} s d \mu \geq \phi\left(A_{1}\right)+\phi\left(A_{2}\right)-2 \varepsilon
$$

so that

$$
\phi\left(A_{1} \cup A_{2}\right) \geq \phi\left(A_{1}\right)+\phi\left(A_{2}\right)
$$

It follows that we have, for every $n$,

$$
\phi\left(A_{1} \cup \cdots \cup A_{n}\right) \geq \phi\left(A_{1}\right)+\cdots+\phi\left(A_{n}\right) .
$$

Since $A \supset A_{1} \cup \cdots \cup A_{n}$, (61) implies

$$
\phi(A) \geq \sum_{n=1}^{\infty} \phi\left(A_{n}\right)
$$

and (58) follows from (59) and (62).

Corollary If $A \in \mathfrak{M}, B \in \mathfrak{M}, B \subset A$, and $\mu(A-B)=0$, then

$$
\int_{\boldsymbol{A}} f d \mu=\int_{B} f d \mu
$$

Since $A=B \cup(A-B)$, this follows from Remark 11.23(e).

11.25 Remarks The preceding corollary shows that sets of ineasure zero are negligible in integration.

Let us write $f \sim g$ on $E$ if the set

$$
\{x \mid f(x) \neq g(x)\} \cap E
$$

has measure zero.

Then $f \sim f ; f \sim g$ implies $g \sim f$; and $f \sim g, g \sim h$ implies $f \sim h$. That is, the relation $\sim$ is an equivalence relation.

If $f \sim g$ on $E$, we clearly have

$$
\int_{\boldsymbol{A}} f d \mu=\int_{\boldsymbol{A}} g d \mu
$$

provided the integrals exist, for every measurable subset $A$ of $E$.

If a property $P$ holds for every $x \in E-A$, and if $\mu(A)=0$, it is customary to say that $P$ holds for almost all $x \in E$, or that $P$ holds almost everywhere on $E$. (This concept of "almost everywhere" depends of course on the particular measure under consideration. In the literature, unless something is said to the contrary, it usually refers to Lebesgue measure.)

If $f \in \mathscr{L}(\mu)$ on $E$, it is clear that $f(x)$ must be finite almost everywhere on $E$. In most cases we therefore do not lose any generality if we assume the given functions to be finite-valued from the outset.

11.26 Theorem If $f \in \mathscr{L}(\mu)$ on $E$, then $|f| \in \mathscr{L}(\mu)$ on $E$, and

$$
\left|\int_{E} f d \mu\right| \leq \int_{E}|f| d \mu
$$

Proof Write $E=A \cup B$, where $f(x) \geq 0$ on $A$ and $f(x)<0$ on $B$. By Theorem 11.24,

$$
\int_{E}|f| d \mu=\int_{A}|f| d \mu+\int_{B}|f| d \mu=\int_{A} f^{+} d \mu+\int_{B} f^{-} d \mu<+\infty,
$$

so that $|f| \in \mathscr{L}(\mu)$. Since $f \leq|f|$ and $-f \leq|f|$, we see that

$$
\int_{\boldsymbol{E}} f d \mu \leq \int_{E}|f| d \mu, \quad-\int_{\boldsymbol{E}} f d \mu \leq \int_{E}|f| d \mu
$$

and (63) follows.

Since the integrability of $f$ implies that of $|f|$, the Lebesgue integral is often called an absolutely convergent integral. It is of course possible to define nonabsolutely convergent integrals, and in the treatment of some problems it is essential to do so. But these integrals lack some of the most useful properties of the Lebesgue integral and play a somewhat less important role in analysis.

11.27 Theorem Suppose $f$ is measurable on $E,|f| \leq g$, and $g \in \mathscr{L}(\mu)$ on $E$. Then $f \in \mathscr{L}(\mu)$ on $E$.

Proof We have $f^{+} \leq g$ and $f^{-} \leq g$.

11.28 Lebesgue's monotone convergence theorem Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
0 \leq f_{1}(x) \leq f_{2}(x) \leq \cdots \quad(x \in E)
$$

Let $f$ be defined by

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. Then

$$
\int_{E} f_{n} d \mu \rightarrow \int_{E} f d \mu \quad(n \rightarrow \infty)
$$

Proof By (64) it is clear that, as $n \rightarrow \infty$,

$$
\int_{E} f_{n} d \mu \rightarrow \alpha
$$

for some $\alpha$; and since $\int f_{n} \leq \int f$, we have

$$
\alpha \leq \int_{E} f d \mu
$$

Choose $c$ such that $0<c<1$, and let $s$ be a simple measurable function such that $0 \leq s \leq f$. Put

$$
E_{n}=\left\{x \mid f_{n}(x) \geq c s(x)\right\} \quad(n=1,2,3, \ldots)
$$

By (64), $E_{1} \subset E_{2} \subset E_{3} \subset \cdots$; and by (65),

$$
E=\bigcup_{n=1}^{\infty} E_{n}
$$

For every $n$,

$$
\int_{E} f_{n} d \mu \geq \int_{E_{n}} f_{n} d \mu \geq c \int_{E_{n}} s d \mu .
$$

We let $n \rightarrow \infty$ in (70). Since the integral is a countably additive set function (Theorem 11.24), (69) shows that we may apply Theorem 11.3 to the last integral in (70), and we obtain

$$
\alpha \geq c \int_{E} s d \mu
$$

Letting $c \rightarrow 1$, we see that

$$
\alpha \geq \int_{E} s d \mu
$$

and (53) implies

$$
\alpha \geq \int_{E} f d \mu
$$

The theorem follows from (67), (68), and (72).

11.29 Theorem Suppose $f=f_{1}+f_{2}$, where $f_{i} \in \mathscr{L}(\mu)$ on $E(i=1,2)$. Then $f \in \mathscr{L}(\mu)$ on $E$, and

$$
\int_{E} f d \mu=\int_{E} f_{1} d \mu+\int_{E} f_{2} d \mu .
$$

Proof First, suppose $f_{1} \geq 0, f_{2} \geq 0$. If $f_{1}$ and $f_{2}$ are simple, (73) follows trivially from (52) and (54). Otherwise, choose monotonically increasing sequences $\left\{s_{n}^{\prime}\right\},\left\{s_{n}^{\prime \prime}\right\}$ of nonnegative measurable simple functions which converge to $f_{1}, f_{2}$. Theorem 11.20 shows that this is possible. Put $s_{n}=s_{n}^{\prime}+s_{n}^{\prime \prime}$. Then

$$
\int_{E} s_{n} d \mu=\int_{E} s_{n}^{\prime} d \mu+\int_{E} s_{n}^{\prime \prime} d \mu,
$$

and (73) follows if we let $n \rightarrow \infty$ and appeal to Theorem 11.28.

Next, suppose $f_{1} \geq 0, f_{2} \leq 0$. Put

$$
A=\{x \mid f(x) \geq 0\}, \quad B=\{x \mid f(x)<0\} .
$$

Then $f, f_{1}$, and $-f_{2}$ are nonnegative on $A$. Hence

$$
\int_{A} f_{1} d \mu=\int_{A} f d \mu+\int_{A}\left(-f_{2}\right) d \mu=\int_{A} f d \mu-\int_{A} f_{2} d \mu .
$$

Similarly, $-f, f_{1}$, and $-f_{2}$ are nonnegative on $B$, so that

$$
\int_{B}\left(-f_{2}\right) d \mu=\int_{B} f_{1} d \mu+\int_{B}(-f) d \mu
$$

or

$$
\int_{B} f_{1} d \mu=\int_{B} f d \mu-\int_{B} f_{2} d \mu
$$

and (73) follows if we add (74) and (75).

In the general case, $E$ can be decomposed into four sets $E_{i}$ on each of which $f_{1}(x)$ and $f_{2}(x)$ are of constant sign. The two cases we have proved so far imply

$$
\int_{E_{i}} f d \mu=\int_{E_{t}} f_{1} d \mu+\int_{E_{i}} f_{2} d \mu \quad(i=1,2,3,4),
$$

and (73) follows by adding these four equations.

We are now in a position to reformulate Theorem 11.28 for series.

11.30 Theorem Suppose $E \in \mathfrak{M}$. If $\left\{f_{n}\right\}$ is a sequence of nonnegative measurable functions and

$$
f(x)=\sum_{n=1}^{\infty} f_{n}(x) \quad(x \in E)
$$

then

$$
\int_{E} f d \mu=\sum_{n=1}^{\infty} \int_{E} f_{n} d \mu
$$

Proof The partial sums of (76) form a monotonically increasing sequence.

11.31 Fatou's theorem Suppose $E \in \mathfrak{M}$. If $\left\{f_{n}\right\}$ is a sequence of nonnegative measurable functions and

$$
f(x)=\liminf _{n \rightarrow \infty} f_{n}(x) \quad(x \in E),
$$

then

$$
\int_{E} f d \mu \leq \liminf _{n \rightarrow \infty} \int_{E} f_{n} d \mu .
$$

Strict inequality may hold in (77). An example is given in Exercise 5.

Proof For $n=1,2,3, \ldots$ and $x \in E$, put

$$
g_{n}(x)=\inf f_{i}(x) \quad(i \geq n)
$$

Then $g_{n}$ is measurable on $E$, and

$$
\begin{aligned}
0 & \leq g_{1}(x) \leq g_{2}(x) \leq \cdots, \\
g_{n}(x) & \leq f_{n}(x), \\
g_{n}(x) & \rightarrow f(x) \quad(n \rightarrow \infty) .
\end{aligned}
$$

By (78), (80), and Theorem 11.28,

$$
\int_{E} g_{n} d \mu \rightarrow \int_{E} f d \mu
$$

so that (77) follows from (79) and (81).

11.32 Lebesgue's dominated convergence theorem Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. If there exists a function $g \in \mathscr{L}(\mu)$ on $E$, such that

$$
\left|f_{n}(x)\right| \leq g(x) \quad(n=1,2,3, \ldots, x \in E)
$$

then

$$
\lim _{n \rightarrow \infty} \int_{E} f_{n} d \mu=\int_{E} f d \mu \text {. }
$$

Because of (83), $\left\{f_{n}\right\}$ is said to be dominated by $g$, and we talk about dominated convergence. By Remark 11.25, the conclusion is the same if (82) holds almost everywhere on $E$.

Proof First, (83) and Theorem 11.27 imply that $f_{n} \in \mathscr{L}(\mu)$ and $f \in \mathscr{L}(\mu)$ on $E$.

Since $f_{n}+g \geq 0$, Fatou's theorem shows that

$$
\int_{E}(f+g) d \mu \leq \liminf _{n \rightarrow \infty} \int_{E}\left(f_{n}+g\right) d \mu,
$$

or

$$
\int_{E} f d \mu \leq \liminf _{n \rightarrow \infty} \int_{E} f_{n} d \mu .
$$

Since $g-f_{n} \geq 0$, we see similarly that

so that

$$
\int_{E}(g-f) d \mu \leq \liminf _{n \rightarrow \infty} \int_{E}\left(g-f_{n}\right) d \mu
$$

$$
-\int_{E} f d \mu \leq \liminf _{n \rightarrow \infty}\left[-\int_{E} f_{n} d \mu\right]
$$

which is the same as

$$
\int_{E} f d \mu \geq \limsup _{n \rightarrow \infty} \int_{E} f d \mu
$$

The existence of the limit in (84) and the equality asserted by ( 84$)$ now follow from (85) and (86).

Corollary If $\mu(E)<+\infty,\left\{f_{n}\right\}$ is uniformly bounded on $E$, and $f_{n}(x) \rightarrow f(x)$ on $E$, then (84) holds.

A uniformly bounded convergent sequence is of ten said to be boundedly convergent.

## COMPARISON WITH THE RIEMANN INTEGRAL

Our next theorem will show that every function which is Riemann-integrable on an interval is also Lebesgue-integrable, and that Riemann-integrable functions are subject to rather stringent continuity conditions. Quite apart from the fact that the Lebesgue theory therefore enables us to integrate a much larger class of functions, its greatest advantage lies perhaps in the ease with which many limit operations can be handled; from this point of view, Lebesgue's convergence theorems may well be regarded as the core of the Lebesgue theory.

One of the difficulties which is encountered in the Riemann theory is that limits of Riemann-integrable functions (or even continuous functions) may fail to be Riemann-integrable. This difficulty is now almost eliminated, since limits of measurable functions are always measurable.

Let the measure space $X$ be the interval $[a, b]$ of the real line, with $\mu=m$ (the Lebesgue measure), and $\mathfrak{M}$ the family of Lebesgue-measurable subsets of $[a, b]$. Instead of

$$
\int_{X} f d m
$$

it is customary to use the familiar notation

$$
\int_{a}^{b} f d x
$$

for the Lebesgue integral of $f$ over $[a, b]$. To distinguish Riemann integrals from Lebesgue integrals, we shall now denote the former by

$$
\mathscr{R} \int_{a}^{b} f d x
$$

### 11.33 Theorem

(a) If $f \in \mathscr{R}$ on $[a, b]$, then $f \in \mathscr{L}$ on $[a, b]$, and

$$
\int_{a}^{b} f d x=\mathscr{R} \int_{a}^{b} f d x
$$

(b) Suppose $f$ is bounded on $[a, b]$. Then $f \in \mathscr{R}$ on $[a, b]$ if and only if $f$ is continuous almost everywhere on $[a, b]$.

Proof Suppose $f$ is bounded. By Definition 6.1 and Theorem 6.4 there is a sequence $\left\{P_{k}\right\}$ of partitions of $[a, b]$, such that $P_{k+1}$ is a refinement of $P_{k}$, such that the distance between adjacent points of $P_{k}$ is less than $1 / k$, and such that

$$
\lim _{k \rightarrow \infty} L\left(P_{k}, f\right)=\mathscr{R} \int \underline{\underline{\int}} f d x, \quad \lim _{k \rightarrow \infty} U\left(P_{k}, f\right)=\mathscr{R} \bar{\int} f d x
$$

(In this proof, all integrals are taken over $[a, b]$.)

If $P_{k}=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$, with $x_{0}=a, x_{n}=b$, define

$$
U_{k}(a)=L_{k}(a)=f(a)
$$

put $U_{k}(x)=M_{i}$ and $L_{k}(x)=m_{i}$ for $x_{i-1}<x \leq x_{i}, 1 \leq i \leq n$, using the notation introduced in Definition 6.1. Then

$$
L\left(P_{k}, f\right)=\int L_{k} d x, \quad U\left(P_{k}, f\right)=\int U_{k} d x
$$

and

$$
L_{1}(x) \leq L_{2}(x) \leq \cdots \leq f(x) \leq \cdots \leq U_{2}(x) \leq U_{1}(x)
$$

for all $x \in[a, b]$, since $P_{k+1}$ refines $P_{k}$. By (90), there exist

$$
L(x)=\lim _{k \rightarrow \infty} L_{k}(x), \quad U(x)=\lim _{k \rightarrow \infty} U_{k}(x)
$$

Observe that $L$ and $U$ are bounded measurable functions on $[a, b]$, that

$$
L(x) \leq f(x) \leq U(x) \quad(a \leq x \leq b)
$$

and that

$$
\int L d x=\mathscr{R} \int_{\underline{I}} f d x, \quad \int U d x=\mathscr{R} \bar{\int} f d x
$$

by (88), (90), and the monotone convergence theorem.

So far, nothing has been assumed about $f$ except that $f$ is a bounded real function on $[a, b]$.

To complete the proof, note that $f \in \mathscr{R}$ if and only if its upper and lower Riemann integrals are equal, hence if and only if

$$
\int L d x=\int U d x
$$

since $L \leq U$, (94) happens if and only if $L(x)=U(x)$ for almost all $x \in[a, b]$ (Exercise 1).

In that case, (92) implies that

$$
L(x)=f(x)=U(x)
$$

almost everywhere on $[a, b]$, so that $f$ is measurable, and (87) follows from (93) and (95).

Furthermore, if $x$ belongs to no $P_{k}$, it is quite easy to see that $U(x)=$ $L(x)$ if and only if $f$ is continuous at $x$. Since the union of the sets $P_{k}$ is countable, its measure is 0 , and we conclude that $f$ is continuous almost everywhere on $[a, b]$ if and only if $L(x)=U(x)$ almost everywhere, hence (as we saw above) if and only if $f \in \mathscr{R}$.

This completes the proof.

The familiar connection between integration and differentiation is to a large degree carried over into the Lebesgue theory. If $f \in \mathscr{L}$ on $[a, b]$, and

$$
F(x)=\int_{a}^{x} f d t \quad(a \leq x \leq b)
$$

then $F^{\prime}(x)=f(x)$ almost everywhere on $[a, b]$.

Conversely, if $F$ is differentiable at every point of $[a, b]$ ("almost everywhere" is not good enough here!) and if $F^{\prime} \in \mathscr{L}$ on $[a, b]$, then

$$
F(x)-F(a)=\int_{a}^{x} F^{\prime}(t) \quad(a \leq x \leq b)
$$

For the proofs of these two theorems, we refer the reader to any of the works on integration cited in the Bibliography.

## INTEGRATION OF COMPLEX FUNCTIONS

Suppose $f$ is a complex-valued function defined on a measure space $X$, and $f=u+i v$, where $u$ and $v$ are real. We say that $f$ is measurable if and only if both $u$ and $v$ are measurable.

It is easy to verify that sums and products of complex measurable functions are again measurable. Since

$$
|f|=\left(u^{2}+v^{2}\right)^{1 / 2}
$$

Theorem 11.18 shows that $|f|$ is measurable for every complex measurable $f$.

Suppose $\mu$ is a measure on $X, E$ is a measurable subset of $X$, and $f$ is a complex function on $X$. We say that $f \in \mathscr{L}(\mu)$ on $E$ provided that $f$ is measurable and

$$
\int_{E}|f| d \mu<+\infty,
$$

and we define

$$
\int_{E} f d \mu=\int_{E} u d \mu+i \int_{E} v d \mu
$$

if (97) holds. Since $|u| \leq|f|,|v| \leq|f|$, and $|f| \leq|u|+|v|$, it is clear that (97) holds if and only if $u \in \mathscr{L}(\mu)$ and $v \in \mathscr{L}(\mu)$ on $E$.

Theorems 11.23(a), $(d),(e),(f), 11.24(b), 11.26,11.27,11.29$, and 11.32 can now be extended to Lebesgue integrals of complex functions. The proofs are quite straightforward. That of Theorem 11.26 is the only one that offers anything of interest:

If $f \in \mathscr{L}(\mu)$ on $E$, there is a complex number $c,|c|=1$, such that

$$
c \int_{E} f d \mu \geq 0 \text {. }
$$

Put $g=c f=u+i v, u$ and $v$ real. Then

$$
\left|\int_{E} f d \mu\right|=c \int_{E} f d \mu=\int_{E} g d \mu=\int_{E} u d \mu \leq \int_{E}|f| d \mu
$$

The third of the above equalities holds since the preceding ones show that $\int g d \mu$ is real.

## FUNCTIONS OF CLASS $\mathscr{L}^{2}$

As an application of the Lebesgue theory, we shall now extend the Parseval theorem (which we proved only for Riemann-integrable functions in Chap. 8) and prove the Riesz-Fischer theorem for orthonormal sets of functions.

11.34 Definition Let $X$ be a measurable space. We say that a complex function $f \in \mathscr{L}^{2}(\mu)$ on $X$ if $f$ is measurable and if

$$
\int_{x}|f|^{2} d \mu<+\infty
$$

If $\mu$ is Lebesgue measure, we say $f \in \mathscr{L}^{2}$. For $f \in \mathscr{L}^{2}(\mu)$ (we shall omit the phrase "on $X$ " from now on) we define

$$
\|f\|=\left\{\int_{x}|f|^{2} d \mu\right\}^{1 / 2}
$$

and call $\|f\|$ the $\mathscr{L}^{2}(\mu)$ norm of $f$.

11.35 Theorem Suppose $f \in \mathscr{L}^{2}(\mu)$ and $g \in \mathscr{L}^{2}(\mu)$. Then $f g \in \mathscr{L}(\mu)$, and

$$
\int_{x}|f g| d \mu \leq\|f\|\|g\|
$$

This is the Schwarz inequality, which we have already encountered for series and for Riemann integrals. It follows from the inequality

$$
0 \leq \int_{x}(|f|+\lambda|g|)^{2} d \mu=\|f\|^{2}+2 \lambda \int_{x}|f g| d \mu+\lambda^{2}\|g\|^{2}
$$

which holds for every real $\lambda$.

11.36 Theorem If $f \in \mathscr{L}^{2}(\mu)$ and $g \in \mathscr{L}^{2}(\mu)$, then $f+g \in \mathscr{L}^{2}(\mu)$, and

$$
\|f+g\| \leq\|f\|+\|g\| .
$$

Proof The Schwarz inequality shows that

$$
\begin{aligned}
\|f+g\|^{2} & =\int|f|^{2}+\int f \bar{g}+\int f g+\int|g|^{2} \\
& \leq\|f\|^{2}+2\|f\|\|g\|+\|g\|^{2} \\
& =(\|f\|+\|g\|)^{2}
\end{aligned}
$$

11.37 Remark If we define the distance between two functions $f$ and $g$ in $\mathscr{L}^{2}(\mu)$ to be $\|f-g\|$, we see that the conditions of Definition 2.15 are satisfied, except for the fact that $\|f-g\|=0$ does not imply that $f(x)=g(x)$ for all $x$, but only for almost all $x$. Thus, if we identify functions which differ only on a set of measure zero, $\mathscr{L}^{2}(\mu)$ is a metric space.

We now consider $\mathscr{L}^{2}$ on an interval of the real line, with respect to Lebesgue measure.

11.38 Theorem The continuous functions form a dense subset of $\mathscr{L}^{2}$ on $[a, b]$.

More explicitly, this means that for any $f \in \mathscr{L}^{2}$ on $[a, b]$, and any $\varepsilon>0$, there is a function $g$, continuous on $[a, b]$, such that

$$
\|f-g\|=\left\{\int_{a}^{b}|f-g|^{2} d x\right\}^{1 / 2}<\varepsilon
$$

Proof We shall say that $f$ is approximated in $\mathscr{L}^{2}$ by a sequence $\left\{g_{n}\right\}$ if $\left\|f-g_{n}\right\| \rightarrow 0$ as $n \rightarrow \infty$.

Let $A$ be a closed subset of $[a, b]$, and $K_{A}$ its characteristic function. Put

$$
t(x)=\inf |x-y| \quad(y \in A)
$$

and

$$
g_{n}(x)=\frac{1}{1+n t(x)} \quad(n=1,2,3, \ldots)
$$

Then $g_{n}$ is continuous on $[a, b], g_{n}(x)=1$ on $A$, and $g_{n}(x) \rightarrow 0$ on $B$, where $B=[a, b]-A$. Hence

$$
\left\|g_{n}-K_{A}\right\|=\left\{\int_{B} g_{n}^{2} d x\right\}^{1 / 2} \rightarrow 0
$$

by Theorem 11.32. Thus characteristic functions of closed sets can be approximated in $\mathscr{L}^{2}$ by continuous functions.

By (39) the same is true for the characteristic function of any measurable set, and hence also for simple measurable functions.

If $f \geq 0$ and $f \in \mathscr{L}^{2}$, let $\left\{s_{n}\right\}$ be a monotonically increasing sequence of simple nonnegative measurable functions such that $s_{n}(x) \rightarrow f(x)$. Since $\left|f-s_{n}\right|^{2} \leq f^{2}$, Theorem 11.32 shows that $\left\|f-s_{n}\right\| \rightarrow 0$.

The general case follows.

11.39 Definition We say that a sequence of complex functions $\left\{\phi_{n}\right\}$ is an orthonormal set of functions on a measurable space $X$ if

$$
\int_{X} \phi_{n} \phi_{m} d \mu= \begin{cases}0 & (n \neq m) \\ 1 & (n=m)\end{cases}
$$

In particular, we must have $\phi_{n} \in \mathscr{L}^{2}(\mu)$. If $f \in \mathscr{L}^{2}(\mu)$ and if

$$
c_{n}=\int_{x} f \bar{\phi}_{n} d \mu \quad(n=1,2,3, \ldots)
$$

we write

$$
f \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$

as in Definition 8.10.

The definition of a trigonometric Fourier series is extended in the same way to $\mathscr{L}^{2}$ (or even to $\mathscr{L}$ ) on $[-\pi, \pi]$. Theorems 8.11 and 8.12 (the Bessel inequality) hold for any $f \in \mathscr{L}^{2}(\mu)$. The proofs are the same, word for word.

We can now prove the Parseval theorem.

### 11.40 Theorem Suppose

$$
f(x) \sim \sum_{-\infty}^{\infty} c_{n} e^{i n x}
$$

where $f \in \mathscr{L}^{2}$ on $[-\pi, \pi]$. Let $s_{n}$ be the nth partial sum of (99). Then

$$
\begin{aligned}
\lim _{n \rightarrow \infty}\left\|f-s_{n}\right\| & =0, \\
\sum_{-\infty}^{\infty}\left|c_{n}\right|^{2} & =\frac{1}{2 \pi} \int_{-\pi}^{\pi}|f|^{2} d x .
\end{aligned}
$$

Proof Let $\varepsilon>0$ be given. By Theorem 11.38, there is a continuous function $g$ such that

$$
\|f-g\|<\frac{\varepsilon}{2}
$$

Moreover, it is easy to see that we can arrange it so that $g(\pi)=g(-\pi)$. Then $g$ can be extended to a periodic continuous function. By Theorem 8.16 , there is a trigonometric polynomial $T$, of degree $N$, say, such that

$$
\|g-T\|<\frac{\varepsilon}{2} \text {. }
$$

Hence, by Theorem 8.11 (extended to $\mathscr{L}^{2}$ ), $n \geq N$ implies

$$
\left\|s_{n}-f\right\| \leq\|T-f\|<\varepsilon
$$

and (100) follows. Equation (101) is deduced from (100) as in the proof of Theorem 8.16.

Corollary If $f \in \mathscr{L}^{2}$ on $[-\pi, \pi]$, and if

$$
\int_{-\pi}^{\pi} f(x) e^{-i n x} d x=0 \quad(n=0, \pm 1, \pm 2, \ldots)
$$

then $\|f\|=0$.

Thus if two functions in $\mathscr{L}^{2}$ have the same Fourier series, they differ at most on a set of measure zero.

11.41 Definition Let $f$ and $f_{n} \in \mathscr{L}^{2}(\mu)(n=1,2,3, \ldots)$. We say that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$ if $\left\|f_{n}-f\right\| \rightarrow 0$. We say that $\left\{f_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$ if for every $\varepsilon>0$ there is an integer $N$ such that $n \geq N, m \geq N$ implies $\left\|f_{n}-f_{m}\right\| \leq \varepsilon$.

11.42 Theorem If $\left\{f_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$, then there exists a function $f \in \mathscr{L}^{2}(\mu)$ such that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$.

This says, in other words, that $\mathscr{L}^{2}(\mu)$ is a complete metric space.

Proof Since $\left\{f_{n}\right\}$ is a Cauchy sequence, we can find a sequence $\left\{n_{k}\right\}$, $k=1,2,3, \ldots$, such that

$$
\left\|f_{n_{k}}-f_{n_{k+1}}\right\|<\frac{1}{2^{k}} \quad(k=1,2,3, \ldots)
$$

Choose a function $g \in \mathscr{L}^{2}(\mu)$. By the Schwarz inequality,

$$
\int_{X}^{0}\left|g\left(f_{n_{k}}-f_{n_{k+1}}\right)\right| d \mu \leq \frac{\|g\|}{2^{k}}
$$

Hence

$$
\sum_{k=1}^{\infty} \int_{X}\left|g\left(f_{n_{k}}-f_{n_{k+1}}\right)\right| d \mu \leq\|g\|
$$

By Theorem 11.30, we may interchange the summation and integration in (102). It follows that

$$
|g(x)| \sum_{k=1}^{\infty}\left|f_{n_{k}}(x)-f_{n_{k+1}}(x)\right|<+\infty
$$

almost everywhere on $X$. Therefore

$$
\sum_{k=1}^{\infty}\left|f_{n_{k+1}}(x)-f_{n_{k}}(x)\right|<+\infty
$$

almost everywhere on $X$. For if the series in (104) were divergent on a set $E$ of positive measure, we could take $g(x)$ to be nonzero on a subset of $E$ of positive measure, thus obtaining a contradiction to (103).

Since the $k$ th partial sum of the series

$$
\sum_{k=1}^{\infty}\left(f_{n_{k+1}}(x)-f_{n_{k}}(x)\right)
$$

which converges almost everywhere on $X$, is

$$
f_{n_{k}+1}(x)-f_{n_{1}}(x)
$$

we see that the equation

$$
f(x)=\lim _{k \rightarrow \infty} f_{n_{k}}(x)
$$

defines $f(x)$ for almost all $x \in X$, and it does not matter how we define $f(x)$ at the remaining points of $X$.

We shall now show that this function $f$ has the desired properties. Let $\varepsilon>0$ be given, and choose $N$ as indicated in Definition 11.41. If $n_{k}>N$, Fatou's theorem shows that

$$
\left\|f-f_{n_{k}}\right\| \leq \liminf _{i \rightarrow \infty}\left\|f_{n_{i}}-f_{n_{k}}\right\| \leq \varepsilon
$$

Thus $f-f_{n_{k}} \in \mathscr{L}^{2}(\mu)$, and since $f=\left(f-f_{n_{k}}\right)+f_{n_{k}}$, we see that $f \in \mathscr{L}^{2}(\mu)$. Also, since $\varepsilon$ is arbitrary,

$$
\lim _{k \rightarrow \infty}\left\|f-f_{n_{k}}\right\|=0
$$

Finally, the inequality

$$
\left\|f-f_{n}\right\| \leq\left\|f-f_{n_{k}}\right\|+\left\|f_{n_{k}}-f_{n}\right\|
$$

shows that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$; for if we take $n$ and $n_{k}$ large enough, each of the two terms on the right of (105) can be made arbitrarily small.

11.43 The Riesz-Fischer theorem Let $\left\{\phi_{n}\right\}$ be orthonormal on $X$. Suppose $\Sigma\left|c_{n}\right|^{2}$ converges, and put $s_{n}=c_{1} \phi_{1}+\cdots+c_{n} \phi_{n}$. Then there exists a function $f \in \mathscr{L}^{2}(\mu)$ such that $\left\{s_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$, and such that

$$
f \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$

Proof For $n>m$,

$$
\left\|s_{n}-s_{m}\right\|^{2}=\left|c_{m+1}\right|^{2}+\cdots+\left|c_{n}\right|^{2}
$$

so that $\left\{s_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$. By Theorem 11.42, there is a function $f \in \mathscr{L}^{2}(\mu)$ such that

$$
\lim _{n \rightarrow \infty}\left\|f-s_{n}\right\|=0
$$

Now, for $n>k$,

$$
\int_{X} f \Phi_{k} d \mu-c_{k}=\int_{X} f \Phi_{k} d \mu-\int_{X} s_{n} \bar{\phi}_{k} d \mu
$$

so that

$$
\left|\int_{X} f \Phi_{k} d \mu-c_{k}\right| \leq\left\|f-s_{n}\right\| \cdot\left\|\phi_{k}\right\|+\left\|f-s_{n}\right\|
$$

Letting $n \rightarrow \infty$, we see that

$$
c_{k}=\int_{X} f \Phi_{k} d \mu \quad(k=1,2,3, \ldots)
$$

and the proof is complete.

11.44 Definition An orthonormal set $\left\{\phi_{n}\right\}$ is said to be complete if, for $f \in \mathscr{L}^{2}(\mu)$, the equations

$$
\int_{x} f \bar{\phi}_{n} d \mu=0 \quad(n=1,2,3, \ldots)
$$

imply that $\|f\|=0$.

In the Corollary to Theorem 11.40 we deduced the completeness of the trigonometric system from the Parseval equation (101). Conversely, the Parseval equation holds for every complete orthonormal set:

11.45 Theorem Let $\left\{\phi_{n}\right\}$ be a complete orthonormal set. If $f \in \mathscr{L}^{2}(\mu)$ and if

$$
f \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$

then

$$
\int_{X}|f|^{2} d \mu=\sum_{n=1}^{\infty}\left|c_{n}\right|^{2}
$$

Proof By the Bessel inequality, $\Sigma\left|c_{n}\right|^{2}$ converges. Putting

$$
s_{n}=c_{1} \phi_{1}+\cdots+c_{n} \phi_{n}
$$

the Riesz-Fischer theorem shows that there is a function $g \in \mathscr{L}^{2}(\mu)$ such that

$$
g \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$

and such that $\left\|g-s_{n}\right\| \rightarrow 0$. Hence $\left\|s_{n}\right\| \rightarrow\|g\|$. Since

$$
\left\|s_{n}\right\|^{2}=\left|c_{1}\right|^{2}+\cdots+\left|c_{n}\right|^{2}
$$

we have

$$
\int_{X}|g|^{2} d \mu=\sum_{n=1}^{\infty}\left|c_{n}\right|^{2}
$$

Now (106), (108), and the completeness of $\left\{\phi_{n}\right\}$ show that $\|f-g\|=0$, so that (109) implies (107).

Combining Theorems 11.43 and 11.45 , we arrive at the very interesting conclusion that every complete orthonormal set induces a 1-1 correspondence between the functions $f \in \mathscr{L}^{2}(\mu)$ (identifying those which are equal almost everywhere) on the one hand and the sequences $\left\{c_{n}\right\}$ for which $\Sigma\left|c_{n}\right|^{2}$ converges, on the other. The representation

$$
f \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$

together with the Parseval equation, shows that $\mathscr{L}^{2}(\mu)$ may be regarded as an infinite-dimensional euclidean space (the so-called "Hilbert space"), in which the point $f$ has coordinates $c_{n}$, and the functions $\phi_{n}$ are the coordinate vectors.

## EXERCISES

1. If $f \geq 0$ and $\int_{E} f d \mu=0$, prove that $f(x)=0$ almost everywhere on $E$. Hint: Let $E_{n}$ be the subset of $E$ on which $f(x)>1 / n$. Write $A=\bigcup E_{n}$. Then $\mu(A)=0$ if and only if $\mu\left(E_{n}\right)=0$ for every $n$.
2. If $\int_{A} f d \mu=0$ for every measurable subset $A$ of a measurable set $E$, then $f(x)=0$ almost everywhere on $E$.
3. If $\left\{f_{n}\right\}$ is a sequence of measurable functions, prove that the set of points $x$ at which $\left\{f_{n}(x)\right\}$ converges is measurable.
4. If $f \in \mathscr{L}(\mu)$ on $E$ and $g$ is bounded and measurable on $E$, then $f g \in \mathscr{L}(\mu)$ on $E$.
5. Put

$$
\begin{array}{rlrl}
g(x) & = \begin{cases}0 & \left(0 \leq x \leq \frac{1}{2}\right), \\
1 & \left(\frac{1}{2}<x \leq 1\right),\end{cases} \\
f_{2 k}(x) & =g(x) & (0 \leq x \leq 1), \\
f_{2 k+1}(x) & =g(1-x) & (0 \leq x \leq 1)
\end{array}
$$

Show that

$$
\liminf _{n \rightarrow \infty} f_{n}(x)=0 \quad(0 \leq x \leq 1)
$$

but

$$
\int_{0}^{1} f_{n}(x) d x=\frac{1}{2} .
$$

[Compare with (77).]

6. Let

$$
f_{n}(x)= \begin{cases}\frac{1}{n} & (|x| \leq n) \\ 0 & (|x|>n)\end{cases}
$$

Then $f_{n}(x) \rightarrow 0$ uniformly on $R^{1}$, but

$$
\int_{-\infty}^{\infty} f_{n} d x=2 \quad(n=1,2,3, \ldots)
$$

(We write $\int_{-\infty}^{\infty}$ in place of $\int_{R 1}$.) Thus uniform convergence does not imply dominated convergence in the sense of Theorem 11.32. However, on sets of finite measure, uniformly convergent sequences of bounded functions do satisfy Theorem 11.32.

7. Find a necessary and sufficient condition that $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Hint: Consider Example 11.6(b) and Theorem 11.33.
8. If $f \in \mathscr{R}$ on $[a, b]$ and if $F(x)=\int_{a}^{x} f(t) d t$, prove that $F^{\prime}(x)=f(x)$ almost everywhere on $[a, b]$.
9. Prove that the function $F$ given by (96) is continuous on $[a, b]$.
10. If $\mu(X)<+\infty$ and $f \in \mathscr{L}^{2}(\mu)$ on $X$, prove that $f \in \mathscr{L}(\mu)$ on $X$. If

$$
\mu(X)=+\infty
$$

this is false. For instance, if

$$
f(x)=\frac{1}{1+|x|}
$$

then $f \in \mathscr{L}^{2}$ on $R^{1}$, but $f \notin \mathscr{L}$ on $R^{1}$.

11. If $f, g \in \mathscr{L}(\mu)$ on $X$, define the distance between $f$ and $g$ by

$$
\int_{x}|f-g| d \mu
$$

Prove that $\mathscr{L}(\mu)$ is a complete metric space.

12. Suppose

(a) $|f(x, y)| \leq 1$ if $0 \leq x \leq 1,0 \leq y \leq 1$,

(b) for fixed $x, f(x, y)$ is a continuous function of $y$,

(c) for fixed $y, f(x, y)$ is a continuous function of $x$.

Put

$$
g(x)=\int_{0}^{1} f(x, y) d y \quad(0 \leq x \leq 1)
$$

Is $g$ continuous?

13. Consider the functions

$$
f_{n}(x)=\sin n x \quad(n=1,2,3, \ldots,-\pi \leq x \leq \pi)
$$

as points of $\mathscr{L}^{2}$. Prove that the set of these points is closed and bounded, but not compact.

14. Prove that a complex function $f$ is measurable if and only if $f^{-1}(V)$ is measurable for every open set $V$ in the plane.
15. Let $\mathscr{R}$ be the ring of all elementary subsets of $(0,1]$. If $0<a \leq b \leq 1$, define

$$
\phi([a, b])=\phi([a, b))=\phi((a, b])=\phi((a, b))=b-a,
$$

but define

$$
\phi((0, b))=\phi((0, b])=1+b
$$

if $0<b \leq 1$. Show that this gives an additive set function $\phi$ on $\mathscr{R}$, which is not regular and which cannot be extended to a countably additive set function on a $\sigma$-ring.

16. Suppose $\left\{n_{k}\right\}$ is an increasing sequence of positive integers and $E$ is the set of all $x \in(-\pi, \pi)$ at which $\left\{\sin n_{k} x\right\}$ converges. Prove that $m(E)=0$. Hint: For every $A \subset E$,

$$
\int_{A} \sin n_{k} x d x \rightarrow 0
$$

and

$$
2 \int_{A}\left(\sin n_{k} x\right)^{2} d x=\int_{A}\left(1-\cos 2 n_{k} x\right) d x \rightarrow m(A) \quad \text { as } k \rightarrow \infty \text {. }
$$

17. Suppose $E \subset(-\pi, \pi), m(E)>0, \delta>0$. Use the Bessel inequality to prove that there are at most finitely many integers $n$ such that $\sin n x \geq \delta$ for all $x \in E$.
18. Suppose $f \in \mathscr{L}^{2}(\mu), g \in \mathscr{L}^{2}(\mu)$. Prove that

$$
\left|\int f g d \mu\right|^{2}=\int|f|^{2} d \mu \int|g|^{2} d \mu
$$

if and only if there is a constant $c$ such that $g(x)=c f(x)$ almost everywhere. (Compare Theorem 11.35.)

## BIBLIOGRAPHY

ARTIN, E.: "The Gamma Function," Holt, Rinehart and Winston, Inc., New York, 1964.

BOAS, R. P.: "A Primer of Real Functions," Carus Mathematical Monograph No. 13, John Wiley \& Sons, Inc., New York, 1960.

BuCK, R. C. (ed.): "Studies in Modern Analysis," Prentice-Hall, Inc., Englewood Cliffs, N.J., 1962.

‚Äî-: "Advanced Calculus," 2d ed., McGraw-Hill Book Company, New York, 1965.

BURKILL, J. C.: "The Lebesgue Integral," Cambridge University Press, New York, 1951. DieudonN√â, J.: "Foundations of Modern Analysis," Academic Press, Inc., New York, 1960.

Fleming, w. H.: "Functions of Several Variables," Addison-Wesley Publishing Company, Inc., Reading, Mass., 1965.

GRaVes, L. M.: "The Theory of Functions of Real Variables," 2d ed., McGraw-Hill Book Company, New York, 1956.

HAlmos, P. R.: "Measure Theory," D. Van Nostrand Company, Inc., Princeton, N.J., 1950.

‚Äî‚Äî: "Finite-dimensional Vector Spaces," 2d ed., D. Van Nostrand Company, Inc., Princeton, N.J., 1958.

HARDY, G. H.: "Pure Mathematics," 9th ed., Cambridge University Press, New York, 1947.

‚Äî and Rogosinski, w.: "Fourier Series," 2d ed., Cambridge University Press, New York, 1950.

Herstein, I. N.: "Topics in Algebra," Blaisdell Publishing Company, New York, 1964. hewitt, E., and STROMBerg, K.: "Real and Abstract Analysis," Springer Publishing Co., Inc., New York, 1965.

KellogG, O. D.: "Foundations of Potential Theory," Frederick Ungar Publishing Co., New York, 1940.

KNOPP, K.: "Theory and Application of Infinite Series," Blackie \& Son, Ltd., Glasgow, 1928.

LANDau, E. G. H.: "Foundations of Analysis," Chelsea Publishing Company, New York, 1951.

MCShane, E. J.: "Integration," Princeton University Press, Princeton, N.J., 1944.

NIVEN, I. M.: "Irrational Numbers," Carus Mathematical Monograph No. 11, John Wiley \& Sons, Inc., New York, 1956.

ROYDEN, H. L.: "Real Analysis," The Macmillan Company, New York, 1963.

RUDIN, w.: "Real and Complex Analysis," 2d ed., McGraw-Hill Book Company, New York, 1974.

SIMmONS, G. F.: "Topology and Modern Analysis," McGraw-Hill Book Company, New York, 1963.

SINGER, I. M., and THORPE, J. A.: "Lecture Notes on Elementary Topology and Geometry," Scott, Foresman and Company, Glenview, Ill., 1967.

S–ºi—Çh, K. —Ç.: "Primer of Modern Analysis," Bogden and Quigley, Tarrytown-onHudson, N.Y., 1971.

SPIVAK, M.: "Calculus on Manifolds," W. A. Benjamin, Inc., New York, 1965.

THURSTON, H. A.: "The Number System," Blackie \& Son, Ltd., London-Glasgow, 1956.

## LIST OF SPECIAL SYMBOLS

The symbols listed below are followed by a brief statement of their meaning and by the number of the page on which they are defined.

$\epsilon$ belongs to $\ldots \ldots \ldots \ldots \ldots \ldots .3$

$\notin$ does not belong to .......... 3

$c, \supset$ inclusion signs $\ldots \ldots \ldots \ldots \ldots 3$

$Q$ rational field $\ldots \ldots \ldots \ldots \ldots \ldots 3$

$<, \leq,>, \geq$ inequality signs.... 3

sup least upper bound......... 4

inf greatest lower bound ....... 4

$R$ real field $\ldots \ldots \ldots \ldots \ldots \ldots \ldots 8$

$+\infty,-\infty, \infty$ infinities $\ldots \ldots \ldots 11,27$

$\bar{z}$ complex conjugate $\ldots \ldots \ldots \ldots \ldots 14$

$\operatorname{Re}(z)$ real part.............14

$\operatorname{Im}(z)$ imaginary part ..........14

$|z|$ absolute value $\ldots \ldots \ldots \ldots \ldots .14$

$\sum$ summation sign ..........15, 59

$R^{k}$ euclidean $k$-space ..........16

0 null vector $\ldots \ldots \ldots \ldots \ldots \ldots \ldots 16$

$\mathbf{x} \cdot \mathbf{y}$ inner product $\ldots \ldots \ldots \ldots \ldots 16$

$|\mathbf{x}|$ norm of vector $\mathbf{x} \ldots \ldots \ldots \ldots 16$ $\left\{x_{n}\right\} \quad$ sequence...............26

$U, \cup$ union . ..............27

$\cap, \cap$ intersection ............27

$(a, b)$ segment $\ldots \ldots \ldots \ldots \ldots \ldots . .31$

$[a, b]$ interval ..............31

$E^{c}$ complement of $E \ldots \ldots \ldots \ldots \ldots 32$

$E^{\prime}$ limit points of $E \ldots \ldots \ldots \ldots \ldots 35$

$E$ closure of $E \ldots \ldots \ldots \ldots \ldots \ldots . \ldots 35$

$\lim \operatorname{limit} \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots 47$

$\rightarrow$ converges to ...........47, 98

lim sup upper limit ...........56

$\lim \inf$ lower limit ............56

$g \circ f$ composition ............ 86

$f(x+)$ right-hand limit .........94

$f(x-)$ left-hand limit ..........94

$f^{\prime}, \mathbf{f}^{\prime}(\mathbf{x})$ derivatives $\ldots \ldots \ldots, 103,112$

$U(P, f), U(P, f, \alpha), L(P, f), L(P, f, \alpha)$

Riemann sums $\ldots \ldots \ldots \ldots .121,122$
$\mathscr{R}, \mathscr{R}(\alpha) \quad$ classes of Riemann (Stieltjes) integrable functions ......121, 122

$\mathscr{C}(X)$ space of continuous functions 150

|| || norm $140,150,326$

exp exponential function ........179

$D_{N} \quad$ Dirichlet kernel ............189

$\Gamma(x)$ gamma function .........192

$\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ standard basis ......205

$L(X), L(X, Y)$ spaces of linear transformations.............207

[A] matrix ...............210

$D_{J} f$ partial derivative .........215

$\nabla f$ gradient...............217

$\mathscr{C}^{\prime}, \mathscr{C}^{\prime \prime} \quad$ classes of differentiable

functions

219,235

$\operatorname{det}[A]$ determinant .232

$J_{t}(\mathbf{x}) \quad$ Jacobian .234

$\frac{\partial\left(y_{1}, \ldots, y_{n}\right)}{\partial\left(x_{1}, \ldots, x_{n}\right)}$ Jacobian $I^{k} \quad k$-cell .................245

$Q^{k} \quad k$-simplex ..............247

$d x_{l} \quad$ basic $k$-form ...........257

$\wedge$ multiplication symbol ........254

$d$ differentiation operator .......260

$\omega_{T}$ transform of $\omega \ldots \ldots \ldots \ldots .262$

$\partial$ boundary operator ...........269

$\nabla \times \mathbf{F}$ curl $\ldots \ldots \ldots \ldots \ldots \ldots . .281$

$\nabla \cdot \mathbf{F}$ divergence ............281

$\mathscr{E}$ ring of elementary sets .......303

$m$ Lebesgue measure .......303, 308

$\mu$ measure ............303, 308

$\mathfrak{M}_{F}, \mathfrak{M}$ families of measurable sets 305 $\{x \mid P\}$ set with property $P \ldots \ldots \ldots .310$

$f^{+}, f^{-}$positive (negative) part

of $f \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots .312$

$K_{E}$ characteristic function .......313

$\mathscr{L}, \mathscr{L}(\mu), \mathscr{L}^{2}, \mathscr{L}^{2}(\mu)$ classes of

Lebesgue-integrable

functions $. \ldots \ldots \ldots \ldots \ldots .315,326$

Abel, N. H., 75, 174

Absolute convergence, 71 of integral, 138

Absolute value, 14

Addition (see Sum)

Addition formula, 178

Additivity, 301

Affine chain, 268

Affine mapping, 266

Affine simplex, 266

Algebra, 161

self-adjoint, 165

uniformly closed, 161

Algebraic numbers, 43

Almost everywhere, 317

Altemating series, 71

Analytic function, 172

Anticommutative law, 256

Arc, 136

Area element, 283

Arithmetic means, 80, 199

Artin, E., 192, 195

Associative law, 5, 28, 259

Axioms, 5

Baire's theorem, 46, 82

Ball, 31

Base, 45

Basic form, 257

Basis, 205

Bellman, R., 198

Bessel inequality, 188, 328

Beta function, 193

Binomial series, 201

Bohr-Mollerup theorem, 193

Borel-measurable function, 313
Borel set, 309

Boundary, 269

Bounded convergence, 322

Bounded function, 89

Bounded sequence, 48

Bounded set, 32

Brouwer's theorem, 203

Buck, R. C., 195

Cantor, G., 21, 30, 186

Cantor set, 41, 81, 138, 168, 309

Cardinal number, 25

Cauchy criterion, 54, 59, 147

Cauchy sequence, 21, 52, 82, 329

Cauchy's condensation test, 61

Cell, 31

$\mathscr{C}^{\prime \prime}$-equivalence, 280

Chain, 268

affine, 268

differentiable, 270

Chain rule, 105, 214

Change of variables. 132, 252, 262

Characteristic function, 313

Circle of convergence, 69

Closed curve, 136

Closed form, 275

Closed set, 32

Closure, 35

uniform, 151, 161

Collection, 27

Column matrix, 217

Column vector, 210

Common refinement, 123

Commutative law, 5, 28

Compact metric space, 36

Compact set, 36
Comparison test, 60

Complement, 32

Complete metric space, 54, 82 , 151,329

Complete orthonormal set, 331

Completion, 82

Complex field, 12, 184

Complex number, 12

Complex plane, 17

Component of a function, 87, 215

Composition, 86, 105, 127, 207

Condensation point, 45

Conjugate, 14

Connected set, 42

Constant function, 85

Continuity, 85

uniform, 90

Continuous functions, space of, 150

Continuous mapping, 85

Continuously differentiable curve, 136

Continuously differentiable mapping, 219

Contraction, 220

Convergence, 47

absolute, 71

bounded, 322

dominated, 321

of integral, 138

pointwise, 144

radius of, 69,79

of sequences, 47

of series, 59

uniform, 147

Convex function, 101

Convex set. 31

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-351.jpg?height=540&width=504&top_left_y=235&top_left_x=257)

Davis, P. J., 192

Decimals, 11

Dedekind, R., 21

Dense subset, 9, 32

Dependent set, 205

Derivative, 104

directional, 218

of a form, 260

of higher order, 110

of an integral, 133, 236, 324

integration of, 134, 324

partial, 215

of power series, 173

total, 213

of a transformation, 214

of a vector-valued function, 112

Determinant, 232

of an operator, 234

product of, 233

Diagonal process, 30, 157

Diameter, 52

Differentiable function, 104, 212

Differential, 213

Differential equation, 119, 170

Differential form (see Form)

Differentiation (see Derivative)

Dimension, 205

Directional derivative, 218

Dirichlet's kemel, 189

Discontinuities, 94

Disjoint sets, 27

Distance, 30

Distributive law, 6, 20, 28

Divergence, 281

Divergence theorem, 253, 272, 288

Divergent sequence, 47

Divergent series, 59

Domain, 24

Dominated convergence theorem, $155,167,321$

Double sequence, 144

e, 63

Eberlein, W. F., 184

Elementary set, 303

Empty set, 3

Equicontinuity, 156
Equivalence relation, 25

Euclidean space, 16, 30

Euler's constant, 197

Exact form, 275

Existence theorem, 170

Exponential function, 178

Extended real number system, 11

Extension, 99

Family, 27

Fatou's theorem, 320

Fej√©r's kernel, 199

Fej√©r's theorem, 199

Field axioms, 5

Fine, N. J., 100

Finite set, 25

Fixed point, 117

theorems, 117, 203, 220

Fleming, W. H., 280

Flip, 249

Form, 254

basic, 257

of class $\mathscr{C}^{\prime}, \mathscr{C}^{\prime \prime}, 254$

closed, 275

derivative of, 260

exact, 275

product of, 258, 260

sum of, 256

Fourier, J. B., 186

Fourier coefficients, 186, 187

Fourier series, 186, 187, 328

Function, 24

absolute value, 88

analytic, 172

Borel-measurable, 313

bounded, 89

characteristic, 313

component of, 87

constant, 85

continuous, 85

from left, 97

from right, 97

continuously differentiable, 219

convex, 101

decreasing, 95

differentiable, 104, 212

exponential, 178

harmonic, 297

increasing, 95

inverse, 90

Lebesgue-integrable, 315

limit, 144

linear, 206

logarithmic, 180

measurable, 310

monotonic, 95

nowhere differentiable continuous, 154

one-to-one, 25

orthogonal, 187

periodic, 183

product of, 85

rational, 88

Riemann-integrable, 121
Function:

simple, 313

sum of, 85

summable, 315

trigonometric, 182

uniformly continuous, 90

uniformly differentiable, 115

vector-valued, 85

Fundamental theorem of calculus, 134,324

Gamma function, 192

Geometric series, 61

Gradient, 217, 281

Graph, 99

Greatest lower bound, 4

Green's identities, 297

Green's theorem, 253, 255, 272. 282

Half-open interval, 31

Harmonic function, 297

Havin, V. P., 113

Heine-Borel theorem, 39

Helly's selection theorem, 167

Herstein, I. N., 65

Hewitt, E., 21

Higher-order derivative, 110

Hilbert space, 332

H√∂lder's inequality, 139

$i, 13$

Identity operator, 232

Image, 24

Imaginary part, 14

Implicit function theorem, 224

Improper integral, 139

Increasing index, 257

Increasing sequence, 55

Independent set, 205

Index of a curve, 201

Infimum, 4

Infinite series, 59

Infinite set, 25

Infinity, 11

Initial-value problem, 119, 170

Inner product, 16

Integrable functions, spaces of, 315,326

Integral:

countable additivity of, 316

differentiation of, 133, 236, 324

Lebesgue, 314

lower, 121, 122

Riemann, 121

Stieltjes, 122

upper, 121, 122

Integral test, 139

Integration:

of derivative, 134, 324

by parts, 134, 139, 141

Interior, 43

Interior point, 32

Intermediate value, 93, 100, 108

Intersection, 27

Interval, 31, 302

Into, 24

Inverse function, 90

Inverse function theorem, 221

Inverse image, 24

Inverse of linear operator, 207

Inverse mapping, 90

Invertible transformation, 207

Irrational number, 1, 10, 65

Isolated point, 32

Isometry, 82, 170

Isomorphism, 21

Jacobian, 234

Kellogg, O. D., 281

Kestelman, H., 167

Knopp, K., 21, 63

Landau, E. G. H., 21

Laplacian, 297

Least upper bound, 4 property, 4, 18

Lebesgue, H.|L., 186

Lebesgue-integrable function, 315

Lebesgue integral, 314

Lebesgue measure, 308

Lebesgue's theorem, 155, 167, 318,321

Left-hand limit, 94

Leibnitz, G.W., 71

Length, 136

L'Hospital's rule, 109, 113

Limit, 47, 83, 144

left-hand, 94

lower, 56

pointwise, 144

right-hand, 94

subsequential, 51

upper, 56

Limit function, 144

Limit point, 32

Line, 17

Line integral, 255

Linear combination, 204

Linear function, 206

Linear mapping, 206

Linear operator, 207

Linear transformation, 206

Local maximum, 107

Localization theorem, 190

Locally one-to-one mapping, 223

Logarithm, 22, 180

Logarithmic function, 180

Lower bound, 3

Lower integral, 121, 122

Lower limit, 56
Mapping, 24

affine, 266

continuous, 85

continuously differentiable, 219

linear, 206

open, 100, 223

primitive, 248

uniformly continuous, 90

(See also Function)

Matrix, 210

product, 211

Maximum, 90

Mean square approximation, 187

Mean value theorem, 108, 235

Measurable function, 310

Measurable set, 305, 310

Measurable space, 310

Measure, 308

outer, 304

Measure space, 310

Measure zero, set of, 309, 317

Mertens, F., 74

Metric space, 30

Minimum, 90

M√∂bius band, 298

Monotone convergence theorem, 318

Monotonic function, 95, 302

Monotonic sequence, 55

Multiplication (see Product)

Negative number, 7

Negative orientation, 267

Neighborhood, 32

Newton's method, 118

Nijenhuis, A., 223

Niven, I., 65, 198

Nonnegative number, 60

Norm, 16, 140, 150, 326

of operator, 208

Normal derivative, 297

Normal space, 101

Normal vector, 284

Nowhere differentiable function, 154

Null space, 228

Null vector, 16

Number:

algebraic, 43

cardinal, 25

complex, 12

decimal, 11

finite, 12

irrational, 1, 10, 65

negative, 7

nonnegative, 60

positive, 7,8

rational, 1

real, 8

One-to-one correspondence, 25

Onto, 24

Open cover, 36
Open mapping, 100, 223

Open set, 32

Order, 3, 17

lexicographic, 22

Ordered field, 7, 20

$k$-tuple, 16

pair, 12

set, $3,18,22$

Oriented simplex, 266

Origin, 16

Orthogonal set of functions, 187

Orthonormal set, 187, 327, 331

Outer measure, 304

Parameter domain, 254

Parameter interval, 136

Parseval's theorem, 191, 198, 328, 331

Partial derivative, 215

Partial sum, 59, 186

Partition, 120

of unity, 251

Perfect set, 32

Periodic function, 183, 190

$\pi, 183$

Plane, 17

Poincar√®'s lemma, 275, 280

Pointwise bounded sequence, 155

Pointwise convergence, 144

Polynomial, 88

trigonometric, 185

Positive orientation, 267

Power series, 69, 172

Primes, 197

Primitive mapping, 248

Product, 5

Cauchy, 73

of complex numbers, 12

of determinants, 233

of field elements, 5

of forms, 258, 260

of functions, 85

inner, 16

of matrices, 211

of real numbers, 19, 20

scalar, 16

of series, 73

of transformations, 207

Projection, 228

Proper subset, 3

Radius, 31,32

of convergence, 69,79

Range, 24, 207

Rank, 228

Rank theorem, 229

Ratio test, 66

Rational function, 88

Rational number, 1

Real field, 8

Real line, 17

Real number, 8

Real part. 14

Rearrangement, 75

Rectifiable curve, 136

Refinement, 123

Reflexive property, 25

Regular set function, 303

Relatively open set, 35

Remainder, 211, 244

Restriction, 99

Riemann, B., 76, 186

Riemann integral, 121

Riemann-Stieltjes integral, 122

Riesz-Fischer theorem, 330

Right-hand limit, 94

Ring, 301

Robison, G. B., 184

Root, 10

Root test, 65

Row matrix, 217

Saddle point, 240

Scalar product, 16

Schoenberg, I. J., 168

Schwarz inequality, 15, 139, 326

Segment, 31

Self-adjoint algebra, 165

Separable space, 45

Separated sets, 42

Separation of points, 162

Sequence, 26

bounded, 48

Cauchy, 52, 82, 329

convergent, 47

divergent, 47

double, 144

of functions, 143

increasing, 55

monotonic, 55

pointwise bounded, 155

pointwise convergent, 144

uniformly bounded, 155

unif ormly convergent, 157

Series, 59

absolutely convergent, 71

alternating, 71

convergent, 59

divergent, 59

geometric, 61

nonabsolutely convergent, 72

power, 69,172

product of, 73

trigonometric, 186

uniformly convergent, 157

Set, 3

at most countable, 25

Borel, 309

bounded, 32

bounded above, 3

Cantor, 41, 81, 138, 168, 309

closed, 32

compact, 36

complete orthonormal, 331

connected, 42

convex, 31

countable, 25
Set,

dense, 9, 32

elementary, 303

empty, 3

finite, 25

independent, 205

infinite, 25

measurable, 305,310

nonempty, 3

open, 32

ordered, 3

perfect, 32, 41

relatively open, 35

uncountable, 25, 30, 41

Set function, 301

$\sigma$-ring, 301

Simple discontinuity, 94

Simple function, 313

Simplex, 247

affine, 266

differentiable, 269

oriented, 266

Singer, I. M., 280

Solid angle, 294

Space:

compact metric, 36

complete metric, 54

connected, 42

of continuous functions, 150

euclidean, 16

Hilbert, 332

of integrable functions, 315,326

measurable, 310

measure, 310

metric, 30

normal, 101

separable, 45

Span, 204

Sphere, 272, 277, 294

Spivak, M., 272, 280

Square root, 2, 81, 118

Standard basis, 205

Standard presentation, 257

Standard simplex, 266

Stark, E. L.. 199

Step function, 129

Stieltjes integral, 122

Stirling's formula, 194, 200

Stokes' theorem, 253, 272, 287

Stone-Weierstrass theorem, 162, 190,246

Stromberg, K., 21

Subadditivity, 304

Subcover, 36

Subfield, 8, 13

Subsequence, 51

Subsequential limit, 51

Subset, 3

dense, 9, 32

proper, 3

Sum, 5

of complex numbers, 12

of field elements, 5

of forms, 256

of functions. 85
Sum ,

of linear transformations, 207

of oriented simplexes, 268

of real numbers, 18

of series, 59

of vectors, 16

Summation by parts, 70

Support, 246

Supremum, 4

Supremum norm, 150

Surface, 254

Symmetric difference, 305

Tangent plane, 284

Tangent vector, 286

Tangential component, 286

Taylor polynomial, 244

Taylor's theorem, 110, 116, 176, 24.

Thorpe, J. A., 280

Thurston, H. A., 21

Torus, 239-240, 285

Total derivative, 213

Transformation (see Function; Mapping)

Transitivity, 25

Triangle inequality, 14, 16, 30, 140

Trigonometric functions, 182

Trigonometric polynomial, 185

Trigonometric series, 186

Uncountable set, 25, 30, 41

Uniform boundedness, 155

Uniform closure, 151

Uniform continuity, 90

Uniform convergence, 147

Uniformly closed algebra, 161

Uniformly continuous mapping, 90

Union, 27

Uniqueness theorem, 119, 258

Unit cube, 247

Unit vector, 217

Upper bound, 3

Upper integral, 121, 122

Upper limit, 56

Value, 24

Variable of integration, 122

Vector, 16

Vector field, 281

Vector space, 16, 204

Vector-valued function, 85 derivative of, 112

Volume, 255, 282

Weierstrass test, 148

Weierstrass theorem, 40, 159

Winding number, 201

Zero set, 98, 117

Zeta function. 141

ISBN 978-0.07-054235.8

MHID 0.07.054235.X

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-354.jpg?height=274&width=559&top_left_y=2205&top_left_x=1279)

www.mhhe.com

## $0=07=054235=x$

$2 A$
300
38
1976

supp.

VATH

# Solutions Manual to Walter <br> Rudin's Principles of <br> Mathematical Analysis 

Roger Cooke, University of Vermont

## Contents

1 The Real and Complex Number Systems , 5

2 Basic Topology 15

3 Numerical Sequences and Series $\quad 29$

4 Continuity 49

5 Differentiation $\quad 67$

6 The Riemann-Stieltjes Integral 93

7 Sequences and Series of Functions 109

8 Some Special Functions 129

9 Functions of Several Variables 153

10 Integration of Differential Forms 175

11 The Lebesgue Theory 231

## Chapter 1

## The Real and Complex Number Systems

Exercise 1.1 If $r$ is rational $(r \neq 0)$ and $x$ is irrational, prove that $r+x$ and $r x$ are irrational.

Solution. If $r$ and $r+x$ were both rational, then $x=r+x-r$ would also be rational. Similarly if $r x$ were rational, then $x=\frac{r x}{r}$ would also be rational.

Exercise 1.2 Prove that there is no rational number whose square is 12.

First Solution. Since $\sqrt{12}=2 \sqrt{3}$, we can invoke the previous problem and prove that $\sqrt{3}$ is irrational. If $m$ and $n$ are integers having no common factor and such that $m^{2}=3 n^{2}$, then $m$ is divisible by 3 (since if $m^{2}$ is divisible by 3 , so is $m$ ). Let $m=3 k$. Then $m^{2}=9 k^{2}$, and we have $3 k^{2}=n^{2}$. It then follows that $n$ is also divisible by 3 contradicting the assumption that $m$ and $n$ have no common factor.

Second Solution. Suppose $m^{2}=12 n^{2}$, where $m$ and $n$ have no common factor. It follows that $m$ must be even, and therefore $n$ must be odd. Let $m=2 r$. Then we have $r^{2}=3 n^{2}$, so that $r$ is also odd. Let $r=2 s+1$ and $n=2 t+1$. Then

$$
4 s^{2}+4 s+1=3\left(4 t^{2}+4 t+1\right)=12 t^{2}+12 t+3
$$

so that

$$
4\left(s^{2}+s-3 t^{2}-3 t\right)=2
$$

But this is absurd, since 2 cannot be a multiple of 4 .

Exercise 1.3 Prove Proposition 1.15, i.e., prove the following statements:

(a) If $x \neq 0$ and $x y=x z$, then $y=z$.

(b) If $x \neq 0$ and $x y=x$, then $y=1$.

(c) If $x \neq 0$ and $x y=1$, then $y=1 / x$.

(d) If $x \neq 0$, then $1 /(1 / x)=x$.

Solution. (a) Suppose $x \neq 0$ and $x y=x z$. By Axiom (M5) there exists an element $1 / x$ such that $1 / x=1$. By (M3) and (M4) we have $(1 / x)(x y)=$ $((1 / x) x) y=1 y=y$, and similarly $(1 / x)(x z)=z$. Hence $y=z$.

(b) Apply (a) with $z=1$.

(c) Apply (a) with $z=1 / x$.

(d) Apply (a) with $x$ replaced by $1 / x, y=1 /(1 / x)$, and $z=x$.

Exercise 1:4 Let $E$ be a nonempty subset of an ordered set; suppose $\alpha$ is a lower bound of $E$, and $\beta$ is an upper bound of $E$. Prove that $\alpha \leq \beta$.

Solution. Since $E$ is nonempty, there exists $x \in E$. Then by definition of lower and upper bounds we have $\alpha \leq x \leq \beta$, and hence by property ii in the definition of an ordering, we have $\alpha<\beta$ unless $\alpha=x=\beta$.

Exercise 1.5 Let $A$ be a nonempty set of real numbers which is bounded below. Let $-A$ be the set of all numbers $-x$, where $x \in A$. Prove that

$$
\inf A=-\sup (-A)
$$

Solution: We need to prove that $-\sup (-A)$ is the greatest lower bound of $A$. For brevity, let $\alpha=-\sup (-A)$. We need to show that $\alpha \leq x$ for all $x \in A$ and $\alpha \geq \beta$ if $\beta$ is any lower bound of $A$.

Suppose $x \in A$. Then, $-x \in-A$, and, hence $-x \leq \sup (-A)$. It follows that $x \geq-\sup (-A)$, i.e., $\alpha \leq x$. Thus $\alpha$ is a lower bound of $A$.

Now let $\beta$ be any lower bound of $A$. This means $\beta \leq x$ for all $x$ in $A$. Hence $-x \leq-\beta$ for all $x \in A$, which says $y \leq-\beta$ for all $y \in-A$. This means $-\beta$ is an upper bound of $-A$. Hence $-\beta \geq \sup (-A)$ by definition of sup, i.e., $\beta \leq-\sup (-A)$, and so $-\sup (-A)$ is the greatest lower bound of $A$.

Exercise 1.6 Fix $b>1$.

(a) If $m, n, p, q$ are integers, $n>0, q>0$, and $r=m / n=p / q$, prove that

$$
\left(b^{m}\right)^{1 / n}=\left(b^{p}\right)^{1 / q} .
$$

Hence it makes sense to define $b^{r}=\left(b^{m}\right)^{1 / n}$.

(b) Prove that $b^{r+s}=b^{r} b^{s}$ if $r$ and $s$ are rational.
(c) If $x$ is real, define $B(x)$ to be the set of all numbers $b^{t}$, where $t$ is rational and $t \leq x$. Prove that

$$
b^{r}=\sup B(r)
$$

when $r$ is rational. Hence it makes sense to define

$$
b^{x}=\sup B(x)
$$

for every real $x$.

(d) Prove that $b^{x+y}=b^{x} b^{y}$ for all real $x$ and $y$.

Solution. (a) Let $k=m q=n p$. Since there is only one positive real number $c$ such that $c^{n q}=b^{k}$ (Theorem 1.21), if we prove that both $\left(b^{m}\right)^{1 / n}$ and $\left(b^{p}\right)^{1 / q}$ have this property, it will follow that they are equal. The proof is then a routine computation: $\left(\left(b^{m}\right)^{1 / n}\right)^{n q}=\left(b^{m}\right)^{q}=b^{m q}=b^{k}$, and similarly for $\left(b^{p}\right)^{1 / q}$.

(b) Let $r=\frac{m}{n}$ and $s=\frac{v}{w}$. Then $r+s=\frac{m w+v n}{n w}$, and

$$
b^{r+s}=\left(b^{m w+v n}\right)^{1 / n w}=\left(\left(b^{m w} b^{v n}\right)\right)^{1 / n w},
$$

by the laws of exponents for integer exponents. By the corollary to Theorem 1.21 we then have

$$
b^{r+s}=\left(b^{m w}\right)^{1 / n w}\left(b^{n v}\right)^{1 / n w}=b^{r} b^{s}
$$

where the last equality follows from part $(a)$.

(c) It will simplify things later on if we amend the definition of $B(x)$ slightly, by defining it as $\left\{b^{t}: t\right.$ rational, $\left.t<x\right\}$. It is then slightly more difficult to prove that $b^{r}=\sup B(r)$ if $r$ is rational, but the technique of Problem 7 comes to our rescue. Here is how: It is obvious that $b^{r}$ is an upper bound of $B(r)$. We need to show that it is the least upper bound. The inequality $b^{1 / n}<t$ if $n>(b-1) /(t-1)$ is proved just as in Problem 7 below. It follows that if $0<x<b^{r}$, there exists an integer $n$ with $b^{1 / n}<b^{r} / x$, i.e., $x<b^{r-1 / n} \in B(r)$. Hence $x$ is not an upper bound of $B(r)$, and so $b^{r}$ is the least upper bound.

(d) By definition $b^{x+y}=\sup B(x+y)$, where $B(x+y)$ is the set of all numbers $b^{t}$ with $t$ rational and $t<x+y$. Now any rational number $t$ that is less than $x+y$ can be written as $r+s$, where $r$ and $s$ are rational, $r<x$, and $s<y$. To do this, let $r$ be any rational number satisfying $t-y<r<x$, and let $s=t-r$. Conversely any pair of rational numbers $r, s$ with $r<x, s<y$ gives a rational sum $t=r+s<x+y$. Hence $B(x+y)$ can be described as the set of all numbers $b^{r} b^{s}$ with $r<x, s<y$, and $r$ and $s$ rational, i.e., $B(x+y)$ is the set of all products $u v$, where $u \in B(x)$ and $v \in B(y)$.

Since any such product is less than $\sup B(x) \sup B(y)$, we see that the number $M=\sup B(x) \sup B(y)$ is an upper bound for $B(x+y)$. On the other hand, suppose $0<c<\sup B(x) \sup B(y)$. Then $c /(\sup B(x))<\sup B(y)$. Let $m=(1 / 2)(c /(\sup B(x))+\sup B(y))$. Then $c / \sup B(x)<m<\sup B(y)$, and there exist $u \in B(x), v \in B(y)$ such that $c / m<u$ and $m<v$. Hence we have
$c=(c / m) m<u v \in B(x+y)$, and so $c$ is not an upper bound for $B(x+y)$. It follows that $\sup B(x) \sup B(y)$ is the least upper bound of $B(x+y)$, i.e.,

$$
b^{x+y}=b^{x} b^{y}
$$

as required.

Exercise 1.7 Fix $b>1, y>0$, and prove that there is a unique real $x$ such that $b^{x}=y$, by completing the following outline. (This $x$ is called the logarithm of $y$ to the base b.)

(a) For any positive integer $n, b^{n}-1 \geq n(b-1)$.

(b) Hence $b-1 \geq n\left(b^{1 / n}-1\right)$.

(c) If $t>1$ and $n>(b-1) /(t-1)$, then $b^{1 / n}<t$.

(d) If $w$ is such that $b^{w}<y$, then $b^{w+(1 / n)}<y$ for sufficiently large $n$; to see this apply part (c) with $t=y \cdot b^{-w}$.

(e) If $b^{w}>y$, then $b^{w-(1 / n)}>y$ for sufficiently large $n$.

(f) Let $A$ be the set of all $w$ such that $b^{w}<y$, and show that $x=\sup A$ satisfies $b^{w}=y$.

(g) Prove that this $x$ is unique.

Solution. (a) The inequality $b^{n}-1 \geq n(b-1)$ is equality if $n=1$. Then, by induction $b^{n+1}-1=b^{n+1}-b+(b-1)=b\left(b^{n}-1\right)+(b-1) \geq b n(b-1)+(b-1)=$ $(b n+1)(b-1) \geq(n+1)(b-1)$.

(b) Replace $b$ by $b^{1 / n}$ in part $(a)$.

(c) The inequality $n>(b-1) /(t-1)$ can be rewritten as $n(t-1)>(b-1)$, and since $b-1 \geq n\left(b^{1 / n}-1\right)$, we have $n(t-1)>n\left(b^{1 / n}-1\right)$, which implies $t>b^{1 / n}$.

(d) The application of part (c) with $t=y \cdot b^{-w}>1$ is immediate.

(e) The application of part $(c)$ with $t=b^{w} \cdot(1 / y)$ yields the result, as in part $(d)$ above.

(f) There are only three possibilities for the number $x=\sup A: 1) b^{x}<y$; 2) $b^{x}>y$; 3) $b^{x}=y$. The first assumption, by part $(d)$, implies that $x+(1 / n) \in A$ for large $n$, contradicting the assumption that $x$ is an upper bound for $A$. The second, by part (e), implies that $x-(1 / n)$ is an upper bound for $A$ if $n$ is large, contradicting the assumption that $x$ is the smallest upper bound. Hence the only remaining possibility is that $b^{x}=y$.

(g) Suppose $z \neq x$, say $z>x$. Then $b^{z}=b^{x+(z-x)}=b^{x} b^{z-x}>b^{x}=y$. Hence $x$ is unique. (It is easy to see that $b^{w}>1$ if $w>0$, since there is a positive rational number $r=\frac{m}{n}$ with $0<r<w$, and $b^{r}=\left(b^{m}\right)^{1 / n}$. Then $b^{m}>1$ since $b>1$, and $\left(b^{m}\right)^{1 / n}>1$ since $1^{n}=1<b^{m}$.)

Exercise 1.8 Prove that no order can be defined in the complex field that turns it into an ordered field. Hint: -1 is a square.

Solution. By Part (a) of Proposition 1.18, either $i$ or $-i$ must be positive. Hence $-1=i^{2}=(-i)^{2}$ must be positive. But then $1=(-1)^{2}$, must also be positive, and this contradicts Part $(a)$ of Proposition 1.18, since 1 and -1 cannot both be positive.

Exercise 1.9 Suppose $z=a+b i, w=c+d i$. Define $z<w$ if $a<c$, and also if $a=c$ but $b<d$. Prove that this turns the set of all complex numbers into an ordered set. (This type of order relation is called a dictionary order, or lexicographic order, for obvious reasons.) Does this ordered set have the least upper bound property?

Solution. We need to show that either $z<w$ or $z=w$, or $w<z$. Now since the real numbers are ordered, we have $a<c$ or $a=c$, or $c<a$. In the first case $z<w$; in the third case $w<z$. Now consider the second case. We must have $b<d$ or $b=d$ or $d<b$. In the first of these cases $z<w$, in the third case $w<z$, and in the second case $z=w$.

We also need to show that if $z<w$ and $w<u$, then $z<u$. Let $u=e+f i$. Since $z<w$, we have either $a<c$ or $a=c$ and $b<d$. Since $w<u$ we have either $c<f$ or $c=f$ and $d<g$. Hence there are four possible cases:

Case 1: $a<c$ and $c<f$. Then $a<f$ and so $z<u$, as required.

Case 2: $a<c$ and $c=f$ and $d<g$. Again $a<f$, and $z<u$.

Case 3: $a=c$ and $b<d$ and $c<f$. Once again $a<f$ and so $z<u$.

Case 4: $a=c$ and $b<d$ and $c=f$, and $d<g$. Then $a=f$ and $b<g$, and so $z<u$.

Exercise 1.10 Suppose $z=a+b i, w=u+i v$, and

$$
a=\left(\frac{|w|+u}{2}\right)^{1 / 2}, \quad b=\left(\frac{|w|-u}{2}\right)^{1 / 2}
$$

Prove that $z^{2}=w$ if $v \geq 0$ and that $(\bar{z})^{2}=w$ if $v \leq 0$. Conclude that every complex number (with one exception) has two complex square roots.

Solution.

$$
z^{2}=(a+b i)^{2}=\left(a^{2}-b^{2}\right)+2 a b i
$$

Now

$$
a^{2}-b^{2}=\frac{|w|+u}{2}-\frac{|w|-u}{2}=u
$$

and, since $(x y)^{1 / 2}=x^{1 / 2} y^{1 / 2}$

$$
2 a b=2\left(\frac{|w|+u}{2} \frac{|w|-u}{2}\right)^{1 / 2}=2\left(\frac{|w|^{2}-u^{2}}{4}\right)^{1 / 2}
$$

Hence

$$
2 a b=2\left(\left(\frac{v}{2}\right)^{2}\right)^{1 / 2}
$$

Now $\left(x^{2}\right)^{1 / 2}=x$ if $x \geq 0$ and $\left(x^{2}\right)^{1 / 2}=-x$ if $x \leq 0$. We conclude that $2 a b=v$ if $v \geq 0$ and $2 a b=-v$ if $v \leq 0$. Hence $z^{2}=w$ if $v \geq 0$. Replacing $b$ by $-b$, we find that $(\bar{z})^{2}=w$ if $v \leq 0$.

Hence every non-zero complex number has (at least) two complex square roots.

Exercise 1.11. If $z$ is a complex number, prove that there exists an $r \geq 0$ and a complex number $w$ with $|w|=1$ such that $z=r w$. Are $w$ and $r$ always uniquely determined by $z$ ?

Solution. If $z=0$, we take $r=0, w=1$. (In this case $w$ is not unique.) Otherwise we take $r=|z|$ and $w=z /|z|$, and these choices are unique, since if $z=r w$, we must have $r=r|w|=|r w|=|z|, z / r$.

Exercise 1.12 If $z_{1}, \ldots, z_{n}$ are complex, prove that

$$
\left|z_{1}+z_{2}+\cdots+z_{n}\right| \leq\left|z_{1}\right|+\left|z_{2}\right|+\cdots+\left|z_{n}\right| .
$$

Solution. The case $n=2$ is Part (e) of Theorem 1.33. We can then apply this result and induction on $n$ to get

$$
\begin{aligned}
\left|z_{1}+z_{2}+\cdots z_{n}\right| & =\left|\left(z_{1}+z_{2}+\cdots+z_{n-1}\right)+z_{n}\right| \\
& \leq\left|z_{1}+z_{2}+\cdots+z_{n-1}\right|+\left|z_{n}\right| \\
& \leq\left|z_{1}\right|+\left|z_{2}\right|+\cdots+\left|z_{n-1}\right|+\left|z_{n}\right|
\end{aligned}
$$

Exercise 1.13 If $x, y$ are complex, prove that

$$
|| x|-| y|| \leq|x-y|
$$

Solution. Since $x=x-y+y$, the triangle inequality gives

$$
|x| \leq|x-y|+|y|
$$

so that $|x|-|y| \leq|x-y|$. Similarly $|y|-|x| \leq|x-y|$. Since $|x|-|y|$ is a real number we have either ||$x|-| y||=|x|-|y|$ or ||$x|-| y||=|y|-|x|$. In either case, we have shown that ||$x|-| y|| \leq|x-y|$.

Exercise 1.14 If $z$ is a complex number such that $|z|=1$, that is, such that $z \ddot{z}=1$, compute

$$
|1+z|^{2}+|1-z|^{2}
$$

Solution. $|1+z|^{2}=(1+z)(1+\bar{z})=1+\bar{z}+z+z \bar{z}=2+z+\bar{z}$. Similarly $|1-z|^{2}=(1-z)(1-\bar{z})=1-z-\bar{z}+z \bar{z}=2-z-\bar{z}$. Hence

$$
|1+z|^{2}+|1-z|^{2}=4
$$

Exercise 1.15 Under what conditions does equality hold in the Schwarz inequality?

Solution. The proof of Theorem 1.35 shows that equality can hold if $B=0$ or if $B a_{j}-C b_{j}=0$ for all $j$, i.e., the numbers $a_{j}$ are proportional to the numbers $b_{j}$. (In terms of linear algebra this means the vectors $\mathbf{a}=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ and $\mathbf{b}=\left(b_{1}, b_{2}, \ldots, b_{n}\right)$ in complex $n$-dimensional space are linearly dependent. Conversely, if these vectors are linearly independent, then strict inequality holds.)

Exercise 1.16 Suppose $k \geq 3, \mathbf{x}, \mathbf{y} \in R^{k},|\mathbf{x}-\mathbf{y}|=d>0$, and $r>0$. Prove: (a) If $2 r>d$, there are infinitely many $\mathbf{z} \in R^{k}$ such that

$$
|\mathbf{z}-\mathbf{x}|=|\mathbf{z}-\mathbf{y}|=r
$$

(b) If $2 r=d$, there is exactly one such $\mathbf{z}$.

(c) If $2 r<d$, there is no such $\mathbf{z}$.

How must these statements be modified if $k$ is 2 or 1 ?

Solution. (a) Let $\mathbf{w}$ be any vector satisfying the following two equations:

$$
\begin{aligned}
\mathbf{w} \cdot(\mathbf{x}-\mathbf{y}) & =0 \\
|\mathbf{w}|^{2} & =r^{2}-\frac{d^{2}}{4}
\end{aligned}
$$

From linear algebra it is known that all but one of the components of a solution $\mathbf{w}$ of the first equation can be arbitrary. The remaining component is then uniquely determined. Also, if $\mathbf{w}$ is any non-zero solution of the first equation, there is a unique positive number $t$ such that $t w$ satisfies both equations. (For example, if $x_{1} \neq y_{1}$, the first equation is satisfied whenever

$$
z_{1}=\frac{z_{2}\left(x_{2}-y_{2}\right)+\cdots+z_{k}\left(x_{k}-y_{k}\right)}{y_{1}-x_{1}}
$$

If $\left(z_{1}, z_{2}, \ldots, z_{k}\right)$ satisfies this equation, so does $\left(t z_{1}, t z_{2}, \ldots, t z_{k}\right)$ for any real number $t$.) Since at least two of these components can vary independently, we can find a solution with these components having any prescribed ratio. This
ratio does not change when we multiply by the positive number $t$ to obtain a solution of both equations. Since there are infinitely many ratios, there are infinitely many distinct solutions. For each such solution $\mathbf{w}$ the vector $\mathbf{z}=$ $\frac{1}{2} x+\frac{1}{2} y+w$ is a solution of the required equation. For

$$
\begin{aligned}
|\mathbf{z}-\mathbf{x}|^{2} & =\left|\frac{\mathbf{y}-\mathbf{x}}{2}+\mathbf{w}\right|^{2} \\
& =\left|\frac{\mathbf{y}-\mathbf{x}}{2}\right|^{2}+2 \mathbf{w} \cdot \frac{\mathbf{x}-\mathbf{y}}{2}+|\mathbf{w}|^{2} \\
& =\frac{d^{2}}{4}+0+r^{2}-\frac{d^{2}}{4} \\
& =r^{2}
\end{aligned}
$$

and a similar relation holds for $|\mathbf{z}-\mathbf{y}|^{2}$.

(b) The proof of the triangle inequality shows that equality can hold in this inequality only if it holds in the Schwarz inequality, i.e., one of the two vectors is a scalar multiple of the other. Further examination of the proof shows that the scalar must be nonnegative. Now the conditions of this part of the problem show that

$$
|\mathbf{x}-\mathbf{y}|=d=|\mathbf{x}-\mathbf{z}|+|\mathbf{z}-\mathbf{y}|
$$

Hence it follows that there is a nonnegative scalar $t$ such that

$$
\mathrm{x}-\mathrm{z}=t(\mathbf{z}-\mathbf{y})
$$

However, the hypothesis also shows immediately that $t=1$, and so $\mathbf{z}$ is uniquely determined as

$$
z=\frac{x+y}{2}
$$

(c) If $\mathbf{z}$ were to satisfy this condition, the triangle inequality would be violated, i.e., we would have

$$
|\mathbf{x}-\mathbf{y}|=d>2 r=|\mathbf{x}-\mathbf{z}|+|\mathbf{z}-\mathbf{y}|
$$

When $k=2$, there are precisely 2 solutions in case $(a)$. When $k=1$, there are no solutions in case $(a)$. The conclusions in cases $(b)$ and $(c)$ do not require modification.

Exercise 1.17 Prove that

$$
|x+y|^{2}+|x-y|^{2}=2|x|^{2}+2|y|^{2}
$$

if $\mathrm{x} \in R^{k}$ and $\mathrm{y} \in R^{k}$. Interpret this geometrically as a statement about parallelograms.

Solution. The proof is a routine computation, using the relation

$$
|x \pm y|^{2}=(x \pm y) \cdot(x \pm y)=|x|^{2} \pm 2 x \cdot y+|y|^{2}
$$

If $\mathrm{x}$ and $\mathrm{y}$ are the sides of a parallelogram, then $\mathrm{x}+\mathrm{y}$ and $\mathbf{x}-\mathbf{y}$ are its diagonals. Hence this result says that the sum of the squares on the diagonals of a parallelogram equals the sum of the squares on the sides.

Exercise 1.18 If $k \geq 2$ and $\mathrm{x} \in R^{k}$, prove that there exists $\mathrm{y} \in R^{k}$ such that $\mathbf{y} \neq \mathbf{0}$ but $\mathbf{x} \cdot \mathbf{y}=0$. Is this also true if $k=1$ ?

Solution. If $\mathrm{x}$ has any components equal to 0 , then $\mathrm{y}$ can be taken to have the corresponding components equal to 1 and all others equal to 0 . If all the components of $\mathrm{x}$ are nonzero, $\mathrm{y}$ can be taken as $\left(-x_{2}, x_{1}, 0, \ldots, 0\right)$. This is, of course, not true when $k=1$, since the product of two nonzero real numbers is nonzero.

Exercise 1.19 Suppose $\mathrm{a} \in R^{k}, \mathrm{~b} \in R^{k}$. Find $\mathrm{c} \in R^{k}$ and $r>0$ such that

$$
|x-a|=2|x-b|
$$

if and only if $|\mathbf{x}-\mathbf{c}|=r$. (Solution: $3 \mathbf{c}=4 \mathbf{b}-\mathbf{a}, 3 r=2|\mathbf{b}-\mathbf{a}| \cdot)$

Solution. Since the solution is given to us, all we have to do is verify it, i.e., we need to show that the equation

$$
|x-a|=2|x-b|
$$

is equivalent to $|\mathbf{x}-\mathbf{c}|=r$, which says

$$
\left|\mathbf{x}-\frac{4}{3} \mathbf{b}+\frac{1}{3} \mathbf{a}\right|=\frac{2}{3}|\mathbf{b}-\mathbf{a}|
$$

If we square both sides of both equations, we an equivalent pair of equations, the first of which reduces to

$$
3|\mathbf{x}|^{2}+2 \mathbf{a} \cdot \mathbf{x}-8 \mathbf{b} \cdot \mathbf{x}-|\mathbf{a}|^{2}+4|\mathbf{b}|^{2}=0
$$

and the second of which reduces to this equation divided by 3 . Hence these equations are indeed equivalent.

Exercise 1.20 With reference to the Appendix, suppose that property (III) were omitted from the definition of a cut. Keep the same definitions of order and addition. Show that the resulting ordered set has the least-upper-bound property, that addition satisfies axioms (A1) to (A4) (with a slightly different zero element!) but that (A5) fails.

Solution. We are now defining a cut to be a proper subset of the rational numbers that contains, along with each of its elements, all smaller rational
numbers. Order is defined by containment. Now given a set $A$ of cuts having an upper bound $\beta$, let $\alpha$ be the union of all the cuts in $A$. Obviously $\alpha$ is properly contained in $\beta$, and so is a proper subset of the rationals. It also obviously satisfies the property that if $p \in \alpha$ and $q<p$, then $q \in \alpha$; hence $\alpha$ is a cut. It is further obvious that $\alpha$ contains each elements of $A$, and so is an upper bound for $A$. It remains to prove that there is no smaller upper bound.

To that end, suppose, $\gamma<\alpha$, then $\alpha$ contains an element $x$ not in $\gamma$. By definition of $\alpha, x$ must belong to some cut $\delta$ in $A$. But then $\gamma<\delta$, and so $\gamma$ is not an upper bound for $A$. Thus $\alpha$ is the least upper bound.

The proof given in the text goes over without any change to show that (A1), (A2), and (A3) hold. As for (A4) let $O=\{r: r \leq 0\}$. We claim $O+\alpha=\alpha$. The proof is easy. First, we obviously have $O+\alpha \subseteq \alpha$. For $r+s \leq s$ if $r \leq 0$. Hence $r+s \in \alpha$ if $s \in \alpha$. Conversely $\alpha \subseteq O+\alpha$, since each $s$ in $\alpha$ can be written as $0+s$.

Unfortunately, if $O^{\prime}=\{r: r<0\}$, there is no element $\alpha$ such that $\alpha+O^{\prime}=$ $O$. For $\alpha+O^{\prime}$ has no largest element. If $x=r+s \in \alpha+O^{\prime}$, where $r \in \alpha$ and $s \in O^{\prime}$, there is an element $t \in O^{\prime}$ with $t>s$, and so $r+t \in \alpha+O^{\prime}$ and $r+t>s$. Since $O$ has a largest element (namely 0 ), these two sets cannot be equal.

## Chapter 2

## Basic Topology

Exercise 2.1 Prove that the empty set is a subset of every set.

Solution. Let $\varnothing$ denote the empty set, and let $E$ be any set. The statement $\varnothing \subset E$ is equivalent to the statement, "If $x \in \varnothing$, then $x \in E$." Since the hypothesis of this if-then statement is false, the implication is true, and we are done.

Exercise 2.2 A complex number $z$ is said to be algebraic if there are integers $a_{0}, \ldots, a_{n}$, not all zero, such that

$$
a_{0} z^{n}+a_{1} z^{n-1}+\cdots+a_{n-1} z+a_{n}=0
$$

Prove that the set of all algebraic numbers is countable. Hint: For every positive integer $N$ there are only finitely many equations with

$$
n+\left|a_{0}\right|+\left|a_{1}\right|+\cdots+\left|a_{n}\right|=N
$$

Solution. Following the hint, we let $A_{N}$ be the set of numbers satisfying one of the equations just listed with $n+\left|a_{0}\right|+\left|a_{1}\right|+\cdots+\left|a_{n}\right|=N$. The set $A_{N}$ is finite, since each equation has only a finite set of solutions and there are only finitely many equations satisfying this condition. By the corollary to Theorem 2.12 the set of algebraic numbers, which is the union $\bigcup_{N=2}^{\infty} A_{N}$, is at most countable. Since all rational numbers are algebraic, it follows that the set of algebraic numbers is exactly countable.

Exercise 2.3 Prove that there exist real numbers which are not algebraic.

Solution. By the previous exercise, the set of real algebraic numbers is countable. If every real number were algebraic, the entire set of real numbers would be countable, contradicting the remark after Theorem 2.14.

Exercise 2.4 Is the set of irrational real numbers countable?

Answer. No. If it were, the set of all real numbers, being the union of the rational and irrational numbers, would be countable.

Exercise 2.5 Construct a bounded set of real numbers with exactly three limit points.

Solution. Let $E$ be the set of numbers of the form $a+\frac{1}{n}$, where $a \in\{1,2,3\}$ and $n \in\{2,3,4,5, \ldots$,$\} . It is clear that \{1,2,3\} \subseteq E^{\prime}$, since every deleted neighborhood of 1,2 , or 3 , contains a point in $E$. Conversely, if $x \notin\{1,2,3\}$, let $\delta=\min \{|x-1|,|x-2|,|x-3|\}$. Then the set $U$ of $y$ such that $|x-y|<\delta / 2$ contains at most a finite number of points of $E$, since the set $V=\left(1,1+\frac{\delta}{2}\right) \cup$ $\left(2,2+\frac{\delta}{2}\right) \cup\left(3,3+\frac{\delta}{2}\right)$ is disjoint from $U$, and $V$ contains all the points of the set $E$ except possibly the finite set of points $a+\frac{1}{n}$ for which $n \leq \frac{2}{\delta}$. If $p_{1}, \ldots, p_{\tau}$ are the points of $E$ in $U$, let $\eta$ be the minimum of $\frac{\delta}{2}$ and the $\left|x-p_{j}\right|$ for which $x \neq p_{j}$. Then the set $W$ of points $y$ such that $|y-x|<\eta$ contains no points of $E$ except possibly $x$. Hence $x \notin E^{\prime}$. Thus $E^{\prime}=\{1,2,3\}$.

Exercise 2.6 Let $E^{\prime}$ be the set of all limit points of a set $E$. Prove that $E^{\prime}$ is closed. Prove that $E$ and $\bar{E}$ have the same limit points. (Recall that $\bar{E}=E \cup E^{\prime}$.) Do $E$ and $E^{\prime}$ always have the same limit points?

Solution. To show that $E^{\prime}$ is closed, we shall show that $\left(E^{\prime}\right)^{\prime} \subseteq E^{\prime}$. In fact, we shall show the even stronger statement that $(\bar{E})^{\prime} \subseteq E^{\prime}$. To do this let $x \in(\bar{E})^{\prime}$, and let $r>0$. We need to show that $x \in E^{\prime}$; that is, since $r>0$ is arbitrary, we need to find a point $z \in E$ with $0<d(z, x)<r$. There certainly is a point $y$ of $\bar{E}$ such that $0<d(y, x)<r$. If $y \in E$, we can take $z=y$, and we are done. If $y \notin E$, then $y \in E^{\prime}$. Let $s=\min (d(x, y), r-d(x, y))$, so that $s>0$. Since $y \in E^{\prime}$, there exists $z \in E$ with $0<d(x, z)<s$. But it then follows that $d(z, x) \geq d(x, y)-d(x, z)>0$ and $d(z, x) \leq d(x, y)+d(y, z)<$ $d(x, y)+r-d(x, y)=r$, and we are done in any case.

To show that $E$ and $\bar{E}$ have the same limit points, we need only show the converse of the preceding containment. But this is easy. Suppose $x \in E^{\prime}$. Since every deleted neighborhood of $x$ contains a point of $E$, a fortiori every deleted neighborhood of $x$ contains a point of $\bar{E}$. Hence $E^{\prime} \subseteq(\bar{E})^{\prime}$.

Certainly $E$ and $E^{\prime}$ may have different sets of limit points. For example if $E=\left\{0,1, \frac{1}{2}, \frac{1}{3}, \ldots, \frac{1}{n}, \ldots\right\}$, then $E^{\prime}=\{0\}$, while $\left(E^{\prime}\right)^{\prime}=\varnothing$.

Exercise 2.7 Let $A_{1}, A_{2}, A_{3}, \ldots$ be subsets of a metric space.

(a) If $B_{n}=\cup_{i=1}^{n} A_{i}$, prove that $\bar{B}_{n}=\cup_{i=1}^{n} \bar{A}_{i}$, for $n=1,2,3, \ldots$.

(b) If $B=\cup_{i=1}^{\infty} A_{i}$, prove that $\bar{B} \supset \cup_{i=1}^{\infty} \bar{A}_{i}$.

Show, by an example, that this inclusion can be proper.

Solution. We first show that $\overline{E \cup F}=\bar{E} \cup \bar{F}$, which follows from the stronger fact that $(E \cup F)^{\prime}=E^{\prime} \cup F^{\prime}$ : To show this, in turn, we note that if $x \in E^{\prime}$, then certainly $x \in(E \cup F)^{\prime}$, and similarly if $x \in F^{\prime}$. Hence $E^{\prime} \cup F^{\prime} \subseteq(E \cup F)^{\prime}$. To show the converse, suppose $x \notin E^{\prime} \cup F^{\prime}$. Then there is a positive number $r$ such that there is no element $y$ of $E$ with $0<d(x, y)<r$, and a positive number $s$ such that there is no element $y$ of $F$ with $0<d(x, y)<s$. Hence if $t=\min (r, s)$, then $t>0$, and there is no element $y$ of $E \cup F$ with $0<d(x, y)<t$. Therefore $x \notin(E \cup F)^{\prime}$.

The general result of $(a)$ now follows easily by induction on $n$, since

$$
\begin{aligned}
\bar{B}_{n} & =\overline{\cup_{i=1}^{n} A_{i}} \\
& =\overline{A_{1} \cup \cup_{i=2}^{n} A_{i}} \\
& =\bar{A}_{1} \cup \cup_{i=2}^{n} A_{i} \\
& =\bar{A}_{1} \cup \cup_{i=2}^{n} \bar{A}_{i} \\
& =\cup_{i=1}^{n} \bar{A}_{i} .
\end{aligned}
$$

Part $(b)$ amounts to the trivial observation that, since $B \supseteq A_{i}$ for all $i$, then $\bar{B} \supseteq \bar{A}_{i}$ for all $i$, and so

$$
\bar{B} \supseteq \cup_{i=1}^{\infty} \bar{A}_{i}
$$

If we let $A_{i}=\left\{r_{i}\right\}$, where $\left\{r_{1}, r_{2}, \ldots, r_{n}, \ldots\right\}$ is an enumeration of the rational numbers, then $B$ is the full set of rational numbers. Hence $\bar{B}=R^{1}$, while $\bar{A}_{i}=A_{i}$ for each $i$, i.e., $\cup \bar{A}_{i}$ is the set of rational numbers.

Exercise 2.8 Is every point of every open set $E \subset R^{2}$ a limit point of $E$. Answer the same question for closed sets in $R^{2}$.

Answer. Yes. Every point of an open set $E$ is a limit point of $E$. To see this, let $E$ be an open set in $R^{2}$, let $\left(x_{1}, x_{2}\right) \in E$, let $s$ be such that $\left(y_{1}, y_{2}\right) \in E$ if $\sqrt{\left(y_{1}-x_{1}\right)^{2}+\left(y_{2}-x_{2}\right)^{2}}<s$, and let $r>0$. Then the point $\left(z_{1}, z_{2}\right)=\left(x_{1}+\right.$ $\left.\frac{1}{2} \min (r, s), x_{2}\right)$ belongs to $E$ and satisfies $0<\sqrt{\left(z_{1}-x_{1}\right)^{2}+\left(z_{2}-x_{2}\right)^{2}}<r$.

There are closed sets for which this statement is not true. For example, any finite set $E$ is closed, yet $E^{\prime}=\varnothing$ for a finite set.

Exercise 2.9 Let $E^{\circ}$ denote the set of all interior points of a set $E$.

(a) Prove that $E^{\circ}$ is always open.

(b) Prove that $E$ is open if and only if $E^{\circ}=E$.

(c) If $G \subset E$ and $G$ is open, prove that $G \subset E^{\circ}$.

(d) Prove that the complement of $E^{\circ}$ is the closure of the complement of $E$.

(e) Do $E$ and $\bar{E}$ always have the same interiors?
$(f)$ Do $E$ and $E^{\circ}$ always have the same closures?

Solution. (a) Let $x \in E^{\circ}$. Then there exists $r>0$ such that $y \in E$ if $d(x, y)<r$. We claim that in fact $y \in E^{\circ}$ if $d(x, y)<r$, so that $x \in\left(E^{\circ}\right)^{\circ}$. Indeed if $d(x, y)<r$, let $s=r-d(x, y)$, so that $s>0$. Then if $d(z, y)<s$, we have (by the triangle inequality) $d(x, z)<r$, and so $z \in E$. By definition this means $y \in E^{\circ}$. Since $y$ was any point with $d(x, y)<r$, it follows that all such points are in $E^{\circ}$, and so $x \in\left(E^{\circ}\right)^{\circ}$.

(b) By definition $E$ is open if and only if each of its points is an interior point, which says precisely that $E=E^{\circ}$.

(c) If $G \subset E$ and $G$ is open; then $G=G^{\circ} \subseteq E^{\circ}$.

(d) Part $(c)$ shows that $E^{\circ}$ is the largest open set contained in $E$, i.e., the union of all open sets contained in $E$. Hence its complement is the intersection of all closed sets containing the complement of $E$, and this, by Theorem 2.27 (c), is the closure of the complement of $E$.

(e) Emphatically not. If $E$ is the rational numbers in the space $R^{1}$, then $E^{\circ}=\varnothing$, while $\bar{E}=R^{1}$, so that the interior of $\bar{E}$ is $R^{1}$.

(f) Emphatically not. If $E$ is the rational numbers in the space $R^{1}$, then $\bar{E}=R^{1}$, while $E^{\circ}=\varnothing$, so that $\overline{E^{\circ}}=\varnothing$.

Exercise 2.10 Let $X$ be an infinite set. For $p \in X$ and $q \in X$, define

$$
d(p, q)= \begin{cases}1, & (\text { if } p \neq q) \\ 0, & (\text { if } p=q)\end{cases}
$$

Prove that this is a metric. Which subsets of the resulting metric space are open? Which are closed? Which are compact?

Solution. It is obvious that $d(p, q)>0$ if $p \neq q$ and $d(p, p)=0$; likewise it is obvious that $d(p, q)=d(q, p)$. To show the triangle inequality $d(x, z) \leq$ $d(x, y)+d(y, z)$, note that the maximal value of the left-hand side is 1 , and can be attained only if $x \neq z$. In that case $y$ cannot be equal to both $x$ and $z$, so that at least one term on the right-hand side is also 1 .

Each one-point set is open in this metric, since $B_{\frac{1}{2}}(x) \subseteq\{x\}$. Therefore every set, being the union of all its one-point subsets, is open. Hence every set, being the complement of its complement, is also closed. Only finite sets are compact, since any infinite subset has an open covering (by the union of its one-point subsets) that cannot be reduced to a finite subcovering.

Exercise 2.11 For $x \in R^{1}$ and $y \in R^{1}$, define

$$
\begin{aligned}
d_{1}(x, y) & =(x-y)^{2} \\
d_{2}(x, y) & =\sqrt{|x-y|}
\end{aligned}
$$

$$
\begin{aligned}
d_{3}(x, y) & =\left|x^{2}-y^{2}\right| \\
d_{4}(x, y) & =|x-2 y| \\
d_{5}(x, y) & =\frac{|x-y|}{1+|x-y|}
\end{aligned}
$$

Determine, for each of these, whether it is a metric or not.

Solution. The function $d_{1}(x, y)$ fails the triangle inequality condition, since

$$
d_{1}(0,1)+d_{1}(1,2)=1+1=2<4=d_{1}(0,2)
$$

The function $d_{2}(x, y)$ meets the triangle inequality condition, since

$$
\sqrt{|x-z|} \leq \sqrt{|x-y|}+\sqrt{|y-z|}
$$

as one can easily see by squaring both sides. Hence $d_{2}$ is a metric.

The function $d_{3}(x, y)$ fails the positivity condition, since $d_{2}(1,-1)=0$. (Restricted to $[0, \infty), d_{3}$ would be a metric.)

Since $d_{4}\left(1, \frac{1}{2}\right)=0$, the function $d_{4}(x, y)$ likewise fails the positivity condition. It also fails the symmetry condition, since $d_{4}(x, y) \neq d_{4}(y, x)$ in general.

The function $d_{5}(x, y)$ is a metric. In fact we can prove more generally that if $d(x, y)$ is a metric, so is $\rho(x, y)=\frac{d(x, y)}{1+d(x, y)}$. It is obvious that $\rho$ meets the nonnegativity and symmetry requirements, and we need only verify the triangle inequality, which in this case says that

$$
\frac{d(x, z)}{1+d(x, z)} \leq \frac{d(x, y)}{1+d(x, y)}+\frac{d(y, z)}{1+d(y, z)}
$$

To do this, let $a=d(x, z), b=d(x, y)$, and $c=d(y, z)$. We need to show that if $a \leq b+c$, then

$$
\frac{a}{1+a} \leq \frac{b}{1+b}+\frac{c}{1+c}
$$

Clearing out the denominators, we find this inequality to be equivalent to

$$
a+a b+a c+a b c \leq b+c+a b+a c+2 b c+2 a b c
$$

which is clearly true.

Exercise 2.12 Let $K \subset R^{1}$ consist of 0 and the numbers $1 / n$, for $n=1$, 2, $3, \ldots$. Prove that $E$ is compact directly from the definition without using the Heine-Borel theorem.

Solution. Suppose $K \subset U_{\alpha}$, where $U_{\alpha}$ is open. Then 0 must be in some set $U_{\alpha_{0}}$. Since $U_{\alpha_{0}}$ is open, there exists $\delta>0$ such that $(-\delta, \delta) \subset U_{\alpha_{0}}$. In particular $1 / n \in U_{\alpha_{0}}$ if $n>\frac{1}{\delta}$. Let $N$ be the largest integer in $\frac{1}{\delta}$, and let $\alpha_{j}, j=1, \ldots, N$, be such that $\frac{1}{j} \in U_{\alpha_{j}}$. Then $K \subset \bigcup_{j=0}^{N} U_{\alpha_{j}}$.

Exercise 2.13 Construct a compact set of real numbers whose limit points form a countable set.

Solution. Let $K=\{0\} \cup\left\{\frac{1}{n}: n=1,2, \ldots\right\} \cup\left\{\frac{1}{m}+\frac{1}{n}: n=m, m+1, \ldots ; m=\right.$ $1,2, \ldots\}$. It is clear that 0 and the points $\frac{1}{m}$ are limit points of $K$. We need only show that these are all the limit points. Since $x \geq 0$ for all $x \in K$ and for any positive number $\varepsilon$ there is only a finite set of numbers in $K$ larger than $1+\varepsilon$, it is clear that no negative number and no number larger than 1 can be a limit point of $K$. Hence we need only consider positive numbers $x$ satisfying $0<x<1$. If $x$ is such a number and $x$ is not one of the points $\frac{1}{m}$, let $p$ be such that $\frac{1}{p+1}<x<\frac{1}{p}$, and let $\varepsilon=\frac{1}{2} \min \left(x-\frac{1}{p+1}, \frac{1}{p}-x\right)$. The intersection of the set $K$ with the interval $(x-\varepsilon, x+\varepsilon)$ is contained in the set of points $\left\{\frac{1}{p+1}+\frac{1}{k}: p+1 \leq k<\frac{1}{\varepsilon}\right\} \cup\left\{\frac{1}{m}+\frac{1}{n}: m \leq n<\frac{1}{p+1}-\frac{1}{p+2} ; m=p+2, \ldots, 2 p+2\right\}$, which is a finite set. Therefore $x$ cannot be a limit point of $K$.

Exercise 2.14 Give an example of an open cover of the segment $(0,1)$ which has no finite subcover.

Solution. Let $A_{n}=\left(\frac{1}{n}, \frac{n-1}{n}\right), n=3,4, \ldots$ If $0<x<1$, then $x \in A_{n}$ if $n>1 / \min (x, 1-x)$, so that $\bigcup_{n=3}^{\infty} A_{n}$ covers $(0,1)$. However, the union any finite collection $\left\{A_{1}, \ldots, A_{N}\right\}$ is an interval $\left(\frac{1}{k}, \frac{k-1}{k}\right)$, which fails to contain the point $\frac{1}{2 k}$.

Exercise 2.15 Show that Theorem 2:36 and its Corollary become false (in $R^{1}$, for example) if the word "compact" is replaced by "closed" or "bounded."

Solution. Theorem 2.36 asserts that if a family of closed subsets has the finite intersection property (any finite collection of the sets has a non-empty intersection), then the entire family has a non-empty intersection. To see why this fails for sets that are merely bounded or merely closed, let $A_{n}=\left(0, \frac{1}{n}\right)$ and $B_{n}=[n, \infty)$. The sets' $A_{n}$ are bounded, and the sets $B_{n}$ are closed. Any finite intersection of the $A_{n}^{\prime} s$ is nonempty, and any finite intersection of the $B_{n}^{\prime} s$ is nonempty, yet $\bigcap_{n=1}^{\infty} A_{n}=\varnothing=\bigcap_{n=1}^{\infty} B_{n}$.

The corollary asserts that a nested sequence of nonempty compact sets has a nonempty intersection, and the examples just given show that compactness cannot be replaced by either closedness or boundedness.

Exercise 2.16 Regard $Q$, the set of all rational numbers, as a metric space, with $d(p, q)=|p-q|$. Let $E$ be the set of all $p \in Q$ such that $2<p^{2}<3$. Show that $E$ is closed and bounded in $Q$, but that $E$ is not compact. Is $E$ open in $Q$ ?

Solution. Suppose $x \in Q \backslash E$. We claim that $x$ is an interior point of the complement of $E$ (which by definition means $E$ is closed). In fact if $x^{2} \leq 2$, then $x^{2}<2$, since there is no rational number whose square is 2 . If $x=0$, let $\delta=1$; otherwise let $\delta=\min \left(\sqrt{\frac{2-x^{2}}{3}}, \frac{2-x^{2}}{3|x|}\right)$. Then if $y \in(x-\delta, x+\delta)$, we have $y^{2}<2$. This is obvious if $x=0$ and $\delta=1$. In the other case let $y=x+h$, where $|h|<\delta$. Then $y^{2}=x^{2}+2 x h+h^{2}<x^{2}+2|x| \delta+\delta^{2}<x^{2}+\frac{2}{3}\left(2-x^{2}\right)+\frac{2-x^{2}}{3}=2$. Hence $x$ is an interior point of the complement of $E$.

Similarly suppose $x^{2} \geq 3$. Since there is no rational number whose square is 3 , we must have $x^{2}>3$. Since $x \neq 0$, we let $\delta=\frac{x^{2}-3}{2|x|}$. Then if $y \in(x-\delta, x+\delta)$, we have $y^{2}>3$. For since $y=x+h$, with $|h|<\delta$, and so $y^{2}=x^{2}+2 x h+h^{2}>$ $x^{2}-2|x| \delta=3$. Thus again $x$ is an interior point of the complement of $E$.

Hence in all cases $Q \backslash E$ is open, so that $E$ is closed.

That $E$ is bounded is obvious, since $E \subset[-2,2]$.

To show that $E$ is not compact, let $U_{n}=\left\{p: 2<p^{2}<3-\frac{1}{n}\right\}, n=2,3, \ldots$. The argument that will be used below to show that $E$ is open shows that $U_{n}$ is open. The sets. $U_{n}$ cover $E$, but no finite collection of them covers $E$. Thus $E$ is not compact.

The set $E$ is also open, since if $2<x^{2}<3$, we can let $\delta$ be the minimum of $\sqrt{\frac{3-x^{2}}{3}}, \frac{3-x^{2}}{3|x|}$, and $\frac{x^{2}-2}{2|x|}$. Then if $y \in(x-\delta, x+\delta)$, we must have $2<y^{2}<3$, by the same set of inequalities that was used above.

Exercise 2.17 Let $E$ be the set of all $x \in[0,1]$ whose decimal expansion contains only the digits 4 and 7 . Is $E$ countable? Is $E$ dense in [0,1]? Is $E$ compact? Is $E$ perfect?

Solution. The set $E$ is not countable, since for any hypothetical list of its elements $a_{1}, a_{2}, \ldots, a_{n}, \ldots$ we can always produce an element $a$ of $E$ not in the list by taking the $n$th digit of $a$ to be 4 if the $n$th digit of $a_{n}$ is 7 and equal to 7 if the $n$th digit of $a_{n}$ is 4 .

The set $E$ is not dense in $[0,1]$, since $E \subset[0.4,0.8]$

The set $E$ is closed and bounded, and therefore compact. To show that $E$ is closed, let $x \in[0,1] \backslash E$, i.e., the decimal expansion of $x$ contains a digit different from 4 and 7 . Let the first such digit occur in the $n$th place $\left(x_{n}\right)$. Let $y$ be any element of $E$, and let the first digit in which $x$ and $y$ differ be the $m$ th digit $\left(m \leq n, x_{m} \neq y_{m}\right)$. Then $|x-y| \geq 10^{-m}-\varepsilon, \varepsilon \leq \sum_{k=m+1}^{\infty} 10^{-k}\left|x_{k}-y_{k}\right|$. Since $y_{k} \in\{4,7\}$ and $x_{k} \in\{0,1,2,3,4,5,6,7,8,9\}$, it follows that $\left|x_{k}-y_{k}\right| \leq 7$. Hence $\varepsilon \leq \frac{7}{9} 10^{-m}$, and it follows that $|x-y| \geq \frac{2}{9 \cdot 10^{m}} \geq \frac{1}{9 \cdot 10^{n}}$. Thus $x$ is an interior point of $[0,1] \backslash E$, and so $E$ is closed.

The set $E$ is perfect. For each $x \in E$ and each $\varepsilon>0$ we can find a point $y \in E$ with $0<|x-y|<\varepsilon$ by changing the $n$th digit of $x$ from 4 to 7 or from 7 to 4 in the $n$th place for any $n>1-\log _{10} \varepsilon$. Hence $x \in E^{\prime}$, i.e., $E \subseteq E^{\prime}$. Since we already know $E$ is closed, it follows that $E=E^{\prime}$.

Exercise 2.18 Is there a non-empty perfect set in $R^{1}$ which contains no rational number?

Answer. Yes. Let $\left\{r_{1}, r_{2}, \ldots, r_{n}, \ldots\right\}$ be the rational numbers in the interval $[-\pi, \pi]$. Let $E_{0}=[-\pi, \pi]$. Now assume that $E_{k}$ has been chosen for $k<n$ in such a way that $E_{k}$ is a pairwise disjoint union of at most $2^{k+1}-1$ closed intervals with irrational endpoints, each of positive length at most $\left(\frac{2}{3}\right)^{k} \pi$ and that $E_{k}$ does not contain $r_{j}$ if $j \leq k$. (All of these conditions hold trivially for $k=0$.) Define a set $F_{k+1}$, which is obtained from $E_{k}$ by removing first the middle third of each of the intervals that constitute $E_{k}$. The result is a set of at most $2^{k+2}-2$ pairwise disjoint intervals having irrational endpoints, each interval being of length at most $\left(\frac{2}{3}\right)^{k+1} \pi$. If $r_{k+1} \notin F_{k+1}$, let $E_{k+1}=F_{k+1}$. If $r_{k+1} \in F_{k+1}$, then $r_{k+1}$ is not the endpoint of the interval $I=[a, b]$ of $F_{k+1}$ that it belongs to. Hence let $\delta$ be an irrational positive number less than the minimum of $r_{k+1}-a$ and $b-r_{k+1}$, and let $E_{k+1}$ be obtained from $F_{k+1}$ by removing the interval $\left(r_{k+1}-\delta, r_{k+1}+\delta\right)$ (which has irrational endpoints). Then $E_{k+1}$ consists of at most $2^{k+2}-1$ pairwise disjoint closed intervals, each of positive length at most $\left(\frac{2}{3}\right)^{k+1} \pi$, and each having irrational endpoints.

The sets $E_{k}$ form a nested sequence of nonempty compact sets. Hence the intersection $E=\bigcap_{k=0}^{\infty}$ is a nonempty compact set. By construction it contains no rational numbers. To show that it is perfect, we merely observe that if $x \in E$, then for each $k$ there is a unique interval $I_{k}=\left[a_{k}, b_{k}\right]$, among the finite set of closed intervals constituting the set $E_{k}$ such that $x \in I_{k}$. Let $y_{k}=a_{k}$ if $a_{k} \neq x$, otherwise let $y_{k}=b_{k}$. In either case $y_{k} \in E$ (since in our construction no endpoint of any $E_{k}$ is ever removed) and $\left|y_{k}-x\right|<2 \cdot 3^{-k} \pi$. Therefore $x \in E^{\prime}$.

Exercise $2.19(a)$ if $A$ and $B$ are disjoint closed sets in some metric space $X$, prove that they are separated.

(b) Prove the same for disjoint open sets.

(c) Fix $p \in X, \delta>0$, define $A$ to be the set of all $q \in X$ for which $d(p, q)<\delta$, define $B$ similarly with $>$ in place of $<$. Prove that $A$ and $B$ are separated.

(d) Prove that every connected space with at least two points is uncountable. Hint: Use $(c)$.

Solution. (a) We are given that $A \cap B=\varnothing$. Since $A$ and $B$ are closed, this means $A \cap \bar{B}=\varnothing=\bar{A} \cap B$, which says that $A$ and $B$ are separated.

(b) Since $X \backslash B$ is a closed set containing $A$, it follows from Theorem 2.27 (c) that $X \backslash B \supseteq \bar{A}$, i.e., that $\bar{A} \cap B=\varnothing$. Similarly $A \cap \bar{B}=\varnothing$.

(c) The sets $A$ and $B$ are disjoint open sets, hence by part (b) they are separated.

(d) Let $x \in X$ and $y \in X$, and let $d(x, y)=d>0$. Then for every $\delta \in(0, d)$, there must be a point $z$ such that $d(x, z)=\delta$. (If not, the sets $A$ and $B$ defined in part $(c)$ would separate $X$.) Hence there is a subset of $X$ that can be placed in one-to-one correspondence with the interval $[0, d]$, and so $X$ is uncountable.

Exercise 2.20 Are closures and interiors of connected sets always connected? (Look at subsets of $R^{2}$.)

Answer. The closure of a connected set is connected. Indeed if $E$ is connected and $E \subseteq F \subseteq \vec{E}$, then $F$ is connected. For, suppose $F=G \cup H$, where $G$ and $H$ are separated, nonempty sets. The set $E$ cannot be contained entirely in $G$. (If it were, since $H$ is nonempty, $H$ would contain a limit point of $E$, hence a limit point of $G$, contrary to hypothesis.) For the same reason $E$ cannot be contained entirely in $H$. Hence $G_{1}=E \cap G$ and $H_{1}=E \cap H$ are nonempty separated sets such that $E=G_{1} \cup H_{1}$, and $E$ is not connected.

The interior of a connected set may fail to be connected, as we see by letting $E$ be the union of two closed disks in $R^{2}$ that are tangent to each other.

Exercise 2.21 Let $A$ and $B$ be separated subsets of some $R^{k}$, suppose a $\in A$, $\mathbf{b} \in B$, and define

$$
\mathbf{p}(t)=(1-t) \mathbf{a}+t \mathbf{b}
$$

for $t \in R^{1}$. Put $A_{0}=\mathbf{p}^{-1}(A), B_{0}=\mathbf{p}^{-1}(B)$. [Thus $t \in A_{0}$ if and only if $\mathbf{p}(t) \in A$.]

(a) Prove that $A_{0}$ and $B_{0}$ are separated subsets of $R^{1}$.

(b) Prove that there exists $t_{0} \in(0,1)$ such that $\mathbf{p}\left(t_{0}\right) \notin A \cup B$.

(c) Prove that every convex subset of $R^{k}$ is connected.

Solution. (a) The definition shows that $A_{0}$ and $B_{0}$ are disjoint. We need only show that neither contains a limit point of the other. Let $x$ be a limit point of $A_{0}$, and suppose $x \in B_{0}$. This means that for any $\delta>0$ there exists $t \in A_{0}$ with $0<|x-t|<\delta, \mathbf{p}(t)=(1-t) \mathbf{b}+t \mathbf{b} \in A$ and $\mathbf{p}(x)=(1-x) \mathbf{a}+x \mathbf{b} \in B$. Now $d(\mathbf{p}(t), \mathbf{p}(x))=|\mathbf{p}(t)-\mathbf{p}(x)|=|x-t||\mathbf{a}-\mathbf{b}| \leq|x-t|(|\mathbf{a}|+|\mathbf{b}|)<M \delta$, where $M=|\mathbf{a}|+|\mathbf{b}|$. Since $\delta$ is arbitrary, this means that $B$ contains a limit point of $A$, contrary to hypothesis. This contradiction shows that $B_{0}$ contains no limit points of $A_{0}$ Likewise $A_{0}$ contains no limit points of $B_{0}$, and so $A_{0}$ and $B_{0}$ are separated.

(b) If $\mathbf{p}(t) \in A \cup B$ for all $t \in[0,1]$, then $[0,1] \subseteq A_{0} \cup B_{0}$. Hence $[0,1]=G \cup H$, where $G=[0,1] \cap A_{0}$ and $H=[0,1] \cap B_{0}$ are both nonempty $(0 \in G$ and $1 \in H)$ and separated. This would mean $[0,1]$ is not connected. Therefore $\mathbf{p}\left(t_{0}\right) \notin A \cup B$ for some $t_{0} \in[0,1]$, and necessarily $t_{0} \in(0,1)$, since $\mathbf{p}(0)=\mathbf{a} \in A$ and $\mathbf{p}(1) \mathbf{b} \in B$.

(c) By definition a convex set $C$ is one for which the mapping $\mathbf{p}$ has the property $\mathbf{p}(t) \in C$ for all $t \in[0,1]$ provided $\mathbf{p}(0)=\mathbf{a} \in C$ and $\mathbf{p}(1)=\mathbf{b} \in C$. Hence by part. (b) there cannot be separated nonempty sets $A$ and $B$ such that $C=A \cup B$.

Exercise 2.22 A metric space is called separable if it contains a countable dense subset. Show that $R^{k}$ is separable. Hint: Consider the set of points which have only rational coordinates.

Solution. We need to show that every non-empty open subset $E$ of $R^{k}$ contains a point with all coordinates rational. Now $E$ contains a ball $B_{r}(\mathbf{x})$, and this ball contains all points $\mathbf{y}$ such that $\left(x_{j}-y_{j}\right)^{2}<\frac{1}{k}$ for $j=1,2, \ldots, k$. Each interval $\left(x_{j}-\frac{1}{k}, x_{j}+\frac{1}{k}\right)$ contains a rational number $r_{j}$, and so the point $\mathbf{r}=\left(r_{1}, \ldots, r_{k}\right)$ belongs to $E$. Thus $E$ contains a point with only rational coordinates.

Exercise 2.23 A collection $\left\{V_{\alpha}\right\}$ of open sets of $X$ is said to be a base for $X$ if the following is true: For every $x \in X$ and every open set $G \subset X$ such that $x \in G$, we have $x \in V_{\alpha} \subset G$ for some $\alpha$. In other words, every open set in $X$ is the union of a subcollection of $\left\{V_{\alpha}\right\}$.

Prove that every separable metric space has a countable base. Hint: Take all neighborhoods with rational radius and center in some countable dense subset of $X$.

Solution. Let $\left\{x_{1}, x_{2}, \ldots, x_{n}, \ldots\right\}$ be a countable dense subset of $X$. For each positive integer $m$ and each positive rational number $r$ let $V_{m, r}=\{y$ : $\left.d\left(y, x_{m}\right)<r\right\}$. The collection $V_{m, r}$ is countable.

Let $x \in X$, and let $G$ be any open subset of $X$ with $x \in G$. Then there exists $\delta>0$ such that $B_{\delta}(x) \subset G$. The open ball $B_{\frac{\delta}{2}}(x)$ contains a point $x_{k}$ for some $k$. Let $r$ be a rational number such that $d\left(x_{k}, x\right)<r<\frac{\delta}{2}$. Then $x \in B_{r}\left(x_{k}\right) \subset B_{\delta}(x) \subset G$, and we are done.

Exer√ßise 2.24 Let $X$ be a metric space in which every infinite subset has a limit point. Prove that $X$ is separable. Hint: Fix $\delta>0$, and pick $x_{1} \in X$. Having chosen $x_{1}, \ldots, x_{j} \in X$, choose $x_{j \rightarrow 1} \in X$, if possible, so that $d\left(x_{j}, x_{j-1}\right) \geq \delta$ for $i=1, \ldots, j$. Show that this process must stop after a finite number of steps, and that $X$ can therefore be covered by finitely many neighborhoods of radius $\delta$. Take $\delta=1 / n(n=1,2,3, \ldots)$, and consider the centers of the corresponding neighborhoods.

Solution. Following the hint, we observe that if the process of constructing $x_{j}$ did not terminate, the result would be an infinite set of points $x_{j}, j=1,2, \ldots$, such that $d\left(x_{i}, x_{j}\right) \geq \delta$ for $i \neq j$. It would then follow that for any $x \in X$, the open ball $B_{\frac{\delta}{2}}(x)$ contains at most one point of the infinite set, hence that no point could be a limit point of this set, contrary to hypothesis. Hence $X$ is totally bounded, i.e., for each $\delta>0$ there is a finite set $x_{1}, \ldots, x_{N}$ _.such that $X=\bigcup_{j / 1}^{N \leftarrow \mathcal{L}} B_{\delta}\left(x_{j}\right)$.

Let $x_{n_{1}}, \ldots, x_{n N_{n}}$ be such that $X=\bigcup_{j / 1}^{N_{n}} B_{\frac{1}{n}}\left(x_{n j}\right), n=1,2, \ldots$ We claim that $\left\{x_{n j}: 1 \leq j \leq N_{n} ; n=1,2, \ldots\right\}$ is a countable dense subset of $X$. Indeed
if $x \in X$ and $\delta>0$, then $x \in B_{\frac{1}{n}}\left(x_{n j}\right)$ for some $x_{n j}$ for some $n>\frac{1}{\delta}$; and hence $d\left(x, x_{n j}\right)<\delta$. By definition, this means that $\left\{x_{n j}\right\}$ is dense in $X$.

Exercise 2.25 Prove that every compact metric space $K$ has a countable base, and that $K$ is therefore separable. Hint: For every positive integer $n$, there are finitely many neighborhoods of radius $1 / n$ whose union covers $K$.

Solution. It is easier simply to refer to the previous problem. The hint shows that $K$ can be covered by a finite union of neighborhoods of radius $1 / n$, and the previous problem shows that this implies that $K$ is separable.

It is not entirely obvious that a metric space with a countable base is separable. To prove this, let $\left\{V_{n}\right\}_{n=1}^{\infty}$ be a countable base, and let $x_{n} \in V_{n}$. The points $V_{n}$ must be dense in $X$. For if $G$ is any non-empty open set, then $G$ contains $V_{n}$ for some $n$, and hence $x_{n} \in G$. (Thus for a metric space, having a countable base and being separable are equivalent.)

Exercise 2.26 Let $X$ be a metric space in which every infinite subset has a limit point. Prove that $X$ is compact. Hint: By Exercises 23 and $24, X$ has a countable base. It follows that every open cover of $X$ has a countable subcover $\left\{G_{n}\right\}_{n=1}, n=1,2,3, \ldots$. If no finite subcollection of $\left\{G_{n}\right\}$ covers $X$, then the complement $F_{n}$ of $G_{1} \cup \cdots \cup G_{n}$ is nonempty for each $n$, but $\cap F_{n}$ is empty. If $E$ is a set which contains a point from each $F_{n}$, consider a limit point of $E$, and obtain a contradiction.

Solution. Following the hint, we consider a set $E$ consisting of one point from the complement of each finite union, i.e., $x_{n} \notin G_{1} \cup \cdots \cup G_{n}$. Since there are infinitely many finite unions and every point is in some set of the covering; the set $E$ cannot be finite. (If $\left\{x_{i_{1}}, \ldots, x_{i_{n}}\right\}$ is any finite subset of $E$, there are sets $G_{j_{1}}, \ldots, G_{j_{n}}$ such that $x_{i_{k}} \in G_{j_{k}}$ for each $k$. Since $E$ contains a point not in $G_{j_{1}} \cup \cdots \cup G_{j_{n}}$, it contains a point different from $x_{1}, \ldots, x_{n}$. Hence $E$ is not finite.)

Now by hypothesis $E$ must have a limit point $z$. The point $z$ must belong to some set $G_{n}$; and since $G_{n}$ is open, there is a number $\delta>0$ such that $B_{\delta}(z) \subseteq G_{n}$. But then $B_{\delta}(z)$ cannot contain $x_{m}$ if $m \geq n$, and so $z$ cannot be a limit point of $\left\{x_{m}\right\}$. We have now reached a contradiction.

Exercise 2.27 Define a point $p$ in a metric space $X$ to be a condensation point of a set $E \subset X$ if every neighborhood of $p$ contains uncountably many points of E.

Suppose $E \subset R^{k}, E$ is uncountable, and let $P$ be the set of all condensation points of $E$. Prove that $P$ is perfect and that at most countably many points of $E$ are not in $P$. In other words, show that $P^{c} \cap E$ is at most countable. Hint:

Let $\left\{V_{n}\right\}$ be a countable base of $R^{k}$, let $W$ be the union of those $V_{n}$ for which $E \cap V_{n}$ is at most countable, and show that $P=W^{c}$.

Solution. Following the hint, we see that $E \cap W$ is at most countable, being a countable union of at-most-countable sets. It remains to show that $P=W^{c}$, and that $P$ is perfect.

If $x \in W^{c}$, and $O$ is any neighborhood of $x$, then $x \in V_{n} \subseteq O$ for some n. Since $x \notin W, V_{n} \cap E$ is uncountable. Hence $O$ contains uncountably many points of $E$, and so $x$ is a condensation point of $E$. Thus $x \in P$, i.e., $W^{c} \subseteq P$.

Conversely if $x \in W$, then $x \in V_{n}$ for some $V_{n}$ such that $V_{n} \cap E$ is countable. Hence $x$ has a neighborhood (any neighborhood contained in $V_{n}$ ) containing at most a countable set of points of $E$, and so $x \notin P$, i.e., $W \subseteq P^{c}$. Hence $P=W^{c}$.

It is clear that $P$ is closed (since its complement $W$ is open), so that we need only show that $P \subseteq P^{\prime}$. Hence suppose $x \in P$, and $O$ is any neighborhood of $x$. (By definition of $P$ this means $O \cap E$ is uncountable.) We need to show that there is a point $y \in P \cap(O \backslash\{x\})$. If this is not the case, i.e., if every point $y$ in $O \backslash\{x\}$ is in $P^{c}$, then for each such point $y$ there is a set $V_{n}$ containing $y$ such that $V_{n} \cap E$ is at most countable. That would mean that $y \in W$, i.e., that $O \backslash\{x\}$ is contained in $W$. It would follow that $O \cap E \subseteq\{x\} \cup(W \cap E)$, and so $O \cap E$ contains at most a countable set of points, contrary to the hypothesis that $x \in P$. Hence $O$ contains a point of $P$ different from $x$, and so $P \subseteq P^{\prime}$. Thus $P$ is perfect.

Remark: This result has now been proved to be true in any separable metric space, not just. $R^{k}$.

Exercise 2.28 Prove that every closed set in a separable metric space is the union of a (possibly empty) perfect set and a set which is at most countable. (Corollary: Every countable closed set in $R^{k}$ has isolated points.) Hint: Use Exercise 27.

Solution. If $E$ is closed, it contains all its limit points, and hence certainly all its condensation points. Thus $E=P \cup(E \backslash P)$, where $P$ is perfect (the set of all condensation points of $E$ ), and $E \backslash P$ is at most countable.

Since a perfect set in a separable metric space has the same cardinality as the real numbers, the set $P$ must be empty if $E$ is countable. The at-mostcountable set $E \backslash P$ cannot be perfect, hence must have isolated points if it is nonempty.

Exercise 2.29 Prove that every open set in $R^{1}$ is the union of an at most countable collection of disjoint segments. Hint: Use Exercise 22.

Solution. Let $O$ be open. For each pair of points $x \in O, y \in O$, we define an equivalence relation $x \sim y$ by saying $x \sim y$ if and only if $[\min (x, y), \max (x, y)] \subset$ $O$. This is an equivalence relation, since $x \sim x([x, x] \subset O$ if $x \in O)$; if $x \sim y$,
then $y \sim x$ (since $\min (x, y)=\min (y, x)$ and $\max (x, y)=\max (y, x))$; and if $x \sim y$ and $y \sim z$, then $x \sim z([\min (x, z), \max (x, z)] \subseteq[\min (x, y), \max (x, y)] \cup$ $[\min (y, z), \max (y, z)] \subseteq O)$. In fact it is easy to prove that

$$
\min (x, z) \geq \min (\min (x, y), \min (y, z))
$$

and

$$
\max (x, z) \leq \max (\max (x, y), \max (y, z))
$$

It follows that $O$ can be written as a disjoint union of pairwise disjoint equivalence classes. We claim that each equivalence class is an open interval.

To show this, for each $x \in O$; let $A=\{z:[z, x] \subseteq O\}$ and $B=\{z:[x, z] \subseteq$ $O\}$, and let $a=\inf A, b=\sup B$. We claim that $(a, b) \subset O$. Indeed if $a<z<b$, there exists $c \in A$ with $c<z$ and $d \in B$ with $d>z$. Then $z \in[c, x] \cup[x, d] \subseteq O$. We now claim that $(a, b)$ is the equivalence class containing $x$. It is clear that each element of $(a, b)$ is equivalent to $x$ by the way in which $a$ and $b$ were chosen. We need to show that if $z \notin(a, b)$, then $z$ is not equivalent to $x$. Suppose that $z<a$. If $z$ were equivalent to $x$, then $[z, x]$ would be contained in $O$, and so we would have $z \in A$. Hence $a$ would not be a lower bound for $A$. Similarly if $z>b$ and $z \sim x$, then $b$ could not be an upper bound for $B$.

We have now established that $O$ is a union of pairwise disjoint open intervals. Such a union must be at most countable, since each open interval contains a rational number not in any other interval.

Exercise 2.30 Imitate the proof of Theorem 2.43 to obtain the following result:

If $R^{k}=\cup_{1}^{\infty} F_{n}$, where each $F_{n}$ is a closed subset of $R^{k}$, then at least one $F_{n}$, has a nonempty interior.

Equivalent statement: If $G_{n}$ is a dense open subset of $R^{k}$, for $n=1,2,3, \ldots$, then $\cap_{1}^{\infty} G_{n}$ is not empty (in fact, it is dense in $R^{k}$ ).

(This is a special case of Baire's theorem; see Exercise 22, Chap. 3, for the general case.)

Solution. The equivalence of the two statements is easily established. Suppose the first statement is true, and $G_{n}$ is a dense open subset of $R^{k}$ for $n=1,2,3, \ldots$. Let $F_{n}=R^{k} \backslash G_{n}$. Then $F_{n}$ is a closed subset of $R^{k}$ having empty interior (if the interior of $F_{n}$ were non-empty, $G_{n}$ would not be dense). Hence by the first statement, the union of the set $F_{n}$ cannot be all of $R^{k}$, and hence the intersection of their complements is not empty.

Conversely, if the second statement holds and $F_{n}$ are closed subsets of $R^{k}$ whose union is all of $R^{k}$, let $G_{n}$ be the complement of $F_{n}$. Since the intersection of the $G_{n}$ 's is empty, at least one of them must fail to be dense in $R^{k}$, which means that its complement contains a non-empty open set.

We now prove the second statement, including the parenthetical remark. Let $G_{n}$ be a sequence of dense open sets in $R^{k}$, and let $O$ be any non-empty
open set in $R^{k}$. Since $O$ is an open set and $G_{1}$ is dense, it must intersect $G_{1}$ in a non-empty open set $O_{1}$. Let $x_{1} \in O_{1}$, and choose $r_{1}>0$ such that the closed ball $\overline{B_{r_{1}}\left(x_{i}\right)}$ is contained in $O_{1}$. Then the open ball $B_{r_{1}}\left(x_{1}\right)$ is nonempty, and hence must intersect $G_{2}$ in a non-empty open set $O_{2}$. Let $x_{2} \in O_{2}$, and choose $r_{2}>0$ such that the closed ball $\overline{B_{\tau_{2}}\left(x_{2}\right)}$ is contained in $\mathrm{O}_{2}$. In this way we obtain a nested sequence of nonempty compact sets (closed balls) $\bar{B}_{1} \supseteq \bar{B}_{2} \supseteq \cdots \supseteq \bar{B}_{n} \supseteq \cdots$. If $x \in \cap \bar{B}_{n}$, then $x \in O_{n}$ for each $n$, and hence $x \in O \cap G_{n}$ for each $n$. Thus $\cap G_{n}$ intersects each non-empty open set $O$ in at least one point, which says precisely that $\cap G_{n}$ is dense in $R^{k}$. Notice that the whole proof works exactly the same way if $R^{k}$ is replaced by $O$, since $G_{n} \cap O$ is
dense in $O$.

Remark: The stronger form of the second statement that we have proved shows that the first statement can also be strengthened. If $\left\{F_{n}\right\}$ is a sequence of closed sets whose union is all of $R^{k}$ and $O$ is any non-empty open set, then the interior of $F_{n} \cap O$ is non-empty for at least one $n$. (Simply apply the original statement with $R^{k}$ replaced by $O$ and $F_{k}$ by $F_{k} \cap O$.)

## Chapter 3

## Numerical Sequences and Series

Exercise 3.1 Prove that convergence of $\left\{s_{n}\right\}$ implies convergence of $\left\{\left|s_{n}\right|\right\}$. Is the converse true?

Solution. Let $\varepsilon>0$. Since the sequence $\left\{s_{n}\right\}$ is a Cauchy sequence, there exists $N$ such that $\left|s_{m}-s_{n}\right|<\varepsilon$ for all $m>N$ and $n>N$. We then have ||$s_{m}|-| s_{n}|| \leq\left|s_{m}-s_{n}\right|<\varepsilon$ for all $m>N$ and $n>N$. Hence the sequence $\left\{\left|s_{n}\right|\right\}$ is also a Cauchy sequence, and therefore must converge.

The converse is not true, as shown by the sequence $\left\{s_{n}\right\}$ with $s_{n}=(-1)^{n}$.

Exercise 3.2 Calculate $\lim _{n \rightarrow \infty}\left(\sqrt{n^{2}+n}-n\right)$.

Solution. Multiplying and dividing by $\sqrt{n^{2}+n}+n$ yields

$$
\sqrt{n^{2}+n}-n=\frac{n}{\sqrt{n^{2}+n}+n}=\frac{1}{\sqrt{1+\frac{1}{n}}+1} \text {. }
$$

It follows that the limit is $\frac{1}{2}$.

Exercise 3.3 If $s_{1}=\sqrt{2}$ and

$$
s_{n+1}=\sqrt{2+\sqrt{s_{n}}} \quad(n=1,2,3 \ldots)
$$

prove that $\left\{s_{n}\right\}$ converges, and that $s_{n}<2$ for $n=1,2,3 \ldots$.

Solution. Since $\sqrt{2}<2$, it is manifest that if $s_{n}<2$, then $s_{n+1}<\sqrt{2+2}=2$. Hence it follows by induction that $\sqrt{2}<s_{n}<2$ for all $n$. In view of this fact,
it also follows that $\left(s_{n}-2\right)\left(s_{n}+1\right)<0$ for all $n>1$, i.e., $s_{n}>s_{n}^{2}-2=s_{n-1}$. Hence the sequence is an increasing sequence that is bounded above (by 2) and so converges. Since the limit $s$ satisfies $s^{2}-s-2=0$, it follows that the limit is 2 .

Exercise 3.4 Find the upper and lower limits of the sequence $\left\{s_{n}\right\}$ defined by

$$
s_{1}=0 ; \quad s_{2 m}=\frac{s_{2 m-1}}{2} ; \quad s_{2 m+1}=\frac{1}{2}+s_{2 m}
$$

Solution. We shall prove by induction that

$$
s_{2 m}=\frac{1}{2}-\frac{1}{2^{m}} \text { and } s_{2 m+1}=1-\frac{1}{2^{m}}
$$

for $m=1,2, \ldots$. The second of these equalities is a direct consequence of the first, and so we need only prove the first. Immediate computation shows that $s_{2}=0$ and $s_{3}=\frac{1}{2}$. Hence assume that both formulas hold for $m \leq r$. Then

$$
s_{2 r+2}=\frac{1}{2} s_{2 r+1}=\frac{1}{2}\left(1-\frac{1}{2^{r}}\right)=\frac{1}{2}-\frac{1}{2^{r+1}} .
$$

This completes the induction. We thus have $\limsup _{n \rightarrow \infty} s_{n}=1$ and $\liminf _{n \rightarrow \infty} s_{n}=\frac{1}{2}$.

Exercise 3.5 For any two real sequences $\left\{a_{n}\right\},\left\{b_{n}\right\}$ prove that

$$
\limsup _{n \rightarrow \infty}\left(a_{n}+b_{n}\right) \leq \limsup _{n \rightarrow \infty} a_{n}+\limsup _{n \rightarrow \infty} b_{n}
$$

provided the sum on the right is not of the form $\infty-\infty$.

Solution. Since the case when $\limsup _{n \rightarrow \infty} a_{n}=+\infty$ and $\limsup _{n \rightarrow \infty} b_{n}=-\infty$ has been excluded from consideration, we note that the inequality is obvious if $\lim \sup a_{n}=+\infty$. Hence we shall assume that $\left\{a_{n}\right\}$ is bounded above.

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-382.jpg?height=100&width=1413&top_left_y=2059&top_left_x=562)
$\left.b_{n_{k}}\right)=\lim \sup \left(a_{n}+b_{n}\right)$. Then choose a subsequence of the positive integers $\left\{k_{m}\right\}$ such that

$$
\lim _{m \rightarrow \infty} a_{n_{k_{m}}}=\limsup _{k \rightarrow \infty} a_{n_{k}}
$$

The subsequence $a_{n_{k_{m}}}+b_{n_{k_{m}}}$ still converges to the same limit as $a_{n_{k}}+b_{n_{k}}$, i.e., to $\limsup _{n \rightarrow \infty}\left(a_{n}+b_{n}\right)$. Hence, since $a_{n_{k}}$ is bounded above (so that $\limsup _{k \rightarrow \infty} a_{n_{k}}$ is finite), it follows that $b_{n_{k_{m}}}$ converges to the difference

$$
\lim _{m \rightarrow \infty} b_{n_{k_{m}}}=\lim _{m \rightarrow \infty}\left(a_{n_{k_{m}}}+b_{n_{k_{m}}}\right)-\lim _{m \rightarrow \infty} a_{n_{k_{m}}}
$$

Thus we have proved that there exist subsequences $\left\{a_{n_{k_{m}}}\right\}$ and $\left\{b_{n_{k_{m}}}\right\}$ which converge to limits $a$ and $b$ respectively such that $a+b=\limsup _{n \rightarrow \infty}\left(a_{n}+b_{n}\right)$. Since $a$ is the limit of a subsequence of $\left\{a_{n}\right\}$ and $b$ is the limit of a subsequence of $\left\{b_{n}\right\}$, it follows that $a \leq \limsup _{n \rightarrow \infty} a_{n}$ and $b \leq \limsup _{n \rightarrow \infty} b_{n}$, from which the desired inequality follows.

Exercise 3.6 Investigate the behavior (convergence or divergence) of $\sum a_{n}$ if (a) $a_{n}=\sqrt{n+1}-\sqrt{n}$;

(b) $a_{n}=\frac{\sqrt{n+1}-\sqrt{n}}{n}$;

(c) $a_{n}=(\sqrt[n]{n}-1)^{n}$;

(d) $a_{n}=\frac{1}{1+z^{n}}$ for complex values of $z$.

Solution. (a) Multiplying and dividing $a_{n}$ by $\sqrt{n+1}+\sqrt{n}$, we find that $a_{n}=\frac{1}{\sqrt{n+1}+\sqrt{n}}$, which is larger than $\frac{1}{2 \sqrt{n+1}}$. The series $\sum a_{n}$ therefore diverges by comparison with the $p$ series $\left(p=\frac{1}{2}\right)$.

Alternatively, since the sum telescopes, the $n$th partial sum is $\sqrt{n+1}-1$, which obviously tends to infinity.

(b) Using the same trick as in part $(a)$, we find that $a_{n}=\frac{1}{n[\sqrt{n+1}+\sqrt{n}]}$, which is less than $\frac{1}{n^{3 / 2}}$. Hence the series converges by comparison with the $p$ series $\left(p=\frac{3}{2}\right)$.

(c) Using the root test, we find that $a_{n}^{\frac{1}{n}}=\sqrt[n]{n}-1$, which tends to zero as $n \rightarrow \infty$. Hence the series converges. (Alternatively, since by part $(c)$ of Theorem 3.20 $\sqrt[n]{n}$ tends to 1 as $n \rightarrow \infty$, we have $a_{n} \leq 2^{-n}$ for all large $n$, and the series converges by comparison with a geometric series.)

(d) If $|z| \leq 1$, then $\left|a_{n}\right| \geq \frac{1}{2}$, so that $a_{n}$ does not tend to zero. Hence the series diverges. If $|z|>1$, the series converges by comparison with a geometric series with $r=\frac{1}{|z|}<1$.

Exercise 3.7 Prove that the convergence of $\Sigma a_{n}$ implies the convergence of

$$
\sum \frac{\sqrt{a_{n}}}{n}
$$

if $a_{n} \geq 0$.

Solution. Since $\left(\sqrt{a_{n}}-\frac{1}{n}\right)^{2} \geq 0$, it follows that

$$
\frac{\sqrt{a_{n}}}{n} \leq \frac{1}{2}\left(a_{n}^{2}+\frac{1}{n^{2}}\right) \text {. }
$$

Now $\Sigma a_{n}^{2}$ converges by comparison with $\Sigma a_{n}$ (since $\Sigma a_{n}$ converges, we have $a_{n}<1$ for large $n$, and hence $\left.a_{n}^{2}<a_{n}\right)$. Since $\Sigma \frac{1}{n^{2}}$ also converges $(p$ series, $p=2$ ), it follows that $\Sigma \frac{\sqrt{a_{n}}}{n}$ converges.

Exercise 3.8 If $\Sigma a_{n}$ converges, and if $\left\{b_{n}\right\}$ is monotonic and bounded, prove that $\Sigma a_{n} b_{n}$ converges.

Solution. We shall show that the partial sums of this series form a Cauchy sequence, i.e., given $\varepsilon>0$ there exists $N$ such that $\left|\sum_{k=m+1}^{n} a_{k} b_{k}\right|\langle\varepsilon$ if $n>$ $m \geq N$. To do this, let $S_{n}=\sum_{k=1}^{n} a_{k}\left(S_{0}=0\right)$, so that $a_{k}=S_{k}-S_{k-1}$ for $k=1,2, \ldots$ Let $M$ be an uppper bound for both $\left|b_{n}\right|$ and $\left|S_{n}\right|$, and let $S=\sum a_{n}$ and $b=\lim b_{n}$. Choose $N$ so large that the following three inequalities hold for all $m>N$ and $n>N$ :

$$
\left|b_{n} S_{n}-b S\right|<\frac{\varepsilon}{3} ; \quad\left|b_{m} S_{m}-b S\right|<\frac{\varepsilon}{3} ; \quad\left|b_{m}-b_{n}\right|<\frac{\varepsilon}{3 M}
$$

Then if $n>m>N$, we have, from the formula for summation by parts,

$$
\sum_{k=m+1}^{n} a_{n} b_{n}=b_{n} S_{n}-b_{m} S_{m}+\sum_{k=m}^{n-1}\left(b_{k}-b_{k+1}\right) S_{k}
$$

Our assumptions yield immediately that $\left|b_{n} S_{n}-b_{m} S_{m}\right|<\frac{2 \varepsilon}{3}$, and

$$
\left|\sum_{k=m}^{n-1}\left(b_{k}-b_{k+1}\right) S_{k}\right| \leq M \sum_{k=m}^{n-1}\left|b_{k}-b_{k+1}\right|
$$

Since the sequence $\left\{b_{n}\right\}$ is monotonic, we have

$$
\sum_{k=m}^{n-1}\left|b_{k}-b_{k+1}\right|=\left|\sum_{k=m}^{n-1}\left(b_{k}-b_{k+1}\right)\right|=\left|b_{m}-b_{n}\right|<\frac{\varepsilon}{3 M}
$$

from which the desired inequality follows.

Exercise 3.9 Find the radius of convergence of each of the following power series
(a) $\sum n^{3} z^{n}$
(b) $\sum \frac{2^{n}}{n !} z^{n}$
(c) $\sum \frac{2^{n}}{n^{2}} z^{n}$
(d) $\sum \frac{n^{3}}{3^{n}} z^{n}$

Solution. (a) The radius of convergence is 1 , since $a_{n}=n^{3}$ satisfies $\lim _{n \rightarrow \infty} \frac{a_{n}}{a_{n+1}}=$

(b) The radius of convergence is infinite, since $a_{n}=\frac{2^{n}}{n !}$ satisfies $\lim _{n \rightarrow \infty} \frac{a_{n}}{a_{n+1}}=$ $\lim _{n \rightarrow \infty} \frac{n+1}{2}=\infty$.

(c) The radius of convergence is $\frac{1}{2}$, since $a_{n}=\frac{2^{n}}{n^{2}}$ satisfies

$$
\lim _{n \rightarrow \infty} \frac{a_{n}}{a_{n+1}}=\lim _{n \rightarrow \infty} \frac{1}{2}\left(1+\frac{1}{n}\right)^{2}=\frac{1}{2}
$$

(d) The radius of convergence is 3 , since $a_{n}=\frac{n^{3}}{3^{n}}$ satisfies

$$
\lim _{n \rightarrow \infty} \frac{a_{n}}{a_{n+1}}=\lim _{n \rightarrow \infty} 3\left(\frac{n}{n+1}\right)^{3}=3
$$

Exercise 3.10 Suppose that the coefficients of the power series $\Sigma a_{n} z^{n}$ are integers, infinitely many of which are distinct from zero. Prove that the radius of convergence is at most 1 .

Solution. The series diverges if $|z|>1$, since its general term does not tend to zero. (Infinitely many terms are larger than 1 in absolute value.)

Exercise 3.11 Suppose $a_{n}>0, s_{n}=a_{1}+\cdots+a_{n}$, and $\Sigma a_{n}$ diverges.

(a) Prove that $\sum \frac{a_{n}}{1+a_{n}}$ diverges.

(b) Prove that

$$
\frac{a_{N+1}}{s_{N+1}}+\cdots+\frac{a_{N+k}}{s_{N+k}} \geq 1-\frac{s_{N}}{s_{N+k}}
$$

and deduce that $\sum \frac{a_{n}}{s_{n}}$ diverges.

(c) Prove that

$$
\frac{a_{n}}{s_{n}^{2}} \leq \frac{1}{s_{n-1}}-\frac{1}{s_{n}}
$$

and deduce that $\sum \frac{a_{n}}{s_{n}^{2}}$ converges.

(d) What can be said about

$$
\sum \frac{a_{n}}{1+n a_{n}} \text { and } \sum \frac{a_{n}}{1_{n}^{2} a_{n}} ?
$$

Solution. (a) If $a_{n}$ does not remain bounded, then $\frac{a_{n}}{1+a_{n}}$ does not tend to zero, and hence the series $\sum \frac{a_{n}}{1+a_{n}}$ diverges. If $a_{n} \leq M$ for all $n$, then $\frac{a_{n}}{1+a_{n}} \geq$ $\frac{1}{1+M} a_{n}$, and hence again the series is divergent.
(b) Replacing each denominator on the left by $s_{N+k}$, we have

$$
\begin{array}{r}
\frac{a_{N+1}}{s_{N+1}}+\cdots+\frac{a_{N+k}}{s_{N+k}} \geq \frac{1}{s_{N+k}}\left(a_{N+1}+a_{N+2}+\cdots+a_{N+k}\right)= \\
\quad=\frac{1}{s_{N+k}}\left(s_{N+k}-s_{N}\right)=1-\frac{s_{N}}{s_{N+k}}
\end{array}
$$

It follows that the partial sums of the series $\sum \frac{a_{n}}{s_{n}}$ do not form a Cauchy sequence. For, no matter how large $N$ is taken, if $N$ is held fixed, the right hand side can be made larger than $\frac{1}{2}$ by taking $k$ sufficiently large (since $S_{N+k} \rightarrow \infty$ ).

(c) We observe that if $n \geq 2$, then

$$
\frac{1}{s_{n-1}}-\frac{1}{s_{n}}=\frac{s_{n}-s_{n-1}}{s_{n-1} s_{n}}=\frac{a_{n}}{s_{n-1} s_{n}} \geq \frac{a_{n}}{s_{n}^{2}}
$$

Since the series $\sum_{n=2}^{\infty} \frac{1}{s_{n-1}}-\frac{1}{s_{n}}$ converges to $\frac{1}{a_{1}}$; it follows by comparison that $\sum \frac{a_{n}}{s_{n}^{2}}$ converges.

(d) The series $\sum \frac{a_{n}}{1+\pi a_{n}}$ may be either convergent or divergent. If the sequence $\left\{n a_{n}\right\}$ is bounded above or has a positive lower bound, it definitely diverges. Thus if $n a_{n} \leq M$, each term is at least $\frac{1}{1+M} a_{n}$, and so the series diverges. If $n a_{n} \geq \varepsilon>0$ for all $n$, then each term is at least $\frac{\varepsilon}{1+\varepsilon} \frac{1}{n}$, and once again the series is divergent.

In general, however, the series $\sum \frac{a_{n}}{1+n a_{n}}$ may converge. For example let $a_{n}=\frac{1}{n^{2}}$ if $n$ is not a perfect square and $a_{n}=\frac{1}{\sqrt{n}}$ if $n$ is a perfect square. The sum of $\frac{a_{n}}{1+n a_{n}}$ over the nonsquares obviously converges by comparison with the $p$ series, $p=2$. As for the sum over the square integers it is $\sum \frac{1}{n+n^{2}}$, which converges by comparison with the $p$ series, $p=2$.

Finally, the series $\sum \frac{a_{n}}{1+n^{2} a_{n}}$ is obviously majorized by the $p$ series with $p=2$, hence converges.

Exercise 3.12 Suppose $a_{n}>0$ and $\sum a_{n}$ converges. Put

$$
r_{n}=\sum_{m=n}^{\infty} a_{n}
$$

(a) Prove that

$$
\frac{a_{m}}{r_{m}}+\cdots+\frac{a_{n}}{r_{n}}>1-\frac{r_{n}}{r_{m}}
$$

if $m<n$, and deduce that $\sum \frac{a_{n}}{r_{n}}$ diverges.

(b) Prove that

$$
\frac{a_{n}}{\sqrt{r_{n}}}<2\left(\sqrt{r_{n}}-\sqrt{r_{n+1}}\right)
$$

and deduce that $\sum \frac{a_{n}}{\sqrt{r_{n}}}$ converges.

Solution. (a) Replacing all the denominators on the left-hand side by the largest one $\left(r_{m}\right)$, we find

$$
\frac{a_{m}}{r_{m}}+\cdots+\frac{a_{n}}{r_{n}}>\frac{a_{m}+\cdots+a_{n}}{r_{m}}=\frac{r_{m}-r_{n+1}}{r_{m}}>1-\frac{r_{n}}{r_{m}}
$$

since $r_{n}>r_{n+1}$.

As in the previous problem, this keeps the partial sums of the series $\sum \frac{a_{n}}{r_{n}}$ from forming a Cauchy sequence. No matter how large $m$ is taken, one can choose $n$ larger so that the difference $\sum_{k=m}^{n} \frac{a_{k}}{r_{k}}$ is at least $\frac{1}{2}$, since $r_{n} \rightarrow 0$ as $n \rightarrow \infty$.

(b) We have

$$
\frac{a_{n}}{\sqrt{r_{n}}}\left(\sqrt{r_{n}}+\sqrt{r_{n+1}}\right)=a_{n}+a_{n} \frac{\sqrt{r_{n+1}}}{\sqrt{r_{n}}}<2 a_{n}=2\left(r_{n}-r_{n+1}\right)
$$

Dividing both sides by $\sqrt{r_{n}}+\sqrt{r_{n+1}}$ now yields the desired inequality.

Since the series $\sum\left(\sqrt{r_{n}}-\sqrt{r_{n+1}}\right)$ converges to $\sqrt{r_{1}}$, it follows by comparison that $\sum \frac{a_{n}}{\sqrt{r_{n}}}$ converges.

Exercise 3.13 Prove that the Cauchy product of two absolutely convergent series converges absolutely.

Solution. Since both the hypothesis and conclusion refer to absolute convergence, we may assume both series consist of nonnegative terms. We let $S_{n}=\sum_{k=0}^{n} a_{n}, T_{n}=\sum_{k=0}^{n} b_{n}$, and $U_{n}=\sum_{k=0}^{n} \sum_{l=0}^{k} a_{l} b_{k-l}$. We need to show that $U_{n}$ remains bounded, given that $S_{n}$ and $T_{n}$ are bounded. To do this we make the convention that $a_{-1}=T_{-1}=0$, in order to save ourselves from having to separate off the first and last terms when we sum by parts. We then have

$$
\begin{aligned}
U_{n} & =\sum_{k=0}^{n} \sum_{l=0}^{k} a_{l} b_{k-l} \\
& =\sum_{k=0}^{n} \sum_{l=0}^{k} a_{l}\left(T_{k-l}-T_{k-l-1}\right) \\
& =\sum_{k=0}^{n} \sum_{j=0}^{k} a_{k-j}\left(T_{j}-T_{j-1}\right) \\
& =\sum_{k=0}^{n} \sum_{j=0}^{k}\left(a_{k-j}-a_{k-j-1}\right) T_{j} \\
& =\sum_{j=0}^{n} \sum_{k=j}^{n}\left(a_{k-j}-a_{k-j-1}\right) T_{j}
\end{aligned}
$$

$$
\begin{aligned}
& =\sum_{j=0}^{n} a_{n-j} T_{j} \\
& \leq T \sum_{m=0}^{n} a_{m} \\
& =T S_{n} \\
& \leq S T .
\end{aligned}
$$

Thus $U_{n}$ is bounded, and hence approaches a finite limit.

Exercise 3.14 If $\left\{s_{n}\right\}$ is a complex sequence, define its arithmetic mean $\sigma_{n}$ by

$$
\sigma_{n}=\frac{s_{0}+s_{1}+\cdots+s_{n}}{n+1} \quad(n=0,1,2, \ldots)
$$

(a) If $\lim s_{n}=s$, prove that $\lim \sigma_{n}=s$.

(b) Construct a sequence $\left\{s_{n}\right\}$ which does not converge, although $\lim \sigma_{n}=0$.

(c) Can it happen that $s_{n}>0$ for all $n$ and that $\lim \sup s_{n}=\infty$, even though $\lim \sigma_{n}=0$ ?

(d) Put $a_{n}=s_{n}-s_{n-1}$ for $n \geq 1$. Show that

$$
s_{n}-\sigma_{n}=\frac{1}{n+1} \sum_{k=1}^{n} k a_{k}
$$

Assume that $\lim \left(n a_{n}\right)=0$ and that $\left\{\sigma_{n}\right\}$ converges. Prove that $\left\{s_{n}\right\}$ converges. [This gives a converse of $(a)$, but under the additional assumption that $n a_{n} \rightarrow 0$.] (e) Derive the last conclusion from a weaker hypothesis: Assume $M<\infty$, $\left|n a_{n}\right| \leq M$ for all $n$, and $\lim \sigma_{n}=\sigma$. Prove that $\lim s_{n}=\sigma$ by completing the following outline:

If $m<n$, then

$$
s_{n}-\sigma_{n}=\frac{m+1}{n-m}\left(\sigma_{n}-\sigma_{m}\right)+\frac{1}{n-m} \sum_{i=m+1}^{n}\left(s_{n}-s_{i}\right)
$$

For these $i$,

$$
\left|s_{n}-s_{i}\right| \leq \frac{(n-i) M}{i+1} \leq \frac{(n-m-1) M}{m+2} .
$$

Fix $\varepsilon>0$ and associate with each $n$ the integer $m$ that satisfies

$$
m \leq \frac{n-\varepsilon}{1+\varepsilon}<m+1
$$

then $(m+1) /(n-m) \leq 1 / \varepsilon$ and $\left|s_{n}-s_{i}\right|<M \varepsilon$. Hence

$$
\limsup _{n \rightarrow \infty}\left|s_{n}-\sigma\right| \leq M \varepsilon
$$

Since $\varepsilon$ was arbitrary, $\lim s_{n}=\sigma$.

Solution. Let $\varepsilon>0$. Let $M=\sup \left\{\left|s_{n}\right|\right\}$, and let $N_{0}$ be the first integer such that $\left|s_{n}-s\right|<\frac{\varepsilon}{2}$ for all $n>N_{0}$. Let $N=\max \left(N_{0},\left[\frac{2\left(N_{0}+1\right)(M+|s|)}{\varepsilon}\right]\right)$. Then if $n>N$, we have

$$
\begin{aligned}
\left|\sigma_{n}-s\right|= & \left|\frac{\left(s_{0}-s\right)+\left(s_{1}-s\right)+\cdots+\left(s_{n}-s\right)}{n+1}\right| \\
\leq & \left|\frac{\left(s_{0}-s\right)+\cdots+\left(s_{N_{0}}-s\right)}{n+1}\right|+ \\
& +\left|\frac{\left.\left(s_{N_{0}+1}\right)-s\right)+\cdots+\left(s_{n}-s\right)}{n+1}\right|
\end{aligned}
$$

The first sum on the right-hand side here is at most $\frac{\left(N_{0}+1\right)(M+|s|)}{n+1}$, and since $n+1>\frac{2\left(N_{0}+1\right)(M+|s|)}{\varepsilon}$, this sum is at most $\frac{\varepsilon}{2}$. The second sum is at most $\frac{\left(n-N_{0}\right) \frac{\varepsilon}{2}}{n+1}$, which is at most $\frac{\varepsilon}{2}$. Thus $\left|\sigma_{n}-s\right|<\varepsilon$ if $n>N$, which was to be proved.

(b) Let $s_{n}=(-1)^{n}$. Here $\sigma_{n}$ is 0 if $n$ is odd and $\frac{1}{n+1}$ if $n$ is even. Thus $\sigma_{n} \rightarrow 0$, though $s_{n}$ has no limit.

(c) Let $s_{n}=\frac{1}{n}$ if $n$ is not a perfect cube and $s_{n}=\sqrt[3]{n}$ if $n$ is a perfect cube. Then if $k^{3} \leq n<(k+1)^{3}$ we have

$$
\begin{aligned}
\sigma_{n} & \leq \frac{1}{n+1} \sum_{m=1}^{n} \frac{1}{m}+\frac{1}{n+1} \sum_{j=1}^{k} j \\
& =\frac{1}{n+1}\left(\sum_{m=1}^{n} \frac{1}{m}\right)+\frac{1}{n+1} \cdot \frac{k(k+1)}{2}
\end{aligned}
$$

The first sum on the right tends to zero by part $(a)$ applied to the sequence $s_{0}=0, s_{n}=\frac{1}{n}$ for $n \geq 1$. As for the last term, since $n \geq k^{3}$, it is less than $\frac{1}{2 k}+\frac{1}{2 k^{2}}$, which tends to zero as $k \rightarrow \infty$. Since $(k+1)^{3}>n$, it follows that $k$ tends to infinity as $n$ tends to infinity, and hence we have $\sigma_{n} \rightarrow 0$, even though $s_{n^{3}} \rightarrow \infty$.

(d) If we set $a_{0}=s_{0}$, we have $s_{n}=\sum_{k=0}^{n} a_{k}$. Then

$$
\begin{aligned}
s_{n}-\sigma_{n}= & s_{n}-\frac{s_{0}+s_{1}+\cdots+s_{n}}{n+1} \\
= & \left(a_{0}+a_{1}+\cdots+a_{n-1}+a_{n}\right)- \\
& \frac{(n+1) a_{0}+n a_{1}+\cdots+2 a_{n-1}+a_{n}}{n+1} \\
= & \frac{a_{1}+2 a_{2}+\cdots+(n-1) a_{n-1}+n a_{n}}{n+1}
\end{aligned}
$$

which was to be proved. If $n a_{n} \rightarrow 0$, then the expression on the right-hand side tends to zero by part (a) with $s_{n}$ replaced by $n a_{n}$. Hence $s_{n}-\sigma_{n} \rightarrow 0$.
(e) If $m<n$ we have

$$
\begin{aligned}
\sigma_{n}-\sigma_{m} & =\frac{s_{0}+\cdots+s_{n}}{n+1}-\frac{s_{0}+\cdots+s_{m}}{m+1} \\
& =\left(s_{0}+\cdots+s_{n}\right)\left(\frac{1}{n+1}-\frac{1}{m+1}\right)+\sum_{i=m+1}^{n} \frac{s_{i}}{m+1} \\
& =\frac{m-n}{m+1} \sigma_{n}+\frac{1}{m+1} \sum_{i=m+1}^{n} s_{i} .
\end{aligned}
$$

If we multiply both sides of this equation by $\frac{m+1}{m-n}$, and then transpose the left-hand side to the right and the term $\sigma_{n}$ to the left, we obtain

$$
-\sigma_{n}=\frac{m+1}{n-m}\left(\sigma_{n}-\sigma_{m}\right)-\frac{1}{n-m} \sum_{i=m+1}^{n} s_{i}
$$

Adding $s_{n}=\frac{1}{n-m} \sum_{i=m+1}^{n} s_{n}$ to both sides then yields the result.

We then have

$$
\left|s_{n}-s_{i}\right|=\left|a_{i+1}+\cdots+a_{n}\right| \leq M\left(\frac{1}{i+1}+\cdots+\frac{1}{n}\right) \leq \frac{(n-i) M}{i+1}
$$

Since the function $\frac{n-x}{x+1}=\frac{n+1}{x+1}-1$ is decreasing, the maximal value of the righthand side here is reached with $i=m+1$, so that $\left|s_{n}-s_{i}\right| \leq \frac{(n-m-1) M}{m+2}$, as asserted.

When we choose $m$ to be the largest integer in $\frac{n-\varepsilon}{1+\varepsilon}$, we clearly have $m<n$. Since $\varepsilon$ is fixed, we can assume $m>\varepsilon$. The inequality $\frac{n-\varepsilon}{1+\varepsilon}<m+1$ can easily be converted to $\frac{n-m-1}{m+2}<\varepsilon$, and the inequality $m \leq \frac{n-\varepsilon}{1+\varepsilon}$ likewise becomes $\frac{m+1}{n-m} \leq \frac{1}{\varepsilon}$. The first of these implies that $m \rightarrow \infty$ as $n \rightarrow \infty$, and we have

$$
\left|s_{n}-\sigma_{n}\right| \leq \frac{1}{\varepsilon}\left|\sigma_{n}-\sigma_{m}\right|+M \varepsilon
$$

for all $n$. This implies that the limit of any subsequence of $\left|s_{n}-\sigma_{n}\right|$ is at most $M \varepsilon$, and since $\varepsilon$ is arbitrary, every convergent subsequence of $\left|s_{n}-\sigma_{n}\right|$ converges to zero. This, of course, implies that $s_{n}-\sigma_{n}$ tends to zero, so that if $\sigma_{n} \rightarrow s$, then $s_{n} \rightarrow s$.

Exercise 3.15 Definition 3.21 can be extended to the case in which the $a_{n}$ lie in some fixed $R^{k}$. Absolute convergence is defined as convergence of $\sum\left|a_{n}\right|$. Show that Theorems 3.22, 3.23, 3.25(a), 3.33, 3.34, 3.42, 3.45, 3.47, and 3.55 are true in this more general setting. (Only slight modifications are required in any of the proofs.)

Solution. (Theorem 3.22). $\sum \mathbf{a}_{n}$ converges if and only if for every $\varepsilon>0$ there is an integer $N$ such that

$$
\left|\sum_{k=n}^{m} \mathbf{a}_{k}\right| \leq \varepsilon
$$

if $m \geq n \geq N$.

It is a trivial remark that, since $\left|a_{j}-b_{j}\right| \leq|\mathbf{a}-\mathbf{b}| \leq\left|a_{1}-b_{1}\right|+\cdots+\left|a_{k}-b_{k}\right|$, the sequence $\left\{\mathbf{a}_{n}\right\}$ converges if and only if each sequence of components $\left\{a_{n j}\right\}$ converges, $j=1, \ldots, k$. Hence the sequence of vector-valued functions converges if and only if each sequence of its components is a Cauchy sequence, and by the same inequalities, this is equivalent to saying that the vector-valued sequence is a Cauchy sequence.

(Theorem 3.23) If $\sum \mathbf{a}_{n}$ converges, then $\lim _{n \rightarrow \infty} \mathbf{a}_{n}=\mathbf{0}$.

Using the remark made in the previous paragraph, if $\sum \mathbf{a}_{n}$ converges, then each sum of components $\sum a_{n j}$ converges. Hence for each $j$ we have $a_{n j} \rightarrow 0$, which, again by the remark, means $\mathbf{a}_{n} \rightarrow \mathbf{0}$.

(Theorem $3.25(a)$ ) If $\left|\mathbf{a}_{n}\right| \leq c_{n}$ for $n \geq N_{0}$, where $N_{0}$ is some fixed integer, and if $\sum c_{n}$ converges, then $\sum \mathbf{a}_{n}$ converges.

Again, the hypothesis implies that $\left|a_{n j}\right| \leq c_{n}$ for $n \geq N_{0}$, so that $\sum a_{n j}$ converges for each $j=1,2, \ldots, k$. Once again, by the remark, this means that $\sum \mathrm{a}_{n}$ converges.

(Theorem 3.33) Given $\sum \mathbf{a}_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|\mathbf{a}_{n}\right|}$. Then

(a) if $\alpha<1, \sum \mathbf{a}_{n}$ converges;

(b) if $\alpha>1, \sum \mathbf{a}_{n}$ diverges;

(c) if $\alpha=1$, the test gives no information.

Part (a) follows from the remarks made above, since $\sqrt[n]{\left|a_{n j}\right|} \leq \sqrt[n]{\left|a_{n}\right|}$. (If $\alpha<1$, then each component series converges.)

As for part $(b)$, if $\alpha>1$, then $\left|\mathbf{a}_{n}\right|>1$ for infinitely many $n$, and hence the series diverges.

(Theorem 3.34) The series $\sum \mathbf{a}_{n}$

(a) converges if $\limsup _{n \rightarrow \infty} \frac{\left|\mathbf{a}_{n+1}\right|}{\left|\mathbf{a}_{n}\right|}<1$,

(b) diverges if $\frac{\left|\mathbf{a}_{n+1}\right|}{\left|\mathbf{a}_{n}\right|} \geq 1$ for $n \geq n_{0}$, where $n_{0}$ is some fixed integer.

(a) The inequality implies that for some constant $A$ and some fixed $r<1$ we have $\left|\mathbf{a}_{n}\right|<A r^{n}$, so that $\sum\left|\mathbf{a}_{n}\right|$ converges. Therefore by 3.25 the series $\sum \mathbf{a}_{n}$ also converges.

(b) As in the numerical case, this inequality implies that $\mathbf{a}_{n}$ does not tend to zero, so that the series must diverge.

(Theorem 3.42) Suppose
(a) the partial sums $\mathbf{A}_{n}$ of $\sum \mathbf{a}_{n}$ form a bounded sequence;

(b) $b_{0} \geq b_{1} \geq b_{2} \geq \cdots$;

(c) $\lim _{n \rightarrow \infty} b_{n}=0$.

Then $\sum b_{n} \mathbf{a}_{n}$ converges.

We reduce this to Theorem 3.22 by showing that the partial sums of the series $\sum b_{n} \mathbf{a}_{n}$ form a Cauchy sequence. In fact

$$
\begin{aligned}
\left|\sum_{n=p}^{q} b_{n} \mathbf{a}_{n}\right| & =\left|\sum_{n=p}^{q-1}\left(b_{n}-b_{n+1}\right) \mathbf{A}_{n}+b_{q} \mathbf{A}_{q}-b_{p} \mathbf{A}_{p-1}\right| \\
& \leq M\left(\sum_{n=p}^{q-1}\left|b_{n}-b_{n+1}\right|+b_{q}+b_{p}\right) \\
& \leq 2 M b_{p} .
\end{aligned}
$$

Now, given $\varepsilon>0$ choose $N$ so large that $b_{p}<\frac{\varepsilon}{2 M}$ for all $p>N$. Then if $q \geq p>N$, we have

$$
\left|\sum_{n=p}^{q} b_{n} \mathbf{a}_{n}\right| \leq 2 M b_{p}<\varepsilon
$$

This proves that the partial sums form a Cauchy sequence, as required.

(Theorem 3.45) If $\sum \mathbf{a}_{n}$ converges absolutely, then $\sum \mathbf{a}_{n}$ converges.

Again this is a consequence of 3.25 , with $c_{n}=\left|\mathbf{a}_{n}\right|$. (Theorem 3.47) If $\sum \mathbf{a}_{n}=\mathbf{A}$ and $\sum \mathbf{b}_{n}=\mathbf{B}$, then $\sum\left(\mathbf{a}_{n}+\mathbf{b}_{n}\right)=\mathbf{A}+\mathbf{B}$ and
$\sum c \mathbf{a}_{n}=c \mathbf{A}$ for any fixed $c$.

This theorem holds for each component of the vectors involved, hence it holds for the vectors themselves.

(Theorem 3.55) If $\sum \mathbf{a}_{n}$ is a series of vectors which converges absolutely, then every rearrangement of $\sum \mathrm{a}_{n}$ converges, and they all converge to the same sum.

Let $\mathbf{A}$ be the sum of the series in its original arrangement, and let $\varepsilon>0$. Choose $N$ so large that $\sum_{k=m}^{n}\left|\mathbf{a}_{k}\right|<\frac{\varepsilon}{2}$ if $n \geq m>N$. Then of course $\left|\sum_{k=1}^{n} \mathbf{a}_{k}-\mathbf{A}\right| \leq$ $\frac{\varepsilon}{2}$ if $n>N$. For any arrangement of the series $\sum \mathrm{a}_{n_{k}}$, Choose $N_{1}$ so large that $\{1,2, \ldots, N\} \subseteq\left\{n_{1}, n_{2}, \ldots, n_{N_{1}}\right\}$. Then if $m>N_{1}$ and $N_{2}$ is such that $\left\{n_{1}, \ldots, n_{m}\right\} \subseteq\left\{1, \ldots, N_{2}\right\}$ have,

$$
\begin{aligned}
\left|\sum_{k=1}^{m} \mathbf{a}_{n_{k}}-\mathbf{A}\right| & \leq\left|\sum_{k=1}^{m} \mathbf{a}_{n_{k}}-\sum_{k=1}^{m} \mathbf{a}_{k}\right|+\left|\sum_{k=1}^{m} \mathbf{a}_{k}-\mathbf{A}\right| \\
& \leq \sum_{k=N+1}^{m}\left|\mathbf{a}_{k}\right|+\frac{\varepsilon}{2} \\
& <\varepsilon
\end{aligned}
$$

Exercise 3.16 Fix a positive number $\alpha$. Choose $x_{1}>\sqrt{\alpha}$, and define $x_{1}, x_{2}$, $x_{3}, \ldots$, by the recursion formula

$$
x_{n+1}=\frac{1}{2}\left(x_{n}+\frac{\alpha}{x_{n}}\right)
$$

(a) Prove that $\left\{x_{n}\right\}$ decreases monotonically and that $\lim x_{n}=\sqrt{\alpha}$.

(b) Put $\varepsilon=x_{n}-\sqrt{\alpha}$, and show that

$$
\varepsilon_{n+1}=\frac{\varepsilon_{n}^{2}}{2 x_{n}}<\frac{\varepsilon_{n}^{2}}{2 \sqrt{\alpha}}
$$

so that, setting $\beta=2 \sqrt{\alpha}$,

$$
\varepsilon_{n+1}<\beta\left(\frac{\varepsilon_{1}}{\beta}\right)^{2^{n}} \quad(n=1,2,3, \ldots,)
$$

(c) This is a good algorithm for computing square roots, since the recursion formula is simple and the convergence is extremely rapid. For example, if $\alpha=3$ and $x_{1}=2$, show that $\varepsilon_{1} / \beta<\frac{1}{10}$, and that therefore

$$
\varepsilon_{5}<4 \cdot 10^{-16}, \quad \varepsilon_{6}<4 \cdot 10^{-32}
$$

Solution. (a) We note that $x_{n}$ is always positive, and that if $x_{n}>\sqrt{\alpha}$, then $x_{n+1}^{2}-\alpha=\frac{1}{4}\left(x_{n}-\frac{\alpha}{x_{n}}\right)^{2}>0$. Thus $x_{n}>\sqrt{\alpha}$ for all $n$. Since $x_{n}>\sqrt{\alpha}$, it follows that $\frac{\alpha}{x_{n}}<\sqrt{\alpha}<x_{n}$. Hence $x_{n}-x_{n+1}=\frac{1}{2}\left(x_{n}-\frac{\alpha}{x_{n}}\right)>0$, and so $\left\{x_{n}\right\}$ decreases to a limit $\lambda \geq \sqrt{\alpha}$, which must satisfy $\lambda=\frac{\alpha}{\lambda}$, i.e., $\lambda=\sqrt{\alpha}$.

(b) We have $\frac{\varepsilon_{n}^{2}}{2 x_{n}}=\frac{x_{n}^{2}-2 x_{n} \sqrt{\alpha}+\alpha}{2 x_{n}}=\frac{1}{2}\left(x_{n}+\frac{\alpha}{x_{n}}\right)-\sqrt{\alpha}=x_{n+1}-\sqrt{\alpha}=\varepsilon_{n+1}$. The inequality then results from the simple fact that $x_{n}>\sqrt{\alpha}$. Thus $\varepsilon_{2}<\frac{\varepsilon_{1}^{2}}{\beta}=$ $\beta\left(\frac{\varepsilon_{1}}{\beta}\right)^{2}$. By induction, if we suppose that $\varepsilon_{n}<\beta\left(\frac{\varepsilon_{1}}{\beta}\right)^{2^{n-1}}$, we find $\varepsilon_{n+1}<\frac{\varepsilon_{n}^{2}}{\beta}<$ $\beta\left(\frac{\varepsilon_{1}}{\beta}\right)^{2^{n}}$.

(d) Taking $x_{1}=2, \alpha=3$, we certainly have $\beta<4$. And, since $\sqrt{3}>\frac{5}{3}$, we deduce that $12 \sqrt{3}>20$, so that $2 \sqrt{3}>10(2-\sqrt{3})$, i.e., $\varepsilon_{1}=2-\sqrt{3}$ and $\beta=2 \sqrt{3}$ satisfy $\varepsilon_{1} / \beta<\frac{1}{10}$, as asserted. It follows that $\varepsilon_{n}<4 \cdot 10^{-2^{n-1}}$. In particular $\varepsilon_{5}<4 \cdot 10^{-16}$ and $\varepsilon_{6}<4 \cdot 10^{-32}$.

Exercise 3.17 Fix $\alpha>1$. Take $x_{1}>\sqrt{\alpha}$, and define

$$
x_{n+1}=\frac{\alpha+x_{n}}{1+x_{n}}=x_{n}+\frac{\alpha-x_{n}^{2}}{1+x_{n}}
$$

(a) Prove that $x_{1}>x_{3}>x_{5}>\cdots$.
(b) Prove that $x_{2}<x_{4}<x_{6}<\cdots$.

(c) Prove that $\lim x_{n}=\sqrt{\alpha}$.

(d) Compare the rapidity of convergence of this process with the one described in Exercise 16.

Solution. Most of the work in this problem is done by the following three identities, whose proofs are routine computations:

$$
\begin{gathered}
\left(1+x_{n}\right)\left(1+x_{n+1}\right)=2\left(1+x_{n}\right)+(\alpha-1), \\
x_{n+1}^{2}-\alpha=-\left[\frac{(\alpha-1)}{\left(1+x_{n}\right)^{2}}\right]\left(x_{n}^{2}-\alpha\right), \\
x_{n+1}^{2}-\alpha=\frac{(\alpha-1)^{2}}{\left(1+x_{n}\right)^{2}\left(1+x_{n-1}\right)^{2}}\left(x_{n-1}^{2}-\alpha\right)= \\
=\left[\frac{\alpha-1}{(\alpha-1)+2\left(1+x_{n-1}\right)}\right]^{2}\left(x_{n-1}^{2}-\alpha\right) .
\end{gathered}
$$

The second of these identities shows that $x_{n}$ and $x_{n+1}$ lie on opposite sides of $\sqrt{\alpha}$. The third shows that $x_{n+1}$ is closer to $\sqrt{\alpha}$ than $x_{n-1}$. Hence, since $x_{1}>\sqrt{\alpha}$ by hypothesis, parts $(a)$ and $(b)$ are proved. As for $(c)$, the third relation shows that $\left|x_{n+1}^{2}-\alpha\right| \leq r^{2}\left|x_{n-1}^{2}-\alpha\right|$, where $r=\frac{\alpha-1}{2+\alpha-1}<1$. It follows that $\left|x_{n+2 k}^{2}-\alpha\right| \leq r^{2 k}\left|x_{n}^{2}-\alpha\right|$, and the right-hand side of this expression tends to zero as $k \rightarrow \infty$. Thus $\lim _{k \rightarrow \infty} x_{n+2 k}=\sqrt{\alpha}$ whether $n$ is odd or even, and so $\lim _{n \rightarrow \infty} x_{n}=\sqrt{\alpha}$.

The convergence in this case is geometric, but not quadratically geometric, as in Exercise 16. The rate of convergence will depend on the size of $\alpha$. For $1<\alpha \leq 2$ we certainly have $x_{n} \geq \alpha-2$ for all $n$, and so in this case $r<\frac{1}{3}$, i.e., $\left|x_{n+1}^{2}-\alpha\right|<\frac{1}{9}\left|x_{n-1}^{2}-\alpha\right|$. This implies that $\left|x_{n+1}-\sqrt{\alpha}\right|<\frac{1}{9} \frac{x_{n-1}+\sqrt{\alpha}}{x_{n+1}+\sqrt{\alpha}}\left|x_{n-1}-\sqrt{\alpha}\right|$. If $n$ is odd, we have $x_{n-1}<x_{n+1}$, and so $\left|x_{n+1}-\sqrt{\alpha}\right|<\frac{1}{9}\left|x_{n-1}-\sqrt{\alpha}\right|$. If $n$ is even, we can at least assume $x_{1}<1.5$ (since $\alpha \leq 2$ ), and so $\frac{x_{n-1}+\sqrt{\alpha}}{x_{n+1}+\sqrt{\alpha}}<1.5$, so that $\left|x_{n+1}-\sqrt{\alpha}\right|<\frac{1.5}{9}\left|x_{n-1}-\sqrt{\alpha}\right|$.

Exercise 3.18 Replace the recursion formula of Exercise 16 by

$$
x_{n+1}=\frac{p-1}{p} x_{n}+\frac{\alpha}{p} x_{n}^{-p+1}
$$

where $p$ is a fixed positive integer, and describe the behavior of the resulting sequences $\left\{x_{n}\right\}$.

Solution. (Exercise 16 is the case $p=2$, of course.) The main work is done by the following easily derived formulas, which hold if $x_{n}>\alpha^{\frac{1}{p}}$.

$$
x_{n+1}-\alpha^{\frac{1}{p}}=\left(x_{n}-\alpha^{\frac{1}{p}}\right)\left[\left(\frac{p-1}{p}\right)-\frac{1}{p}\left(\left(\frac{\alpha^{\frac{1}{p}}}{x_{n}}\right)+\cdots+\left(\frac{\alpha^{\frac{1}{p}}}{x_{n}}\right)^{p-1}\right)\right]
$$

$$
\begin{aligned}
& <\left(x_{n}-\alpha^{\frac{1}{p}}\right)\left(\frac{p-1}{p}\right)\left(1-\left(\frac{\alpha^{\frac{1}{p}}}{x_{n}}\right)^{p-1}\right) \\
& =\left(x_{n}-\alpha^{\frac{1}{p}}\right)\left(\frac{p-1}{p x_{n}^{p-1}}\right)\left(x_{n}^{p-1}-\left(\alpha^{\frac{1}{p}}\right)^{p-1}\right) \\
& =\left(x_{n}-\alpha^{\frac{1}{p}}\right)^{2} \cdot \frac{p-1}{p x_{n}^{p-1}} \cdot\left[x_{n}^{p-2}+x_{n}^{p-3} \alpha^{\frac{1}{p}}+\cdots+\alpha^{\frac{p-2}{p}}\right] \\
& <\left(x_{n}-\alpha^{\frac{1}{p}}\right)^{2} \cdot \frac{(p-1)^{2}}{p x_{n}} \\
& <\left(x_{n}-\alpha^{\frac{1}{p}}\right)^{2} \cdot \frac{(p-1)^{2}}{p \alpha^{\frac{1}{p}}} .
\end{aligned}
$$

Thus we can guarantee quadratic-geometric convergence if we start with $x_{1}-\alpha^{\frac{1}{p}}=\varepsilon_{1}<\beta=\frac{p \alpha^{\frac{1}{p}}}{(p-1)^{2}}$. In that case we obtain the same inequalities as in Exercise 16 , and $x_{n} \rightarrow \alpha^{\frac{1}{p}}$.

Exercise 3.19 Associate to each sequence $a=\left\{\alpha_{n}\right\}$, in which $\alpha_{n}$ is 0 or 2 , the real number

$$
x(a)=\sum_{n=1}^{\infty} \frac{\alpha_{n}}{3^{n}}
$$

Prove that the set of all $x(a)$ is precisely the Cantor set described in Sec. 2.44.

Solution. We note that the open middle third removed at the first stage of the construction is precisely the set of points whose ternary expansions must have a 1 as their first digit. (The numbers $\frac{1}{3}$ and $\frac{2}{3}$ can be written with a 1 in this place, since

$$
\begin{aligned}
& \frac{1}{3}=\frac{1}{3}+\frac{0}{9}+\cdots+\frac{0}{3^{n}}+\cdots \\
& \frac{2}{3}=\frac{1}{3}+\frac{2}{9}+\cdots+\frac{2}{3^{n}}+\cdots
\end{aligned}
$$

However, these numbers can also be written as

$$
\begin{aligned}
& \frac{1}{3}=\frac{0}{3}+\frac{2}{9}+\cdots+\frac{2}{3^{n}}+\cdots \\
& \frac{2}{3}=\frac{2}{3}+\frac{0}{9}+\cdots+\frac{0}{3^{n}}+\cdots
\end{aligned}
$$

Thus the points retained in the Cantor set after the first dissection are precisely those whose ternary expansions may be written without a 1 in the first digit. The same argument shows that the points retained in the Cantor set after the $n$th dissection are precisely those whose ternary expansions may be written without using a 1 in any of the first $n$ digits. It then follows that the Cantor set is the set of points in $[0,1]$ whose ternary expanions can be written without using any 1's, i.e., it is precisely the set of numbers $x(a)$ just described.

Exercise 3.20 Suppose $\left\{p_{n}\right\}$ is a Cauchy sequence in a metric space $X$, and some subsequence $\left\{p_{n}\right\}$ converges to a point $p \in X$. Prove that the full sequence $\left\{p_{n}\right\}$ converges to $p$.

Solution. Let $\varepsilon>0$. Choose $N_{1}$ so large that $d\left(p_{m}, p_{n}\right)<\frac{\varepsilon}{2}$ if $m>N_{1}$ and $n>N$, we have

$$
d\left(p_{n}, p\right) \leq d\left(p_{n}, p_{n_{N+1}}\right)+d\left(p_{n_{N+1}}, p\right)<\varepsilon
$$

For the first term on the right is less than $\frac{\varepsilon}{2}$ since $n>N_{1}$ and $n_{N+1}>N+1>$ $N_{1}$. The second term is less than $\frac{\varepsilon}{2}$ by the choice of $N$.

Exercise 3.21 Prove the following analogue of Theorem 3.10(b): If $\left\{E_{n}\right\}$ is a sequence of closed and bounded sets in a complete metric space $X$, if $E_{n} \supset E_{n+1}$,

then $\cap_{1}^{\infty} E_{n}$ consists of exactly one point.

$$
\lim _{n \rightarrow \infty} \operatorname{diam} E_{n}=0
$$

Solution. Choose $x_{n} \in E_{n}$. (We use the axiom of choice here.) The sequence $\left\{x_{n}\right\}$ is a Cauchy sequence, since the diameter of $E_{n}$ tends to zero as $n$ tends to infinity and $E_{n}$ contains $E_{n+1}$. Since the metric space $X$ is complete, the sequence $x_{n}$ converges to a point $x$, which must belong to $E_{n}$ for all $n$, since $E_{n}$ is closed and contains $x_{m}$ for all $m \geq n$. There cannot be a second point $y$ in all of the $E_{n}$, since for any point $y \neq x$ the diameter of $E_{n}$ is less than $d(x, y)$ for large $n$.

Exercise 3.22 Suppose $X$ is a complete metric space, and $\left\{G_{n}\right\}$ is a sequence of dense open subsets of $X$. Prove Baire's theorem, namely that $\cap_{1}^{\infty} G_{n}$ is not empty. (In fact, it is dense in $X$.) Hint: Find a shrinking sequence of neighborhoods $E_{n}$ such that $\bar{E}_{n} \subset G_{n}$, and apply Exercise 21 .

Solution. Let $F_{n}$ be the complement of $G_{n}$, so that $F_{n}$ is closed and contains no open sets. We shall prove that any nonempty open set $U$ contains a point not in any $F_{n}$, hence in all $G_{n}$. To this end, we note that $U$ is not contained in $F_{1}$, so that there is a point $x_{1} \in U \backslash F_{1}$. Since $U \backslash F_{1}$ is open, there exists $r_{1}>0$ such that $B_{1}$, defined as the open ball of radius $r_{1}$ about $x_{1}$, is contained in $U \backslash F_{1}$. Let $E_{1}$ be the open ball of radius $\frac{r_{1}}{2}$ about $x_{1}$, so that the closure of $E_{1}$ is contained in $B_{1}$. Now $F_{2}$ does not contain $E_{1}$, and so we can find a point $x_{2} \in E_{1} \backslash F_{2}$. Since $E_{1} \backslash F_{2}$ is an open set, there exists a positive number $r_{2}$ such that $B_{2}$, the open ball of radius $R_{2}$ about $x_{2}$, is contained in $E_{1} \backslash F_{2}$, which in turn is contained in $U \backslash\left(F_{1} \cup F_{2}\right)$. We let $E_{2}$ be the open ball of radius $\frac{r_{2}}{2}$ about $x_{2}$, so that $\bar{E}_{2} \subseteq B_{2}$. Proceeding in this way, we construct a sequence of open balls $E_{j}$; such that $E_{j} \supseteq \bar{E}_{j+1}$, and the diameter of $E_{j}$ tends to zero. By the previous exercise, there is a point $x$ belonging to all the sets $\bar{E}_{j}$, hence to all the sets $U \backslash\left(F_{1} \cup F_{2} \cup \cdots \cup F_{n}\right)$. Thus the point $x$ belongs to $U \cap\left(\cap_{1}^{\infty} G_{n}\right)$.

Exercise 3.23 Suppose $\left\{p_{n}\right\}$ and $\left\{q_{n}\right\}$ are Cauchy sequences in a metric space $X$. Show that the sequence $\left\{d\left(p_{n}, q_{n}\right)\right\}$ converges. Hint: For any $m, n$,

$$
d\left(p_{n}, q_{n}\right) \leq d\left(p_{n}, p_{m}\right)+d\left(p_{m}, q_{m}\right)+d\left(q_{m}, q_{n}\right)
$$

it follows that

$$
\left|d\left(p_{n}, q_{n}\right)-d\left(p_{m}, q_{m}\right)\right|
$$

is small if $m$ and $n$ are large.

Solution. The inequality in the hint, which is an extension of the triangle inequality, shows that

$$
d\left(p_{n}, q_{n}\right)-d\left(p_{m}, q_{m}\right) \leq d\left(p_{n}, p_{m}\right)+d\left(q_{m}, q_{n}\right)
$$

and since the same inequality holds with $m$ and $n$ reversed, it follows that

$$
\left|d\left(p_{n}, q_{n}\right)-d\left(p_{m}, q_{m}\right)\right| \leq d\left(p_{n}, p_{m}\right)+d\left(q_{m}, q_{n}\right)
$$

Now if $\varepsilon>0$, choose $N_{1}$ and $N_{2}$ so that $d\left(p_{n}, p_{m}\right)<\frac{\varepsilon}{2}$ if $m>N_{1}, n>N_{1}$, and $d\left(q_{n}, q_{m}\right)<\frac{\varepsilon}{2}$ if $m>N_{2}, n>N_{2}$. Then let $N=\max \left(N_{1}, N_{2}\right)$. It follows immediately that $\left|d\left(p_{n}, q_{n}\right)-d\left(p_{m}, q_{m}\right)\right|<\varepsilon$ if $m>N$ and $n>N$. Since the real numbers are a complete metric space, it follows that $\left\{d\left(p_{n}, q_{n}\right)\right\}$ converges.

Exercise 3.24 Let $X$ be a metric space.

(a) Call two Cauchy sequences $\left\{p_{n}\right\},\left\{q_{n}\right\}$ in $X$ equivalent if

$$
\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)=0
$$

Prove that this is an equivalence relation.

(b) Let $X^{*}$ be the set of all equivalence classes so obtained. If $P \in X^{*}$ and $Q \in X^{*},\left\{p_{n}\right\} \in P,\left\{q_{n}\right\} \in Q$, define

$$
\Delta(P, Q)=\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)
$$

by Exercise 23, this limit exists. Show that the number $\Delta(P, Q)$ is unchanged if $\left\{p_{n}\right\}$ and $\left\{q_{n}\right\}$ are replaced by equivalent sequences, and hence that $\Delta$ is a distance function in $X^{*}$.

(c) Prove that the resulting metric space $X^{*}$ is complete.

(d) For each $p \in X$, there is a Cauchy sequence all of whose terms are $p$; let $P_{p}$ be the element of $X^{*}$ which contains this sequence. Prove that

$$
\Delta\left(P_{p}, P_{q}\right)=d(p, q)
$$

for all $p, q \in X$. In other words, the mapping $\varphi$ defined by $\varphi(p)=P_{p}$ is an isometry (i.e., a distance-preserving mapping) of $X$ into $X^{*}$.
(e) Prove that $\varphi(X)$ is dense in $X$, and that $\varphi(X)=X^{*}$ if $X$ is complete. By $(d)$, we may identify $X$ and $\varphi(X)$ and thus regard $X$ as embedded in the complete metric space $X^{*}$. We call $X^{*}$ the completion of $X$

Solution. (a). We need to show that: 1) $\left\{p_{n}\right\}$ is equivalent to itself; 2) if $\left\{p_{n}\right\}$ is equivalent to $\left\{q_{n}\right\}$, then $\left\{q_{n}\right\}$ is equivalent to $\left\{p_{n}\right\}$; and 3 ) if $\left\{p_{n}\right\}$ is equivalent to $\left\{q_{n}\right\}$ and $\left\{q_{n}\right\}$ is equivalent to $\left\{r_{n}\right\}$, then $\left\{p_{n}\right\}$ is equivalent to $\left\{r_{n}\right\}$. These follow from the properties of any metric. Thus 1) follows, since $d\left(p_{n}, p_{n}\right)=0$ for all $n$; 2) follows since $d\left(p_{n}, q_{n}\right)=d\left(q_{n}, p_{n}\right)$; and 3) follows from the triangle inequality, i.e., $d\left(p_{n}, r_{n}\right) \leq d\left(p_{n}, q_{n}\right)+d\left(q_{n}, r_{n}\right)$, so that if $d\left(p_{n}, q_{n}\right) \rightarrow 0$ and $d\left(q_{n}, r_{n}\right) \rightarrow 0$, then $d\left(p_{n}, r_{n}\right) \rightarrow 0$.

(b) Let $\left\{p_{n}\right\}$ be equivalent to $\left\{p_{n}^{\prime}\right\}$ and $\left\{q_{n}\right\}$ equivalent to $\left\{q_{n}^{\prime}\right\}$. Then, since we know in advance that all the limits exist, we have

$$
\lim _{n \rightarrow \infty} d\left(p_{n}^{\prime}, q_{n}^{\prime}\right) \leq \lim _{n \rightarrow \infty}\left(d\left(p_{n}^{\prime}, p_{n}\right)+d\left(p_{n}, q_{n}\right)+d\left(q_{n}, q_{n}^{\prime}\right)\right)=\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)
$$

By symmetry, however, we must also have the opposite inequality, so that the two limits are actually equal.

Now $X^{*}$ is a metric space; for $\Delta(P, Q) \geq 0$, by definition $\Delta(P, Q)=0$ means $P=Q$, and symmetry and the triangle inequality on $X^{*}$ follow from the same properties on $X$.

(c) Suppose $\left\{P_{k}\right\}$ is a Cauchy sequence in $X^{*}$. Choose Cauchy sequences $\left\{p_{k n}\right\}$ in $X$ such that $\left\{p_{k n}\right\} \in P_{k}, k=1,2, \ldots$. For each $k$, let $N_{k}$ be the first positive integer such that $d\left(p_{k n}, p_{k m}\right)<2^{-k}$ if $m \geq N_{k}$ and $n \geq N_{k}$. Let $p_{k}=p_{k N_{k}}$. Observe that $d\left(p_{k}, p_{k n}\right)<2^{-k}$ for any $n \geq N_{k}$, so that $\lim _{n \rightarrow \infty} d\left(p_{k}, p_{k n}\right) \leq 2^{-k}$. (This limit exists since the sequence all of whose terms equal $p_{k}$ is a Cauchy sequence.) Also, for any $k, l$, and $n$ we have

$$
d\left(p_{k}, p_{l}\right) \leq d\left(p_{k}, p_{k n}\right)+d\left(p_{k n}, p_{l n}\right)+d\left(p_{l n}, p_{l}\right)
$$

Hence, taking $n$ sufficiently large and assuming $k<l$, we obtain

$$
d\left(p_{k}, p_{l}\right) \leq 2^{-k}+\Delta\left(P_{k}, P_{l}\right)+2^{-k}+2^{-l}<3 \cdot 2^{-k}+\Delta\left(P_{k}, P_{l}\right)
$$

It follows that $\left\{p_{k}\right\}$ is a Cauchy sequence. Let $P$ be the element of $X^{*}$ containing $\left\{p_{k}\right\}$. We claim $P_{k} \rightarrow P$ in $X^{*}$. For

$$
\begin{aligned}
\Delta\left(P_{k}, P\right) & =\lim _{n \rightarrow \infty} d\left(p_{k n}, p_{n}\right) \\
& \leq \lim _{n \rightarrow \infty}\left(d\left(p_{k n}, p_{k}\right)+d\left(p_{k}, p_{n}\right)\right) \\
& \leq 2^{-k}+\limsup _{n \rightarrow \infty} \Delta\left(P_{k}, P_{n}\right)+3 \cdot 2^{-k}
\end{aligned}
$$

Thus if $\varepsilon>0$, choose $N_{1}=2+\left[\frac{-\log \varepsilon}{\log 2}\right]$, and $N_{2}$ such that $\Delta\left(P_{k}, P_{l}\right)<\frac{\varepsilon}{2}$ if $k>N_{2}$ and $l>N_{2}$. Let $N=\max \left(N_{1}, N_{2}\right)$. We claim that if $k>N$, then $d\left(P_{k}, P\right)<\varepsilon$. Indeed this follows, since we then have $2^{-k+2}<\frac{\varepsilon}{2}$ and $\lim \sup _{n \rightarrow \infty} \Delta\left(P_{k}, P_{n}\right) \leq \frac{\varepsilon}{2}$. We have thus finally proved that $X^{*}$ is complete.
(d) The assertion $\Delta\left(P_{p}, P_{q}\right)=d(p, q)$ is the trivial assertion that if $p_{n}=p$ and $q_{n}=q$ for all $n$, then

$$
\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)=d(p, q)
$$

(e) Let $P$ be any element of $X^{*}$, and let $\varepsilon>0$. We shall find $p \in X$ such that $\Delta\left(P, P_{p}\right)<\varepsilon$. To this end, let $\left\{p_{n}\right\} \in P$ and let $N$ be such that $d\left(p_{n}, p_{m}\right)<\frac{\varepsilon}{2}$ if $n>N$ and $m>N$. Let $p=p_{N+1}$. Then $\Delta\left(P, P_{p}\right)=\lim d\left(p_{n}, p\right) \leq \frac{\varepsilon}{2}$, and we are done.

If $X$ is already complete, then for each $P \in X^{*}$ and $\left\{p_{n}\right\} \in P$ there exists $p \in X$ such that $p_{n} \rightarrow p$. This $p$ is obviously the same for any sequence equivalent to $\left\{p_{n}\right\}$, and it is clear that $P=P_{p}$. Hence $\varphi(X)=X^{*}$ when $X$ is complete.

It should be remarked that $X^{*}$ is unique, in the sense that if $Y$ and $Z$ are any two complete metric spaces, each containing a dense subset isometric to $X$, then $Y$ is isometric to $Z$. Indeed let $\varphi$ and $\psi$ be isometries of $X$ into $Y$ and $Z$ respectively, such that $\varphi(X)$ is dense in $Y$ and $\psi(X)$ is dense in $Z$. We construct an isometry of $Y$ onto $Z$ as follows. For each $y \in Y$, there is a sequence $\left\{x_{n}\right\} \subset X$ such that $\varphi\left(x_{n}\right) \rightarrow y$. The sequence $\left\{x_{n}\right\}$ is a Cauchy sequence in $X$, and hence $\left\{\psi\left(x_{n}\right)\right\}$ is a Cauchy sequence in $Z$ (since $\psi$ preserves distance). Since $Z$ is complete, there is an element $z$ such that $\psi\left(x_{n}\right) \rightarrow z$. We define $\theta(y)=z$. We claim first of all that this definition is unambiguous. For if $y$ is given and some other sequence $\left\{x_{n}^{\prime}\right\}$ in $X$ is such that $\left\{\varphi\left(x_{n}^{\prime}\right)\right\}$ converges to $y$, then $d_{Z}\left(\psi\left(x_{n}\right), \psi\left(x_{n}^{\prime}\right)\right)=d_{X}\left(x_{n}, x_{n}^{\prime}\right)=d_{Y}\left(\varphi\left(x_{n}\right), \varphi\left(x_{n}^{\prime}\right) \rightarrow 0\right.$, and hence $\psi\left(x_{n}^{\prime}\right) \rightarrow z$ also. The mapping $\theta$ is an isometry, since if $y_{1}=\lim \varphi\left(x_{1 n}\right)$ and $y_{2}=\lim \varphi\left(x_{2 n}\right)$, then

$$
\begin{aligned}
d_{Z}\left(\theta\left(y_{1}\right), \theta\left(y_{2}\right)\right) & =\lim d_{Z}\left(\psi\left(x_{1 n}\right), \psi\left(x_{2 n}\right)\right) \\
& =\lim d_{X}\left(x_{1 n}, x_{2 n}\right) \\
& =\lim d_{Y}\left(\varphi\left(x_{1 n}\right), \varphi\left(x_{2 n}\right)\right) \\
& =d_{Y}\left(y_{1}, y_{2}\right) .
\end{aligned}
$$

(Here we have used the fact that if $p_{n} \rightarrow p$ and $q_{n} \rightarrow q$, then $d\left(p_{n}, q_{n}\right) \rightarrow d(p, q)$, which in turn follows from the inequality

$$
\left|d(p, q)-d\left(p_{n}, q_{n}\right)\right| \leq d\left(p, p_{n}\right)+d\left(q, q_{n}\right)
$$

proved in Exercise 23 above.)

Finally $\theta(Y)=Z$, since one can easily define an inverse mapping $\eta: Z \rightarrow Y$ by merely reversing the steps used to define $\theta$.

Exercise 3.25 Let $X$ be the metric space whose points are the rational numbers, with the metric $d(x, y)=|x-y|$. What is the completion of this space? (Compare Exercise 24.)

Answer. By the remarks at the end of Exercise 24, the completion of a metric space $X$ is any complete metric space containing a dense subset isometric to the space $X$. Since the real numbers have this property, the completion of the rational numbers is the real numbers. A Cauchy sequence of rational numbers converges to a unique real number, of course, and two sequences are equivalent if and only if they converge to the same real number. Hence we have also a more direct reason for claiming that the completion of the rational numbers is the real numbers.

## Chapter 4

## Continuity

Exercise 4.1 Suppose $f$ is a real function defined on $R^{1}$ which satisfies

$$
\lim _{h \rightarrow 0}[f(x+h)-f(x-h)]=0
$$

for every $x \in R^{1}$. Does this imply that $f$ is continuous?

Solution. No. In fact even the stronger statement

$$
\lim _{h \rightarrow 0} \frac{f(x+h)-f(x-h)}{h^{n}}=0
$$

for every $x \in R^{1}$, where $n$ is an arbitrary positive number, does not imply that $f$ is continuous, since this property is possessed by the function

$$
f(x)= \begin{cases}1 & \text { if } x \text { is an integer } \\ 0 & \text { if } x \text { is not an integer. }\end{cases}
$$

(If $x$ is an integer, then $f(x+h)-f(x-h) \equiv 0$ for all $h$; while if $x$ is not an integer, $f(x+h)-f(x-h)=0$ for $|h|<\min (x-[x], 1+[x]-x)$.

Exercise 4.2 If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, prove that

$$
f(\bar{E}) \subset \overline{f(E)}
$$

for every set $E \subset X$. ( $\bar{E}$ denotes the closure of $E$.) Show, by an example, that $f(\bar{E})$ can be a proper subset of $\overline{f(E)}$.

Solution. Let $x \in \bar{E}$. We need to show that $f(x) \in \overline{f(E)}$. To this end, let $O$ be any neighborhood of $f(x)$. Since $f$ is continuous, $f^{-1}(O)$ contains (is) a neighborhood of $x$. Since $x \in \bar{E}$, there is a point $u$ of $E$ in $f^{-1}(O)$. Hence $f(u) \in O \cap f(E)$. Since $O$ was any neighborhood of $f(x)$, it follows that
$f(x) \in f(E)$.

Consider $f: R^{1} \rightarrow R^{1}$ given by $f(x)=\frac{x}{1+x^{2}}$, and let $E=\bar{E}=[1, \infty)$, so that $f(E)=f(\bar{E})=\left(0, \frac{1}{2}\right]$, yet $\overline{f(E)}=\left[0, \frac{1}{2}\right]$.

Exercise 4.3 Let $f$ be a continuous real function on a metric space $X$. Let $Z(f)$ (the zero set of $f$ ) be the set of all $p \in X$ at which $f(p)=0$. Prove that $Z(f)$ is closed.

Solution. $Z(f)=f^{-1}(\{0\})$, which is the inverse image of a closed set. Hence $Z(f)$ is closed.

Exercise 4.4 Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$, and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p)=f(p)$ for all $p \in E$, prove that $g(p)=f(p)$ for all $p \in X$. (In other words, a continuous mapping is determined by its values on a dense subset of its domain.)

Solution. To prove that $f(E)$ is dense in $f(X)$, simply use Exercise 2 above: $f(X)=f(\bar{E}) \subseteq \overline{f(E)}$.

The function $\varphi: X \rightarrow R^{1}$ given by

$$
\varphi(p)=d_{Y}(f(p), g(p))
$$

is continuous, since

$$
\left|d_{Y}(f(p), g(p))-d_{Y}(f(q), g(q))\right| \leq d_{Y}(f(p), f(q))+d_{Y}(g(p), g(q))
$$

(This inequality follows from the triangle inequality, since

$$
d_{Y}(f(p), g(p)) \leq d_{Y}(f(p), f(q))+d_{Y}(f(q), g(q))+d_{Y}(g(q), g(p))
$$

and the same inequality holds with $p$ and $q$ interchanged. The absolute value $\left|d_{Y}(f(p), g(p))-d_{Y}(f(q), g(q))\right|$ must be either $d_{Y}(f(p), g(p))-d_{Y}(f(q), g(q))$ or $d_{Y}(f(q), g(q))-d_{Y}(f(p), g(p))$, and the triangle inequality shows that both of these numbers are at most $d_{Y}(f(p), f(q))+d_{Y}(g(p), g(q))$.)

By the previous problem, the zero set of $\varphi$ is closed. But by definition

$$
Z(\varphi)=\{p: f(p)=g(p)\}
$$

Hence the set of $p$ for which $f(p)=g(p)$ is closed. Since by hypothesis it is dense, it must be $X$.

Exercise 4.5 If $f$ is a real continuous function defined on a closed set $E \subset R^{1}$, prove that there exist continuous real functions $g$ on $R^{1}$ such that $g(x)=f(x)$ for all $x \in E$. (Such functions $g$ are called continuous extensions of $f$ from $E$ to $R^{1}$.) Show that the result becomes false if the word "closed" is omitted. Extend the result to vector-valued functions. Hint: Let the graph of $g$ be a straight line on each of the segments which constitute the complement of $E$ (compare Exercise 29, Chap. 2). The result remains true if $R^{1}$ is replaced by any metric space, but the proof is not so simple.

Solution. Following the hint, let the complement of $E$ consist of a countable collection of finite open intervals $\left(a_{k}, b_{k}\right)$ together with possibly one or both of the the semi-infinite intervals $(b,+\infty)$ and $(-\infty, a)$. The function $f(x)$ is already defined at $a_{k}$ and $b_{k}$, as well as at $a$ and $b$ (if these last two points exist). Define $g(x)$ to be $f(b)$ for $x>b$ and $f(a)$ for $x<a$ if $a$ and $b$ exist. On the interval $\left(a_{k}, b_{k}\right)$ let

$$
g(x)=f\left(a_{k}\right)+\frac{x-a_{k}}{b_{k}-a_{k}}\left(f\left(b_{k}\right)-f\left(a_{k}\right)\right) .
$$

Of course we let $g(x)=f(x)$ for $x \in E$. It is now fairly clear that $g(x)$ is continuous. A rigorous proof proceeds as follows. Let $\varepsilon>0$. To choose $\delta>0$ such that $|x-u|<\delta$ implies $|g(x)-g(u)|<\varepsilon$, we consider three cases.

i. If $x>b$, let $\delta=x-b$. Then if $|x-u|<\delta$, it follows that $u>b$ also, so that $g(u)=f(b)=g(x)$, and $|g(u)-g(x)|=0<\varepsilon$. Similarly if $x<a$, let $\delta=a-x$.

ii. If $a_{k}<x<b_{k}$ and $f\left(a_{k}\right)=f\left(b_{k}\right)$, let $\delta=\min \left(x-a_{k}, b_{k}-x\right)$. Since $|x-u|<\delta$ implies $a_{k}<u<b_{k}$, so that $g(u)=f\left(a_{k}\right)=f\left(b_{k}\right)=g(x)$, we again have $|g(x)-g(u)|=0<\varepsilon$. If $a_{k}<x<b_{k}$ and $f\left(a_{k}\right) \neq f\left(b_{k}\right)$, let $\delta=\min \left(x-a_{k}, b_{k}-x, \frac{\left(b_{k}-a_{k}\right) \varepsilon}{\left|f\left(b_{k}\right)-f\left(a_{k}\right)\right|}\right)$. Then if $|x-u|<\delta$, we again have $a_{k}<u<b_{k}$ and so

$$
|g(x)-g(u)|=\frac{|x-u|}{b_{k}-a_{k}}\left|f\left(b_{k}\right)-f\left(a_{k}\right)\right|<\varepsilon .
$$

iii. If $x \in E$, let $\delta_{1}$ be such that $|f(u)-f(x)|<\varepsilon$ if $u \in E$ and $|x-u|<\delta_{1}$. (Subcase a). If there are points $x_{1} \in E \cap\left(x-\delta_{1}, x\right)$ and $x_{2} \in E \cap\left(x, x+\delta_{1}\right)$, let $\delta=\min \left(x-x_{1}, x_{2}-x\right)$. If $|u-x|<\delta$ and $u \in E$, then $|f(u)-f(x)|<\varepsilon$ by definition of $\delta_{1}$. if $u \notin E$, then, since $x_{1}, x$, and $x_{2}$ are all in $E$, it follows that $u \in\left(a_{k}, b_{k}\right)$, where $a_{k} \in E, b_{k} \in E$, and $\left|a_{k}-x\right|<\delta$ and $\left|b_{k}-x\right|<\delta$, so that $\left|f\left(a_{k}\right)-f(x)\right|<\varepsilon$ and $\left|f\left(b_{k}\right)-f(x)\right|<\varepsilon$. If $f\left(a_{k}\right)=f\left(b_{k}\right)$, then $f(u)=f\left(a_{k}\right)$ also, and we have $|f(u)-f(x)|<\varepsilon$. If $f\left(a_{k}\right) \neq f\left(b_{k}\right)$, then

$$
\begin{aligned}
|f(u)-f(x)| & =\left|f\left(a_{k}\right)-f(x)+\frac{u-a_{k}}{b_{k}-a_{k}}\left(f\left(b_{k}\right)-f\left(a_{k}\right)\right)\right| \\
& =\left|\frac{b_{k}-u}{b_{k}-a_{k}}\left(f\left(a_{k}\right)-f(x)\right)+\frac{u-a_{k}}{b_{k}-a_{k}}\left(f\left(b_{k}\right)-f(x)\right)\right| \\
& <\frac{b_{k}-u}{b_{k}-a_{k}} \varepsilon+\frac{u-a_{k}}{b_{k}-a_{k}} \varepsilon \\
& =\varepsilon
\end{aligned}
$$

(Subcase b). Suppose $x_{2}$ does not. exist, i.e., either $x=a_{k}$ or $x=a_{k}$ and $b_{k}>a_{k}+\delta_{1}$. Let us consider the second of these cases and show how to get $|f(u)-f(x)|<\varepsilon$ for $x<u<x+\delta$. If $f\left(a_{k}\right)=f\left(b_{k}\right)$, let $\delta=\delta_{1}$. If $u>x$ we have $a_{k}<u<b_{k}$ and $f(u)=f\left(a_{k}\right)=f(x)$. If $f\left(a_{k}\right) \neq f\left(b_{k}\right)$, let $\delta=$ $\min \left(\delta_{1}, \frac{\left(b_{k}-a_{k}\right) \varepsilon}{\left|f\left(b_{k}\right)-f\left(a_{k}\right)\right|}\right)$. Then, just as in Subcase a, we have $|f(u)-f(x)|<\varepsilon$.

The case when $x=b_{k}$ for some $k$ and $a_{k}<x-\delta_{1}$ is handled in exactly the same way.

If $x=b$, let $\delta=\delta_{1}$. If $u>x$ we have $f(x)-f(u)$; and if $u<x$ and $u \notin E$, we use the same argument as in Subcases a and $b$.

The case $x=a$ is handled similarly.

The extension of this result to vector-valued functions is immediate: Simply extend each component of the function. A vector-valued function is continuous if and only if each of its components is continuous.

Exercise 4.6 If $f$ is defined on $E$, the graph of $f$ is the set of points $(x, f(x))$ for $x \in E$. In particular, if $E$ is the set of real numbers and $f$ is real-valued, the graph of $f$ is a subset of the plane.

Suppose $E$ is compact, and prove that $f$ is continuous on $E$ if and only if its graph is compact.

Solution. Let $Y$ be the co-domain of the function $f$. We invent a new metric space $E \times Y$ as the set of pairs of points $(x, y), x \in E, y \in Y$, with the metric $\rho\left(\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)\right)=d_{E}\left(x_{1}, x_{2}\right)+d_{Y}\left(y_{1}, y_{2}\right)$. The function $\varphi(x)=(x, f(x))$ is then a mapping of $E$ into $E \times Y$.

We claim that the mapping $\varphi$ is continuous if $f$ is continuous. Indeed, let $x \in X$ and $\varepsilon>0$ be given. Choose $\eta>0$ so that $d_{Y}(f(x), f(u))<\frac{\varepsilon}{2}$ if $d_{E}(x, y)<\eta$. Then let $\delta=\min \left(\eta, \frac{\varepsilon}{2}\right)$. It is easy to see that $\rho(\varphi(x), \varphi(u))<\varepsilon$ if $d_{E}(x, u)<\delta$. Conversely if $\varphi$ is continuous, it is obvious from the inequality $\rho(\varphi(x), \varphi(u)) \geq d_{Y}(f(x), f(u))$ that $f$ is continuous.

From these facts we deduce immediately that the graph of a continuous function $f$ on a compact set $E$ is compact, being the image of $E$ under the continuous mapping $\varphi$. Conversely, if $f$ is not continuous at some point $x$, there is a sequence of points $x_{n}$ converging to $x$ such that $f\left(x_{n}\right)$ does not converge to $f(x)$. If no subsequence of $f\left(x_{n}\right)$ converges, then the sequence $\left\{\left(x_{n}, f\left(x_{n}\right)\right\}_{n=1}^{\infty}\right.$ has no convergent subsequence, and so the graph is not compact. If some subsequence of $f\left(x_{n}\right)$ converges, say $f\left(x_{n_{k}}\right) \rightarrow z$, but $z \neq f(x)$, then the graph of $f$ fails to contain the limit point $(x, z)$, and hence is not closed. A fortiori it is not compact.

Exercise 4.7 If $E \subset X$ and if $f$ is a function defined on $X$, the restriction of $f$ to $E$ is the function $g$ whose domain of definition is $E$, such that $g(p)=f(p)$ for $p \in E$. Define $f$ and $g$ on $R^{2}$ by $f(0,0)=g(0,0)=0, f(x, y)=x y^{2} /\left(x^{2}+y^{4}\right)$, $g(x, y)=x y^{2} /\left(x^{2}+y^{6}\right)$ if $(x, y) \neq(0,0)$. Prove that $f$ is bounded on $R^{2}$, that $g$ is unbounded in every neighborhood of $(0,0)$, and that $f$ is not continuous at $(0,0)$; nevertheless, the restrictions of both $f$ and $g$ to every straight line in $R^{2}$ are continuous!

Solution. The fact that $|f(x, y)| \leq \frac{1}{2}$ is an easy consequence of the inequality $\left(x-y^{2}\right)^{2} \geq 0$. The fact that $\lim _{y \rightarrow 0} g\left(y^{3}, y\right)=\lim _{y \rightarrow 0} \frac{y^{5}}{2 y^{6}}=\lim _{y \rightarrow 0} \frac{1}{2 y}=\infty$ shows that $g$ is unbounded on every neighborhood of infinity. The fact that $\lim _{y \rightarrow 0} f\left(y^{2}, y\right)=$ $\lim _{y \rightarrow 0} \frac{y^{4}}{2 y^{4}}=\frac{1}{2}$ shows that $f$ is not continuous at $(0,0)$.

Since $f$ and $g$ are continuous except at $(0,0)$, it is obvious that their restrictions to any line that does not pass through $(0,0)$ are continuous. Now a line that does pass through $(0,0)$ has an equation that is either $x=0$ or $y=a x$ for some $a$. Both $f$ and $g$ are constantly 0 on the first of these, and on the second we have $f(x, a x)=a^{2} x^{3} /\left(x^{2}+a^{4} x^{4}\right)=a^{2} x /\left(1+a^{4} x^{2}\right)$, while $g(x, a x)=a^{2} x^{3} /\left(x^{2}+a^{6} x^{6}\right)=a^{2} x /\left(1+a^{6} x^{4}\right)$. Both of the latter are obviously continuous functions.

Exercise 4.8 Let $f$ be a real uniformly continuous function on the bounded set $E$ in $R^{\mathrm{1}}$. Prove that $f$ is bounded on $E$.

Show that the conclusion is false if boundedness of $E$ is omitted from the hypothesis.

Let $a=\inf E$ and $b=\sup E$, and let $\delta>0$ be such that $|f(x)-f(y)|<1$ if $x, y \in E$ and $|x-y|<\delta$. Now choose a positive integer $N$ larger than $(b-a) / \delta$, and consider the $N$ intervals $I_{k}=\left[a+\frac{k-1}{b-a}, a+\frac{k}{b-a}\right], k=1,2, \ldots, N$. For each $k$ such that $I_{k} \cap E \neq \varnothing$ let $x_{k} \in E \cap I_{k}$. Then let $M=1+\max \left\{\left|f\left(x_{k}\right)\right|\right\}$. If $x \in E$, we have $\left|x-x_{k}\right|<\delta$ for some $k$, and hence $|f(x)|<M$.

The function $f(x)=x$ is uniformly continuous on the entire line, but not. bounded.

Exercise 4.9 Show that the requirement in the definition of uniform continuity can be rephrased as follows, in terms of diameters of sets: To every $\varepsilon>0$ there exists a $\delta>0$ such that $\operatorname{diam} f(E)<\varepsilon$ for all $E \subset X$ with $\operatorname{diam} E<\delta$.

Solution. Suppose $f$ is uniformly continuous and $\varepsilon>0$ is given. Choose any positive number $\alpha$ smaller than $\varepsilon$. Then there exists $\delta>0$ such that $d_{Y}(f(x), f(u))<\alpha$ if $d_{X}(x, u)<\delta$. Hence if $E$ is any set of diameter less than $\delta$ and $x$ and $u$ are any two points in $E$ we have $d_{Y}(f(x), f(u))<\alpha$, so that $\operatorname{diam} f(E) \leq \alpha<\varepsilon$.

Conversely if $f$ satisfies the condition stated in the problem, it is obvious that for any $\varepsilon>0$ there exists $\delta>0$ such that $d_{Y}(f(x), f(u))<\varepsilon$ whenever $d_{X}(x, u)<\delta$. (Choose $\delta>0$ corresponding to $\varepsilon$ in the condition of the problem and then let $E$ be the two-point set $\{x, u\}$.)

Exercise 4.10 Complete the details of the following alternate proof of Theorem 4.19: If $f$ is not uniformly continuous, then for some $\varepsilon>0$ there are sequences $\left\{p_{n}\right\},\left\{q_{n}\right\}$ in $X$ such that $d_{X}\left(p_{n}, q_{n}\right) \rightarrow 0$ but $d_{Y}\left(f\left(p_{n}\right), f\left(q_{n}\right)\right)>\varepsilon$. Use Theorem 2.37 to obtain a contradiction.

Solution. Theorem 4.19 asserts that a continuous function on a compact set is uniformly continuous. By Theorem 2.37 there are subsequences $\left\{p_{n_{k}}\right\}$ and $\left\{q_{n_{k}}\right\}$ that converge to points $p$ and $q$ respectively. Since $d_{X}\left(p_{n}, q_{n}\right) \rightarrow 0$, it follows that $p=q$. However, since $f$ is continuous, it follows from Theorem 4.2 that $f\left(p_{n_{k}}\right)$ and $f\left(q_{n_{k}}\right)$ converge to $f(p)$, which, since $d_{Y}\left(f\left(p_{n_{k}}\right), f\left(q_{n_{k}}\right) \leq\right.$ $d_{Y}\left(f\left(p_{n_{k}}\right), f(p)\right)+d_{Y}\left(f(p), f\left(q_{n_{k}}\right)\right)$, implies that $d_{Y}\left(f\left(p_{n_{k}}\right), f\left(q_{n_{k}}\right)\right) \rightarrow 0$, contradicting the inequality $d_{Y}\left(f\left(p_{n_{k}}\right), f\left(q_{n_{k}}\right)\right)>\varepsilon$.

Exercise 4.11 Suppose $f$ is a uniformly continuous mapping of a metric space $X$ into a metric space $Y$ and prove that $\left\{f\left(x_{n}\right)\right\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence $\left\{x_{n}\right\}$ in $X$. Use this result to give an alternative proof of the theorem stated in Exercise 13.

Solution. Suppose $\left\{x_{n}\right\}$ is a Cauchy sequence in $X$. Let $\varepsilon>0$ be given. Let $\delta>0$ be such that $d_{Y}(f(x), f(u))<\varepsilon$ if $d_{X}(x, u)<\delta$. Then choose $N$ so that $d_{X}\left(x_{n}, x_{m}\right)<\delta$ if $n, m>N$. Obviously $d_{Y}\left(f\left(x_{n}\right), f\left(x_{m}\right)\right)<\varepsilon$ if $m, n>N$, showing that $\left\{f\left(x_{n}\right)\right\}$ is a Cauchy sequence.

Now let $f$ be a uniformly continuous function defined on a dense subset $E$ of $X$, mapping $E$ into a complete metric space $Y$ (for example, $Y$ could be the real numbers). To prove that $f$ has a unique continuous extension to all of $X$, proceed as follows. For each $x \in X \backslash E$ let $\left\{x_{n}\right\}$ be a sequence of points in $E$ converging to $x$. Define $f(x)$ to be the limit of the Cauchy sequence $\left\{f\left(x_{n}\right)\right\}$. This definition is unambiguous; for if $\left\{u_{n}\right\}$ also converges to $x$, then the sequence $\left\{y_{n}\right\}$ defined by

$$
y_{n}= \begin{cases}x_{n / 2} & \text { if } n \text { is even } \\ u_{(n+1) / 2} & \text { if } n \text { is odd }\end{cases}
$$

also converges to $x$. Hence $\left\{f\left(y_{n}\right)\right\}$ is a Cauchy sequence in $Y$, and so all subsequences of $\left\{f\left(y_{n}\right)\right\}$ converge to the same limit. In particular $\left\{f\left(x_{n}\right)\right\}$ and $\left\{f\left(u_{n}\right)\right\}$ both converge to the same value.

The extended function is also uniformly continuous. For if $\varepsilon>0$, let $\delta>0$ be such that $d_{Y}(f(x), f(u))<\frac{\varepsilon}{3}$ if $x, u \in E$ and $d_{X}(x, u)<\delta$. Then if $x \in E$, $u \in X \backslash E$, and $d_{X}(x, u)<\delta$, choose $v \in E$ with $d_{X}(v, u)<\delta-d_{X}(x, u)$ and $d_{Y}(f(v), f(u))<\frac{\varepsilon}{3}$ (this is possible because of the definition of $f(u)$ ). We then have $d_{X}(x, v) \leq d_{X}(x, u)+d_{X}(u, v)<\delta$, and so

$$
d_{Y}(f(x), f(u)) \leq d_{Y}(f(x), f(v))+d_{Y}(f(v), f(u))<\frac{2 \varepsilon}{3}<\varepsilon
$$

Similarly if $x \in X \backslash E, u \in X \backslash E$, and $d_{X}(x, u)<\delta$, choose $v, w \in E$ with $d_{X}(v, u)<\frac{1}{2}\left(\delta-d_{X}(x, u)\right), d_{X}(x, w)<\frac{1}{2}\left(\delta-d_{X}(x, u)\right), d_{Y}(f(v), f(u))<\frac{\varepsilon}{3}$,
and $d_{Y}(f(w), f(x))<\frac{\varepsilon}{3}$. We then have

$$
d_{X}(v, w) \leq d_{X}(v, u)+d_{X}(u, x)+d_{X}(x, w)<\delta
$$

and hence

$$
d_{Y}(f(x), f(u)) \leq d_{Y}(f(x), f(w))+d_{Y}(f(w), f(v))+d_{Y}(f(v), f(u))<\varepsilon
$$

The uniqueness of this extension follows from Exercise 4 above.

Exercise 4.12 A uniformly continuous function of a uniformly continuous function is uniformly continuous.

State this more precisely and prove it.

Solution. Let $f: X \rightarrow Y$ and $g: Y \rightarrow Z$ be uniformly continuous. Then $g \circ f: X \rightarrow Z$ is uniformly continuous, where $g \circ f(x)=g(f(x))$ for all $x \in X$.

To prove this fact, let $\varepsilon>0$ be given. Then, since $g$ is uniformly continuous, there exists $\eta>0$ such that $d_{Z}(g(u), g(v))<\varepsilon$ if $d_{Y}(u, v)<\eta$. Since $f$ is uniformly continuous, there exists $\delta>0$ such that $d_{Y}(f(x), f(y))<\eta$ if $d_{X}(x, y)<\delta$.

It is then obvious that $d_{Z}(g(f(x)), g(f(y)))<\varepsilon$ if $d_{X}(x, y)<\delta$, so that $g \circ f$ is uniformly continuous.

Exercise 4.13 Let $E$ be a dense subset of a metric space $X$, and let $f$ be a uniformly continuous real function defined on $E$. Prove that $f$ has a continuous extension from $E$ to $X$ (see Exercise 5 for terminology). (Uniqueness follows from Exercise 4.) Hint: For each $p \in X$ and each positive ineger $n$, let $V_{n}(p)$ be the set of all $q \in E$ with $d(p, q)<1 / n$. Use Exercise 9 to show that the intersection of the closures of the sets $f\left(V_{1}(p)\right), f\left(V_{2}(p)\right), \ldots$, consists of a single point, say $g(p)$, of $R^{1}$. Prove that the function $g$ so defined is the desired extension of $f$.

Could the range space $R^{1}$ be replaced by $R^{n}$. By any compact metric space? By any complete metric space? By any metric space?

Solution. We shall carry out the proof in the context of any complete metric space, showing that the range space could be $R^{n}$ or any compact metric space.

The diameter of the closure of $f\left(V_{i}(p)\right)$ is the same as the diameter of $f\left(V_{i}(p)\right)$ itself. Hence by Exercise 9 above these diameters tend to zero. Since they form a nested sequence of nonempty closed sets, their intersection must consist of a single point, which can be defined to be $g(p)$. If $p \in E$, the intersection of these sets is just $f(p)$ (since $f(p)$ is in all the sets, and only one point belongs to all of them), so that $g$ coincides with $f$ on $E$. It remains to show that $g$ is continuous. This proof is identical to the proof given in Exercise 11 above, which depends only on the fact that for $u \in X \backslash E$ and $\varepsilon>0, \delta>0$ there is a point $v \in E$
with $d_{X}(v, u)<\delta$ and $d_{Y}(f(v), f(u))<\varepsilon$. This condition clearly holds in the present context as well.

In general this theorem fails on an incomplete metric space. For example, take $X$ to be the real numbers, $Y$ and $E$ the rational numbers, and let $f: E \rightarrow Y$ be given by $f(x)=x$. There is no possible extension of $f$ to a mapping from $X$ into $Y$. (There is a unique extension of $f$ to a mapping from $X$ into $X$, but its range is not contained in $Y$. If there were an extension of $f$ to a mapping from $X$ into $Y$, there would be two extensions of $f$ to mappings from $X$ into $X$, contradicting the uniqueness of the extension.)

Exercise 4.14 Let $I=[0,1]$ be the closed unit interval. Suppose $f$ is a continuous mapping of $I$ into $I$. Prove that $f(x)=x$ for at least one $x \in I$.

Solution. If $f(0)=0$ or $f(1)=1$, we are done. If not, then $0<f(0)$ and $f(1)<1$. Hence the continuous function $g(x)=x-f(x)$ satisfies $g(0)<0<$ $g(1)$. By the intermediate value theorem, there must be a point $x \in(0,1)$ where
$g(x)=0$.

Exercise 4.15 Call a mapping from $X$ into $Y$ open if $f(V)$ is an open set in $Y$ whenever $V$ is an open set in $X$.

Prove that every continuous open mapping of $R^{1}$ into $R^{1}$ is monotonic.

Solution. Suppose $f$ is continuous and not monotonic, say there exist points $a<b<c$. with $f(a)<f(b)$, and $f(c)<f(b)$. Then the maximum value of $f$ on the closed interval $[a, c]$ is assumed at a point $u$ in the open interval $(a, c)$. If there is also a point $v$ in the open interval $(a, c)$ where $f$ assumes its minimum value on $[a, c]$, then $f(a, c)=[f(v), f(u)]$. If no such point $v$ exists, then $f(a, c)=(d, f(u)$, where $d=\min (f(a), f(c))$. In either case, the image of $(a, c)$ is not open.

Exercise 4.16 Let $[x]$ denote the largest integer contained in $x$, that is $[x]$ is the integer such that $x-1<[x] \leq x$; and let $(x)=x-[x]$ denote the fractional part of $x$. What discontinuities do the functions $[x]$ and $(x)$ have?

Solution. The two functions have the same discontinuities, since each can be written as the difference of the continuous function $f(x)=x$ and the other function. Now the function $[x]$ is constant on each open interval $(k, k+1)$; hence its only possible discontinuities are the integers. These are of course real discontinuities, since if $\varepsilon=1$, there is no $\delta>0$ such that $|[x]-[k]|<\varepsilon$. whenever $|x-k|<\delta$. (For if any $\delta$ is given, let $\eta=\min (1, \delta)$. Then $[k]-\left[k-\frac{\eta}{2}\right]=1$.)

Exercise 4.17 Let $f$ be a real function defined on $(a, b)$. Prove that the set of points at which $f$ has a simple discontinuity is at most countable. Hint: Let $E$ be the set on which $f(x-)<f(x+)$. With each point $x$ of $E$ associate a triple $(p, q, r)$ of rational numbers such that

(a) $f(x-)<p<f(x+)$,

(b) $a<q<t<x$ implies $f(t)<p$,

(c) $x<t<r<b$ implies $f(t)>p$.

The set of such triples is countable. Show that each triple is associated with at most one point of $E$. Deal similarly with the other possible types of simple discontinuities.

Solution. The existence of three such rational numbers $(p, q, r)$ for each simple discontinuity of this type follows from the assumption $f(x-)<f(x+)$, and the definition of $f(x-)$ and $f(x+)$. We need to show that a given triple $(p, q, r)$ cannot be associated with any other discontinuity of this type. To that end, suppose $y>x$ and $f(y-)<f(y+)$. If we do not have $f(y-)<p<f(y+)$, then the triple chosen for $y$ will differ from $(p, q, r)$ in its first element. Hence suppose $f(y-)<p<f(y+)$. In this case we definitely cannot have $r>y$, since there are points $t \in(x, y)$ such that $f(t)<p$ (if there weren't, we would have $f(y-) \geq p)$.

We have thus shown that the set of points $x \in(a, b)$ at which $f(x-)<f(x+)$ is at most countable. The proof that the set of points at which $f(x-)>f(x+)$ is at most countable is, of course, nearly identical.

Now consider the set of points $x$ at which $\lim _{t \rightarrow x} f(t)$ exists, but is not equal to $f(x)$. For each point $x \in(a, b)$ such that $\lim _{t \rightarrow x} f(t)<f(x)$, we take a triple $(p, q, r)$ of rational numbers such that

(a) $\lim _{t \rightarrow x} f(t)<p<f(x)$,

(b) $a<q<t<x$ or $x<t<r<b$ implies $f(t)<p$.

As before, if $y>x$ and $\lim _{t \rightarrow y} f(t)<f(y)$, the triple associated with $y$ will be different from that associated with $x$. For even if $\lim _{t \rightarrow y} f(t)<p<f(y)$, we cannot have $r>y$, since $f(y)>p$ and $x<y$.

The proof that the set of points $x \in(a, b)$ at which $\lim _{t \rightarrow x} f(t)>f(x)$ is countable is nearly identical.

Hence, the number of points in $[a, b]$ at which $f$ has a discontinuity of first kind is countable.

Exercise 4.18 Every rational $x$ can be written in the form $x=m / n$, where $n>0$ and $m$ and $n$ are integers without any common divisors. When $x=0$, we take $n=1$. Consider the function $f$ defined on $R^{1}$ by

$$
f(x)= \begin{cases}0 & (x \text { irrational }) \\ \frac{1}{n} & \left(x=\frac{m}{n}\right)\end{cases}
$$

Prove that $f$ is continuous at every irrational point, and that $f$ has a simple discontinuity at every rational point.

Solution. We shall show that $\lim _{t \rightarrow x} f(t)=0$ for every $t$. Both assertions follow immediately from this fact. To this end, let $\varepsilon>0$ be given, and let $x$ be any real number. Let $N$ be the unique positive integer such that $N \leq 1 / \varepsilon<N+1$, and for each positive integer $n=1,2, \ldots, N$, let $k_{n}$ be the unique integer such that

$$
\frac{k_{n}}{n} \leq x<\frac{k_{n}+1}{n}
$$

Then for each such $n$ let $\delta_{n}=\frac{1}{n}$ if $x=\frac{k_{n}}{n}$, otherwise let $\delta_{n}=\min (x-$ $\left.\frac{k}{n}, \frac{k_{n}+1}{n}-x\right)$. Finally let $\delta=\min \left(\delta_{1}, \ldots, \delta_{N}\right)$. We claim that $|f(t)|<\varepsilon$ if $0<|x-t|<\delta$. This is obvious if $t$ is irrational, while if $t$ is rational and $t=\frac{m}{n}$, we necessarily have $n>N$ by the choice of the numbers $\delta_{n}$ for $n \leq N$. Hence if $t$ is rational, then $f(t) \leq \frac{1}{N+1}<\varepsilon$. The proof is now complete.

Exercise 4.19 Suppose $f$ is a real function with domain $R^{1}$ which has the intermediate-value property: If $f(a)<c<f(b)$, then $f(x)=c$ for some $x$ between $a$ and $b$.

Suppose also, for every rational $r$, that the set of all $x$ with $f(x)=r$ is closed.

Prove that $f$ is continuous.

Fint: If $x_{n} \rightarrow x_{0}$ but $f\left(x_{n}\right)>r>f\left(x_{0}\right)$ for some $r$ and all $n$, then $f\left(t_{n}\right)=r$ for some $t_{n}$ between $x_{0}$ and $x_{n}$; thus $t_{n} \rightarrow x_{0}$. Find a contradiction. (N. M. Fine, Amer. Math. Monthly, vol. 73, 1966, p. 782.)

Solution. The contradiction is evidently that $x_{0}$ is a limit point of the set of $t$ such that $f(t)=r$, yet, $x_{0}$ does not belong to this set. This contradicts the hypothesis that the set is closed.

Exercise 4.20 If $E$ is a nonempty subset of a metric space $X$, define the distance from $x \in X$ to $E$ by

$$
\rho_{E}(x)=\inf _{z \in E} d(x, z)
$$

(a) Prove that $\rho_{E}(x)=0$ if and only if $x \in \bar{E}$.

(b) Prove that $\rho_{E}$ is a uniformly continuous function on $X$ by showing that

$$
\left|\rho_{E}(x)-\rho_{E}(y)\right| \leq d(x, y)
$$

for all $x \in X$ and $y \in X$.

Hint: $\rho_{E}(x) \leq d(x, z) \leq d(x, y)+d(y, z)$, so that

$$
\rho_{E}(x) \leq d(x, y)+\rho_{E}(y) .
$$

Solution. (a) For each positive integer $n$, let $z_{n} \in E$ be such that $\rho_{E}(x) \leq$ $d\left(x, z_{n}\right)<\rho_{E}(x)+\frac{1}{n}$. It follows that $d\left(x, z_{n}\right) \rightarrow \rho_{E}(x)$. If $\rho_{E}(x)=0$, this means $z_{n} \rightarrow x$, i.e., $x \in \bar{E}$. Conversely, if $x \in \bar{E}$, there exists a sequence $\left\{z_{n}\right\}_{n=1}^{\infty} \subseteq E$ such that $z_{n} \rightarrow x$, and this means $d\left(z_{n}, x\right) \rightarrow 0$, so that $\rho_{E}(x)=0$.

(b) The last inequality given in the hint follows form the first by taking the infimum over $z$ on the right-hand side. This inequality immediately implies that

$$
\rho_{E}(x)-\rho_{E}(y) \leq d(x, y)
$$

By interchanging $x$ and $y$, we also obtain

$$
\rho_{E}(y)-\rho_{E}(x) \leq d(y, x)=d(x, y)
$$

Since $\left|\rho_{E}(x)-\rho_{E}(y)\right|$ must be either $\rho_{E}(x)-\rho_{E}(y)$ or $\rho_{E}(y)-\rho_{E}(x)$, it follows that

$$
\left|\rho_{E}(x)-\rho_{E}(y)\right| \leq d(x, y)
$$

Exercise 4.21 Suppose $K$ and $F$ are disjoint sets in a metric space $X, K$ is compact, $F$ is closed. Prove that there exists $\delta>0$ such that $d(p, q)>\delta$ if $p \in K, q \in F$. Hint: $\rho_{F}$ is a continuous positive function on $K$.

Show that the conclusion may fail for two disjoint closed sets if neither is compact.

Solution. Following the hint, we observe that $\rho_{F}(x)$ must attain its minimum value on $K$, i.e., there is some point $r \in K$ such that

$$
\rho_{F}(r)=\min _{q \in K} \rho_{F}(q)
$$

Since $F$ is closed and $r \notin F$, it follows from Exercise 4.20 that $\rho_{F}(r)>0$. Let $\delta$ be any positive number smaller than $\rho_{F}(r)$. Then for any $p \in F, q \in K$, we have

$$
d(p, q) \geq \rho_{F}(q) \geq \rho_{F}(r)>\delta
$$

This proves the positive assertion.

As for closed sets in general, one could let $F=\{1,2,3, \ldots\}$ and $K=\{1+$ $\left.\frac{1}{2}, 2+\frac{1}{3}, 3+\frac{1}{4} \cdots\right\}$ in $R^{1}$, or one could let $F=\{(x, y): y=0\}$ and $K=\{(x, y)$ : $\left.y=\frac{1}{1+x^{2}}\right\}$ in $R^{2}$. In both cases there are sequences of points $p_{n} \in F, q_{n} \in K$ such that $d\left(p_{n} ; q_{n}\right) \rightarrow 0$.

Exercise 4.22 Let $A$ and $B$ be disjoint nonempty closed sets in a metric space $X$, and define

$$
f(p)=\frac{\rho_{A}(p)}{\rho_{A}(p)+\rho_{B}(p)} \quad(p \in X)
$$

Show that $f$ is a continuous function on $X$ whose range lies in $[0,1]$, that $f(p)=0$ precisely on $A$ and $f(p)=1$ precisely on $B$. This establishes a converse of Exercise 3: Every closed set $A \subset X$ is $Z(f)$ for some continuous real $f$ on $X$. Setting

$$
V=f^{-1}\left(\left[0, \frac{1}{2}\right)\right), \quad W=f^{-1}\left(\left(\frac{1}{2}, 1\right]\right)
$$

show that $V$ and $W$ are open and disjoint, and that $A \subset V, B \subset W$. (Thus pairs of disjoint closed sets in a metric space can be covered by pairs of disjoint open sets. This property of metric spaces is called normality.)

Solution. The continuity of $f$ follows from the fact that the quotient of two continuous real-valued continuous functions is continuous wherever the denominator is non-zero. Now the denominator of the fraction that defines $f$ cannot be zero, since the first term is zero only on $A$ and the second is zero only on $B$, while $A$ and $B$ are disjoint. The fact that $f(p)=0$ if and only if $p \in A$ follows from Exercise 20 and the fact that $A$ is closed. Likewise the fact that $f(p)=1$ if and only if $p \in B$ follows from Exercise 20 and the fact that $B$ is closed. The assertion about $V$ and $W$ is immediate, since $V$ and $W$ are the inverse images of disjoint open sets containing 0 and 1 respectively.

Exercise 4.23 A real-valued function $f$ defined in $(a, b)$ is said to be convex if

$$
f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)
$$

whenever $a<x<b, a<y<b, 0<\lambda<1$. Prove that every convex function is continuous. Prove that every increasing convex function of a convex function is convex. (For example, if $f$ is convex, so is $e^{f}$.)

If $f$ is convex in $(a, b)$ and if $a<s<t<u<b$, show that

$$
\frac{f(t)-f(s)}{t-s} \leq \frac{f(u)-f(s)}{u-s} \leq \frac{f(u)-f(t)}{u-t}
$$

Solution. Fix any points $c$, $d$ with $a<c<d<b$, let $\eta>0$ be any fixed positive number with $\eta<\frac{d-c}{2}$ and consider any two points $x, y$ satisfying $c+\eta \leq x<y \leq d-\eta$. The inequality in the definition implies that $f(t)$ is bounded above on $[c, d]$. Indeed, if $c<t<d$, taking $\lambda=\frac{t-c}{d-c}$, we have $t=(1-\lambda) c+\lambda d$, and so, if $M=\max (f(c), f(d))$, we have

$$
f(t) \leq(1-\lambda) f(c)+\lambda f(d) \leq(1-\lambda) M+\lambda M=M
$$

It is less obvious that $f$ is also bounded below on $[c, d]$. In fact if $\frac{c+d}{2}<t<d$, we have

$$
\frac{c+d}{2}=(1-\lambda) c+\lambda t
$$

where $\lambda=\frac{d-c}{2(t-c)}$, so that

$$
f\left(\frac{c+d}{2}\right) \leq\left(\frac{2 t-(c+d)}{2(t-c)}\right) f(c)+\left(\frac{d-c}{2(t-c)}\right) f(t)
$$

which implies

$$
\left.f(t) \geq\left(\frac{2(t-c)}{d-c}\right) f\left(\frac{c+d}{2}\right)-\frac{2 t-(c+d)}{d-c}\right) f(c) \geq-2\left|f\left(\frac{c+d}{2}\right)\right|-|f(c)|
$$

The proof that $f$ is bounded below on $\left[c, \frac{c+d}{2}\right]$ is similar. Hence there exists $M$ such that $|f(t)| \leq M$ for all $t \in[c, d]$.

We can also write

$$
x=(1-\lambda) c+\lambda y
$$

where $\lambda=\frac{x-c}{y-c} \in(0,1)$. Accordingly we have

$$
\begin{aligned}
& f(x)-f(y) \leq(1-\lambda)(f(c)-f(y))= \\
& \quad=\frac{y-x}{y-c}(f(c)-f(y)) \leq \frac{y-x}{\eta}|f(c)-f(y)|
\end{aligned}
$$

Thus

$$
f(x)-f(y) \leq \frac{2 M}{\eta}(y-x)
$$

Similarly, writing $y=\lambda x+(1-\lambda) d$, where $\lambda=\frac{d-y}{d-x} \in(0,1)$, we find

$$
\begin{aligned}
f(y)-f(x) \leq(1-\lambda)(f(d)- & f(x))= \\
& =\frac{y-x}{d-x}(f(d)-f(x)) \leq \frac{y-x}{\eta}|f(d)-f(x)|
\end{aligned}
$$

Hence we also have

$$
f(y)-f(x) \leq \frac{2 \dot{M}}{\eta}(y-x)
$$

Therefore

$$
|f(y)-f(x)| \leq \frac{2 M}{\eta}|y-x|
$$

for all $x, y \in[c+\eta, d-\eta]$. Since $c, d$, and $\eta$ are arbitrary, it follows that $f$ is continuous on $(a, b)$.

If $f(x)$ is convex on $(a, b)$, and $g(x)$ is an increasing convex function on $f((a, b))$, we have

$$
g(f(\lambda x+(1-\lambda) y)) \leq g(\lambda f(x)+(1-\lambda) f(y)) \leq \lambda g(f(x))+(1-\lambda) g(f(y))
$$

The inequality

can be rewritten as

$$
\frac{f(t)-f(s)}{t-s} \leq \frac{f(u)-f(s)}{u-s}
$$

$$
f(t) \leq \frac{t-s}{u-s} f(u)+\left(1-\frac{t-s}{u-s}\right) f(s)
$$

which is precisely the definition of convexity if we note that

$$
t=\lambda u+(1-\lambda) s
$$

when $\lambda=\frac{t-s}{u-s}$.

The other inequality is proved in exactly the same way. Exercise 4.24 Assume that $f$ is a continuous real function defined in $(a, b)$
such that

$$
f\left(\frac{x+y}{2}\right) \leq \frac{f(x)+f(y)}{2}
$$

for all $x, y \in(a, b)$. Prove that $f$ is convex.

Solution. We shall prove that

$$
f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)
$$

for all "dyadic rational" numbers, i.e., all numbers of the form $\lambda=\frac{k}{2^{n}}$, where $k$ is a nonnegative integer not larger than $2^{n}$. We do this by induction on $n$. The case $n=0$ is trivial (since $\lambda=0$ or $\lambda=1$ ). In the case $n=1$ we have $\lambda=0$ or $\lambda=1$ or $\lambda=\frac{1}{2}$. The first two cases are again trivial, and the third is precisely the hypothesis of the theorem. Suppose the result is proved for $n \leq r$, and consider $\lambda=\frac{k}{2^{r+1}}$. If $k$ is even, say $k=2 l$, then $\frac{k}{2^{r+1}}=\frac{l}{2^{r}}$, and we can appeal to the induction hypothesis. Now suppose $k$ is odd. Then $1 \leq k \leq 2^{r+1}-1$, and so the numbers $l=\frac{k-1}{2}$ and $m=\frac{k+1}{2}$ are integers with $0 \leq l<m \leq 2^{r}$.
We can now write

$$
\lambda=\frac{s+t}{2}
$$

where $s=\frac{k-1}{2^{r+1}}=\frac{l}{2^{r}}$ and $t=\frac{k+1}{2^{r+1}}=\frac{m}{2^{r}}$. We then have

$$
\lambda x+(1-\lambda) y=\frac{[s x+(1-s) y]+[t x+(1-t) y]}{2}
$$

Hence by the hypothesis of the theorem and the induction hypothesis we have

$$
\begin{aligned}
f(\lambda x+(1-\lambda) y) & \leq \frac{f(s x+(1-s) y)+f(t x+(1-t) y)}{2} \\
& \leq \frac{s f(x)+(1-s) f(y)+t f(x)+(1-t) f(y)}{2} \\
& =\left(\frac{s+t}{2}\right) f(x)+\left(1-\frac{s+t}{2}\right) f(y) \\
& =\lambda f(x)+(1-\lambda) f(y) .
\end{aligned}
$$

This completes the induction.

Now for each fixed $x$ and $y$ both sides of the inequality

$$
f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)
$$

are continuous functions of $\lambda$. Hence the set on which this inequality holds (the inverse image of the closed set $[0, \infty)$ under the mapping $\lambda \mapsto \lambda f(x)+(1-$ d) $f(y)-f(\lambda x+(1-\lambda) y))$ is a closed set. Since it contains all the points $\frac{k}{2^{n}}$, $0 \leq k \leq n, n=1,2, \ldots$, it must contain the closure of this set of points, i.e., it must contain all of $[0,1]$. Thus $f$ is convex.

Exercise 4.25 If $A \subset R^{k}$ and $B \subset R^{k}$, define $A+B$ to be the set of all sums $\mathrm{x}+\mathrm{y}$ with $\mathrm{x} \in A, \mathrm{y} \in B$.

(a) If $K$ is compact and $C$ is closed in $R^{k}$, prove that $K+C$ is closed.

Hint: Take $\mathbf{z} \notin K+C$, put $F=\mathbf{z}-C$, the set of all $\mathbf{z}-\mathbf{y}$ with $\mathrm{y} \in C$. Then $K$ and $F$ are disjoint. Choose $\delta$ as in Exercise 21. Show that the open ball with center $\mathrm{z}$ and radius $\delta$ does not intersect $K+C$.

(b) Let $\alpha$ be an irrational number. Let $C_{1}$ be the set of all integers. Let $C_{2}$ be the set of all $n \alpha$ with $n \in C_{1}$. Show that $C_{1}$ and $C_{2}$ are closed subsets of $R^{1}$ whose sum $C_{1}+C_{2}$ is not closed, by showing that $C_{1}+C_{2}$ is a countable dense subset of $R^{1}$.

Solution. (a) It is clear that the set $F$ defined in the hint is a closed set. It is disjoint from $K$, since $\mathbf{z} \notin K+C$. Let $\delta$ be such that $|\mathbf{p}-\mathbf{q}|>\delta$ if $\mathbf{p} \in F$ and $\mathrm{q} \in K$. We claim that there is no point of $K+C$ inside the ball of radius $\delta$ about z. For suppose $\mathbf{w}$ were such a point. By definition we would have $\mathbf{w}=\mathbf{u}+\mathbf{v}$, where $\mathbf{u} \in K$ and $\mathbf{v} \in C$. But then we would have

$$
|\mathbf{u}-(\mathbf{z}-\mathbf{v})|=|\mathbf{w}-\mathbf{z}|<\delta
$$

which is a contradiction, since $\mathbf{u} \in K$ and $\mathbf{z}-\mathbf{v} \in F$. Thus $K+C$ is closed.

(b) Neither of the sets $C_{1}$ and $C_{2}$ has any limit points; hence both are closed sets. For each fixed integer $N \geq 2$, consider the fractional parts $\beta_{1}=\alpha-[\alpha]$, $\beta_{2}=2 \alpha-[2 \alpha], \ldots, \beta_{N}=N \alpha-[\hat{N} \alpha]$. There must be some half-open interval
$\left[\frac{k-1}{N-1}, \frac{k}{N-1}\right), k=1,2, \ldots, N-1$ containing two of the numbers $\beta_{1}, \ldots, \beta_{N}$, since there are $N$ numbers and only $N-1$ intervals. (Note: No two of these numbers are equal, since $\beta_{i}=\beta_{j}, i \neq j$, would imply

$$
\alpha=\frac{[i \alpha]-[j \alpha]}{i-j}
$$

i.e., $\alpha$ would be a rational number.) Now the inequalities

$$
0<(i \alpha-[i \alpha])-(j \alpha-[j \alpha])<\frac{1}{N-1}
$$

say that $(i-j) \alpha+([j \alpha]-[i \alpha]) \in\left(0, \frac{1}{N-1}\right)$, that is, there is certainly a point of $C_{1}+C_{2}$ in $\left(0, \frac{1}{N-1}\right)$ for any $N \geq 2$. We shall now prove that there is a point of $C_{1}+C_{2}$ in $\left(\frac{k}{n}, \frac{k+1}{n}\right)$ for any integer $k$ and any positive integer $n$. To do so, fix the integer $q$ such that $q n \leq k<(q+1) n$, and choose $y \in C_{1}+C_{2}$ such that $0<y<\frac{1}{n}$. Then $x=n y \in C_{1}+C_{2}$ and $0<x<1$. Hence there is a positive integer $p$ such that $k<p x+q n<k+1$. This says precisely that

$$
\frac{k}{n}<p y+q<\frac{k+1}{n}
$$

and certainly $p y+q \in C_{1}+C_{2}$. Now let $O$ be any nonempty open subset of $R^{1}$. Then $O$ contains an interval $(a, b)$. If $n>\frac{2}{b-a}$, there is an integer $k$ such that $\left(\frac{k}{n}, \frac{k+1}{n}\right) \subset(a, b)$. This interval, as just shown, contains a point of $C_{1}+C_{2}$, and hence $O$ contains such a point. Therefore $C_{1}+C_{2}$ is dense in $R^{1}$. Since it is a countable set, it is not all of $R^{1}$, and hence not closed.

Exercise 4.26 Suppose $X, Y, Z$ are metric spaces and $Y$ is compact. Let $f$ $\operatorname{map} X$ into $Y$, let $g$ be a continuous one-to-one mapping of $Y$ into $Z$, and put $h(x)=g(f(x))$ for $x \in X$.

Prove that $f$ is uniformly continuous if $h$ is uniformly continuous.

Hint: $g^{-1}$ has compact domain $g(Y)$, and $f(x)=g^{-1}(h(x))$.

Prove also that $f$ is continuous if $h$ is continuous.

Show (by modifying Example 4.21, or by finding a different example) that the compactness of $Y$ cannot be omitted from the hypotheses, even when $X$ and $Z$ are compact.

Solution. Theorem 4.17 asserts that $g^{-1}$ is continuous, and since its domain is compact, it is uniformly continuous. Exercise 12 above then implies that $f$ is uniformly continuous. The same argument, with the word "uniformly" omitted,

To get a counterexample when $Y$ is not compact, let $X=[0,1]=Z, Y=$ $\{0\} \cup[1, \infty)$, and let $f: X \rightarrow Y$ and $g: Y \rightarrow Z$ be given by

$$
\begin{aligned}
& f(x)= \begin{cases}\frac{1}{x}, & 0<x \leq 1 \\
0, & x=0\end{cases} \\
& g(y)= \begin{cases}\frac{1}{y}, & 1 \leq y<\infty \\
0, & y=0\end{cases}
\end{aligned}
$$

Then $h(x)=g(f(x))=x$, so that $h$ is uniformly continuous, and $g$ is continuous and one-to-one, yet $f$ is not even continuous.

## Chapter 5

## Differentiation

Exercise 5.1 Let $f$ be defined for all real $x$, and suppose that

$$
|f(x)-f(y)| \leq(x-y)^{2}
$$

for all real $x$ and $y$. Prove that $f$ is constant.

Solution. Dividing by $x-y$, and letting $x \rightarrow y$, we find that $f^{\prime}(y)=0$ for all $y$. Hence $f$ is constant.

Exercise 5.2 Suppose $f^{\prime}(x)>0$ in $(a, b)$. Prove that $f$ is strictly increasing in $(a, b)$, and let $g$ be its inverse function. Prove that $g$ is differentiable, and that

$$
g^{\prime}(f(x))=\frac{1}{f^{\prime}(x)} \quad(a<x<b)
$$

Solution. For any $c, d$ with $a<c<d<b$ there exists a point $p \in(c, d)$ such that $f(d)-f(c)=f^{\prime}(p)(d-c)>0$. Hence $f(c)<f(d)$.

We know from Theorem 4.17 that the inverse function $g$ is continuous. (Its restriction to each closed subinterval $[c, d]$ is continuous, and that is sufficient.) Now observe that if $f(x)=y$ and $f(x+h)=y+k$, we have

$$
\frac{g(y+k)-g(y)}{k}-\frac{1}{f^{\prime}(x)}=\frac{1}{\frac{f(x+h)-f(x)}{h}}-\frac{1}{f^{\prime}(x)}
$$

Since we know $\lim \frac{1}{\varphi(t)}=\frac{1}{\lim \varphi(t)}$ provided $\lim \varphi(t) \neq 0$, it follows that for any $\varepsilon>0$ there exists $\eta>0$ such that

$$
\left|\frac{1}{\frac{f(x+h)-f(x)}{h}}-\frac{1}{f^{\prime}(x)}\right|<\varepsilon
$$

if $0<|h|<\eta$. Since $h=g(y+k)-g(y)$, there exists $\delta>0$ such that $0<|h|<\eta$ if $0<|k|<\delta$. The proof is now complete.

Exercise 5.3 Suppose $g$ is a real function on $R^{1}$ with bounded derivative (say $\left.\left|g^{\prime}\right| \leq M\right)$. Fix $\varepsilon>0$, and define $f(x)=x+\varepsilon g(x)$. Prove that $f$ is one-to-one if $\varepsilon$ is small enough. (A set of admissible values of $\varepsilon$ can be determined which depends only on $M$.)

Solution. If $0<\varepsilon<\frac{1}{M}$, we certainly have

$$
f^{\prime}(x) \geq 1-\varepsilon M>0
$$

and this implies that $f(x)$ is one-to-one, by the preceding problem.

Exercise 5.4 If

$$
C_{0}+\frac{C_{1}}{2}+\cdots+\frac{C_{n-1}}{n}+\frac{C_{n}}{n+1}=0
$$

where $C_{0}, \ldots, C_{n}$ are real constants, prove that the equation

$$
C_{0}+C_{1} x+\cdots+C_{n-1} x^{n-1}+C_{n} x^{n}=0
$$

has at least one real root between 0 and 1 .

Solution. Consider the polynomial

$$
p(x)=C_{0} x+\frac{C_{1}}{2} x^{2}+\cdots+\frac{C_{n-1}}{n} x^{n}+\frac{C_{n}}{n+1} x^{n+1}
$$

whose derivative is

$$
p^{\prime}(x)=C_{0}+C_{1} x+\cdots+C_{n-1} x^{n-1}+C_{n} x^{n}
$$

It is obvious that $p(0)=0$, and the hypothesis of the problem is that $p(1)=0$. Hence Rolle's theorem implies that $p^{\prime}(x)=0$ for some $x$ between 0 and 1 .

Exercise 5.5 Suppose $f$ is defined and differentiable for every $x>0$, and $f^{\prime}(x) \rightarrow 0$ as $x \rightarrow+\infty$. Put $g(x)=f(x+1)-f(x)$. Prove that $g(x) \rightarrow 0$ as
$x \rightarrow+\infty$.

Solution. Let $\varepsilon>0$. Choose $x_{0}$ such that $\left|f^{\prime}(x)\right|<\varepsilon$ if $x>x_{0}$. Then for any $x \geq x_{0}$ there exists $x_{1} \in(x, x+1)$ such that

$$
f(x+1)-f(x)=f^{\prime}\left(x_{1}\right)
$$

Since $\left|f^{\prime}\left(x_{1}\right)\right|<\varepsilon$, it follows that $|f(x+1)-f(x)|<\varepsilon$, as required.

Exercise 5.6 Suppose

(a) $f$ is continuous for $x \geq 0$,

(b) $f^{\prime}(x)$ exists for $x>0$,

(c) $f(0)=0$,

(d) $f^{\prime}$ is monotonically increasing.

Put

$$
g(x)=\frac{f(x)}{x} \quad(x>0)
$$

and prove that $g$ is monotonically increasing.

Solution. By the mean-value theorem

$$
f(x)=f(x)-f(0)=f^{\prime}(c) x
$$

for some $c \in(0, x)$. Since $f^{\prime}$ is monotonically increasing, this result implies that $f(x)<x f^{\prime}(x)$. It therefore follows that

$$
g^{\prime}(x)=\frac{x f^{\prime}(x)-f(x)}{x^{2}}>0,
$$

so that $g$ is also monotonically increasing.

Exercise 5.7 Suppose $f^{\prime}(x)$ and $g^{\prime}(x)$ exist, $g^{\prime}(x) \neq 0$, and $f(x)=g(x)=0$. Prove that

$$
\lim _{t \rightarrow x} \frac{f(t)}{g(t)}=\frac{f^{\prime}(x)}{g^{\prime}(x)}
$$

(This holds also for complex functions.)

Solution. Since $f(x)=g(x)=0$, we have

$$
\begin{aligned}
\lim _{t \rightarrow x} \frac{f(t)}{g(t)} & =\lim _{t \rightarrow x} \frac{\frac{f(t)-f(x)}{t-x}}{\frac{g(t)-g(x)}{t-x}} \\
& =\frac{\lim _{t \rightarrow x} \frac{f(t)-f(x)}{t-x}}{\lim _{t \rightarrow x} \frac{g(t)-g(x)}{t-x}} \\
& =\frac{f^{\prime}(x)}{g^{\prime}(x)}
\end{aligned}
$$

Exercise 5.8 Suppose $f^{\prime}$ is continuous on $[a, b]$ and $\varepsilon>0$. Prove that there exists $\delta>0$ such that

$$
\left|\frac{f(t)-f(x)}{t-x}-f^{\prime}(x)\right|<\varepsilon
$$

whenever $0<|t-x|<\delta, a \leq x \leq b, a \leq t \leq b$. (This could be expressed by saying that $f$ is uniformly differentiable on $[a, b]$ if $f^{\prime}$ is continuous on $[a, b]$.) Does this hold for vector-valued functions too?

Solution. Let $\delta$ be such that $\left|f^{\prime}(x)-f^{\prime}(u)\right|<\varepsilon$ for all $x, u \in[a, b]$ with $|x-u|<\delta$. Then if $0<|t-x|<\delta$ there exists $u$ between $t$ and $x$ such that

$$
\frac{f(t)-f(x)}{t-x}=f^{\prime}(u)
$$

and hence, since $|u-x|<\delta$,

$$
\left|\frac{f(t)-f(x)}{t-x}-f^{\prime}(x)\right|=\left|f^{\prime}(u)-f^{\prime}(x)\right|<\varepsilon .
$$

Since this result holds for each component of a vector-valued function $\mathbf{f}(x)$, it must hold also for $\mathbf{f}$.

Exercise 5.9 Let $f$ be a continuous real function on $R^{1}$, of which it is known that $f^{\prime}(x)$ exists for all $x \neq 0$ and that $f^{\prime}(x) \rightarrow 3$ as $x \rightarrow 0$. Does it follow that $f^{\prime}(0)$ exists?

Solution. Yes. By L'Hospital's rule

$$
\lim _{t \rightarrow 0} \frac{f(t)-f(0)}{t}=\lim _{t \rightarrow 0} f^{\prime}(t)=3
$$

and this by definition means that $f^{\prime}(0)=3$.

Exercise 5.10 Suppose $f$ and $g$ are complex differentiable functions on $(0,1)$, $f(x) \rightarrow 0, g(x) \rightarrow 0, f^{\prime}(x) \rightarrow A, g^{\prime}(x) \rightarrow B$ as $x \rightarrow 0$, where $A$ and $B$ are complex numbers, $B \neq 0$. Prove that

$$
\lim _{x \rightarrow 0} \frac{f(x)}{g(x)}=\frac{A}{B}
$$

Compare with Example 5.18. Hint:

$$
\frac{f(x)}{g(x)}=\left\{\frac{f(x)}{x}-A\right\} \cdot \frac{x}{g(x)}+A \cdot \frac{x}{g(x)}
$$

Apply Theorem 5.13 to the real and imaginary parts of $f(x) / x$ and $g(x) / x$.

Solution. We can make $f$ and $g$ continuous on $[0,1)$ by simply defining $f(0)=$ $0=g(0)$. Then Exercise 9 applied to the real and imaginary parts of $f$ and $g$ show that $f^{\prime}(0)=A$ and $g^{\prime}(0)=B$. (These are one-sided derivatives, since $f$ and $g$ are not defined for negative values of $x$; however, we could extend them as odd functions, since both are 0 at 0 ). We could then apply Exercise 7, whose proof does not use anything but the definition of the derivative and some general facts about limits. In this way we get the result without resorting to the combinatorial trick referred to in the hint. This result shows that many of the facts ordinarily proved for real functions by use of the mean-value theorem and L'Hospital's rule remain true for complex-valued functions, even though, as Example 5.18 shows, these theorems are not true for complex-valued functions.

Exercise 5.11 Suppose $f$ is defined in a neighborhood of $x$, and suppose $f^{\prime \prime}(x)$ exists. Show that

$$
\lim _{h \rightarrow 0} \frac{f(x+h)+f(x-h)-2 f(x)}{h^{2}}=f^{\prime \prime}(x)
$$

Solution. For a real-valued function this is a routine application of L'Hospital's rule:

$$
\begin{aligned}
\lim _{h \rightarrow 0} \frac{f(x+h)+f(x-h)-2 f(x)}{h^{2}} & =\lim _{h \rightarrow 0} \frac{f^{\prime}(x+h)-f^{\prime}(x-h)}{2 \hbar} \\
& =\frac{1}{2} \lim _{h \rightarrow 0} \frac{f^{\prime}(x+h)-f^{\prime}(x)}{h}+ \\
& +\frac{f^{\prime}(x)-f^{\prime}(x-h)}{h} \\
& =f^{\prime \prime}(x) .
\end{aligned}
$$

For complex-valued functions the result follows from separate consideration of real and imaginary parts.

The limit will be zero at $x=0$ for any odd function $f$ whatsoever, even if the function is not continuous. For example we could take $f(x)=\operatorname{sgn}(x)$, which is +1 for $x>0,0$ for $x=0$, and -1 for $x<0$

Exercise 5.12 If $f(x)=|x|^{3}$, compute $f^{\prime}(x), f^{\prime \prime}(x)$ for all real $x$, and show that $f^{(3)}(0)$ does not exist.

Solution. For $x>0$ we have $f^{\prime}(x)=3 x^{2}, f^{\prime \prime}(x)=6 x$, and for $x<0 f^{\prime}(x)=$ $-6 x^{2}, f^{\prime \prime}(x)=-6 x$, i.e., $f^{\prime}(x)=3 x|x|$, and $f^{\prime \prime}(x)=6|x|$ for $x \neq 0$. By Exercise 9 , it therefore follows that $f^{\prime}(0)$ exists and equals 0 , and then another application of Exercise 9 shows that $f^{\prime \prime}(0)$ also exists and equals 0 . However

$$
\frac{f^{\prime \prime}(x)-f^{\prime \prime}(0)}{x}=6 \operatorname{sgn}(x),
$$

which has no limit at 0 . Hence $f^{(3)}(0)$ does not exist.

Exercise 5.13 Suppose $a$ and $c$ are real numbers, $c>0$, and $f$ is defined on $[-1,1]$ by

$$
f(x)= \begin{cases}x^{a} \sin \left(x^{-c}\right) & (\text { if } x \neq 0) \\ 0 & \text { (if } x=0)\end{cases}
$$

Prove the following statements:

(a) $f$ is continuous if and only if $a>0$.

(b) $f^{\prime}(0)$ exists if and only if $a>1$.

(c) $f^{\prime}$ is bounded if and only if $a \geq 1+c$.

(d) $f^{\prime}$ is continuous if and onlyl if $a>1+c$.

(e) $f^{\prime \prime}(0)$ exists if and only if $a>2+c$.

(f) $f^{\prime \prime}$ is bounded if and only if $a \geq 2+2 c$.

(g) $f^{\prime \prime}$ is continuous if and only if $a>2+2 c$.

Solution. We remark editorially that there are two difficulties with this problem. One is that we haven't yet introduced the function sin. To overcome that problem we can rely on our intuitive notion or use the Taylor series if we have to. The second problem is more serious, however: What do $x^{a}$ and $x^{-c}$ mean when $x<0$ ? In general these will be complex-valued functions. It might be better to use absolute values in both cases. Thus we shall amend the problem by defining $f(x)=|x|^{a} \sin \left(|x|^{-c}\right)$ when $x \neq 0$.

(a) Since $f$ is infinitely differentiable except at $x=0$, the only question of continuity is at $x=0$. Let $t_{n}=2 \pi\left(n+\frac{1}{8}\right), x_{n}=t_{n}^{-\frac{1}{c}}$ and $y_{n}=\frac{1}{\sqrt{2}} t_{n}^{-\frac{a}{c}}$. Notice that $f\left(x_{n}\right)=y_{n}$ and that $y_{n}$ tends to $\frac{1}{\sqrt{2}}$ if $a=0$ and to $+\infty$ if $a<0$. Hence the function cannot be continuous if $a \leq 0$. On the other hand, we have

$$
|f(x)-f(0)|=|f(x)| \leq|x|^{a}
$$

so that if $a>0$ and $\varepsilon$ is given, we can choose $\delta=\varepsilon^{\frac{1}{a}}$, and then $|x-0|<\delta$ implies $|f(x)-f(0)|<\varepsilon$, i.e., $f(x)$ is continuous at $x=0$. (b) If $f^{\prime}(0)$ exists, then $f$ is continuous at 0 , so that $a>0$. Notice that

$$
\frac{f\left(x_{n}\right)-f(0)}{x_{n}}=\frac{y_{n}}{x_{n}}=\frac{1}{\sqrt{2}} t_{n}^{\frac{1-a}{c}}
$$

which tends to $\frac{1}{\sqrt{2}}$ if $a=1$ and to $+\infty$ if $0<a<1$. Hence $f^{\prime}(0)$ does not exist if $a \leq 1$. On the other hand if $a>1$ we have

$$
0 \leq \frac{f(x)-f(0)}{x}<|x|^{a-1} \rightarrow 0
$$

and so $f^{\prime}(0)=0$.

(c) For $x \neq 0$ we have

$$
f^{\prime}(x)=\operatorname{sgn}(x)|x|^{a-1}\left[a \sin \left(|x|^{-c}\right)-c|x|^{-c} \cos \left(|x|^{-c}\right)\right]
$$

Hence $f^{\prime}\left(x_{n}\right)=\frac{1}{\sqrt{2}}\left[a x_{n}^{a-1}-c x_{n}^{-c+a-1}\right]$, which tends to $-\infty$ if $a<1+c$. On the other hand we have

$$
\left|f^{\prime}(x)\right| \leq|a||x|^{a-1}+c|x|^{a-1-c}
$$

which is certainly bounded on $[-1,1]$ if $a \geq 1+c$.

(d) If $f^{\prime}$ is continuous, it is bounded, and so $a \geq 1+c$. However if $a=1+c$, then

$$
f^{\prime}\left(x_{n}\right)=\frac{1}{\sqrt{2}}\left[(1+c) t_{n}^{-1}-c\right]
$$

which tends to $-\frac{c}{\sqrt{2}}$ as $n \rightarrow \infty$, while $x_{n} \rightarrow 0$. Hence $f^{\prime}$ is not continuous at 0 unless $a>1+c$. If $a>1+c$, the inequality

$$
\left|f^{\prime}(x)\right| \leq|a||x|^{a-1}+c|x|^{a-1-c}
$$

implies that $f(x) \rightarrow 0$ as $x \rightarrow 0$, and so $f^{\prime}$ is continuous.

(e) If $f^{\prime \prime}(0)$ exists, then $f^{\prime}$ must be continuous at 0 , and so $a \geq 1+c$. Now for $x \neq 0$

$$
\frac{f^{\prime}(x)-f^{\prime}(0)}{x}=\operatorname{sgn}(x)\left[a|x|^{a-2} \sin \left(|x|^{-c}\right)-c|x|^{a-c-2} \cos \left(|x|^{-c}\right)\right.
$$

Taking $x=x_{n}$, we find that this difference quotient equals

$$
\frac{1}{\sqrt{2}}\left[a t_{n}^{\frac{2-a}{c}}-c t_{n}^{\frac{c+2-a}{c}}\right]
$$

which tends to $\frac{1}{\sqrt{2}}$ if $a=c+2$ and to $-\infty$ if $a<c+2$. Hence $f^{\prime \prime}(0)$ exists only if $a>c+2$.

On the other hand, if $a>c+2$, we have the inequality

$$
\left|\frac{f^{\prime}(x)-f^{\prime}(0)}{x}\right| \leq a|x|^{a-2}+c|x|^{a-c-2}
$$

from which it follows immediately that $f^{\prime \prime}(0)=0$.

$(f)$ For $x \neq 0$ we have

$$
\begin{aligned}
f^{\prime \prime}(x)=\operatorname{sgn}(x)\left[a(a-1)|x|^{a-2}-c^{2}|x|^{a-2 c-2}\right] \sin \left(|x|^{-c}\right] & \\
& -c(2 a-c-1)|x|^{a-c-1} \cos \left(|x|^{-c}\right] .
\end{aligned}
$$

In particular

$$
f^{\prime \prime}\left(x_{n}\right)=\frac{1}{\sqrt{2}}\left[a(a-1) t_{n}^{\frac{2-a}{c}}-c^{2} t_{n}^{\frac{2+2 c-a}{c}}-c(2 a-c-1) t_{n}^{\frac{c+1-a}{c}}\right]
$$

which tends to $-\infty$ if $a<2+2 c$. On the other hand, we have the inequality

$$
\left|f^{\prime \prime}(x)\right| \leq|a||a-1||x|^{a-2}+c^{2}|x|^{a-2 c-2}+c|2 a-c-1||x|^{a-c-1}
$$

and the right-hand side is certainly bounded if $a \geq 2+2 c$.

(g) If $f^{\prime \prime}$ is continuous, then it is bounded, and hence $a \geq 2+2 c$. If $a=2+2 c$, we have

$$
f^{\prime \prime}\left(x_{n}\right)=\frac{1}{\sqrt{2}}\left[(2 c+2)(2 c+1) t_{n}^{-2}-c^{2}-c(3+3 c) t_{n}^{\frac{-c-1}{c}}\right]
$$

which tends to $-\frac{c^{2}}{\sqrt{2}}$, so that $f^{\prime \prime}$ is not continuous at 0 . On the other hand, if $a>2+2 c$, the inequality

$$
\left|f^{\prime \prime}(x)\right| \leq|a||a-1||x|^{a-2}+c^{2}|x|^{a-2 c-2}+c|2 a-c-1||x|^{a-c-1}
$$

shows that $f^{\prime \prime}(x) \rightarrow 0$ as $x \rightarrow 0$, and hence $f^{\prime \prime}$ is continuous.

Exercise 5.14 Let $f$ be a differentiable real function defined in $(a, b)$. Prove that $f$ is convex if and only if $f^{\prime}$ is monotonically increasing. Assume next that $f^{\prime \prime}(x)$ exists for every $x \in(a, b)$, and prove that $f$ is convex if and only if $f^{\prime \prime}(x) \geq 0$ for all $x \in(a, b)$.

Suppose first that $f^{\prime}$ is montonically increasing, and that $x<y$. We wish to show that if $0<\lambda<1$, then

$$
f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)
$$

Letting $z=\lambda x+(1-\lambda) y$, we have $\lambda=\frac{y-z}{y-x}, 1-\lambda=\frac{z-x}{y-x}$, and $x<z<y$. Now the required inequality can be written

$$
(1-\lambda)[f(y)-f(z)] \geq \lambda[f(z)-f(x)]
$$

which, when we insert the values of $\lambda$ and $1-\lambda$, and multiply by the positive number $\frac{y-x}{(z-x)(y-z)}$, becomes

$$
\frac{f(y)-f(z)}{y-z} \geq \frac{f(z)-f(x)}{z-x}
$$

Since the left-hand side is $f^{\prime}(d)$ for some $d \in(z, y)$, the right-hand side is $f^{\prime}(c)$ for some $c \in(x, z)$, and $f^{\prime}$ is nondecreasing, we have the required inequality.

By Exercise 23 of Chapter 4 we know that if $f$ is convex on $(a, b)$ and $a<c<d<p<q<b$, then

$$
\frac{f(d)-f(c)}{d-c} \leq \frac{f(p)-f(d)}{p-d} \leq \frac{f(q)-f(p)}{q-p}
$$

Hence, if $f^{\prime}$ exists, letting $d \rightarrow c$ and $q \rightarrow p$, we find

$$
f^{\prime}(c) \leq f^{\prime}(p)
$$

so that $f^{\prime}$ is nondecreasing.

Finally if $f^{\prime \prime}$ exists, we know that $f^{\prime}$ is nondecreasing if and only if $f^{\prime \prime}(x) \geq 0$ for all $x \in(a, b)$. Hence $f$ is convex if and only if $f^{\prime \prime}(x) \geq 0$ for all $x \in(a, b)$.

Exercise 5.15 Suppose $a \in R^{1}, f$ is a twice-differentiable real function on $(a, \infty)$, and $M_{0}, M_{1}, M_{2}$ are the least upper bounds of $|f(x)|,\left|f^{\prime}(x)\right|,\left|f^{\prime \prime}(x)\right|$, respectively, on $(a, \infty)$. Prove that

$$
M_{1}^{2} \leq 4 M_{0} M_{2}
$$

Hint: If $h>0$, Taylor's theorem shows that

$$
f^{\prime}(x)=\frac{1}{2 h}[f(x+2 h)-f(x)]-h f^{\prime \prime}(\xi)
$$

for some $\xi \in(x, x+2 h)$. Hence

$$
|f(x)| \leq h M_{2}+\frac{M_{0}}{h}
$$

To show that $M_{1}^{2}=4 M_{0} M_{2}$ can actually happen, take $a=-1$, define

$$
f(x)= \begin{cases}2 x^{2}-1, & (-1<x<0) \\ \frac{x^{2}-1}{x^{2}+1}, & (0 \leq x<\infty)\end{cases}
$$

and show that $M_{0}=1, M_{1}=4, M_{2}=4$.

Does $M_{1}^{2} \leq 4 M_{1} M_{2}$ hold for vector-valued functions too?

Solution. The inequality is obvious if $M_{0}=+\infty$ or $M_{2}=+\infty$, so we shall assume that $M_{0}$ and $M_{2}$ are both finite. We need to show that

$$
\left|f^{\prime}(x)\right| \leq 2 \sqrt{M_{0} M_{2}}
$$

for all $x>a$. We note that this is obvious if $M_{2}=0$, since in that case $f^{\prime}(x)$ is constant, $f(x)$ is a linear function, and the only bounded linear function is a constant, whose derivative is zero. Hence we shall assume from now on that $0<M_{2}<+\infty$ and $0<M_{0}<+\infty$.

Following the hint, we need only choose $h=\sqrt{\frac{M_{0}}{M_{2}}}$, and we obtain

$$
\left|f^{\prime}(x)\right| \leq 2 \sqrt{M_{0} M_{2}}
$$

which is precisely the desired inequality.

The case of equality follows, since the example proposed satisfies

$$
f(x)=1-\frac{2}{x^{2}+1}
$$

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-427.jpg?height=137&width=1451&top_left_y=2398&top_left_x=321)
for $x>0$ and $f^{\prime}(x)=4 x$ for $x<0$. It thus follows from Exercise 9 above that $f^{\prime}(0)=0$, and that $f^{\prime}(x)$ is continuous. Likewise $f^{\prime \prime}(x)=4$ for $x<0$
and $f^{\prime \prime}(x)=\frac{4-4 x^{2}}{\left(x^{2}+1\right)^{3}}=-4 \frac{x^{2}-1}{\left(x^{2}+1\right)^{3}}$. This shows that $\left|f^{\prime \prime}(x)\right|<4$ for $x>0$ and also that $\lim _{x \rightarrow 0} f^{\prime \prime}(x)=4$. Hence Exercise 9 again implies that $f^{\prime \prime}(x)$ is continuous and $f^{\prime \prime}(0)=4$.

On $n$-dimensional space let $\mathbf{f}(x)=\left(f_{1}(x), \ldots, f_{n}(x)\right), M_{0}=\sup |\mathbf{f}(x)|$, $M_{1}=\sup \left|\mathbf{f}^{\prime}(x)\right|$, and $M_{2}=\sup \left|\mathbf{f}^{\prime \prime}(x)\right|$. Just as in the numerical case, there is nothing to prove if $M_{2}=0$ or $M_{0}=+\infty$ or $M_{2}=+\infty$, and so we assume $0<M_{0}<+\infty$ and $0<M_{2}<\infty$. Let $a$ be any positive number less than $M_{1}$, let $x_{0}$ be such that $\left|\mathbf{f}^{\prime}\left(x_{0}\right)\right|>a$, and let $\mathbf{u}=\frac{1}{\left|\mathbf{f}^{\prime}\left(x_{0}\right)\right|} \mathbf{f}^{\prime}\left(x_{0}\right)$. Consider the real-valued function $\varphi(x)=\mathbf{u} \cdot \mathbf{f}(x)$. Let $N_{0}, N_{1}$, and $N_{2}$ be the suprema of $|\varphi(x)|,\left|\varphi^{\prime}(x)\right|$, and $\left|\varphi^{\prime \prime}(x)\right|$ respectively. By the Schwarz inequality we have (since $|\mathbf{u}|=1) N_{0} \leq M_{0}$ and $N_{2} \leq M_{2}$, while $N_{1} \geq \varphi\left(x_{0}\right)=\left|\mathbf{f}^{\prime}\left(x_{0}\right)\right|>a$. We therefore have $a^{2}<4 N_{0} N_{2} \leq 4 M_{0} M_{2}$. Since $a$ was any positive number less than $M_{1}$, we have $M_{1}^{2} \leq 4 M_{0} M_{2}$, i.e., the result holds also for vector-valued functions.

Equality can hold on any $R^{n}$, as we see by taking $\mathrm{f}(x)=(f(x), 0, \ldots, 0)$ or $\mathrm{f}(x)=(f(x), f(x), \ldots, f(x))$, where $f(x)$ is a real-valued function for which
equality holds.

Exercise 5.16 Suppose $f$ is twice-differentiable on $(0, \infty), f^{\prime \prime}$ is bounded on $(0, \infty)$, and $f(x) \rightarrow 0$ as $x \rightarrow \infty$. Prove that $f^{\prime}(x) \rightarrow 0$ as $x \rightarrow \infty$.

Solution. We shall prove an even stronger statement. If $f(x) \rightarrow L$ as $x \rightarrow \infty$ and $f^{\prime}(x)$ is uniformly continuous on $(0, \infty)$, then $f^{\prime}(x) \rightarrow 0$ as $x \rightarrow \infty$.

For, if not, let $x_{n} \rightarrow \infty$ be a sequence such that $f\left(x_{n}\right) \geq \varepsilon>0$ for all $n$. (We can assume $f\left(x_{n}\right)$ is positive by replacing $f$ with $-f$ if necessary.) Let $\delta$ be such that $\left|f^{\prime}(x)-f^{\prime}(y)\right|<\frac{\varepsilon}{2}$ if $|x-y|<\delta$. We then have $f^{\prime}(y)>\frac{\varepsilon}{2}$ if
$\left|y-x_{n}\right|<\delta$, and so

$$
\left|f\left(x_{n}+\delta\right)-f\left(x_{n}-\delta\right)\right| \geq 2 \delta \cdot \frac{\varepsilon}{2}=\delta \varepsilon
$$

But, since $\delta \varepsilon>0$, there exists $X$ such that

$$
|f(x)-L|<\frac{1}{2} \delta \varepsilon
$$

for all $x>X$. Hence for all large $n$ we have

$$
\left|f\left(x_{n}+\delta\right)-f\left(x_{n}-\delta\right)\right| \leq\left|f\left(x_{n}+\delta\right)-L\right|+\left|L-f\left(x_{n}-\delta\right)\right|<\delta \varepsilon
$$

and we have reached a contradiction.

The problem follows from this result, since if $f^{\prime \prime}$ is bounded, say $\left|f^{\prime \prime}(x)\right| \leq M$, then $\left|f^{\prime}(x)-f^{\prime}(y)\right| \leq M|x-y|$, and $f^{\prime}$ is certainly uniformly continuous.
Exercise 5.17 Suppose $f$ is a real, three times differentiable function on $[-1,1]$,
such that

$$
f(-1)=0, \quad f(0)=0, \quad f(1)=1, \quad f^{\prime}(0)=0
$$

Prove that $f^{(3)}(x) \geq 3$ for some $x \in(-1,1)$.

Note that equality holds for $\frac{1}{2}\left(x^{3}+x^{2}\right)$.

Hint: Use Theorem 5.15 with $\alpha=1$ and $\beta= \pm 1$, to to show that there are $s \in(0,1)$ and $t \in(-1,0)$ such that

$$
f^{(3)}(s)+f^{(3)}(t)=6
$$

Solution. Following the hint, we observe that Theorem 5.15 (Taylor's formula with remainder) implies that

$$
\begin{aligned}
f(1) & =f(0)+f^{\prime}(0)+\frac{1}{2} f^{\prime \prime}(0)+\frac{1}{6} f^{(3)}(s) \\
f(-1) & =f(0)-f^{\prime}(0)+\frac{1}{2} f^{\prime \prime}(0)-\frac{1}{6} f^{(3)}(t)
\end{aligned}
$$

for some $s \in(0,1), t \in(-1,0)$. By subtracting the second equation from the first and using the given values of $f(1), f(-1)$, and $f^{\prime}(0)$, we obtain

$$
1=\frac{1}{6}\left(f^{(3)}(s)+f^{(3)}(t)\right)
$$

which is the desired result. Note that we made no use of the hypothesis $f(0)=0$.

Exercise 5.18 Suppose $f$ is a real function on $[a, b], n$ is a positive integer, and $f^{(n-1)}$ exists for every $t \in[a, b]$. Let $\alpha, \beta$, and $P$ be as in Taylor's theorem
(5.15). Define

$$
Q(t)=\frac{f(t)-f(\beta)}{t-\beta}
$$

for $t \in[a, b], t \neq \beta$, differentiate

$$
f(t)-f(\beta)=(t-\beta) Q(t)
$$

$n-1$ times at $t=\alpha$, and derive the following version of Taylor's theorem:

$$
f(\beta)=P(\beta)+\frac{Q^{n-1}(\alpha)}{(n-1) !}(\beta-\alpha)^{n}
$$

Solution. The function $Q(t)$ is differentiable $n-1$ times except possibly at $t=\beta$, so we don't have to worry when differentiating $n-1$ times at $t=\alpha$. It is easy to prove by induction that

$$
f^{(k)}(t)=(t-\beta) Q^{(k)}(t)+k Q^{(k-1)}(t)
$$

for $0<k \leq n-1$. Hence

$$
\frac{1}{k !} f^{(k)}(\alpha)(\beta-\alpha)^{k}=-\frac{(\beta-\alpha)^{k+1}}{k !} Q^{(k)}(\alpha)+\frac{(\beta-\alpha)^{k}}{(k-1) !} Q^{(k-1)}(\alpha)
$$

Then, because the sum telescopes, we find

$$
\sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k !}(\beta-\alpha)^{k}=f(\beta)-\frac{Q^{n-1}(\alpha)}{(n-1) !}(\beta-\alpha)^{n}
$$

which can be rewritten as

$$
f(\beta)=P(\beta)+\frac{Q^{(n-1)}(\alpha)}{(n-1) !}(\beta-\alpha)^{n}
$$

Exercise 5.19 Suppose $f$ is defined in $(-1,1)$ and $f^{\prime}(0)$ exists. Suppose $-1<$ $\alpha_{n}<\beta_{n}<1, \alpha_{n} \rightarrow 0$, and $\beta_{n} \rightarrow 0$ as $n \rightarrow \infty$. Define the difference quotients

$$
D n=\frac{f\left(\beta_{n}\right)-f\left(\alpha_{n}\right)}{\beta_{n}-\alpha_{n}}
$$

Prove the following statements:

(a) If $\alpha_{n}<0<\beta_{n}$, then $\lim D_{n}=f^{\prime}(0)$.

(b) If $0<\alpha_{n}<\beta_{n}$ and $\beta_{n} /\left(\beta_{n}-\alpha_{n}\right)$ is bounded, then $\lim D_{n}=f^{\prime}(0)$.

(c) if $f^{\prime}$ is continuous in $(-1,1)$, then $\lim D_{n}=f^{\prime}(0)$.

Give an example in which $f$ is differentiable in $(-1,1)$ (but $f^{\prime}$ is not continuous at 0 ) and in which $\alpha_{n}, \beta_{n}$ tends to 0 in such a way that $\lim D_{n}$ exists but is different from $f^{\prime}(0)$. Solution. We assume that $\alpha_{n} \beta_{n} \neq 0$ throughout, i.e., that neither $\alpha_{n}$ nor $\beta_{n}$ is
zero.

(a) Write

$$
\begin{aligned}
D_{n} & =\frac{f\left(\beta_{n}\right)-f(0)}{\beta_{n}-\alpha_{n}}+\frac{f(0)-f\left(\alpha_{n}\right)}{\beta_{n}-\alpha_{n}} \\
& =\frac{\beta_{n}}{\beta_{n}-\alpha_{n}} \frac{f\left(\beta_{n}\right)-f(0)}{\beta_{n}}+\frac{-\alpha_{n}}{\beta_{n}-\alpha_{n}} \frac{f\left(\alpha_{n}\right)-f(0)}{\alpha_{n}} .
\end{aligned}
$$

Now let $\varepsilon>0$. Choose $\delta>0$ such that

$$
\left|\frac{f(x)-f(0)}{x}-f^{\prime}(0)\right|<\varepsilon
$$

if $0<|x|<\delta$. Then choose $N$ so that $0<\beta_{n}<\delta$ and $-\delta<\alpha_{n}<0$ for $n>N$. Then for all $n>N$ we have

$$
\begin{aligned}
\left|D_{n}-f^{\prime}(0)\right| & \leq \frac{\beta_{n}}{\beta_{n}-\alpha_{n}}\left|\frac{f\left(\beta_{n}\right)-f(0)}{\beta_{n}}-f^{\prime}(0)\right|+ \\
& \quad+\frac{-\alpha_{n}}{\beta_{n}-\alpha_{n}}\left|\frac{f\left(\alpha_{n}\right)-f(0)}{\alpha_{n}}-f^{\prime}(0)\right| \\
& <\frac{\beta_{n}}{\beta_{n}-\alpha_{n}} \varepsilon+\frac{-\alpha_{n}}{\beta_{n}-\alpha_{n}} \varepsilon \\
& =\varepsilon .
\end{aligned}
$$

(b) If $\frac{\beta_{n}}{\beta_{n}-\alpha_{n}} \leq M$ for all $n$, and $0<\alpha_{n}<\beta_{n}$, then surely $\frac{\alpha_{n}}{\beta_{n}-\alpha_{n}}<M$ for all $n$. Hence if $\varepsilon>0$ is given, choose $N$ so that

$$
\left|\frac{f(x)-f(0)}{x}-f^{\prime}(0)\right|<\frac{\varepsilon}{2 M}
$$

if $0<|x|<\delta$. Then choose $N$ so that $0<\beta_{n}<\delta$ (hence also $0<\alpha_{n}<\delta$ ) for $n>N$. Then for all $n>N$ we have

$$
\begin{aligned}
\left|D_{n}-f^{\prime}(0)\right| & \leq \frac{\beta_{n}}{\beta_{n}-\alpha_{n}}\left|\frac{f\left(\beta_{n}\right)-f(0)}{\beta_{n}}-f^{\prime}(0)\right|+ \\
& \quad+\frac{\alpha_{n}}{\beta_{n}-\alpha_{n}}\left|\frac{f\left(\alpha_{n}\right)-f(0)}{\alpha_{n}}-f^{\prime}(0)\right| \\
& <\frac{\beta_{n}}{\beta_{n}-\alpha_{n}} \frac{\varepsilon}{2 M}+\frac{\alpha_{n}}{\beta_{n}-\alpha_{n}} \frac{\varepsilon}{2 M} \\
& <\varepsilon .
\end{aligned}
$$

(c) By the mean-value theorem there exists $\gamma_{n}$ between $\alpha_{n}$ and $\beta_{n}$ such that $D_{n}=f^{\prime}\left(\gamma_{n}\right)$. Since $\gamma_{n} \rightarrow 0$ and $f^{\prime}$ is continuous, it follows that $D_{n} \rightarrow f^{\prime}(0)$.

Let $f(x)$ be any function such that $f^{\prime}(0)$ exists but $\lim _{x \rightarrow 0} f^{\prime}(x)$ does not exist.

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-431.jpg?height=79&width=1445&top_left_y=1783&top_left_x=393)
would have $\left|f^{\prime}(x)\right|>1+\left|f^{\prime}(0)\right|$ for all sufficiently small nonzero $x$, and this contradicts the intermediate-value property of derivatives. Hence there is a sequence $x_{n} \rightarrow 0, x_{n} \neq 0$, such that $\lim _{n \rightarrow \infty} f^{\prime}\left(x_{n}\right)=L \neq f^{\prime}(0)$. Let $\beta_{n}=x_{n}$, and let $y_{n}$ be such that $0<\left|y_{n}-x_{n}\right|<\frac{1}{2}\left|x_{n}\right|$ and

$$
\left|\frac{f\left(y_{n}\right)-f\left(x_{n}\right)}{y_{n}-x_{n}}-f^{\prime}\left(x_{n}\right)\right|<\frac{\left|L-f^{\prime}(0)\right|}{2 n}
$$

It is then immediate that

$$
\lim _{n \rightarrow \infty} \frac{f\left(y_{n}\right)-f\left(x_{n}\right)}{y_{n}-x_{n}}=L \neq f^{\prime}(0)
$$

A suitable example of such a function $f(x)$ is

$$
f(x)= \begin{cases}x^{2} \sin \left(\frac{1}{x}\right), & x \neq 0 \\ 0, & x=0\end{cases}
$$

In this case we can get the counterexample in a slightly different form by taking. $x_{n}=\frac{1}{2 \pi n}$ and $y_{n}=\frac{1}{2 \pi\left(n+\frac{1}{4}\right)}$. We then have $f^{\prime}(0)=0$ and

$$
\frac{f\left(y_{n}\right)-f\left(x_{n}\right)}{y_{n}-x_{n}}=\frac{2 n}{\pi\left(n+\frac{1}{4}\right)} \rightarrow \frac{2}{\pi}
$$

Exercise 5.20 Formulate and prove an inequality which follows from Taylor's theorem and which remains valid for vector-valued functions.

Solution. There is a variety of possibilities, of which we choose just one: Suppose $f(x)$ has continuous derivatives up to order $n$ on $[a, b]$. Then there exists $c \in$ $(a, b)$ such that

$$
|f(b)-P(b)| \leq\left|\frac{f^{n}(c)}{n !}\right|(b-a)^{n}
$$

To prove this assertion true for a vector-valued function $f$, we merely observe that it holds for each scalar-valued function $\mathbf{u} \cdot \mathrm{f}$ if $\mathbf{u}$ is any fixed vector of length 1. It is obviously true if $|\mathbf{f}(b)-\mathbf{P}(b)|=0$, and in all other cases it follows by taking $\mathbf{u}=\frac{1}{|\mathbf{f}(b)-\mathbf{P}(b)|}(\mathbf{f}(b)-\mathbf{P}(b))$.

Exercise 5.21 Let $E$ be a closed subset of $R^{1}$. We saw in Exercise 22, Chap. 4 , that there is a real continuous function $f$ on $R^{1}$ whose zero set is $E$. Is it possible, for each closed set $E$, to find such an $f$ which is differentiable on $R^{1}$, or one which is $n$ times differentiable, or even one which has derivatives of all orders on $R^{1}$ ?

Solution. Yes, it is possible. The proof depends on the following lemma:

Let $a$ and $b$ be any real numbers with $a<b$, and let $f(x)$ be defined for all real numbers $x$ by the formulas

$$
f(x)= \begin{cases}e^{\frac{1}{(x-a)(x-b)}}, & a<x<b \\ 0, & x \leq a \text { or } x \geq b .\end{cases}
$$

Then $f$ has derivatives of all orders on $R^{1}$.

It is obvious that $f$ has derivatives of all orders at every point except possibly $a$ and $b$. To prove that derivatives exist at these points we need two sublemmas: For each nonnegative integer $n$ there exists a polynomial $p_{n}(z, w)$ such that

$$
f^{(n)}(x)=p_{n}\left(\frac{1}{x-a}, \frac{1}{x-b}\right) e^{\frac{1}{(x-a)(x-b)}}
$$

for $a<x<b$.

The proof of this sublemma uses only the partial-fraction decomposition

$$
\frac{1}{(x-a)(x-b)}=\frac{1}{b-a}\left[\frac{1}{x-b}-\frac{1}{x-a}\right]
$$

together with the chain rule and the fact that the partial derivative of a polynomial is again a polynomial. We omit the details.

The second sublemma is stated as a formula: For every nonnegative integer $n$,

$$
\lim _{x \downarrow a} \frac{e^{\frac{1}{(x-a)(x-b)}}}{(x-a)^{n}}=0
$$

Its proof is a consequence of Taylor's formula. To be specific, Taylor's formula with remainder implies the following result:

For each nonnegative integer $k$ and each positive number $t$

$$
e^{t}>\frac{1}{k !} t^{k}
$$

This last result follows easily since there is a point $t_{k} \in(0, t)$ for which

$$
e^{t}=1+t+\frac{t^{2}}{2 !}+\cdots+\frac{t^{k-1}}{(k-1) !}+\frac{e^{t_{k}}}{k !} t^{k}
$$

every term in this last sum is positive, and $e^{t_{k}}>1$.

We now apply this result with $k=n$ and $t=\frac{1}{(x-a)(b-x)}$, to obtain

$$
\begin{aligned}
e^{\frac{1}{(x-a)(x-b)}} & =\frac{1}{e^{\frac{1}{(x-a)(b-x)}}} \\
& <n !(b-x)^{n}(x-a)^{n}
\end{aligned}
$$

for all $n=0,1, \ldots$ In particular

$$
e^{\frac{1}{(x-a)(x-b)}}<n !(b-a)^{n}(x-a)^{n}=K_{n}(x-a)^{n}
$$

Since the $k$ th derivative of $e^{\frac{1}{(x-a)(x-b)}}$ is a polynomial in $\frac{1}{x-a}$ and $\frac{1}{x-b}$, each derivative also satisfies such an estimate. It follows from this last result that

$$
\lim _{x \downarrow a} p\left(\frac{1}{z-b}, \frac{1}{z-a}\right) e^{\frac{1}{(x-a)(x-b)}}=0
$$

for any polynomial $p(z, w)$, and hence that $f^{(n)}(a)=0$ for all $n$. The proof that $f^{(n)}(b)=0$ is similar. We observe that the zero set of $f(x)$ is the complement of the open interval $(a, b)$.

Identical reasoning shows that the function

$$
f(x)= \begin{cases}e^{\frac{1}{a-x}}, & x>a \\ 0, & x \leq a\end{cases}
$$

has derivatives of all orders, and its zero set is the complement of the semiinfinite open interval $(a,+\infty)$. A similar function can be constructed for a semi-infinite open interval $(-\infty, b)$.

Now let $F$ be any non-empty closed set. The complement of $F$ consists of a countable set of pairwise disjoint finite open intervals $\left(a_{k}, b_{k}\right)$, together with possibly one or two semi-infinite open intervals. Define $f(x)$ to be zero on $F$, let $f(x)=e^{\frac{1}{\left(x-a_{k}\right)\left(x-b_{k}\right)}}$ in each finite open interval complementary to $F$ with endpoints in $F, f(x)=e^{\frac{1}{a-x}}$ for $x>a$ if the complement of $F$ contains a semi-infinite interval $(a,+\infty)$ with endpoint $a \in F$, and $f(x)=e^{\frac{1}{x-b}}$ if the complement of $F$ contains a semi-infinite interval $(-\infty, b)$ with endpoint $b \in F$.

It is now obvious that $f$ is zero precisely on $F$, and that $F$ has derivatives of all orders at each point of the complement of $F$ and at each interior point of $F$.

It remains to be shown that $f$ has derivatives of all orders at each boundary point $x$ of $F$. There are actually 4 cases to consider, but all are handled alike, and we shall settle for just one typical case, in which there is a decreasing sequence of points $x_{p} \in F, x_{p} \rightarrow x$, and a decreasing sequence of points $y_{p} \notin F$, $y_{p} \rightarrow x$, but no increasing sequence of points $z_{p} \in F, z_{p} \rightarrow x$. This means either $x=b_{k}$ for some $k$ or $x=b$. Now for each $y$ such that $x<y<x_{1}$ there is a complementary interval to $F$, say $\left(a_{l}, b_{l}\right) \subset\left(x, x_{1}\right)$, with $a_{k}<y<b_{k}$. Then for all nonnegative integers $k$ and $n$ we have

$$
0<f^{(k)}(y)<K_{n, k}\left(y-a_{l}\right)^{n}<K_{n, k}(y-x)^{n}
$$

where $K_{n, k}$ is a positive constant independent of $l$, hence independent of $y$. It therefore follows, upon taking $n=2$ that if $x_{1}>y>x$, then

$$
\left|\frac{f(y)-f(x)}{(y-x)}\right| \leq K_{2,0}(y-x)
$$

(We have just proved this inequality for $y \notin F$, and $f(y)=f(x)=0$ if $y \in F$.)
Hence the right-handed derivative

$$
f_{+}^{\prime}(x)=\lim _{y \downarrow x} \frac{f(y)-f(x)}{y-x}
$$

is zero. That the left-hand derivative is zero follows from the fact that $x=b_{k}$ or $x=b$. Hence $f^{\prime}(x)=0$. We now assume by induction that $f^{(k-1)}(x)=0$ Then the inequality $f^{(k-1)}(y) \leq K_{2, k-1}(y-x)^{2}$ shows that

$$
f_{+}^{(k)}(x)=\lim _{y \downarrow x} \frac{f^{(k-1)}(y)}{y-x}=0
$$

Again, the left-hand $k$ th derivative is zero since $x=b_{k}$ or $x=b$. It follows easily that $f^{(k)}(x)$ exists and equals zero for all $k$.

Exercise 5.22 Suppose $f$ is a real function on $(-\infty, \infty)$. Call $x$ a fixed point
of $f$ if $f(x)=x$. (a) If $f$ is differentiable and $f^{\prime}(t) \neq 1$ for every real $t$, prove that $f$ has at most
one fixed point.

(b) Show that the function $f$ defined by

$$
f(t)=t+\left(1+e^{t}\right)^{-1}
$$

has no fixed point, although $0<f^{\prime}(t)<1$ for all real $t$.

(c) However, if there is a constant $A<1$ such that $\left|f^{\prime}(t)\right| \leq A$ for all real $t$, prove that a fixed point $x$ of $f$ exists, and that $x=\lim x_{n}$, where $x_{1}$ is an arbitrary real number and

for $n=1,2,3, \ldots$.

$$
x_{n+1}=f\left(x_{n}\right)
$$

(d) Show that the process described in (c) can be visualized by the zig-zag path

$$
\left(x_{1}, x_{2}\right) \rightarrow\left(x_{2}, x_{2}\right) \rightarrow\left(x_{2}, x_{3}\right) \rightarrow\left(x_{3}, x_{3}\right) \rightarrow\left(x_{3}, x_{4}\right) \rightarrow \cdots
$$

Solution. (a) If a function $f(x)$ has two fixed points $x$ and $y, x \neq y$, the mean-value theorem implies that there exists a point $z$ between $x$ and $y$ such

so that $f^{\prime}(z)=1$.

$$
y-x=f(y)-f(x)=f^{\prime}(z)(y-x)
$$

(b) The equation $f(t)=t$ implies that $\left(1+e^{t}\right)^{-1}=0$, which is clearly impossible, while $f^{\prime}(t)=1-\frac{e^{t}}{\left(1+e^{t}\right)^{2}}$ always lies in $(0,1)$.

(c) Since $f^{\prime}$ is bounded, $f$ is uniformly continuous, and we observe that the sequence $\left\{x_{n}\right\}_{n=1}^{\infty}$ is a Cauchy sequence. Indeed, if $n>m>N$, we have

$$
\left|x_{n}-x_{m}\right| \leq\left|x_{n}-x_{n-1}\right|+\left|x_{n-1}-x_{n-2}\right|+\cdots+\left|x_{m+1}-x_{m}\right|
$$

Now it is easy to show by induction, using the mean-value theorem and the fact
that $\left|f^{\prime}(x)\right| \leq A$ for all $x$, that

$$
\left|x_{n+1}-x_{n}\right| \leq A^{n-1}\left|x_{2}-x_{1}\right|
$$

for $n \geq 1$. We therefore have

$$
\begin{aligned}
\left|x_{n}-x_{m}\right| & \leq\left|x_{2}-x_{1}\right|\left(A^{n-2}+A^{n-3}+\cdots+A^{m-1}\right) \\
& <\frac{1}{1-A} A^{m-1}\left|x_{2}-x_{1}\right| \\
& \leq \frac{\left|x_{2}-x_{1}\right|}{1-A} A^{N}
\end{aligned}
$$

Since $0 \leq A<1$, it follows that $A^{N} \rightarrow 0$ as $N \rightarrow \infty$, and so this is a Cauchy sequence. Let its limit be $x$. We claim that $x$ is a fixed point. Indeed, $x=$

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-436.jpg?height=737&width=1111&top_left_y=309&top_left_x=496)

Figure 5.1: Finding a fixed point

$\lim _{n \rightarrow \infty} x_{n}=\lim _{n \rightarrow \infty} x_{n+1}=\lim _{n \rightarrow \infty} f\left(x_{n}\right)=f\left(\lim _{n \rightarrow \infty} x_{n}\right)=f(x)$, since $f$ is continuous. There can of course be only one fixed point because of the result proved in $(a)$. (d) The procedure described can be depicted on the graph of the function $f$, i.e., the set of points $(x, f(x))$, as follows: Let $x_{1}$ be any abscissa; locate the point $\left(x_{1}, f\left(x_{1}\right)\right)$ on the graph. Thereafter, for each point $\left(x_{n}, y_{n}\right)$ located on the graph, let the abscissa of $\left(x_{n+1}, y_{n+1}\right)$ be the ordinate of $\left(x_{n}, y_{n}\right)$, i.e., $x_{n+1}=y_{n}$. Thus, from a point $\left(x_{n}, y_{n}\right)$ on the graph of $f$ we move horizontally to the line $y=x$, then vertically back to the graph of $f$. It is clear visually that this process leads to the point of intersection of the graph of $f$ with the line $y=x$, as illustrated in Fig. 1 for the case of $f(x)=2-\frac{1}{2} x$, where $P_{1}=(2,1), P_{2}=(1,1)$, $P_{3}=\left(1, \frac{3}{2}\right), P_{4}=\left(\frac{3}{2}, \frac{3}{2}\right), P_{5}=\left(\frac{3}{2}, \frac{5}{4}\right)$, and $P_{6}=\left(\frac{5}{4}, \frac{5}{4}\right)$. (The fixed point is $\left(\frac{4}{3}, \frac{4}{3}\right)$, which is the point of intersection of the graph of $f$ and the line $y=x$.)

Exercise 5.23 The function $f$ defined by

$$
f(x)=\frac{x^{3}+1}{3}
$$

has three fixed points, say $\alpha, \beta, \gamma$, where

$$
-2<\alpha<-1, \quad 0<\beta<1, \quad 1<\gamma<2
$$

For arbitrarily chosen $x_{1}$, define $\left\{x_{n}\right\}$ by setting $x_{n+1}=f\left(x_{n}\right)$.

(a) If $x_{1}<\alpha$, prove that $x_{n} \rightarrow-\infty$ as $n \rightarrow \infty$.

(b) If $\alpha<x_{1}<\gamma$, prove that $x_{n} \rightarrow \beta$ as $n \rightarrow \infty$.

(c) If $\gamma<x_{1}$, prove that $x_{n} \rightarrow+\infty$ as $n \rightarrow \infty$.

Thus $\beta$ can be located by this method, but $\alpha$ and $\gamma$ cannot.

Solution. We shall make use of the auxiliary functions

$$
g(x)=f(x)-x=\frac{x^{3}+1}{3}-x
$$

and

$$
h(x)= \begin{cases}\frac{g(x)-g(\beta)}{x-\beta}, & x \neq \beta \\ g^{\prime}(\beta) & x=\beta\end{cases}
$$

i.e., $g(x)=\frac{x^{2}+\beta x+\beta^{2}}{3}-1$. We observe that the fixed points of $f$ are the zeros of $g$. Since $g(-2)=-\frac{1}{3}<0, g(-1)=1>0, g(0)=\frac{1}{3}>0, g(1)=-\frac{1}{3}<0$, and $g(2)=1>0$, the intermediate value theorem shows that $\alpha, \beta$, and $\gamma$ are located in the intervals they are asserted to be in.

Since $g(\alpha)=g(\beta)=g(\gamma)=0$, it follows that $h(\alpha)=h(\gamma)=0$. Since $h$ is a quadratic function, it has only the two zeros $\alpha$ and $\gamma$, and in particular $h(x)$ is negative for $\alpha<x<\gamma$. Now the minimum value of $h(x)$ is attained at $x=-\frac{\beta}{2}$, and this minimum value is $c$, where $c=\frac{\beta^{2}}{4}-1$. Thus $-1<c<0$. In particular, for $\alpha<x<\gamma$ there is a number $r \in(0,1)$ such that

$$
f(x)-x=r(\beta-x)
$$

i.e.,

$$
f(x)-\beta=s(x-\beta)
$$

where $s=1-r$ is also in the interval $(0,1)$. This means that $f(x)-\beta$ and $x-\beta$ both have the same sign, but that $|f(x)-\beta|<|x-\beta|$. Thus $f(x)$ is always between $\beta$ and $x$. Therefore the sequence $\left\{x_{n}\right\}$ is monotonic and converges to a fixed point in the interval whose endpoints are $x_{1}$ and $\beta$. Since the only fixed point in this interval is $\beta$, the sequence must converge to $\beta$.

If $x<\alpha$ (resp. $x>\gamma$ ), it is easy to see that $f(x)<x$ (resp. $f(x)>x$ ). Thus the sequence $\left\{x_{n}\right\}$ is monotonically decreasing (resp. increasing), and hence either tends to $-\infty$ (resp. $+\infty$ ) or converges to a fixed point $\delta$ in the interval $\left(-\infty, x_{1}\right)$ (resp. $\left(x_{1},+\infty\right)$ ). Since there are no fixed points in this interval, it follows that $x_{n} \rightarrow-\infty$ (resp. $x_{n} \rightarrow+\infty$ ).

Exercise 5.24 The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$.

Fix some $\alpha>1$, and put

$$
f(x)=\frac{1}{2}\left(x+\frac{\alpha}{x}\right), \quad g(x)=\frac{\alpha+x}{1+x} .
$$

Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$. Try to explain, on the basis of properties of $f$ and $g$, why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^{\prime}$ and $g^{\prime}$, draw the zig-zag suggested in Exercise 22.)

Do the same when $0<\alpha<1$.

Solution. We recall that in Chap. 3 we proved that the first function leads to $\left|x_{n}-\sqrt{\alpha}\right| \leq A r^{2^{n}}$ for some $r \in(0,1)$, while the second leads only to $\left|x_{n}-\sqrt{\alpha}\right| \leq$ $A r^{n}$. The exact values of $A$ and $r$ depend on $\alpha$ and $x_{1}$.

The best explanation of the difference between the two methods is that

$$
\begin{aligned}
& f(x)-\sqrt{\alpha}=\frac{1}{2}\left(1-\frac{\sqrt{\alpha}}{x}\right)(x-\sqrt{\alpha}), \\
& g(x)-\sqrt{\alpha}=\frac{1-\sqrt{\alpha}}{1+x}(x-\sqrt{\alpha}) .
\end{aligned}
$$

The first of these makes it plain that if $x>\sqrt{\alpha}$, the same will be true of $f(x)$, though $f(x)$ will be closer to $\alpha$ than $x$ by a factor that is at most $\frac{1}{2}$ and tends to zero as $x$ tends to $\sqrt{\alpha}$, i.e., the relative improvement in accuracy itself improves as the recursion proceeds. The second equality shows that $g(x)-\alpha$ is on the opposite side of $\sqrt{\alpha}$ from $x$ if $\alpha>1$, though closer by a factor that is at least the absolute value of $\frac{1-\sqrt{\alpha}}{1+x_{1}}$. Hence the relative improvement in accuracy as the recursion proceeds is limited.

In terms of the zigzag pattern, when we use $g$, the zigzag keeps circulating around the point of intersection of the graph of $g$ and the line $y=x$ instead of moving steadily toward it in a staircase pattern.

When $0<\alpha<1$, the zigzag does stay on one side of the point of intersection of the two curves. However, the relative improvement is still at best a factor of $\frac{1-\sqrt{\alpha}}{2}$ when $x$ is close to $\sqrt{\alpha}$.

Exercise 5.25 Suppose $f$ is twice differentiable in $[a, b], f(a)<0, f(b)>0$, $f^{\prime}(x) \geq \delta>0$, and $0 \leq f^{\prime}(x) \leq M$ for all $x \in[a, b]$. Let $\xi$ be the unique point
in $(a, b)$ at which $f(\xi)=0$.

Complete the details in the following outline of Newton's method for com(a) Choose $x_{1} \in(\xi, b)$, and define $x_{n}$ by

$$
x_{n+1}=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)}
$$

Interpret this geometrically, in terms of a tangent to the graph of $f$.

(b) Prove that $x_{n+1}<x_{n}$, and that

$$
\lim _{n \rightarrow \infty} x_{n}=\xi
$$

(c) Use Taylor's theorem to show that

$$
x_{n+1}-\xi=\frac{f^{\prime \prime}\left(t_{n}\right)}{2 f^{\prime}\left(x_{n}\right)}\left(x_{n}-\xi\right)^{2}
$$

for some $t_{n} \in\left(\xi, x_{n}\right)$.

(d) If $A=M / 2 \delta$, deduce that

$$
0 \leq x_{n+1}-\xi \leq \frac{1}{A}\left[A\left(x_{1}-\xi\right)\right]^{2^{n}}
$$

(Compare with Exercises 16 and 18, Chap. 3.)

(e) Show that Newton's method amounts to finding a fixed point of the function $g$ defined by

$$
g(x)=x-\frac{f(x)}{f^{\prime}(x)}
$$

How does $g^{\prime}(x)$. behave for $x$ near $\xi$ ?

(f) Put $f(x)=x^{1 / 3}$ on $(-\infty, \infty)$ and try Newton's method. What happens?

Solution. We remark at the outset that $x_{1}$ can be found by trying $z_{0}=\frac{a+b}{2}$. If $f\left(z_{0}\right)>0$, take $x_{1}=z_{0}$. Otherwise let $z_{n+1}=\left(b+z_{n}\right) / 2$, and let $x_{1}$ be the first $z_{n}$ for which $f\left(z_{n}\right)>0$. (In a finite number of steps we must reach such a point since $z_{n} \uparrow b$ and $f(b)>0$ )

(a) The tangent line to the graph of $f$ at the point $x_{n}$ has the equation $y-$ $f\left(x_{n}\right)=f^{\prime}\left(x_{n}\right)\left(x-x_{n}\right)$. Setting $y=0$ in this equation and solving for $x$ gives $x=x_{n+1}$. Thus the interpretation of Newton's method is that we approximate the point where the graph of $f$ intersects the $x$-axis by the point at which its tangent line at $\left(x_{n}, f\left(x_{n}\right)\right)$ intersects the $x$-axis.

(b) We can assume by induction that $f\left(x_{n}\right)>0$, and hence, since $f^{\prime}\left(x_{n}\right)>0$, it follows immediately that $x_{n+1}<x_{n}$. Notice that there exists $c$ between $x_{n}$ and $x_{n+1}$ such that $f\left(x_{n+1}\right)=f\left(x_{n}\right)-f^{\prime}(c)\left(x_{n}-x_{n+1}\right)>f\left(x_{n}\right)-f^{\prime}\left(x_{n}\right)\left(x_{n}-\right.$ $\left.x_{n+1}\right)=0$ since $f^{\prime}(c)<f^{\prime}\left(x_{n}\right)$ and $x_{n}-x_{n+1}>0$. Thus it follows that $\xi<x_{n+1}<x_{n}$. Hence $\left\{x_{n}\right\}$ converges to a limit $\eta$ satisfying $\eta \geq \xi$. Now, however, we have

$$
\eta=\eta-\frac{f(\eta)}{f^{\prime}(\eta)}
$$

from which it follows that $f(\eta)=0$, i.e., $\eta=\xi$.

(c) The required equality can be written as

$$
x_{n}-\xi-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)}=\frac{f^{\prime \prime}\left(t_{n}\right)}{2 f^{\prime}\left(x_{n}\right)}\left(x_{n}-\xi\right)^{2}
$$

while Taylor's theorem can be written as

$$
f(\xi)=f\left(x_{n}\right)+f^{\prime}\left(x_{n}\right)\left(\xi-x_{n}\right)+\frac{f^{\prime \prime}\left(t_{n}\right)}{2}\left(\xi-x_{n}\right)^{2}
$$

Since $f(\xi)=0$, it is clear that these two equations are equivalent.

(d) Since $0 \leq f^{\prime \prime}\left(t_{n}\right) \leq M$ and $f^{\prime}\left(x_{n}\right)>\delta$, we have

$$
0 \leq x_{n+1}-\xi \leq A\left(x_{n}-\xi\right)^{2}
$$

In particular

and then an easy induction gets the general result.

$$
0 \leq x_{2}-\xi \leq A\left(x_{1}-\xi\right)^{2}=\frac{1}{A}\left[A\left(x_{1}-\xi\right)\right]^{2}
$$

We found this kind of convergence in Exercises 16 and 18 of Chap. 3 with the recursion relation

$$
x_{n+1}=\frac{p-1}{p} x_{n}+\frac{\alpha}{p} x_{n}^{-p+1}
$$

We now recognize this recursion as Newton's method for the function $f(x)=$ $x^{p}-\alpha$ on the interval $[1, \sqrt{\alpha}+1]$. Exercise 16 of Chap. 2 was the special case

(e) Obviously the equation $g(x)=x$ is equivalent to the equation $f(x)=0$.

Since $g^{\prime}(x)=\frac{f(x) f^{\prime \prime}(x)}{\left[f^{\prime}(x)\right]^{2}}$, we see that $g^{\prime}(x)$ tends to zero as $x$ tends to $\xi$, i.e., the graph of $g(x)$ meets the line $y=x$ at a $45^{\circ}$ degree angle at the point
$(\xi, \xi)$.

$(f)$ The fixed point of $f(x)$ is $x=0$. However $f^{\prime}(x) \rightarrow \infty$ as $x \rightarrow 0$, and $f^{\prime}(0)$ does not exist. This destroys the convergence of Newton's method. In fact, if $x_{n} \neq 0$, then $x_{n+1}=-2 x_{n}$, so that $x_{n}$ oscillates wildly: $\lim \sup x_{n}=+\infty$,
$\lim \inf x_{n}=-\infty$

Exercise 5.26 Suppose $f$ is differentiable on $[a, b], f(a)=0$, and there is a real number $A$ such that $\left|f^{\prime}(x)\right| \leq A|f(x)|$ on $[a, b]$. Prove that $f(x)=0$ for all $x \in[a, b]$. Hint: Fix $x_{n} \in[a, b]$, let

for $a \leq x \leq x_{0}$. For any $\operatorname{such} x$,

$$
M_{0}=\sup |f(x)|, \quad M_{1}=\sup \left|f^{\prime}(x)\right|
$$

$$
|f(x)| \leq M_{1}\left(x_{0}-a\right) \leq A\left(x_{0}-a\right) M_{n}
$$

Hence $M_{0}=0$ if $A\left(x_{0}-a\right) \leq 1$. That is, $f=0$ on $\left[a, x_{0}\right]$. Proceed.

Solution. If we anticipate the fundamental result that the function $f(x)=e^{x}$ satisfies $f^{\prime}(x)=f(x)$, Exercise 2 above yields the result that $\ln x$ is differentiable and has derivative $\frac{1}{x}$. Hence by the chain rule for any positive differentiable function $f(x)$ the function $g(x)=\ln f(x)$ is differentiable and $g^{\prime}(x)=\frac{f^{\prime}(x)}{f(x)}$. (Unfortunately this fundamental result is not proved until Chapter 7 , so we shall
just have to wait. However, since certain other functions such as $\sin x$ and $\cos x$ have been introduced without any formal definition, and their derivatives have been assumed known, we might as well continue along this line of reasoning.)

Now suppose there is an interval $(c, d) \subset[a, b]$ such that $f(c)=0$ but $f(x) \neq 0$ for $c<x<d$. By passing to consideration of $-f(x)$ if necessary, we can assume $f(x)>0$ for $c<x<d$. The function $g(x)=\ln f(x)$ is then defined for $c<x<d$, and its derivative satisfies

$$
\left|g^{\prime}(x)\right|=\left|\frac{f^{\prime}(x)}{f(x)}\right| \leq A
$$

The mean-value theorem them implies that

$$
g(x) \geq g\left(\frac{c+d}{2}\right)-A\left(\frac{d-c}{2}\right)
$$

for all $x \in(c, d)$. But this is a contradiction, since $g(x) \rightarrow-\infty$ as $x \rightarrow c$.

This finishes the proof, except that it assumes we know the derivative of $e^{x}$. If we don't assume that, we have to fall back on the hint. In that case, let $x_{0}=a+\frac{1}{2 A}$, and let $M_{0}=\sup \left\{|f(x)|: a \leq x \leq x_{0}\right\}$. We then have

$$
|f(x)| \leq M_{1}(x-a) \leq A M_{0}\left(x_{0}-a\right)=\frac{1}{2} M_{0}
$$

for all $x \in\left[a, x_{0}\right]$. But by definition of $M_{0}$ this implies $M_{0} \leq \frac{1}{2} M_{0}$, so that $M_{0} \leq 0$, i.e., $M_{0}=0$. We now start over with $a$ replaced by $x_{0}, x_{1}=x_{0}+\frac{1}{2 A}$. In a finite number of steps, we will have $b<x_{n}+\frac{1}{2 A}$, so that $f(x)=0$ for $a \leq x \leq b$.

Exercise 5.27 Let $\phi$ be a real function defined on a rectangle $R$ in the plane, given by $a \leq x \leq b, \alpha \leq y \leq \beta$. A solution of the initial-value problem

$$
y^{\prime}=\phi(x, y), \quad y(a)=c \quad(\alpha \leq c \leq \beta)
$$

is, by definition, a differentiable function $f$ on $[a, b]$ such that $f(a)=c, \alpha \leq$ $f(x) \leq \beta$, and

$$
f^{\prime}(x)=\phi(x, f(x)) \quad(a \leq x \leq b) .
$$

Prove that such a problem has at most one such solution if there is a constant $A$ such that

$$
\left|\phi\left(x, y_{2}\right)-\phi\left(x, y_{1}\right)\right| \leq A\left|y_{2}-y_{1}\right|
$$

whenever $\left(x, y_{1}\right) \in R$ and $\left(x, y_{2}\right) \in R$.

Hint: Apply Exercise 26 to the difference of two solutions. Note that this uniqueness theorem does not hold for the initial-value problem

$$
y^{\prime}=y^{1 / 2}, \quad y(0)=0
$$

which has two solutions: $f(x)=0$ and $f(x)=x^{2} / 4$. Find all other solutions.

Solution. Following the hint, we observe that if $f(x)=f_{2}(x)-f_{1}(x)$, then

$$
\begin{aligned}
\left|g^{\prime}(x)\right| & =\left|f_{2}^{\prime}(x)-f_{1}^{\prime}(x)\right| \\
& =\left|\phi\left(x, f_{2}(x)\right)-\phi\left(x, f_{1}(x)\right)\right| \\
& \leq A\left|f_{2}(x)-f_{1}(x)\right| \\
& =A|g(x)|
\end{aligned}
$$

By the initial condition $g(a)=f_{2}(a)-f_{1}(a)=c-c=0$. Hence by the preceding exercise $g(x)=0$ for all $x \in[a, b]$.

As for the equation $y^{\prime}=\sqrt{y}$, if $f(x)$ is a solution and $f(x)>0$ on an interval $(a, b)$, while $f(a)=0$, we observe that $g(x)=\sqrt{f(x)}$ satisfies $g^{\prime}(x)=$ $\frac{1}{2}(f(x))^{-1 / 2} f^{\prime}(x)=\frac{1}{2}$, so that for some constant $c$ we have $g(x)=\frac{1}{2}(x+c)$.
Thus

$$
f(x)=(g(x))^{2}=\frac{1}{4}(x+c)^{2}
$$

Since $f(a)=0$, it follows that $c=-a$, i.e., $f(x)=\frac{(x-a)^{2}}{4}$. Thus the only
possible solutions are

$$
f(x)= \begin{cases}0, & 0 \leq x \leq a \\ \frac{(x-a)^{2}}{4}, & a \leq x\end{cases}
$$

Here $a \geq 0$ is arbitrary.

Exercise 5.28 Formulate and prove an analogous uniqueness theorem for systems of differential equations of the form

$$
y_{j}^{\prime}=\phi_{j}\left(x, y_{1}, \ldots, y_{k}\right), \quad y_{j}=c_{j} \quad(j=1, \ldots, k)
$$

Note that this can be rewritten in the form

$$
\mathbf{y}^{\prime}=\phi(x, y), \quad \mathbf{y}(a)=\mathbf{c}
$$

where $\mathbf{y}=\left(y_{1}, \ldots, y_{k}\right)$ ranges over a $k$-cell, $\phi$ is the mapping of a $(k+1)$-cell into the Euclidean $k$-space whose components are the functions $\phi_{1}, \ldots, \phi_{k}$, and $\mathbf{c}$ is the vector $\left(c_{1}, \ldots, c_{k}\right)$. Use Exercise 26 for vector-valued functions.

Solution. The result is the following:

Let $\phi$ be a vector-valued function defined on $a(k+1)$-cell $D=[a, b] \times C$ in $R^{k+1}$ whose range is contained in $R^{k}$, and suppose that there exists a constant A such that

$$
\left|\phi\left(x, \mathbf{y}_{2}\right)-\phi\left(x, \mathbf{y}_{1}\right)\right| \leq A\left|\mathbf{y}_{2}-\mathbf{y}_{1}\right|
$$

for all $\mathbf{y}_{1} \in C, \mathbf{y}_{2} \in C$. Then the initial-value problem

$$
\mathbf{y}^{\prime}=\phi(x, y) \quad \mathbf{y}(a)=\mathrm{c}
$$

has at most one solution $\mathbf{y}:[a, b] \rightarrow C$.

The main tool needed to prove this result is the analogue of Exercise 26 for vector-valued functions, which does hold. Indeed the proof is identical, considering that the original proof depends only on the inequality $|f(d)-f(c)| \leq$ $\left|f^{\prime}(r)\right|(d-c)$ for some $r \in(c, d)$, and this inequality is certainly valid for vectorvalued functions. Once that result is obtained, the preceding exercise can be applied verbatim.

Exercise 5.29 Specialize Exercise 28 by considering the system

$$
\begin{aligned}
y_{j}^{\prime} & =y_{j+1} \quad(j=1, \ldots, k-1), \\
y_{k}^{\prime} & =f(x)-\sum_{j=1}^{k} g_{j}(x) y_{j},
\end{aligned}
$$

where $f, g_{1}, \ldots, g_{k}$ are continuous real functions on $[a, b]$, and derive a uniqueness theorem for solutions of the equation

$$
y^{(k)}+g_{k}(x) y^{(k-1)}+\cdots+g_{2}(x) y^{\prime}+g_{1}(x) y=f(x)
$$

subject to initial conditions

$$
y(a)=c_{1}, \quad y^{\prime}(a)=c_{2}, \quad \ldots, \quad y^{(k-1)}(a)=c_{k} .
$$

Solution. We let $\mathbf{y}=\left(y_{1}, y_{2}, y_{3}, \ldots, y_{k}\right)=\left(y, y^{\prime}, y^{\prime \prime}, \ldots, y^{(k-1)}\right)$ and $\phi(x, \mathbf{y})=$ $\left(y_{2}, y_{3}, \ldots, y_{k}, f(x)-\sum_{j=1}^{k} g_{j}(x) y_{j}\right)$. We then observe that if $\mathbf{y}_{i}=\left(y_{i 1}, \ldots, y_{i k}\right)$, then

$$
\left|\phi\left(x, \mathbf{y}_{2}\right)-\phi\left(x, \mathbf{y}_{1}\right)\right|=\left|\left(y_{22}-y_{12}, y_{23}-y_{13}, \ldots, \sum_{j=1}^{k} g_{j}(x)\left(y_{1 j}-y_{2 j}\right)\right)\right|
$$

If $M=\sup \left\{\left|g_{j}(x)\right|: a \leq x \leq b, 1 \leq j \leq k\right\}$, we then have

$$
\left|\phi\left(x, \mathbf{y}_{2}\right)-\phi\left(x, \mathbf{y}_{1}\right)\right| \leq(M+1) \sum_{j=1}^{k}\left|y_{2 j}-y_{1 j}\right| \leq k(M+1)\left|\mathbf{y}_{2}-\mathbf{y}_{1}\right|
$$

This provides the hypothesis of the theorem for any $(k+1)$-cell $[a, b] \times C$ whatsoever in $R^{k+1}$. Hence there is at most one solution to this initial-value problem.

## Chapter 6

## The Riemann-Stieltjes Integral

Exercise 6.1 Suppose $\alpha$ increases on $[a, b], a \leq x_{0} \leq b, \alpha$ is continuous at $x_{0}$, $f\left(x_{0}\right)=1$, and $f(x)=0$ if $x \neq x_{0}$. Prove that $f \in \mathcal{R}(\alpha)$ and that $\int f d \alpha=0$.

Solution. Let $\varepsilon>0$, and let $\delta$ be such that $\left|\alpha(x)-\alpha\left(x_{0}\right)\right|<\varepsilon$ if $\left|x-x_{0}\right|<\delta$. Now consider any partiion $a=t_{0}<t_{1}<\cdots<t_{n}=b$ with $n \geq 2$ such that $\left|t_{i}-t_{i-1}\right|<\frac{\delta}{2}$. There exists an index $i$ such that $t_{i-1}<x_{0}<t_{i+1}$ (there may possibly be 2 such indices). We then have, for any choice of $t_{0}^{*}, t_{1}^{*}, \ldots, t_{n}^{*}$,

$$
\begin{aligned}
\left|\sum_{j=1}^{n} f\left(t_{j}^{*}\right)\left(\alpha\left(t_{j}\right)-\alpha\left(t_{j-1}\right)\right)\right| & \leq\left|f\left(t_{i}^{*}\right)\right|\left[\alpha\left(t_{i}\right)-\alpha\left(t_{i-1}\right) \mid+\right. \\
& +\left|f\left(t_{i+1}^{*}\right)\right|\left[\alpha\left(t_{i+1}\right)-\alpha\left(t_{i}\right)\right] \\
& \leq \alpha\left(t_{i+1}\right)-\alpha\left(t_{i-1}\right)<\varepsilon .
\end{aligned}
$$

By definition of the Riemann-Stieltjes integral, this means that $f \in \mathcal{R}(\alpha)$ and $\int f d \alpha=0$.

Exercise 6.2 Suppose $f \geq 0, f$ is continuous on $[a, b]$, and $\int_{a}^{b} f(x) d x=0$. Prove that $f(x)=0$ for all $x \in[a, b]$. (Compare this with Exercise 1.)

Solution. Suppose $f\left(x_{0}\right) \neq 0$ for some $x_{0} \in[a, b]$. Since $f(x)$ is continuous on $[a, b]$ and $\frac{f\left(x_{0}\right)}{2}>0$, there exists $\delta>0$ such that $\left|f(x)-f\left(x_{0}\right)\right|<\frac{f\left(x_{0}\right)}{2}$ for all $x \in[a, b]$ such that $\left|x-x_{0}\right|<\delta$. Let $\eta=\min \left(\delta, \max \left(x_{0}-a, b-x_{0}\right)\right)$, so that $\eta>0$. Let $I$ be the interval $\left[x_{0}-\eta, x_{0}\right]$ if it is contained in $[a, b]$; otherwise let $I=\left[x_{0}, x_{0}+\eta\right]$. Whichever is the case, $I \subseteq[a, b]$ and $f(x)=$
$f\left(x_{0}\right)+\left(f(x)-f\left(x_{0}\right)\right) \geq f\left(x_{0}\right)-\left|f(x)-f\left(x_{0}\right)\right|>\frac{f\left(x_{0}\right)}{2}$ for all $x \in I$. The functions $f_{1}(x)$ and $f_{2}(x)$ defined as

$$
f_{1}(x)=\left\{\begin{array}{ll}
f(x), & x \in I, \\
0, & x \notin I,
\end{array} \quad f_{2}(x)= \begin{cases}f(x), & x \notin I \\
0, & x \in I\end{cases}\right.
$$

are both nonnegative, bounded, and continuous except possibly at the two endpoints of the interval $I$. They are therefore both Riemann-integrable. Consideration of Riemann sums shows that

$$
\int_{a}^{b} f_{1}(x) d x \geq \eta \frac{\varepsilon}{2}
$$

and

$$
\int_{a}^{b} f_{2}(x) d x \geq 0
$$

It therefore follows that

$$
\int_{a}^{b} f(x) d x=\int_{a}^{b} f_{1}(x) d x+\int_{a}^{b} f_{2}(x) d x \geq \eta \frac{\varepsilon}{2}>0
$$

contradicting the hypothesis that $\int_{a}^{b} f(x) d x=0$.

Exercise 6.3 Define three functions $\beta_{1}, \beta_{2}, \beta_{3}$ as follows: $\beta_{j}(x)=0$ if $x<0$, $\beta_{j}(x)=1$ if $x>0$ for $j=1,2,3$ and $\beta_{1}(0)=0, \beta_{2}(0)=1, \beta_{3}(0)=\frac{1}{2}$. Let $f$ be a bounded function on $[-1,1]$.

(a) Prove that $f \in \mathcal{R}\left(\beta_{1}\right)$ if and only if $f(0-)=f(0)$ and that then

$$
\int f d \beta_{i}=f(0)
$$

(b) State and prove a similar result for $\beta_{2}$.

(c) Prove that $f \in \mathcal{R}\left(\beta_{3}\right)$ if and only if $f$ is continuous at 0 .

(d) If $f$ is continuous at 0 , prove that

$$
\int f d \beta_{1}=\int f d \beta_{2}=\int f d \beta_{3}=f(0)
$$

Solution. Let $t_{0}<t_{1}<\cdots<t_{n-1}<t_{n}$ be any partition of any interval containing 0 . Since the upper Riemann-Stieltjes sums become smaller and the lower ones larger when a point is added to any partition, in deciding whether a function is integrable or not, we may assume that 0 is one of the points of
the partition. Let $k$ be the index such that $t_{k}=0$, so that the upper and lower Riemann-Stieltjes sums

$$
\sum_{i=1}^{n} M_{i}\left(\beta_{j}\left(t_{i}\right)-\beta_{j}\left(t_{i-1}\right)\right), \quad j=1,2,3
$$

and

$$
\sum_{i=1}^{n} m_{i}\left(\beta_{j}\left(t_{i}\right)-\beta_{j}\left(t_{i-1}\right)\right), \quad j=1,2,3
$$

are respectively $M_{k}$ and $m_{k}, M_{k-1}$ and $m_{k-1}, \frac{M_{k-1}+M_{k}}{2}$ and $\frac{m_{k-1}+m_{k}}{2}$.

(a) Since $m_{k} \leq f(x) \leq M_{k}$ for $0 \leq x \leq t_{k+1}$ in the first case, the sets of upper and lower sums contain elements arbitrarily near to each other if and only if for each $\varepsilon$ there is a partition with $M_{k}-m_{k}<\varepsilon$. If such a partition exists, let $\delta=t_{k+1}$. Then we have $|f(x)-f(0)| \leq M_{k}-m_{k}<\varepsilon$ for $0 \leq x \leq \delta$, and hence $\lim _{x \rightarrow 0+}=f(0)$. Conversely, if $\lim _{x \rightarrow 0+}=f(0)$, then for any $\varepsilon$, let $\delta>0$ be such that $|f(x)-f(0)|<\delta$ if $0<x<\delta$, and let $P$ be a partition with $t_{k}=0$, $t_{k+1}<\delta$. It is then clear that both upper and lower Riemann sums differ from $f(0)$ by less than $\varepsilon$, i.e., $\int f d \beta_{1}=f(0)$.

(b) $f \in \mathcal{R}\left(\beta_{2}\right)$ if and only if $\lim _{x \rightarrow 0-} f(x)=f(0)$ and if this condition holds, then $\int f d \beta_{2}=f(0)$. The proof is identical to the proof just given, except that "+" is replaced by "-."

(c) In the third case, the upper and lower Riemann-Stieltjes sums differ by $\frac{\left(M_{k}-m_{k}\right)+\left(M_{k-1}+m_{k-1}\right.}{2}$. If, given $\varepsilon$, there exists a partition containing 0 for which this difference is less than $\frac{\varepsilon}{2}$, let $\delta=\min \left(t_{k+1},-t_{k-1}\right)$. Then for $-\delta \leq x \leq \delta$ we certainly have

$$
|f(x)-f(0)| \leq \max \left(\frac{M_{k}-m_{k}}{2}, \frac{M_{k-1}-m_{k-1}}{2}\right) \leq M_{k}-m_{k}+M_{k-1}-m_{k-1}<\varepsilon
$$

so that $f$ is continuous at 0 . The same argument shows that in this case

$$
\int f d \beta_{3}=f(0)
$$

(d) This result is contained in $(a)-(c)$.

Exercise 6.4 If $f(x)=0$ for all irrational $x, f(x)=1$ for all rational $x$, prove that $f \notin \mathcal{R}$ on $[a, b]$ for any $a<b$.

Solution. Every upper Riemann sum equals $b-a$, and every lower Riemann sum equals 0 . Hence the set of upper sums and the set of lower sums do not have a common bound.

Exercise 6.5 Suppose $f$ is a bounded real function on $[a, b]$ and $f^{2} \in \mathcal{R}$ on $[a, b]$. Does it follow that $f \in \mathcal{R}$ ? Does the answer change if we assume that $f^{3} \in \mathcal{R}$ ?

Solution. The integrability of $f^{2}$ does not imply the integrability of $f$. For example, one could let $f(x)=-1$ if $x$ is irrational and $f(x)=1$ if $x$ is rational. Then every upper Riemann sum of $f$ is $b-a$ and every lower sum is $a-b$. However, $f^{2}$, being the constant function $I$, is integrable.

The integrability of $f^{3}$ does imply the integrability of $f$, by Theorem 6.11 with $\varphi(u)=\sqrt[3]{u}$.

Exercise 6.6 Le $P$ be the Cantor set constructed in Sec. 2.44. Let $f$ be a bounded real function on $[0,1]$ which is continuous at every point outside $P$. Prove that $f \in \mathcal{R}$ on $[0,1]$. [Hint: $P$ can be covered by finitely many segments whose total length can be made as small as desired. Proceed as in Theorem 6.10.]

Solution. Let $M=\sup \{|f(x)|: a \leq x \leq b\}$, and let $\varepsilon>0$ be given. Cover $P$ by a finite collection of open intervals $O=\bigcup_{i=1}^{k}\left(a_{i}, b_{i}\right)$ such that $\sum\left(b_{i}-\right.$ $\left.a_{i}\right)<\frac{\varepsilon}{4 M}$. Let $\theta=\inf \{|x-y|: x \in P, y \in[a, b] \backslash O\}$. Since $x$ and $y$ range over disjoint compact sets, $\theta$ is a positive number. On the compact set $E=\left\{x: d(x, P) \geq \frac{1}{2} \theta\right\}$ the function $f$ is uniformly continuous. Let $\delta>0$ be such that $|f(x)-f(y)|<\frac{\varepsilon}{2(b-a)}$ if $x, y \in E$ and $|x-y|<\delta$. Then consider any partition $\left\{t_{j}\right\}$ of $[a, b]$ with $\max \left(t_{j}-t_{j-1}\right)<\min \left(\delta, \frac{1}{2} \theta\right)$. The difference between the upper and lower Riemann sums for this partition can be expressed as two sums:

$$
\sum\left(M_{j}-m_{j}\right)\left(t_{j}-t_{j-1}\right)=\Sigma_{1}+\Sigma_{2}
$$

where $\Sigma_{1}$ contains all the terms for which $\left[t_{j-1}, t_{j}\right]$ is contained in $E$ and $\Sigma_{2}$ all the other terms. It is then obvious that

$$
\Sigma_{1}<\frac{\varepsilon}{2(b-a)} \sum\left(t_{j}-t_{j-1}\right) \leq \frac{\varepsilon}{2},
$$

and, since each interval $\left[t_{j-1}, t_{j}\right]$ that occurs in $\Sigma_{2}$ is contained in $O$,

$$
\Sigma_{2}<2 M \frac{\varepsilon}{4 M}=\frac{\varepsilon}{2}
$$

Therefore the upper and lower Riemann sums for any such partition differ by less than $\varepsilon$, and so $f$ is Riemann integrable.

Exercise 6.7 Suppose $f$ is a real function on $[0,1]$ and $f \in \mathcal{R}$ on $[c, 1]$ for every $c>0$. Define

$$
\int_{0}^{1} f(x) d x=\lim _{c \rightarrow 0+} \int_{c}^{1} f(x) d x
$$

if this limit exists (and is finite).

(a) If $f \in \mathcal{R}$ on $[0, I]$ show that this definition of the integral agrees with the old one.

(b) Construct a function $f$ such that the above limit exists, although it fails to exist with $|f|$ in place of $f$.

Solution. (a) Suppose $f \in \mathcal{R}$ on $[0,1]$. Let $\varepsilon>0$ be given, and let $M=$ $\sup \{|f(x)|: 0 \leq x \leq 1\}$. Let $c \in\left(0, \frac{\varepsilon}{4 M}\right]$ be fixed, and consider any partition of $[0,1]$ containing $c$ for which the upper and lower Riemann sums $\sum M_{j}\left(t_{j}-t_{j-1}\right)$ and $\sum m_{j}\left(t_{j}-t_{j-1}\right)$ of $f$ differ by less than $\frac{\varepsilon}{4}$. Then the partition of $[c, 1]$ formed by the points of this partition that lie in this interval certainly has the property that its upper and lower Riemann sums $\sum^{\prime} M_{j}\left(t_{j}-t_{j-1}\right)$ and $\sum^{\prime} m_{j}\left(t_{j}-t_{j-1}\right)$ differ by less than $\frac{\varepsilon}{4}$. Moreover, the terms of the original upper and lower Riemann sums not found in the sums for the smaller interval amount to less than $\frac{\varepsilon}{4}$. In short, we have shown that for $c<\frac{\varepsilon}{4 M}$ and a suitable partition containing $c$,

$$
\sum M_{j}\left(t-j-t_{j-1}\right)-\frac{\varepsilon}{4}<\int_{0}^{1} f(x) d x \leq<\sum m_{j}\left(t_{j}-t_{j-1}\right)+\frac{\varepsilon}{4}
$$

and

$$
\sum^{\prime} M_{j}\left(t_{j}-t_{j-1}\right)-\frac{\varepsilon}{4}<\int_{c}^{1} f(x) d x<\sum^{\prime} m_{j}\left(t_{j}-+\frac{\varepsilon}{4}\right.
$$

Moreover, we have also shown that

$$
\left|\sum M_{j}\left(t_{j}-t_{j-1}\right)-\sum{ }^{\prime} M_{j}\left(t_{j}-t_{j-1}\right)\right|<\frac{\varepsilon}{4}
$$

and

$$
\left|\sum m_{j}\left(t_{j}-t_{j-1}\right)-\sum^{\prime} m_{j}\left(t_{j}-t_{j-1}\right)\right|<\frac{\varepsilon}{4}
$$

combining these inequalities, we find that

$$
\left|\int_{0}^{1} f(x) d x-\int_{c}^{1} f(x) d x\right|<\varepsilon
$$

if $0<c<\frac{\varepsilon}{4 M}$.u

(b) Let

$$
f(x)=(-1)^{n}(n+1)
$$

for $\frac{1}{n+1}<x \leq \frac{1}{n}, n=1,2, \ldots$. Then if $\frac{1}{N+1} \leq c \leq \frac{1}{N}$ we have

$$
\int_{c}^{1} f(x) d x=(-1)^{N}(N+1)\left(\frac{1}{N}-c\right)+\sum_{k=1}^{N-1} \frac{(-1)^{k}}{k}
$$

Since $0 \leq \frac{1}{N}-c \leq \frac{1}{N}-\frac{1}{N+1}=\frac{1}{N(N+1)}$, the first term on the right-hand side tends to zero as $c \downarrow 0$, while the sum approaches $\ln 2$. Hence this integral approaches a limit. However,

$$
\int_{c}^{1}|f(x)| d x=(N+1)\left(\frac{1}{N}-c\right)+\sum_{k=1}^{N-1} \frac{1}{k}
$$

and in this case the first term on the right-hand side tends to zero as $c \downarrow 0$, while the sum tends to infinity.

Exercise 6.8 Suppose $f \in \mathcal{R}$ on $[a, b]$ for every $b>a$, where $a$ is fixed. Define

$$
\int_{a}^{\infty} f(x) d x=\lim _{x \rightarrow \infty} \int_{a}^{b} f(x) d x
$$

if this limit exists (and is finite). In that case, we say that the integral on the left converges. If it also converges after $f$ has been replaced by $|f|$, it is said to converge absolutely. that

Assume that $f(x) \geq 0$ and that $f$ decreases monotonically on $[1, \infty)$. Prove

$$
\int_{1}^{\infty} f(x) d x
$$

converges if and only if

$$
\sum_{n=1}^{\infty} f(n)
$$

converges. (This is the so-called "integral test" for convergence of series.)

Solution. Since both the series and the integral are increasing functions of their upper limits, it suffices to show that they are bounded together. Define $f(x)=f(1)$ for $0 \leq x \leq 1$. Then consider a partition of $[0, n]$ consisting of the $n+1$ points $0,1,2, \ldots, n$. The upper Riemann sum for this partition is $\sum_{k=0}^{n-1} f(k)$ and the lower Riemann sum is $\sum_{k=1}^{n} f(k)$. Hence we have

$$
\sum_{k=1}^{n} f(k) \leq \int_{0}^{n} f(x) d x=f(0)+\int_{1}^{n} f(x) d x \leq \sum_{k=0}^{n-1} f(k)=f(0)+\sum_{k=1}^{n-1} f(k) .
$$

This shows that

$$
-f(0)+\sum_{k=1}^{n} f(k) \leq \int_{1}^{n} f(x) d x \leq \sum_{k=1}^{n-1} f(x)
$$

and hence the sum and the integral converge or diverge together.

Exercise 6.9 Show that integration by parts can sometimes be applied to the "improper" integrals defined in Exercises 7 and 8. (State appropriate hypotheses, formulate a theorem, and prove it.) For instance show that

$$
\int_{0}^{\infty} \frac{\cos x}{1+x} d x=\int_{0}^{\infty} \frac{\sin x}{(1+x)^{2}} d x
$$

Solution. Without striving for ultimate generality we can get the main ideas in the following theorem:

Theorem. Let $f(x)$ and $g(x)$ be continuously differentiable functions defined on $[a, \infty)$ such that $\lim _{b \rightarrow \infty} f(b) g(b)$ exists and the integral $\int_{a}^{\infty} f(x) g^{\prime}(x) d x$ converges.

Then $\int_{a}^{\infty} f^{\prime}(x) g(x) d x$ converges and

$$
\int_{a}^{\infty} f^{\prime}(x) g(x) d x=\lim _{b \rightarrow \infty}[f(b) g(b)-f(a) g(a)]-\int_{a}^{\infty} f(x) g^{\prime}(x) d x
$$

Proof. For each finite value of $b$ larger than $a$ the standard rule for integration by parts gives

$$
\int_{a}^{b} f^{\prime}(x) g(x) d x=[f(b) g(b)-f(a) g(a)]-\int_{a}^{b} f(x) g^{\prime}(x) d x
$$

The hypotheses of the theorem guarantee that the limit on the right exists. Therefore, by definition, the integral on the left converges.

Applying this result with $f(x)=\sin x, g(x)=\frac{1}{1+x}$, we find, since $f(0) g(0)=$ 0 and $\lim _{b \rightarrow \infty} f(b) g(b)=0$, while $\int_{0}^{\infty} f(x) g^{\prime}(x) d x$ converges absolutely, that

$$
\int_{0}^{\infty} \frac{\cos x}{1+x} d x=\int_{0}^{\infty} \frac{\sin x}{(1+x)^{2}} d x
$$

Exercise 6.10 Let $p$ and $q$ be positive real number ssuch that

$$
\frac{1}{p}+\frac{1}{q}=1
$$

Prove the following statements.

(a) If $u \geq 0$ and $v \geq 0$, then

$$
u v \leq \frac{u^{p}}{p}+\frac{v^{q}}{q}
$$

Equality holds if and only if $u^{p}=v^{q}$.
(b) if $f \in \mathcal{R}(\alpha), g \in \mathcal{R}(\alpha), f \geq 0, g \geq 0$, and

$$
\int_{a}^{b} f^{p} d \alpha=1=\int_{a}^{b} g^{q} d \alpha
$$

then

$$
\int_{a}^{b} f g d \alpha \leq 1
$$

(c) If $f$ and $g$ are complex functions in $\mathcal{R}(\alpha)$, then

$$
\left|\int_{a}^{b} f g d \alpha\right| \leq\left\{\int_{a}^{b}|f|^{p} d \alpha\right\}^{1 / p}\left\{\left.\int_{a}^{b}|g|^{q} d \alpha\right|^{1 / q} .\right.
$$

This is H√∂lder's inequality. When $p=q=2$ it is usually called the Schwarz inequality. (Note that Theorem 1.35 is a very special case of this.)

(d) Show that H√∂lder's inequality is also true for the "improper" integrals described in Exercises 7 and 8.

Solution. (a) The inequality is obvious if either $u=0$ or $v=0$, and equality holds in that case if and only if $u=v=0$. Hence assume $v>0$. Keep $v$ fixed. The inequality implies that $p>1$ and $q>1$, and hence the function $\varphi(u)=\frac{u^{p}}{p}+\frac{v^{q}}{q}-u v$ satisfies

$$
\lim _{u \rightarrow+\infty} \varphi(u)=+\infty
$$

We also have $\varphi^{\prime}(0)=-v<0$. Hence the function $\varphi(u)$ has a minimum at some point $u_{0}$ on $(0, \infty)$ at which $0=\varphi^{\prime}\left(u_{0}\right)=u_{0}^{p-1}-v$, i.e., $u_{0}=v^{\frac{1}{p-1}}=v^{q-1}$ and $u_{0}^{p}=v^{q}$. Note that $\varphi\left(u_{0}\right)=\frac{v^{q}}{p}+\frac{v^{q}}{q}-v^{q-1} v=v^{q}-v^{q}=0$. Since this point is the only critical point for $\varphi$, we have $\varphi(u)>0$ for all $u \neq u_{0}$, as required.

(b) Simply integrate the inequality

$$
f(x) g(x) \leq \frac{f(x)^{p}}{p}+\frac{g(x)^{q}}{q}
$$

(c) The inequality is obviously equality if either of the two integrals on the righthand side is zero. For the vanishing of, say $\int_{a}^{b}|f|^{p} d \alpha$ implies the vanishing of $\int_{a}^{b} M|f| d \alpha$ and hence the vanishing of $\int_{a}^{b}|g||f| d \alpha$ if $|g(x)| \leq M$ for all $x$. Hence we now assume that $\int_{a}^{b}|f|^{p} d \alpha>0$ and $\int_{a}^{b}|g|^{q} d \alpha>0$. In part $(b)$ we replace $f(x)$ by $\frac{|f(x)|}{\left(\int_{a}^{b}|f| p d \alpha\right)^{1 / p}}$ and $g(x)$ by $\frac{|g(x)|}{\left(\int_{a}^{b}|g|^{q} d \alpha\right)^{1 / q}}$. We then need only invoke the inequality $\left|\int_{a}^{b} h d \alpha\right| \leq \int_{a}^{b}|h| d \alpha$.
(d) The inequality holds on each finite interval. If either of the factors on the right-hand side diverges as $b \rightarrow \infty$, the inequality is obvious. If they both converge, it follows that the left-hand side converges absolutely, and to a limit not larger than the limit of the right-hand side.

Exercise 6.11 Let $\alpha$ be a fixed increasing function on $[a, b]$. For $u \in \mathcal{R}(\alpha)$ define

$$
\|u\|_{2}=\left\{\int_{a}^{b}|u|^{2} d \alpha\right\}^{1 / 2}
$$

Suppose $f, g$, and $h \in \mathcal{R}(\alpha)$, and prove the triangle inequality

$$
\|f-h\|_{2} \leq\|f-g\|_{2}+\|g-h\|_{2}
$$

as a consequence of the Schwarz inequality, as in the proof of Theorem 1.37.

Solution. We have

$$
\begin{aligned}
\|f-h\|_{2}^{1} & =\int_{a}^{b}|f-h|^{2} d \alpha \\
& =\int_{a}^{b}|(f-g)+(g-h)|^{2} d \alpha \\
& =\int_{a}^{b}|f-g|^{2} d \alpha+2 \int_{a}^{b}|f-g \| g-h| d \alpha+\int_{a}^{b}|g-h|^{2} d \alpha \\
& \leq\|f-g\|_{2}^{2}+2\|f-g\|_{2}\|g-h\|_{2}+\|g-h\|_{2}^{2} \\
& =\left(\|f-g\|_{2}+\|g-h\|_{2}\right)^{2},
\end{aligned}
$$

from which the desired inequality follow when square roots are taken.

Exercise 6.12 With the notations of Exercise 11, suppose $f \in \mathcal{R}(\alpha)$ and $\varepsilon>0$. Prove that there exists a continuous function $g$ on $[a, b]$ such that $\|f-g\|_{2}<\varepsilon$. Hint: Let $P=\left\{x_{0}, \ldots, x_{n}\right\}$ be a suitable partition of $[a, b]$, define

$$
g(t)=\frac{x_{i}-t}{\Delta x_{i}} f\left(x_{i-1}\right)+\frac{t-x_{i-1}}{\Delta x_{i}} f\left(x_{i}\right)
$$

if $x_{i-1} \leq t \leq x_{i}$.

Solution. Since $g(t)$ is defined on $\left[x_{i-1}, x_{i}\right]$ as the weighted average of the values of $f(x)$ at the endpoints, the weights being proportional to the distances from $t$ to the endpoints, it is clear that $g(t)$ is piecewise linear, hence continuous. For the same reason the maximum value of the function $h=|g-f|$ on the interval $\left[x_{i-1}, x_{i}\right]$ will be at most $M_{i}-m_{i}$ where $M_{i}$ and $m_{i}$ are the maximum
and minimum values of $f$ on this interval. Let $M$ be the maximum of $|f(x)|$ for $a \leq x \leq b$. If the partition is chosen so that

$$
\sum\left(M_{i}-m_{i}\right)\left[\alpha\left(t_{i}\right)-\alpha\left(t_{i-1}\right]<\frac{\varepsilon^{2}}{2 M}\right.
$$

then we will have

$$
\sum\left(M_{i}-m_{i}\right)^{2}\left[\alpha\left(t_{i}\right)-\alpha\left(t_{i-1}\right] \leq 2 M \sum\left(M_{i}-m_{i}\right)\left[\alpha\left(t_{i}\right)-\alpha\left(t_{i-1}\right)\right]<\varepsilon^{2}\right.
$$

and hence the upper Riemann integral for $|g-f|^{2}$ for this partition will also be less than $\varepsilon^{2}$. Therefore $\|g-f\|_{2}<\varepsilon$, as required.

Exercise 6.13 Define

$$
f(x)=\int_{x}^{x+1} \sin \left(t^{2}\right) d t
$$

(a). Prove that $|f(x)|<1 / x$ if $x>0$.

Hint: Put $t^{2}=u$ and integrate by parts to show that $f(x)$ is equal to

$$
\frac{\cos \left(x^{2}\right)}{2 x}-\frac{\cos \left[(x+1)^{2}\right]}{2(x+1)}-\int_{x^{2}}^{(x+1)^{2}} \frac{\cos u}{4 u^{3 / 2}} d u
$$

Replace $\cos u$ by -1 .

(b) Prove that

$$
2 x f(x)=\cos \left(x^{2}\right)-\cos \left[(x+1)^{2}\right]+r(x)
$$

where $|r(x)|<c / x$, and $c$ is constant.

(c) Find the upper and lower limits of $x f(x)$ as $x \rightarrow \infty$.

(d) Does $\int_{0}^{\infty} \sin \left(t^{2}\right) d t$ converge?

Solution. (a) This inequality is obvious if $0<x \leq 1$. Hence we assume $x>1$. Following the hint, we observe that

$$
\begin{aligned}
f(x) & <\frac{\cos \left(x^{2}\right)}{2 x}-\frac{\cos \left[(x+1)^{2}\right]}{2(x+1)}+\frac{1}{2 x}-\frac{1}{2(x+1)} \\
& =\frac{1+\cos \left(x^{2}\right)}{2 x}-\frac{1+\cos \left[(x+1)^{2}\right]}{2(x+1)} \\
& \leq \frac{1+\cos \left(x^{2}\right)}{2 x} \\
& \leq \frac{1}{x}
\end{aligned}
$$

A similar argument shows that

$$
\begin{aligned}
f(x) & >\frac{\cos \left(x^{2}\right)}{2 x}-\frac{\cos \left[(x+1)^{2}\right]}{2(x+1)}-\frac{1}{2 x}+\frac{1}{2(x+1)} \\
& =\frac{-1+\cos \left(x^{2}\right)}{2 x}-\frac{-1+\cos \left[(x+1)^{2}\right]}{2(x+1)} \\
& =\frac{-1+\cos \left(x^{2}\right)}{2 x}+\frac{1-\cos \left[(x+1)^{2}\right]}{2(x+1)} \\
& \geq \frac{-1+\cos \left(x^{2}\right)}{2 x} \\
& \geq \frac{-1}{x} .
\end{aligned}
$$

(b) The expression just written for $f(x)$ shows that

$$
2 x f(x)=\cos \left(x^{2}\right)-\cos \left[(x+1)^{2}\right]+r(x)
$$

where

$$
r(x)=\left(\frac{1}{x+1}\right) \cos \left[(x+1)^{2}\right]-\frac{x}{2} \int_{x^{2}}^{(x+1)^{2}} \frac{\cos u}{u^{3 / 2}} d u
$$

If we integrate by parts again, we find that

$$
\int_{x^{2}}^{(x+1)^{2}} \frac{\cos u}{u^{3 / 2}} d u=\frac{\sin \left[(x+1)^{2}\right]}{(x+1)^{3}}-\frac{\sin \left(x^{2}\right)}{x^{3}}+\frac{3}{2} \int_{x^{2}}^{(x+1)^{2}} \frac{\sin u}{x^{5 / 2}} d u
$$

We now observe that the absolute value of this last integral is at most

$$
\frac{3}{2} \int_{x^{2}}^{\infty} \frac{1}{u^{5 / 2}} d u=-\left.u^{-3 / 2}\right|_{x^{2}} ^{\infty}=x^{-3}
$$

It then follows by collecting the terms that

$$
|r(x)|<\frac{3}{x}
$$

(c) Since $r(x) \rightarrow 0$, the upper and lower limits of $x f(x)$ will be the corresponding limits of

$$
\frac{\cos \left(x^{2}\right)-\cos \left[(x+1)^{2}\right]}{2}=\sin \left(x^{2}+x+\frac{1}{2}\right) \sin \left(x+\frac{1}{2}\right) \text {. }
$$

We can write this last expression as $\sin s \sin \left(s^{2}+\frac{1}{4}\right)$, where $s=x+\frac{1}{2}$. We claim that the upper limit of this expression is 1 and the lower limit is -1 . Indeed, let $\varepsilon>0$ be given. Choose $n$ to be any positive integer larger than $\frac{2-\varepsilon}{8 \varepsilon}$. Then the interval $\left(\frac{1}{4}+\left(\left(2 n+\frac{1}{2}\right) \pi-\varepsilon\right)^{2}, \frac{1}{4}+\left(\left(2 n+\frac{1}{2}\right) \pi+\varepsilon\right)^{2}\right)$ is longer than $2 \pi$, and hence there exists a point $t \in\left(\left(2 n+\frac{1}{2}\right) \pi-\varepsilon,\left(2 n+\frac{1}{2}\right) \pi+\varepsilon\right)$
at which $\sin \left(t^{2}+\frac{1}{4}\right)=1$ and also a point $u$ in the same interval at which $\sin \left(u^{2}+\frac{1}{4}\right)=-1$. But then $t f(t)>1-\varepsilon$ and $u f(u)<-1+\varepsilon$. It follows that the upper limit is 1 and the lower limit is -1 . (This argument actually shows that the limit points of $x f(x)$ fill up the entire interval $[-1,1]$.)

(d) The integral does converge. We observe that for integers $N$ we have

$$
\begin{aligned}
\int_{0}^{N} \sin \left(t^{2}\right) d t & =\sum_{k=0}^{N} f(k) \\
& =f(0)+\sum_{k=1}^{N} \frac{r(k)}{k}+\sum_{k=1}^{N} \frac{\cos \left(k^{2}\right)-\cos \left[(k+1)^{2}\right]}{k} \\
& =f(0)+\sum_{k=1}^{N} \frac{r(k)}{k}+\left[\frac{\cos 1}{2}-\frac{\cos \left[(N+1)^{2}\right]}{N}\right]+\sum_{k=2}^{N} \frac{\cos \left(k^{2}\right)}{k(k-1)} .
\end{aligned}
$$

The first sum on the right converges since $|r(k)|<\frac{3}{k}$, and the rest obviously converges. Hence we will be finished if we show that

$$
\lim _{x \rightarrow \infty} \int_{[x]}^{x} \sin \left(t^{2}\right) d t=0
$$

where $[x]$ is the integer such that $[x] \leq x<[x]+1$. But this is easily done using integration by parts. The integral equals

$$
\frac{\cos \left([x]^{2}\right)}{2[x]}-\frac{\cos \left(x^{2}\right)}{x^{2}}-\int_{[x]^{2}}^{x^{2}} \frac{\cos u}{4 u^{3 / 2}} d u
$$

and this expression obviously tends to zero as $x \rightarrow \infty$.

Exercise 6.14 Deal similarly with

$$
f(x)=\int_{x}^{x+1} \sin \left(e^{t}\right) d t
$$

Show that

$$
e^{x}|f(x)|<2
$$

and that

$$
e^{x} f(x)=\cos \left(e^{x}\right)-e^{-1} \cos \left(e^{x+1}\right)+r(x)
$$

where $|r(x)|<C e^{-x}$ for some constant $C$.

Solution. The arguments are completely analogous to the preceding problem. The substitution $u=e^{t}$ changes $f(x)$ into

$$
f(x)=\int_{e^{x}}^{e^{x+1}} \frac{\sin u}{u} d u
$$

and then integration by parts yields

$$
f(x)=\frac{\cos \left(e^{x}\right)}{e^{x}}-\frac{\cos \left(e^{x+1}\right)}{e^{x+1}}-\int_{e^{x}}^{e^{x+1}} \frac{\cos u}{u^{2}} d u
$$

from which it then follows that

$$
-\frac{1-\cos \left(e^{x}\right)}{e^{x}} \leq f(x) \leq \frac{1+\cos \left(e^{x}\right)}{e^{x}}
$$

We have the equality

$$
e^{x} f(x)=\cos \left(e^{x}\right)-e^{-1} \cos \left(e^{x+1}\right)-e^{x} \int_{e^{x}}^{e^{x+1}} \frac{\cos u}{u^{2}} d u
$$

and one more integration by parts shows that

$$
\left|e^{x} \int_{e^{x}}^{e^{x+1}} \frac{\cos u}{u^{2}} d u\right|<\frac{3}{e^{x}}
$$

In this case $f(x)$ decreases so rapidly that there is no difficulty at all proving the convergence of the integral.

Exercise 6.15. Suppose $f$ is a real, continuously differentiable function on $[a, b]$, $f(a)=f(b)=0$, and

$$
\int_{a}^{b} f^{2}(x) d x=1
$$

Prove that

$$
\int_{a}^{b} x f(x) f^{\prime}(x) d x=-\frac{1}{2}
$$

and that

$$
\int_{a}^{b}\left[f^{\prime}(x)\right]^{2} d x \cdot \int_{a}^{b} x^{2} f^{2}(x) d x \geq \frac{1}{4}
$$

Solution. To prove the first assertion we merely integrate by parts, taking $u=x$, $d v=f(x) f^{\prime}(x) d x$, so that $d u=d x$ and $v=\frac{1}{2} f^{2}(x)$. Since $v$ vanishes at both endpoints, the result is

$$
\int_{a}^{b} x f(x) f^{\prime}(x) d x=-\frac{1}{2} \int_{a}^{b} f^{2}(x) d x=-\frac{1}{2}
$$

The second inequality is an immediate consequence of the Schwarz inequality applied to the two functions $x f(x)$ and $f^{\prime}(x)$.

Exercise 6.16 For $1<s<\infty$, define

$$
\zeta(s)=\sum_{n=1}^{\infty} \frac{1}{n^{s}}
$$

(This is Riemann's zeta function, of great importance in the study of the distribution of prime numbers.) Prove that

(a) $\quad \zeta(s)=s \int_{1}^{\infty} \frac{[x]}{x^{s+1}} d x$

and that

$$
\zeta(s)=\frac{s}{s-1}-s \int_{1}^{\infty} \frac{x-[x]}{x^{s+1}} d x
$$

where $[x]$ denotes the greatest integer $\leq x$.

Prove that the integral in (b) converges for all $x>0$.

Hint: To prove (a) compute the difference between the integral over $[1, N]$ and the $N$ th partial sum of the series that defines $\zeta(s)$.

Solution. (a) Ignoring the author's advice, we note that

$$
\begin{aligned}
s \int_{1}^{\infty} \frac{[x]}{x^{s+1}} d x & =s \sum_{n=1}^{\infty} n \int_{n}^{n+1} \frac{1}{x^{s+1}} d x \\
& =\sum_{n=1}^{\infty} n\left[\frac{1}{n^{s}}-\frac{1}{(n+1)^{s}}\right] \\
& =1\left[\frac{1}{1^{s}}-\frac{1}{2^{s}}\right]+2\left[\frac{1}{2^{s}}-\frac{1}{3^{s}}\right]+\cdots \\
& =\sum_{n=1}^{\infty} \frac{1}{n^{s}} \\
& =\zeta(s) .
\end{aligned}
$$

(b) This result is a trivial consequence of $(a)$ and the identity

$$
\frac{s}{s-1}=\int_{1}^{\infty} \frac{x}{x^{s+1}} d x
$$

Exercise 6.17 Suppose $\alpha$ increases monotonically on $[a, b], g$ is continuous, and $g(x)=G^{\prime}(x)$ for $a \leq x \leq b$. Prove that

$$
\int_{a}^{b} \alpha(x) g(x) d x=G(b) \alpha(b)-G(a) \alpha(a)-\int_{z}^{b} G d \alpha
$$

Hint: Take $g$ real, without loss of generality. Given $P=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$, choose $t_{i} \in\left(x_{i-1}, x_{i}\right)$ so that $g\left(t_{i}\right) \Delta x_{i}=G\left(x_{i}\right)-G\left(x_{i-1}\right)$. Show that

$$
\sum_{i=1}^{n} \alpha\left(x_{i}\right) g\left(t_{i}\right) \Delta x_{i}=G(b) \alpha(b)-G(a) \alpha(a)-\sum_{i=1}^{n} G\left(x_{i-1}\right) \Delta \alpha_{i}
$$

Solution. The identity just given is a trivial consequence of Abel's method of rearranging the sums:

$$
\begin{aligned}
\sum_{i=1}^{n} \alpha\left(x_{i}\right) g\left(t_{i}\right) \Delta x_{i} & =\sum_{i=1}^{n} \alpha\left(x_{i}\right)\left(G\left(x_{i}\right)-G\left(x_{i-1}\right)\right) \\
& \left.=G\left(x_{n}\right) \alpha\left(x_{n}\right)-G\left(x_{0}\right) \alpha\left(x_{0}\right)-\sum_{i=1}^{G}\left(x_{i-1}\right)\left(\alpha\left(x_{i}\right)-\alpha_{i-1}\right)\right)
\end{aligned}
$$

Now the fact that $G(x)$ is continuous and $\alpha$ is nondecreasing means that the right-hand side can be made arbitrarily close to

$$
G(b) \alpha(b)-G(a) \alpha(a)-\int_{a}^{b} G d \alpha
$$

whenever the partition is sufficiently fine. It does not follow immediately that the function $\alpha(x) g(x)$ is integrable on $[a, b]$. However, since $\alpha$ is nondecreasing, its only discontinuties are jumps, and for any given $\varepsilon>0$ there can be only a finite number of jumps larger than $\varepsilon$. These can be enclosed in a finite number of open intervals of arbitrarily small length. We can then argue, as in Exercise 6 above, that any partition that is sufficiently fine will have upper and lower Riemann sums that differ by less than $\varepsilon$. Hence $\alpha(x) g(x)$ is integrable, and its integral is given by the stated relation.

Exercise 6.18 Let $\gamma_{1}, \gamma_{2}, \gamma_{3}$ be curves in the complex plane defined on $[0,2 \pi]$ by

$$
\gamma_{1}(t)=e^{i t}, \quad \gamma_{2}(t)=e^{2 i t}, \quad \gamma_{3}(t)=e^{2 \pi i t \sin (1 / t)}
$$

Show that these curves have the same range, that $\gamma_{1}$ and $\gamma_{2}$ are rectifiable, that the length of $\gamma_{1}$ is $2 \pi$, that the length of $\gamma_{2}$ is $4 \pi$, and that $\gamma_{3}$ is not rectifiable.

Solution. Since $e^{i t}$ has period $2 \pi$ it is obvious that $\gamma_{1}$ and $\gamma_{2}$ have the same range, namely the set of all complex numbers of absolute value 1 . To show that this is also the range of $\gamma_{3}$, we need to show that the mapping $t \mapsto 2 \pi t \sin (1 / t)$, $0 \leq t \leq 2 p i$, covers an interval of length $2 \pi$, i.e., that the mapping $t \mapsto t \sin (1 / t)$, $0 \leq t \leq 2 \pi$ covers an interval of length 1 . (We naturally take the value to be zero when $t=0$.) Since this range is connected, it suffices to find two points $a$ and $b$ in the range with $a-b>1$. We choose those points to be $a=\frac{3}{\pi}$ (the image of $t=\frac{6}{\pi}$ ) and $b=\frac{-2}{3 \pi}$, (the image of $t=\frac{2}{3 \pi}$ ). We have $a-b=\frac{11}{3 \pi}>1$.

The rectification of $\gamma_{1}$ and $\gamma_{2}$ is straightforward:

$$
\begin{gathered}
l\left(\gamma_{1}\right)=\int_{0}^{2 \pi}\left|\gamma_{1}^{\prime}(t)\right| d t=2 \pi \\
l\left(\dot{\gamma}_{2}\right)=\int_{0}^{2 \pi}\left|\gamma_{2}^{\prime}(t)\right| d t=\int_{0}^{2 \pi} 2 d t=4 \pi
\end{gathered}
$$

To show that $\gamma_{3}$ is not rectifiable, we observe that its length would be

$$
\int_{0}^{2 \pi}\left|\sin (1 / t)-\frac{1}{t} \cos (1 / t)\right| d t \geq \int_{0}^{2 \pi}\left|\frac{\cos (1 / t)}{t}\right| d t-2 \pi
$$

By making the substitution $u=\frac{1}{t}$ in this last integral we get

$$
\int_{\frac{1}{2 \pi}}^{\infty}\left|\frac{\cos u}{u}\right| d u
$$

But we already know that this integral diverges, since

$$
\sum_{n=1}^{\infty} \int_{2 n \pi}^{\left(2 n+\frac{1}{2}\right) \pi} \frac{\cos u}{u} d u \geq \sum_{n=1}^{\infty} \frac{1}{\left(2 n+\frac{1}{2}\right) \pi}=\infty
$$

Exercise 6.19 Let $\gamma_{1}$ be a curve in $R^{k}$ defined on $[a, b]$; let $\phi$ be a continuous 1-1 mapping of $[c, d]$ onto $[a, b]$ such that $\phi(c)=a$, and define $\gamma_{2}(x)=\gamma_{1}(\phi(x))$. Prove that $\gamma_{2}$ is an arc, a closed curve, or a rectifiable curve if and only if the same is true of $\gamma_{1}$. Prove that $\gamma_{1}$ and $\gamma_{2}$ have the same length.

Solution. We know that $\phi$ has a continuous 1-1 inverse $\varphi$, and that the composition of one-to-one functions is one-to-one. Hence, since $\gamma_{1}(x)=\gamma_{2}(\varphi(x))$, we see that $\gamma_{1}$ and $\gamma_{2}$ are both arcs (one-to-one) if either is. Since necessarily $\phi(d)=b$, we see that $\gamma_{1}(a)=\gamma_{1}(b)$ if and only if $\gamma_{2}(c)=\gamma_{2}(d)$. Hence both are closed curves if either is. Finally, since $\phi$ and $\varphi$ establish a one-toone correspondence between partitions $\left\{s_{i}\right\}$ of $[a, b]$ and $\left\{t_{i}\right\}$ of $[c, d]$ such that $\sum\left|\gamma_{1}\left(s_{i}\right)-\gamma_{1}\left(s_{i-1}\right)\right|=\sum\left|\gamma_{2}\left(t_{i}\right)-\gamma_{2}\left(t_{i-1}\right)\right|$, it follows that the two curves have the same length.

## Chapter 7

## Sequences and Series of Functions

Exercise 7.1 Prove that every uniformly convergent sequence of bounded functions is uniformly bounded.

Solution. Let $\left\{f_{n}(x)\right\}_{n=1}^{\infty}$ be a uniformly convergent sequence of bounded functions, say $\left|f_{n}(x)\right| \leq M_{n}$ for all $x$ and all $n$. Since the sequence converges uniformly, it is a uniformly Cauchy sequence. Hence there exists $N$ such that $\left|f_{m}(x)-f_{n}(x)\right|<1$ for all $m, n \geq N$. In particular if $m \geq N$, we have $\left|f_{m}(x)\right| \leq$ $\left|f_{N}(x)\right|+\left|f_{m}(x)-f_{N}(x)\right| \leq M_{N}+1$, and therefore if $M=1+\max \left(M_{1}, \ldots, M_{N}\right)$ we have $\left|f_{n}(x)\right| \leq M$ for all $n$ and $x$.

Exercise 7.2 If $\left\{f_{n}\right\}$ and $\left\{g_{n}\right\}$ converge uniformly on a set $E$, prove that $\left\{f_{n}+\right.$ $\left.g_{n}\right\}$ converges uniformly on $E$. If, in addition, $\left\{f_{n}\right\}$ and $\left\{g_{n}\right\}$ are sequences of bounded functions, prove that $\left\{f_{n} g_{n}\right\}$ converges uniformly on $E$.

Solution. Let $f$ and $g$ denote the limits of the two sequences. Let $\varepsilon>0$. There exist $N_{1}$ and $N_{2}$ such that $\left|f_{n}(x)-f(x)\right|<\frac{\varepsilon}{2}$ for all $x$ if $n>N_{1}$ and $\left|g_{n}(x)-g(x)\right|<\frac{\varepsilon}{2}$ for all $x$ if $n>N_{2}$. Let $N=\max \left(N_{1}, N_{2}\right)$. Then for $n>N$ we have, for all $x$,

$$
\left|\left(f_{n}+g_{n}\right)(x)-(f+g)(x)\right| \leq\left|f_{n}(x)-f(x)\right|+\left|g_{n}(x)-g(x)\right|<\varepsilon
$$

Hence $\left\{f_{n}+g_{n}\right\}$ converges uniformly.

Suppose now that each of the functions $f_{n}$ and $g_{n}$ is bounded. By the previous problem, both sequences are uniformly bounded. Hence there exists $M$ such that $\left|f_{n}(x)\right| \leq M$ and $\left|g_{n}(x)\right| \leq M$ for all $n$ and all $x$. It follows that $|g(x)| \leq M$ also. Then, given $\varepsilon>0$, choose $N_{1}$ and $N_{2}$ such that $\left|f_{n}(x)-f(x)\right| \leq$ $\frac{\varepsilon}{2 M}$ for all $x$ and all $n>N_{1}$ and $\left|g_{n}(x)-g(x)\right|<\frac{\varepsilon}{2 M}$ for all $x$ and $n>N_{2}$.

Again let $N=\max \left(N_{1}, N_{2}\right)$. We then have, for all $x$ and all $n>N$,

$$
\begin{aligned}
\left|f_{n}(x) g_{n}(x)-f(x) g(x)\right| & \leq\left|f_{n}(x) g_{n}(x)-f_{n}(x) g(x)\right|+ \\
& \quad+\left|f_{n}(x) g(x)-f(x) g(x)\right| \\
& \leq M\left|g_{n}(x)-g(x)\right|+M\left|f_{n}(x)-f(x)\right| \\
& <M \frac{\varepsilon}{2 M}+M \frac{\varepsilon}{2 M} \\
& =\varepsilon
\end{aligned}
$$

Exercise 7.3 Construct sequences $\left\{f_{n}\right\},\left\{g_{n}\right\}$ which converge uniformly on some set $E$, but such that $\left\{f_{n} g_{n}\right\}$ does not converge uniformly on $E$ (of course, $\left\{f_{n} g_{n}\right\}$ must converge on $E$ ).

Solution. Let $f_{n}(x)=x$ for all $x$ and all $n$, and let $g_{n}(x)=\frac{1}{n}$ for all $x$ and all $n$. Then $f_{n}(x)$ converges uniformly to $x$, and $g_{n}(x)$ converges uniformly to 0 . Therefore $f_{n}(x) g_{n}(x)$. converges to 0 , but not uniformly. In fact for every $n$ there is an $x$, namely $x=n$, such that $f_{n}(x) g_{n}(x)=1$. Hence, no matter how large $n$ is taken, the inequality $\left|f_{n}(x) g_{n}(x)\right|<1$ will never hold for all $x$.

## Exercise 7.4 Consider

$$
f(x)=\sum_{n=1}^{\infty} \frac{1}{1+n^{2} x}
$$

For what values of $x$ does the series converge absolutely? On what intervals does it converge uniformly? On what intervals does it fail to converge uniformly? Is $f$ continuous wherever the series converges? Is $f$ bounded?

Solution. The series converges for all $x$ except 0 and $x=\frac{-1}{n^{2}}, n=1,2, \ldots$. For $x=0$ all the terms of the series are defined, but the terms do not tend to 0 . For $x=\frac{-1}{n^{2}}$ the $n$th term is not defined. For all other values of $x$ the series converges. By Theorem 7.10 (the Weierstrass $M$-test) the series converges uniformly on the interval $[\delta, \infty)$ if $\delta>0$, since on that interval

$$
\frac{1}{1+n^{2} x} \leq \frac{1}{n^{2} \delta}
$$

Likewise, the series converges uniformly on $(-\infty,-\delta]$ except at the points $x=$ $-\frac{1}{n^{2}}$, since for $n \geq \sqrt{\frac{2}{\delta}}$ we have

$$
\left|\frac{1}{1+n^{2} x}\right| \leq \frac{1}{n^{2}} \cdot \frac{1}{\delta-\frac{1}{n^{2}}} \leq \frac{2}{\delta n^{2}}
$$

The series does not converge uniformly on any interval having 0 as an endpoint. This is easy to see in the case when 0 is the left-hand endpoint. For each
of the terms of the series is a bounded function on $[0, \infty)$. If the series converged uniformly, the limit would be bounded by Problem 1 above. But we have

$$
f\left(\frac{1}{m^{2}}\right) \geq \sum_{n=1}^{m} \frac{1}{1+\frac{n^{2}}{m^{2}}} \geq \frac{m}{2}
$$

Likewise the series cannot be a uniformly Cauchy series (i.e., the sequence of partial sums cannot be a uniformly Cauchy sequence) on any interval $(-\delta, 0)$, since, no matter how large $n$ is taken, there is a point $x$ in this interval, namely $x=-\frac{1}{2 n^{2}}$, at which the $n$th term has the value 2 . Hence, if $S_{n}$ denotes the sum of the first $n$ terms, then $\left|S_{n}(x)-S_{n-1}(x)\right|=2$.

The uniform convergence shows that the limiting function $f(x)$ is continuous wherever it is defined on $(-\infty, \delta] \cup[\delta,+\infty)$. Since $\delta$ is arbitrary, $f(x)$ is continuous wherever it is defined. The argument given above shows that $f(x)$ is not bounded.

Exercise 7.5 Let

$$
f_{n}(x)= \begin{cases}0 & \left(x<\frac{1}{n+1}\right) \\ \sin ^{2} \frac{\pi}{x} & \left(\frac{1}{n+1} \leq x \leq \frac{1}{n}\right) \\ 0 & \left(\frac{1}{n}<x\right)\end{cases}
$$

Show that $\left\{f_{n}\right\}$ converges to a continuous function, but not uniformly. Use the series $\sum f_{n}$ to show that absolute convergence, even for all $x$ does not imply uniform convergence.

Solution. The limit of $f_{n}(x)$ is zero. If $x \leq 0$ or $x \geq 1$, then $f_{n}(x)=0$ for all $n$, and so this assertion is obvious. If $0<x<1$, then $f_{n}(x)=0$ for all $n \geq \frac{1}{x}$, and so once again the assertion is obvious.

The convergence is not uniform, since, no matter how large $n$ is taken, there is a point $x$, namely $x=\frac{1}{2 n+\frac{1}{2}}$, for which $f_{n}(x)=1$.

The series $\sum f_{n}(x)$ converges to 0 for $x \leq 0$ and $x \geq 1$, and to $\sin ^{2} \frac{\pi}{x}$ for $0<x<1$. Since the terms are nonnegative, the series obviously converges absolutely. Since the sum is not continuous at 0 , the series does not converge uniformly on any interval containing 0 .

Exercise 7.6 Prove that the series

$$
\sum_{n=1}^{\infty}(-1)^{n} \frac{x^{2}+n}{n^{2}}
$$

converges uniformly in every bounded interval, but does not converge absolutely for any value of $x$.

Solution. The series is the sum of two series:

$$
x^{2} \sum_{n=1}^{\infty} \frac{(-1)^{n}}{n^{2}}+\sum_{n=1}^{\infty} \frac{(-1)^{n}}{n^{2}}
$$

The first of these converges both uniformly and absolutely on any bounded interval $[a, b]$ by the $M$-test (with $M_{n}=\frac{M^{2}}{n^{2}}$, where $M=\max (|a|,|b|)$ ). The second is independent of $x$ and converges, hence it converges uniformly in $x$. By Exercise 2 above, the sum of the two series converges uniformly. The series does not converge absolutely since the absolute value of each term
is at least $\frac{1}{n}$ for any $x$.

Exercise 7.7 For $n=1,2,3, \ldots, x$ real, put

$$
f_{n}(x)=\frac{x}{1+n x^{2}}
$$

Show that $\left\{f_{n}\right\}$ converges uniformly to a function $f$, and that the equation

$$
f^{\prime}(x)=\lim _{n \rightarrow \infty} f_{n}^{\prime}(x)
$$

is correct if $x \neq 0$, but false if $x=0$.

Solution. The Schwarz inequality, which implies that $\left|f_{n}(x)\right| \leq \frac{|x|}{2 \sqrt{n}|x|}=\frac{1}{2 \sqrt{n}}$ for $x \neq 0$, shows that $f_{n}(x)$ tends uniformly to 0 . Now $f_{n}^{\prime}(x)=\frac{1-n x^{2}}{\left(1+n x^{2}\right)^{2}}$, which tends to 0 if $x \neq 0$, though $f_{n}^{\prime}(0)=1$ for all $n$.

## Exercise 7.8 If

$$
I(x)= \begin{cases}0 & (x \leq 0) \\ 1 & (x>0)\end{cases}
$$

if $\left\{x_{n}\right\}$ is a sequence of distinct points of $(a, b)$, and if $\sum\left|c_{n}\right|$ converges, prove
that the series

$$
f(x)=\sum_{n=1}^{\infty} c_{n} I\left(x-x_{n}\right) \quad(a \leq x \leq b)
$$

converges uniformly, and that $f$ is continuous for every $x \neq x_{n}$.

Solution. The uniform convergence is a consequence of the $M$-test with $M_{n}=$ $\left|c_{n}\right|$. Hence $f$ is continuous wherever each of the individual terms is continuous,
in particular, at least for $x \neq x_{n}$.

Exercise 7.9 Let $\left\{f_{n}\right\}$ be a sequence of continuous functions which converges uniformly to a function $f$ on a set $E$. Prove that

$$
\lim _{n \rightarrow \infty} f_{n}\left(x_{n}\right)=f(x)
$$

for every sequence of points $x_{n} \in E$ such that $x_{n} \rightarrow x$, and $x \in E$. Is the converse of this true?

Solution. Let $\varepsilon>0$. Choose $N_{1}$ so large that $\left|f_{m}(x)-f(x)\right|<\frac{\varepsilon}{2}$ for all $m>N_{1}$. Then, since $f$ is continuous at $x$, choose $\delta>0$ so small that $|f(y)-f(x)|<\frac{\varepsilon}{2}$ if $|y-x|<\delta$. Finally, choose $N_{2}$ so large that $\left|x_{n}-x\right|<\delta$ if $n>N_{2}$. Then if $n>\max \left(N_{1}, N_{2}\right)$ we have

$$
\left|f_{n}\left(x_{n}\right)-f(x)\right| \leq\left|f_{n}\left(x_{n}\right)-f\left(x_{n}\right)\right|+\left|f\left(x_{n}\right)-f(x)\right|<\varepsilon
$$

The converse is not true in general. For example let $f_{n}(x)$ be given by $f_{n}(x)=\sin ^{2} \pi x$ for $n \leq|x| \leq n+1$ and $f_{n}(x)=0$ for $|x| \leq n$ or $|x| \geq n+1$. Thus $f_{n}(x)$ tends to zero, since $f_{n}(x)=0$ if $n \geq|x|$, but $f_{n}(x)$ does not converge uniformly, since $f_{n}\left(n+\frac{1}{2}\right)=1$ Then for any convergent sequence, say $x_{n} \rightarrow x$, let $N \geq \max \left(|x|,\left|x_{1}\right|,\left|x_{2}\right|, \ldots,\left|x_{n}\right|, \ldots\right)$. We then have $f_{n}\left(x_{n}\right)=0$ for all $n \geq N$, and so $f_{n}\left(x_{n}\right) \rightarrow f(x)$.

This condition does guarantee uniform convergence on any compact set, however. For if $\left\{f_{n}(x)\right\}$ is not a uniformly Cauchy sequence, then for some $\varepsilon_{0}>0$ there is a sequence of integers $n_{1}<n_{2}<\cdots$ and a sequence of points $x_{1}, x_{2}, \ldots$ such that

$$
\left|f_{n_{2 k-1}}\left(x_{k}\right)-f_{n_{2 k}}\left(x_{k}\right)\right| \geq \varepsilon_{0}
$$

for $k=1,2, \ldots$. Since $K$ is compact, some subsequence of $\left\{x_{k}\right\}$ converges, say $x_{k_{r}} \rightarrow x$ as $r \rightarrow \infty$. Now define $y_{n}=x$ for all $n \neq n_{2 k_{r}}, n \neq n_{2 k_{r}-1}$, and let $y_{n_{2 k_{r}-1}}=y_{n_{2 k}}=x_{k_{r}}$, so that so that $y_{n} \rightarrow x$. Then the sequence $\left\{z_{n}\right\}=\left\{f_{n}\left(y_{n}\right)\right\}$ is not a Cauchy sequence, since $\left|z_{n_{2 k_{r}}}-z_{n_{2 k_{r}-1}}\right| \geq \varepsilon_{0}$.

Exercise 7:10 Let $(x)=x-n$, where $n$ is the unique integer such that $n \leq$ $x<n+1$. Prove that

$$
f(x)=\sum_{n=1}^{\infty} \frac{(n x)}{n^{2}}
$$

is discontinuous at a dense set of points.

Solution. We shall prove that $f(x)$ is discontinuous at every rational number. Since $f(x)$ has period 1 , it suffices to prove this for $0 \leq x<1$. To that end, let $x=\frac{p}{q}$ where $p$ and $q$ are relatively prime integers, $0 \leq p<q$. We "stratify" the sum that defines $f(x)$ by grouping all the indices $n$ that are congruent modulo $q$, i.e., we let $n=k q+r$, where $1 \leq r \leq q$ :

$$
f(x)=\sum_{k=0}^{\infty} \sum_{r=1}^{q} \frac{((k q+r) x)}{(k q+r)^{2}}
$$

Reversing the order of summation, we find

$$
f(x)=f_{1}(x)+f_{2}(x)+\cdots+f_{q-1}(x)+f_{q}(x)
$$

where

$$
f_{r}(x)=\sum_{k=0}^{\infty} \frac{((k q+r) x)}{(k q+r)^{2}}
$$

Now it is easy to see that $f_{1}(x), \ldots, f_{q-1}(x)$ are continuous at $x=\frac{p}{q}$. For if $1 \leq r<q$, then $((k q+r) x)$ is continuous at that point, since $(x)$ is continuous at the point $x=(k q+r) \frac{p}{q}=k p+\frac{r p}{q}$. (This point is not an integer, since $p$ and $q$ are relatively prime.) Since the series defining $f_{r}(x)$ converges uniformly, its limit is continuous at each point where all of the terms are continuous. In particular $f_{r}(x)$ is continuous at $x=\frac{p}{q}$, for $1 \leq r<q$.

We shall now show that $f_{q}(x)$ is discontinuous at $x=\frac{p}{q}$. It will then follow that $f(x)$ is discontinuous at that point. Observe that

$$
f_{q}(x)=\frac{1}{q^{2}} \sum_{k=0}^{\infty} \frac{((k+1) q x)}{(k+1)^{2}}=\frac{1}{q^{2}} \sum_{k=1}^{\infty} \frac{(k q x)}{k^{2}}
$$

so that

$$
f_{q}\left(\frac{p}{q}\right)=\frac{1}{q^{2}} \sum_{k=1}^{1} \frac{(k p)}{k^{2}}=0
$$

We shall prove that $\lim _{x \omega \frac{p^{+}}{q}} f_{q}(x)>0$, and this will show that $f_{q}(x)$ is discontinuous at $x=\frac{p}{q}$. Since all the terms of the series for $f_{q}$ are nonnegative, it suffices to show that the limit of the first term is positive. To that end, let $\delta=\frac{1}{2 q}$. If $\frac{p}{q}-\delta<x<\frac{p}{q}$, then $p-\frac{1}{2}<q x<p$, and hence $(q x)>\frac{1}{2}$, from which it follows that $f_{q}(x) \geq \frac{1}{2 q^{2}}$. Therefore the lower left-hand limit of $f_{q}(x)$ at $x=\frac{p}{q}$ is at least $\frac{1}{2 q^{2}}$.

Since, by the $M$-test with $M_{n}=\frac{1}{n^{2}}$, this series converges uniformly and each of its terms is Riemann-integrable, it follows from Theorem 7.16 that the sum of the series is Riemann-integrable.

Exercise 7.11 Suppose $\left\{f_{n}\right\},\left\{g_{n}\right\}$ are defined on $E$ and

(a) $\sum f_{n}$ has uniformly bounded partial sums;

(b) $g_{n} \rightarrow 0$ uniformly on $E$;
(c) $g_{1}(x) \geq g_{2}(x) \geq g_{3}(x) \geq \cdots$ for every $x \in E$.

Prove that $\sum f_{n} g_{n}$ converges uniformly on $E$. Hint: Compare with Theorem 3.42 .

Solution. Following the hint, we let $S_{N}(x)=\sum_{n=1}^{N} f_{n}(x) g_{n}(x)$ and $F_{N}(x)=$ $\sum_{n=1}^{N} f_{n}(x)\left(F_{0}(x)=0\right)$, so that $\left|F_{N}(x)\right| \leq B$ for all $x$. Then if $N>M$, we have

$$
\begin{aligned}
\left|S_{N}(x)-S_{M}(x)\right|= & \left|\sum_{n=M+1}^{N}\left[F_{n}(x)-F_{n-1}(x)\right] g_{n}(x)\right| \\
= & \mid F_{N}(x) g_{N}(x)-F_{M}(x) g_{M+1}(x)+ \\
& \quad+\sum_{n=M+1}^{N-1} F_{n}(x)\left[g_{n}(x)-g_{n+1}(x) \mid\right. \\
\leq & B\left\{\left|g_{N}(x)\right|+\left|g_{M+1}(x)\right|+\sum_{n=M+1}^{N-1}\left[g_{n}(x)-g_{n+1}(x)\right]\right\} \\
= & B\left[\left|g_{N}(x)\right|+\left|g_{M+1}(x)\right|+g_{M+1}(x)-g_{N}(x)\right],
\end{aligned}
$$

and this last expression can be made uniformly small by choosing $M$ sufficiently large by hypothesis $(b)$. Hypothesis $(c)$ was used in moving the summation sign outside the absolute value.

Exercise 7.12 Suppose $g$ and $f_{n}(n=1,2,3, \ldots)$ are defined on $(0, \infty)$, are Riemann-integrable on $[t, T]$ whenever $0<t<T<\infty,\left|f_{n}\right| \leq g, f_{n} \rightarrow f$ uniformly on every compact subset of $(0, \infty)$, and

$$
\int_{0}^{\infty} g(x) d x<\infty
$$

Prove that

$$
\lim _{n \rightarrow \infty} \int_{0}^{\infty} f_{n}(x) d x=\int_{0}^{\infty} f(x) d x
$$

(See Exercises 7 and 8 of Chap. 6 for the relevant definitions.)

This is a rather weak form of Lebesgue's dominated convergence theorem (Theorem 11.32). Even in the context of the Riemann integral, uniform convergence can be replaced by pointwise convergence if it is assumed that $f \in \mathcal{R}$. (See the articles by F. Cunningham in Math. Mag., vol. 40, 1967, pp. 179-186, and by H. Kestelman in Amer. Math. Monthly, vol. 77, 1970, pp. 182-187.)

Solution. We shall prove that $\int_{0}^{\infty} f_{n}(x) d x$ converges for each $n$, that the limit $\lim _{n \rightarrow \infty} \int_{0}^{\infty} f_{n}(x) d x$ exists, that $\int_{0}^{\infty} f(x) d x$ converges and that these last two quantities are equal.

Since we obviously have $|f(x)| \leq g(x)$ also, it follows that for any interval $[r, s] \subset(0, \infty)$ we have

$$
\begin{gathered}
\left|\int_{r}^{s} f_{n}(x) d x\right| \leq \int_{r}^{s} g(x) d x \\
\left|\int_{r}^{s} f(x) d x\right| \leq \int_{r}^{s} g(x) d x \\
\left|\int_{\tau}^{s} f_{n}(x)-f(x) d x\right| \leq 2 \int_{r}^{s} g(x) d x .
\end{gathered}
$$

Now let $\varepsilon>0$. Choose $a$ and $b$ with $0<a<b<\infty$ so that if $0<c<a<$ $b<d<\infty$, then

$$
\left|\int_{c}^{d} g(x) d x-\int_{0}^{\infty} g(x) d x\right|<\frac{\varepsilon}{2}
$$

It follows in particular that if $d>e>b$ we have

$$
\begin{aligned}
\int_{e}^{d} g(x) d x & =\int_{\frac{a}{2}}^{d} g(x) d x-\int_{\frac{a}{2}}^{e} g(x) d x \\
& \left.\leq \mid \int_{0}^{\infty} g(x) d x-\int_{\frac{a}{2}}^{d} g(x) d x\right) \mid \\
& \quad+\left|\int_{0}^{\infty} g(x) d x-\int_{\frac{a}{2}}^{e} g(x) d x\right| \\
& <\varepsilon .
\end{aligned}
$$

Then for any $d>e>b>r$ and any $n$ we certainly have

$$
\left|\int_{\tau}^{d} f_{n}(x) d x-\int_{r}^{e} f_{n}(x) d x\right|=\left|\int_{d}^{e} f_{n}(x) d x\right| \leq \int_{d}^{e} g(x) d x<\varepsilon
$$

Thus by the Cauchy criterion $\lim _{d \rightarrow \infty} \int_{r}^{d} f_{n}(x) d x$ exists. A similar argument shows that all the improper integrals in question converge. Moreover the argument shows that

$$
\left|\int_{c}^{d} \varphi(x) d x-\int_{0}^{\infty} \varphi(x) d x\right|<\varepsilon
$$

when $0<c<a<b<d$, whether $\varphi(x)=f_{n}(x), \varphi(x)=f(x)$, or $\varphi(x)=g(x)$.

We now merely observe that

$$
\begin{aligned}
\left|\int_{0}^{\infty} f_{n}(x) d x-\int_{0}^{\infty} f(x) d x\right| \leq\left|\int_{0}^{\infty} f_{n}(x) d x-\int_{c}^{d} f_{n}(x) d x\right|+ \\
+\left|\int_{c}^{d}\left[f_{n}(x)-f(x)\right] d x\right|+ \\
+\left|\int_{c}^{d} f(x) d x-\int_{0}^{\infty} f(x) d x\right|
\end{aligned}
$$

Given $\varepsilon>0$ we can choose $c$ and $d$ so that the first and last terms on the right are less than $\frac{\varepsilon}{3}$ (for all $n$, in the case of the first term). Then, since $f_{n}(x) \rightarrow f(x)$ uniformly on $c, d$, we can choose $n_{0}$ so large that the middle term is less than $\frac{\varepsilon}{3}$ if $n>n_{0}$.

Exercise 7.13 Assume that $\left\{f_{n}\right\}$ is a sequence of monotonically increasing functions on $R^{1}$ with $0 \leq f_{n}(x) \leq 1$ for all $x$ and all $n$.

(a) Prove that there is a function $f$ and a sequence $\left\{n_{k}\right\}$ such that

$$
f(x)=\lim _{k \rightarrow \infty} f_{n_{k}}(x)
$$

for every $x \in R^{1}$. (The existence of such a pointwise convergent subsequence is usually called Helly's selection theorem.)

(b) If, moreover $f$ is continuous, prove that $f_{n_{k}} \rightarrow f$ uniformly on $R^{1}$.

Hint: (i) Some subsequence $\left\{f_{n_{i}}\right\}$ converges at all rational points $r$, say to $f(r)$. (ii) Define $f(x)$ for any $x \in R^{1}$ to be $\sup f(r)$, the sup being taken over all $r \leq x$. (iii) Show that $f_{n_{i}}(x) \rightarrow f(x)$ at every $x$ at which $f$ is continuous. (This where monotonicity is strongly used.) (iv) A subsequence of $\left\{f_{n_{i}}\right\}$ converges at every point of discontinuity of $f$ since there are at most countably many such points. This proves $(a)$. To prove $(b)$, modify your proof of $(i i i)$ appropriately.

Solution. (a) Following the hint, we enumerate the rational numbers (or any countable dense set). as $\left\{r_{n}\right\}$ and use the well-known diagonal procedure to get first a subsequence that converges at $r_{1}$, then a further subsequence that converges at $r_{2}$, etc. The sequence formed by taking the $n$th term of the $n$th subsequence is itself a subsequence and converges at each $r_{n}$. (Note that we have not used the fact that $0 \leq f_{n}(x) \leq 1$ for all $x$ and $n$, only the much weaker fact that for each $x$ there is an $M(x)$ such that $\left|f_{n}(x)\right| \leq M(x)$ for all $x$ and $n$.) Let the function $f(x)$ be defined as $f\left(r_{k}\right)=\lim _{n_{i}} f_{n_{i}}\left(r_{k}\right)$ and $f(x)=\sup \left\{f\left(r_{k}\right): r_{k} \leq x\right\}$ for all other $x$. The second definition could be taken as the general one if we wished, since it is consistent with the definition already given at the points $x=r_{k}$.

Since each of the functions is nondecreasing, it is clear that the function $f(x)$ is nondecreasing. By its definition it is continuous from the left. Suppose $f(x)$ is continuous at $x_{0}$. Let $\varepsilon>0$ be given. Choose rational numbers $r$ and $s$ with $r \leq x_{0} \leq s, f\left(x_{0}\right)-\frac{\varepsilon}{4} \leq f(r) \leq f\left(x_{0}\right) \leq f(s) \leq f\left(x_{0}\right)+\frac{\varepsilon}{4}$. Then choose $i_{0}$ so large that $\left|f_{n_{i}}(t)-f(t)\right|<\frac{\varepsilon}{4}$ for all $i>i_{0}, t=r$ or $t=s$. We then have

$$
f\left(x_{0}\right)-\frac{\varepsilon}{2} \leq f(r)-\frac{\varepsilon}{4}<f_{n_{i}}(r) \leq f_{n_{i}}\left(x_{0}\right) \leq f_{n_{i}}(s)<f(s)+\frac{\varepsilon}{4} \leq f\left(x_{0}\right)+\frac{\varepsilon}{2} .
$$

Hence $\left|f\left(x_{0}\right)-f_{n_{i}}\left(x_{0}\right)\right|<\varepsilon$ if $i>i_{0}$, which proves the convergence at points of continuity. One more application of the diagonal procedure now allows us to assure that some subsequence converges at every point (since the set of of discontinuities of a nondecreasing function is countable). We can then modify the definition of $f(x)$ at these points.

The claim that the convergence is uniform if the limit is continuous is not true. Let

$$
f_{n}(x)= \begin{cases}\frac{1}{3}+\frac{x}{1+3|x|}, & \text { if } x \leq n \\ 1, & \text { if } x>n\end{cases}
$$

It is clear that $\lim _{n \rightarrow \infty} f_{n}(x)=\frac{1}{3}+\frac{x}{1+3|x|}$ for each $x$, yet $f_{n}(y)-f(y) \geq \frac{1}{3}$ if $y>n$, so that the convergence is not uniform. Here the functions $f_{n}(x)$ are not continuous, but they could easily be made so without violating the conditions of the problem.

To get uniform convergence we must assume in addition that $f(x) \rightarrow 1$ as $x \rightarrow \infty$ and $f(x) \rightarrow 0$ as $x \rightarrow-\infty$. Let us grant these relations and assume that $f(x)$ is continuous at all points $x$. To simplify the notation we shall write $f_{k}$ instead of $f_{n_{k}}$. Given $\varepsilon>0$, choose an interval $[a, b]$ such that $f(x)<\frac{\varepsilon}{2}$ if $x \leq a$ and $f(x)>1-\frac{\varepsilon}{2}$ if $x>b$. Then, since $f(x)$ is uniformly continuous on $[a, b]$, let $a=t_{0}<t_{1}<\cdots<t_{n}=b$ be such that $f\left(t_{i}\right)-f\left(t_{i-1}\right)<\frac{\varepsilon}{5}$. Choose $k$ so large that $\left|f_{l}\left(t_{i}\right)-f\left(t_{i}\right)\right|<\frac{\varepsilon}{5}$ for all $i=1, \ldots, n$ and all $l>k$.Then for all $y \geq b=t_{n}$ we have

$$
1 \geq f_{l}(y) \geq f_{l}\left(t_{k}\right)>1-\frac{4 \varepsilon}{5}
$$

and

$$
1 \geq f(y)>1-\frac{\varepsilon}{2}>1-\frac{4 \varepsilon}{5} .
$$

Hence certainly

$$
\left|f_{l}(y)-f(y)\right| \leq 1-\left(1-\frac{4 \varepsilon}{5}\right)<\varepsilon
$$

for all $l>k$ and all $y \geq b$.

A similar argument shows that $f_{l}$ converges uniformly to $f$ on $(-\infty, a]$. The argument that $f_{l}$ converges uniformly to $f$ on $\left[t_{i-1}, t_{i}\right]$ is identical to that given above.

Exercise 7.14 Let $f$ be a continuous real function on $R^{1}$ with the following properties: $0 \leq f(t) \leq 1, f(t+2)=f(t)$ for every $t$, and

$$
f(t)= \begin{cases}0 & \left(0 \leq t \leq \frac{1}{3}\right) \\ 1 & \left(\frac{2}{3} \leq t \leq 1\right)\end{cases}
$$

Put $\Phi(t)=(x(t), y(t))$, where

$$
x(t)=\sum_{n=1}^{\infty} 2^{-n} f\left(3^{2 n-1} t\right), \quad y(t)=\sum_{n=1}^{\infty} 2^{-n} f\left(3^{2 n} t\right)
$$

Prove that $\Phi$ is continuous and that $\Phi$ maps $I=[0,1]$ onto the unit square $I^{2} \subset R^{2}$. In fact, show that $\Phi$ maps the Cantor set onto $I^{2}$.

Hint: Each $\left(x_{0}, y_{0}\right) \in I^{2}$ has the form

$$
x_{0}=\sum_{n=1}^{\infty} 2^{-n} a_{2 n-1}, \quad y_{0}=\sum_{n=1}^{\infty} 2^{-n} a_{2 n}
$$

where each $a_{i}$ is 0 or 1 . If

$$
t_{0}=\sum_{i=1}^{\infty} 3^{-i-1}\left(2 a_{i}\right)
$$

show that $f\left(3^{k} t_{0}\right)=a_{k}$, and hence that $x\left(t_{0}\right)=x_{0}, y\left(t_{0}\right)=y_{0}$.

(This simple example of a so-called "space-filling curve" is due to I. J. Schoenberg, Bull. A.M.S., vol. 44, 1938, p. 519.)

Solution. We note that $3^{k} t_{0}$ is the sum of the even integer $2\left(3^{k-2} a_{1}+\cdots+\right.$ $\left.3 a_{k-2}+a_{k-1}\right)$ and a fractional part $\sum_{i=k}^{\infty} \frac{2 a_{i}}{3^{i-k+1}}$. This fractional part lies in $\left[\frac{2}{3}, 1\right]$ if $a_{k}=1$, while if $a_{k}=0$ it is at least 0 and at most $\frac{2}{9}$. Thus it lies in the interval $\left[0, \frac{1}{3}\right]$ if $a_{k}=0$. In either case $f\left(3^{k} t_{0}\right)=a_{k}$, as claimed. We therefore have

$$
x\left(t_{0}\right)=\sum_{n=1}^{\infty} 2^{-n} a_{2 n-1}=x_{0}, \quad y\left(t_{0}\right)=\sum_{n=1}^{\infty} 2^{-n} a_{2 n}=y_{0}
$$

as asserted.

Exercise 7.15 Suppose $f$ is a real continuous function on $R^{1}, f_{n}(t)=f(n t)$ for $n=1,2,3, \ldots$, and $\left\{f_{n}\right\}$ is equicontinuous on $[0,1]$. What conclusion can you draw about $f$ ?

Solution. The function $f(t)$ must be constant on $[0, \infty)$. For if $f(x) \neq f(y)$ and $0 \leq x<y<\infty$, say $|f(x)-f(y)|=\varepsilon>0$, it follows that $\left|f_{n}\left(\frac{x}{n}\right)-f_{n}\left(\frac{y}{n}\right)\right|=\varepsilon$ for all $n$. Since $\frac{x-y}{n} \rightarrow 0$, it follows that the family $\left\{f_{n}\right\}$ cannot be equicontinuous on $[0,1]$, or, indeed, on any neighborhood of 0 .

Exercise 7.16 Suppose $\left\{f_{n}\right\}$ is an equicontinuous sequence of functions on a compact set $K$, and $\left\{f_{n}\right\}$ converges pointwise on $K$. Prove that $\left\{f_{n}\right\}$ converges uniformly on $K$.

Solution. Let $\varepsilon>0$. Choose $\delta>0$ such that $\left|f_{n}(x)-f_{n}(y)\right|<\frac{\varepsilon}{3}$ for all $n \neq m$ if $x, y \in K$ and $|x-y|<\delta$. Choose a finite number of points $x_{1}, \ldots, x_{N}$ such that for every $x \in K$ there exists $j$ with $\left|x-x_{j}\right|<\delta$. (Such a finite set exists; otherwise we could inductively select a sequence $\left\{x_{n}\right\}$ such that $\left|x_{m}-x_{n}\right| \geq \delta$
for all $n$, and this sequence would have no Cauchy subsequence, contradicting the compactness of $K$.) Then choose $n_{0}$ so large that $\left|f_{m}\left(x_{j}\right)-f_{n}\left(x_{j}\right)\right|<\frac{\varepsilon}{3}$ for all $m, n>n_{0}$ and all $j=1,2, \ldots, N$. Then for any point $x \in K$, fix $j$ so that $\left|x-x_{j}\right|<\delta$. If $m, n>n_{0}$ we have

$$
\left|f_{m}(x)-f_{n}(x)\right| \leq\left|f_{m}(x)-f_{m}\left(x_{j}\right)\right|+\left|f_{m}\left(x_{j}\right)-f_{n}\left(x_{j}\right)\right|+\left|f_{n}\left(x_{j}\right)-f_{n}(x)\right| .
$$

The first and last terms are smaller than $\frac{\varepsilon}{3}$ because $\left|x-x_{j}\right|<\delta$; the middle term is smaller than $\frac{\varepsilon}{3}$ since $m, n>n_{0}$. Thus the sequence is a uniformly Cauchy sequence.

Exercise 7.17 Define the notions of uniform convergence and equicontinuity for mappings into any metric space. Show that Theorems 7.9 and 7.12 are valid for mappings into any metric space, that Theorems 7.8 and 7.11 are valid for mappings into any complete metric space, and that Theorems 7.10, 7.16, 7.17, 7.24 , and 7.25 hold for vector-valued functions, that is, for mappings into any $R^{n}$.

Solution. Let $X$ and $Y$ be any metric spaces. The sequence $\left\{f_{n}\right\}$, where $f_{n}$ : $X \rightarrow Y$, converges uniformly to $f: X \rightarrow Y$ if for every $\varepsilon>0$ there exists $N$ such that $d_{Y}\left(f_{n}(x), f(x)\right)<\varepsilon$ for all $x \in X$ and all $n>N$. A family of functions $\mathcal{F}$ is equicontinuous if for every $\varepsilon>0$ there exists $\delta>0$ such that $d_{Y}\left(f\left(x_{1}\right), f\left(x_{2}\right)\right)<\varepsilon$ for all $f \in \mathcal{F}$ whenever $d_{X}\left(x_{1}, x_{2}\right)<\delta$.

An immediate consequence of this definition is that $\left\{f_{n}\right\}$ converges uniformly to $f$ if and only if $M_{n} \rightarrow 0$, where $M_{n}=\sup _{x \in X} d_{Y}\left(f_{n}(x), f(x)\right.$ ) (Theorem 7.9).

The same $\frac{\varepsilon}{3}$ argument that proves Theorem 7.12 shows that the uniform limit of a sequence of continuous functions is continuous.

The Cauchy convergence criterion accepts the additional word uniformly without any change, provided $Y$ is complete. Suppose for every $\varepsilon>0$ there exists $N$ such that $d_{Y}\left(f_{m}(x), f_{n}(x)\right)<\varepsilon$ for all $m, n>N$ and all $x$. Then, in particular, for each $x \in X$, the sequence $\left\{f_{n}(x)\right\}$ is a Cauchy sequence in $Y$. Since $Y$ is complete, this sequence converges to a value that we shall call $f(x)$. We now claim that $\left\{f_{n}\right\}$ converges uniformly to $f$. Indeed, given $\varepsilon>0$ choose $N$ so that $d_{Y}\left(f_{m}(x), f_{n}(x)\right)<\frac{\varepsilon}{2}$ if $m, n>N$. Since a metric is a continuous function, it follows that $d_{Y}\left(f(x), f_{n}(x)\right) \leq \frac{\varepsilon}{2}<\varepsilon$ if $n>N$, that is $\left\{f_{n}\right\}$ converges uniformly to $f$. This is Theorem 7.8.

Suppose now $\left\{f_{n}\right\}$ converges uniformly to $f, Y$ is complete, $x_{0} \in X$, and $\lim _{x \rightarrow x_{0}} f_{n}(x)=A_{n}$ for $n=1,2, \ldots$. Then $\left\{A_{n}\right\}$ converges, and $\lim _{x \rightarrow x_{0}} f(x)=$ $\lim _{n \rightarrow \infty} A_{n}$. (This is Theorem 7.11.) The proof is as follows. Given $\varepsilon>0$ choose $N$ so that $d_{Y}\left(f(x), f_{n}(x)\right)<\frac{\varepsilon}{3}$ for all $x$ if $n \geq N$. Let $n>N$ be fixed. Choose $\delta>0$ (depending on $n$ and $\varepsilon$ in general) such that $d_{Y}\left(f_{n}(x), A_{n}\right)<\frac{\varepsilon}{3}$ if $0<d_{X}\left(x, x_{0}\right)<\delta$. (The fact that $\lim _{x \rightarrow x_{0}} f_{n}(x)=A_{n}$ implies that there must exist $x$ satisfying these inequalities, i.e., that $x_{0}$ is an accumulation point of $X$.) We then have, for $m, n>N$,

$$
d_{Y}\left(A_{m}, A_{n}\right) \leq d_{Y}\left(A_{m}, f_{m}(x)\right)+d_{Y}\left(f_{m}(x), f_{n}(x)\right)+d_{Y}\left(f_{n}(x), f(x)\right)
$$

The middle term is less than $\frac{\varepsilon}{3}$ for all $m, n>N$ and all $x \in X$. If $m$ and $n$ are then fixed integers larger than $N$, the first and last terms can be made smaller than $\frac{\varepsilon}{3}$ by choosing $x$ sufficiently close to $x_{0}$. Hence we have $d_{Y}\left(A_{m}, A_{n}\right)<\varepsilon$ if $m, n>N$. Since $Y$ is complete, the sequence $\left\{A_{n}\right\}$ converges, say to $A$. Now observe that

$$
d_{Y}(f(x), A) \leq d_{Y}\left(f(x), f_{n}(x)\right)+d_{Y}\left(f_{n}(x), A_{n}\right)+d_{Y}\left(A_{n}, A\right)
$$

If $N$ is chosen sufficiently large, the first and last terms on the right-hand side will be less than $\frac{\varepsilon}{3}$ (for all $x$, in the case of the first term). For a fixed $n$ satisfying these conditions, if $\delta>0$ is sufficiently small, the second term will be less than $\frac{\varepsilon}{3}$ whenever $0<d_{X}\left(x, x_{0}\right)<\delta$, and hence $d_{Y}(f(x), A)<\varepsilon$ if $0<d_{X}\left(x, x_{0}\right)<\delta$.

The proof of the stated theorems for vector-valued functions is a consequence of the obvious facts that a vector-valued function $\mathrm{f}$ is integrable, differentiable or continuous if and only if each of its components has the corresponding property, and that a series of vector-valued functions $\left\{f_{n}\right\}$ is Cauchy, bounded, convergent, uniformly convergent, majorized by a convergent sequence, equicontinuous, etc., if and only if each component has those properties. A typical proof proceeds as follows (Theorem 7.25). Suppose $\left\{\mathfrak{f}_{n}\right\}$ is a bounded equicontinuous sequence of vector-valued functions on a compact set $K$. Let $\left\|\mathbf{f}_{n}(x)\right\| \leq M$ for all $x \in K$ and all $n$, and given $\varepsilon>0$ choose $\delta>0$ such that $\left\|\mathfrak{f}_{n}(x)-\mathbf{f}_{n}(y)\right\|<\varepsilon$ whenever $d(x, y)<\delta$. Then for each component $f_{n}^{i}$ of $\mathrm{f}_{n}$ we have $\left|f_{n}^{i}(x)\right| \leq\left\|\mathbf{f}_{n}\right\| \leq M$ and $\left|f_{n}^{i}(x)-f_{n}^{i}(y)\right| \leq\left\|\mathfrak{f}_{n}(x)-f_{n}(y)\right\|<\varepsilon$ whenever $d(x, y)<\delta$. Hence each sequence of components $\left\{f_{n}^{i}\right\}_{n}, i=1, \ldots, k$, is bounded and equicontinuous. Therefore for each $i$ there is a subsequence $\left\{n_{r}\right\}$ such that $f_{n_{r}}^{i}$ converges uniformly. By refining to subsubsequences, we can obtain a single subsequence $\left\{n_{r}\right\}$ such that $\left\{f_{n_{r}}^{i}\right\}$ converges uniformly for all $i$, say to $f^{i}(x)$. Then, given $\varepsilon>0$, choose $r_{0}$ so large that $\left|f_{n_{r}}^{i}(x)-f^{i}(x)\right|<\frac{\varepsilon}{k}$ for $i=1,2, \ldots, k$ and $r>r_{0}$. It then follows that $\left\|\mathbf{f}_{n_{r}}(x)-\mathbf{f}(x)\right\|<\varepsilon$ if $r>r_{0}$. The proofs of the other results all follow this model argument.

Exercise 7.18 Let $\left\{f_{n}\right\}$ be a uniformly bounded sequence of functions which are Riemann integrable on $[a, b]$, and put

$$
F_{n}(x)=\int_{a}^{x} f_{n}(t) d t \quad(a \leq x \leq b)
$$

Prove that there exists a subsequence $\left\{F_{n_{k}}\right\}$ which converges uniformly on $[a, b]$.

Solution. Let $M$ be such that $\left|f_{n}(x)\right| \leq M$ for all $n$ and $x$. Then clearly $\left|F_{n}(x)\right| \leq M(b-a)$ for all $n$, so that $\left\{F_{n}\right\}$ is uniformly bounded. Also, given $\varepsilon>0$, let $\delta=\frac{\varepsilon}{M}$. Then if $x<y$ and $|x-y|<\delta$, we have

$$
\left|F_{n}(y)-F_{n}(x)\right|=\left|\int_{x}^{y} f_{n}(t) d t\right|<M|x-y|<\varepsilon
$$

Hence $\left\{F_{n}\right\}$ is also uniformly equicontinuous. Therefore by Ascoli's Theorem (Theorem 7.25), there exists a uniformly convergent subsequence of $\left\{F_{n}\right\}$.

Exercise 7.19 Let $K$ be a compact metric space, let $S$ be a subset of $\mathcal{C}(K)$. Prove that $S$ is compact (with respect to the metric defined in Section 7.14) if and only if $S$ is uniformly closed, pointwise bounded, and equicontinuous. (If $S$ is not equicontinuous, then $S$ contains a sequence which has no equicontinuous subsequence, hence has no sequence that converges uniformly on $K$.)

Solution. First suppose $S$ is compact. Then we know that $S$ has to be closed and bounded (in this metric bounded means the same thing as uniformly bounded). If $S$ is not equicontinuous, then there exists $\varepsilon>0$ such that for all $\delta>0$ there exist $x, y$ and $g \in S$ such that $d(x, y)<\delta$ and $|g(x)-g(y)| \geq \varepsilon$. Let $x_{n}, y_{n} \in K$ and $g_{n} \in S$ be such that $d\left(x_{n}, y_{n}\right)<\frac{1}{n}$ and $\left|g_{n}\left(x_{n}\right)-g_{n}\left(y_{n}\right)\right| \geq \varepsilon$. Then no subsequence of $\left\{g_{n}\right\}$ can be equicontinuous, since $\mid g_{n_{k}}\left(x_{n_{k}}-g_{n_{k}}\left(y_{n_{k}}\right) \geq \varepsilon\right.$. Hence by Theorem 7.24 no subsequence of $\left\{g_{n}\right\}$ can converge in $\mathcal{C}(K)$, and so $S$ cannot be compact. We conclude, then, that if $S$ is compact, then $S$ is closed, bounded, and equicontinuous.

Conversely, if $S$ is closed, pointwise bounded, and equicontinuous, then every sequence $\left\{g_{n}\right\}$ contains a subsequence that converges uniformly, hence converges in the metric of $\mathcal{C}(K)$ (by Ascoli's theorem). Since $S$ is closed, the limit belongs to $S$, and so $S$ is compact by Exercise 26 of Chapter 2 .

Exercise 7.20 If $f$ is continuous on $[0,1]$ and if

$$
\int_{0}^{1} f(x) x^{n} d x=0 \quad(n=0,1,2 \ldots)
$$

prove that $f(x)=0$ on $[0,1]$. Hint: The integral of the product of $f$ with any polynomial is zero. Use the Weierstrass theorem to show that $\int_{0}^{1} f^{2}(x) d x=0$.

Solution. There exists a sequence of polynomials $p_{n}(x)$ such that $p_{n}(x)$ converges uniformly to $f(x)$. Since $f$ is bounded, $\left\{p_{n}\right\}$ is uniformly bounded, and hence $p_{n} f$ converges uniformly to $f^{2}$. Then by Theorem 7.16

$$
\int_{0}^{1} f^{2}(x) d x=\lim _{n \rightarrow \infty} \int_{0}^{1} p_{n}(x) f(x) d x=0
$$

But we know already (Exercise 2 of Chapter 6 ) that this implies $f^{2}(x) \equiv 0$.

Exercise 7.21 Let $K$ be the unit circle in the complex plane (i.e., the set of all $z$ with $|z|=1$ ), and let $\mathcal{A}$ be the algebra of all functions of the form

$$
f\left(e^{i \theta}\right)=\sum_{n=0}^{N} c_{n} e^{i n \theta}(\theta \text { real })
$$

The $\mathcal{A}$ separates points on $K$, and $\mathcal{A}$ vanishes at no point of $\dot{K}$, but nevertheless there are continuous functions on $K$ which are not in the uniform closure of $\mathcal{A}$. Hint: For every $f \in \mathcal{A}$

$$
\int_{0}^{2 \pi} f\left(e^{i \theta}\right) e^{i \theta} d \theta=0
$$

and this is also true for every $f$ in the closure of $\mathcal{A}$.

Solution. The function $f(z)=z \in \mathcal{A}$ separates points on $K$ and never vanishes. The equality given in the hint is a straighforward computation. It implies that the continuous function $\frac{1}{z}$, which is $e^{-i \theta}$, is not in the uniform closure of $\mathcal{A}$, since

$$
\int_{0}^{2 \pi} e^{-i \theta} e^{i \theta} d \theta=2 \pi
$$

Exercise 7.22 Assume $f \in \mathcal{R}(\alpha)$ on $[a, b]$, and prove that there are polynomials $P_{n}$ such that

$$
\lim _{n \rightarrow \infty} \int_{a}^{b}\left|f-P_{n}\right|^{2} d \alpha=0
$$

(Compare with Exercise 12, Chap. 6.)

Solution. The parenthetical remark refers to the proof that there is a sequence of continuous functions $\left\{f_{n}\right\}$ such that

$$
\lim _{n \rightarrow \infty} \int_{a}^{b}\left|f-f_{n}\right|^{2} d \alpha=0
$$

All that is now needed is to note that one can find polynomials $P_{n}$ such that $\left|f_{n}(x)-P_{n}(x)\right|<\frac{1}{n}$ for all $x \in[a, b]$ and all $n$.

Exercise 7.23 Put $P_{n}=0$, and define, for $n=0,1,2, \ldots$,

$$
P_{n+1}(x)=P_{n}(x)+\frac{x^{2}-P_{n}^{2}(x)}{2}
$$

Prove that

$$
\lim _{n \rightarrow \infty} P_{n}(x)=|x|
$$

uniformly on $[-1,1]$.

(This makes it possible to prove the Stone-Weierstrass theorem without first proving Theorem 7.26.

Hint: Use the identity

$$
|x|-P_{n+1}(x)=\left[|x|-P_{n}(x)\right]\left[1-\frac{|x|+P_{n}(x)}{2}\right]
$$

to prove that $0 \leq P_{n}(x) \leq P_{n+1}(x) \leq|x|$ if $|x| \leq 1$, and that

$$
|x|-P_{n}(x) \leq|x|\left(1-\frac{|x|}{2}\right)^{n}<\frac{2}{n+1}
$$

if $|x| \leq 1$.

Solution. The identity given in the hint is a trivial consequence of the identity $x^{2}-P_{n}^{2}(x)=\left[|x|-P_{n}(x)\right]\left[|x|+P_{n}(x)\right]$. Then, granting that $0 \leq P_{n}(x) \leq|x|$, we conclude that $0 \leq 1-\frac{|x|+P_{n}(x)}{2}<1$ for $|x| \leq 1$, and hence that $0 \leq$ $|x|-P_{n+1}(x) \leq|x|-P_{n}(x)$, which gives all of the desired inequalities. An immediate corollary of the same identity (obtained by replacing $P_{n}(x)$ by 0 in the second factor on the right-hand side) is

$$
|x|-P_{n+1}(x) \leq\left[|x|-P_{n}(x)\right]\left(1-\frac{|x|}{2}\right),
$$

and this inequality makes it possible to obtain the inequality

$$
|x|-P_{n}(x) \leq|x|\left(1-\frac{|x|}{2}\right)^{n}
$$

by induction on $n$. Finally, by symmetry, the maximum of $|x|\left(1-\frac{|x|}{2}\right)^{n}$ on $[-1,1]$ is its maximum on $[0,1]$, and this can be found by simple calculus to occur at $x=\frac{2}{n+1}$. Since this function is always less than $|x|$, the final inequality now follows.

Exercise 7.24 Let $X$ be a metric space, with metric $d$. Fix a point $a \in X$. Assign to each $p \in X$ the function $f_{p}$ defined by

$$
f_{p}(x)=d(x, p)-d(x, a) \quad(x \in X)
$$

Prove that $\left|f_{p}(x)\right| \leq d(a, p)$ for all $x \in X$, and therefore $f_{p} \in \mathcal{C}(X)$. Prove that

$$
\left\|f_{p}-f_{q}\right\|=d(p, q)
$$

for all $p, q \in X$.

If $\Phi(p)=f_{p}$, it follows that $\Phi$ is an isometry (a distance-preserving mapping) of $X$ onto $\Phi(X) \subset \mathcal{C}(X)$.

Let $Y$ be the closure of $\Phi(X)$ in $\mathcal{C}(X)$. Show that $Y$ is complete.

Conclusion: $X$ is isometric to a dense subset of a complete metric space $Y$. (Exercise 24, Chap. 3 contains a different proof of this.)

Solution. The inequality $\left|f_{p}(x)\right| \leq d(a, p)$ is well-known, i.e., the fact that

$$
|d(x, p)-d(x, a)| \leq d(a, p)
$$

and follows from the triangle inequality by merely transposing a term. (The left-hand side is either $d(x, p)-d(x, a)$ or $d(x, a)-d(x, p)$. Whichever is the case, if the subtracted term is moved to the other side, we have the ordinary triangle inequality.)

As for the isometry, we certainly have, for all $x$,

$$
\left|f_{q}(x)-f_{p}(x)\right|=|d(x, q)-d(x, p)| \leq d(p, q)
$$

and equality holds here if $x=q$ or $x=p$. Hence the supremum over all $x$ is exactly $d(p, q)$.

As for the closure $Y$ of $\Phi(X)$ being complete, it is a closed subset of a complete metric space, hence necessarily complete. By definition of closure, $\Phi(X)$ is dense in $Y$.

Exercise 7.25 Suppose $\phi$ is a continuous bounded real function in the strip defined by $0 \leq x \leq 1,-\infty<y<\infty$. Prove that the initial-value problem

$$
y^{\prime}=\phi(x, y), \quad y(0)=c
$$

has a solution. (Note that the hypotheses of this existence theorem are less strigent than those of the corresponding uniqueness theorem; see Exercise 27, Chap. 5.)

Hint: Fix $n$. For $i=0, \ldots, n$; put $x_{i}=i / n$. Let $f_{n}$ be a continuous function on $[0,1]$ such that $f_{n}(0)=c$,

$$
f_{n}^{\prime}(t)=\phi\left(x_{i}, f_{n}\left(x_{i}\right)\right) \quad \text { if } x_{i}<t<x_{i+1}
$$

and put

$$
\Delta_{n}(t)=f_{n}^{\prime}(t)-\phi\left(t, f_{n}(t)\right)
$$

except at the points $x_{i}$, where $\Delta_{n}(t)=0$. Then

$$
f_{n}(x)=c+\int_{0}^{x}\left[\phi\left(t, f_{n}(t)\right)+\Delta_{n}(t)\right] d t
$$

Choose $M<\infty$ so that $|\phi| \leq M$. Verify the following assertions.

(a) $\left|f_{n}^{\prime}\right| \leq M,\left|\Delta_{n}\right| \leq 2 M, \Delta_{n} \in \mathcal{R}$, and $\left|f_{n}\right| \leq|c|+M=M_{1}$, say, on $[0,1]$ for all $n$.

(b) $\left\{f_{n}\right\}$ is equicontinuous on $[0,1]$, since $\left|f_{n}^{\prime}\right| \leq M$.

(c) Some $\left\{f_{n_{k}}\right\}$ converges to some $f$, uniformly on $[0,1]$.

(d) Since $\phi$ is uniformly continuous on the rectangle $0 \leq x \leq 1,|y| \leq M_{1}$,

$$
\phi\left(t, f_{n_{k}}(t)\right) \rightarrow \phi(t, f(t))
$$

uniformly on $[0,1]$.
(e) $\Delta_{n}(t) \rightarrow 0$ uniformly on $[0,1]$, since

$$
\Delta_{n}(t)=\phi\left(x_{i}, f_{n}\left(x_{i}\right)\right)-\phi\left(t, f_{n}(t)\right)
$$

in $\left(x_{i}, x_{i+1}\right)$.

(f) Hence

$$
f(x)=c+\int_{0}^{x} \phi(t, f(t)) d t
$$

This $f$ is a solution of the given problem.

Solution. It will save trouble if we assume that $\phi$ is a bounded continuous mapping from $[0,1] \times R^{k}$ into $R^{k}$ and that $c$ is a vector in $R^{k}$. That way we can do Exercise 26 simultaneously with this one. Since we are defining the functions $f_{n}(t)$ to be piecewise-linear, there is no difficulty in doing this with vector-valued functions. We simply define $f_{n}(t)=c+t \phi(0, c)$ for $0 \leq t \leq x_{1}$, and then, by induction on $i$,

$$
f_{n}(t)=f_{n}\left(x_{i}\right)+\left(t-x_{i}\right) \phi\left(x_{i}, f_{n}\left(x_{i}\right)\right)
$$

for $x_{i}<t \leq x_{i+1}$.

Then, if $\Delta_{n}(t)$ is defined as indicated, we have $f_{n}^{\prime}(t)=\Delta_{n}(t)+\phi\left(t, f_{n}(t)\right)$ except at a finite set of points, and therefore

$$
f_{n}(x)=f_{n}(0)+\int_{0}^{x}\left[\phi\left(t, f_{n}(t)\right)+\Delta_{n}(t)\right] d t
$$

(a) The assertions $\left|f_{n}^{\prime}\right| \leq M$ and $\left|\Delta_{n}\right| \leq 2 M$ are immediate consequences of the definitions of these two functions and the fact that $|\phi(x, y)| \leq M$ for all $x$ and $y$ (here in general $y \in R^{k}$ ). Since $\Delta_{n}(t)$ is bounded and continuous except at $x_{i}$, it is Riemann-integrable. The inequality $\left|f_{n}\right| \leq|c|+M=M_{1}$ is then immediate.

(b) $\left|f_{n}(x)-f_{n}(y)\right| \leq \int_{x}^{y}\left|f_{n}^{\prime}(t)\right| d t \leq M|x-y|$.

(c) This is Ascoli's Theorem (Theorem 7.25).

(d) Given $\dot{\varepsilon}>0$ let $\delta>0$ be such that $|\phi(t, y)-\phi(t, z)|<\varepsilon$ if $|y-z|<\delta$, for all $t \in[0,1]$, and $y, z \in R^{k}$. Then if $\left|f_{n_{k}}(t)-f(t)\right|<\delta$ for all $t$ (which is the case if $k$ is large), we have $\left|\phi\left(t, f_{n_{k}}(t)\right)-\phi(t, f(t))\right|<\varepsilon$ for all $t$.

(e) For each $t$ and $n$ let $i(n)$ be chosen so that $t \in\left[x_{i(n)}, x_{i(n)+1}\right]$, so that $\left|t-x_{i(n)}\right| \leq \frac{1}{n}$. Since $f_{n_{k}}(t)$ converges uniformly to $f(t)$ and $x_{i(n)} \rightarrow t$, it follows that $\phi\left(x_{i(n)}, f_{n}\left(x_{i(n)}\right)\right)-\phi\left(t, f_{n}(t)\right) \rightarrow 0$.

$(f)$ We now invoke Theorem 7.16 to get

$$
f(x)=c+\int_{0}^{x} \phi(t, f(t)) d t
$$

Clearly $f(0)=c$, and since the right-hand side has a continuous derivative, so does the left-hand side, and $f^{\prime}(x)=\phi(x, f(x))$.

Exercise 7.26 Prove an analogous existence theorem for the initial-value problem

$$
\mathbf{y}^{\prime}=\Phi(x, \mathbf{y}), \quad \mathbf{y}(0)=\mathbf{c}
$$

where now $\mathrm{c} \in R^{k}, \mathrm{y} \in R^{k}$, and $\Phi$ is a continuous bounded mapping of the part of $R^{k+1}$ defined by $0 \leq x \leq 1, \mathrm{y} \in R^{k}$ into $R^{k}$. (Compare Exercise 28, Chap. 5.) Hint: Use the vector-valued version of Theorem 7.25 .

Solution. Since we were foresightful enough to make all the necessary notes in the solution of the previous problem, there is nothing to be done. Observe that an $k$-th order initial-value problem

$$
y^{(k)}=\phi\left(x, y, y^{\prime}, y^{\prime \prime}, \ldots, y^{(k-1)}\right)
$$

with $y(0)=c_{0}, y^{\prime}(0)=c_{1}, \ldots, y^{(k-1)}(0)=c_{k-1}$ falls under this theorem if we let

$$
\Phi\left(x, y_{1}, y_{2}, \ldots, y_{k}\right)=\left(y_{2}, y_{3}, \ldots, y_{k}, \phi\left(x, y_{1}, \ldots, y_{k-1}\right)\right)
$$

$\mathbf{y}(0)=\left(c_{0}, \ldots, c_{k-1}\right)$ Any solution of this problem provides a solution of the $k$-th order equation (namely $y_{1}$ if $\mathbf{y}=\left(y_{1}, \ldots, y_{k}\right)$ ).

## Chapter 8

## Some Special Functions

Exercise 8.1 Define

$$
f(x)= \begin{cases}e^{-1 / x^{2}} & (x \neq 0) \\ 0 & (x=0)\end{cases}
$$

Prove that $f$ has derivatives of all orders at $x=0$ and that $f^{(n)}(0)=0$ for $n=1,2,3, \ldots$.

Solution. We have $\lim _{x \rightarrow 0} x^{k} e^{-1 / x^{2}}=0$ for all $k=0, \pm 1, \pm 2, \ldots$ by L'Hospital's rule. It is easily shown by induction that there is a polynomial $p_{n}$ such that $f^{(n)}(x)=p_{n}\left(\frac{1}{x}\right) e^{-1 / x^{2}}$ for $x \neq 0$. Assuming (by induction) that $f^{(n)}(0)=0$, we then have $f^{(n+1)}(0)=\lim _{x \rightarrow 0} q_{n}\left(\frac{1}{x}\right) e^{-1 / x^{2}}=0$, where $q_{n}(x)=x p_{n}(x)$.

Exercise 8.2 Let $a_{i j}$ be the number in the $i$ th row and $j$ th column of the array

$$
\begin{array}{ccccc}
-1 & 0 & 0 & 0 & \ldots \\
\frac{1}{2} & -1 & 0 & 0 & \ldots \\
\frac{1}{4} & \frac{1}{2} & -1 & 0 & \ldots \\
\frac{1}{8} & \frac{1}{4} & \frac{1}{2} & -1 & \ldots
\end{array}
$$

so that

$$
a_{i j}= \begin{cases}0 & (i<j) \\ -1 & (i=j) \\ 2^{j-i} & (i>j)\end{cases}
$$

Prove that

$$
\sum_{i} \sum_{j} a_{i j}=-2, \quad \sum_{j} \sum_{i}=0 .
$$

Solution. This is a routine computation:

$$
\begin{aligned}
\sum_{i} \sum_{j} a_{i j} & =\sum_{i=1}^{\infty}\left[-1+\sum_{j=1}^{i-1} 2^{j-i}\right] \\
& =\sum_{i=1}^{\infty}\left[-1+\left(1-2^{1-i}\right)\right] \\
& =\sum_{i=1}^{\infty}-2^{1-i}=-2
\end{aligned}
$$

while

$$
\begin{aligned}
\sum_{j} \sum_{i} a_{i j} & =\sum_{j=1}^{\infty}\left[-1+\sum_{i=j+1}^{\infty} 2^{j-i}\right] \\
& =\sum_{j=1}^{\infty}[-1+1] \\
& =0 .
\end{aligned}
$$

Exercise 8.3 Prove that

$$
\sum_{i} \sum_{j} a_{i j}=\sum_{j} \sum_{i} a_{i j}
$$

if $a_{i j} \geq 0$ for all $i$ and $j$ (the case $+\infty=+\infty$ may occur).

Solution. In fact the only case that we need to consider is the case when one of the two sums is infinite. If either sum is finite, we merely invoke Theorem 8.3, which explicitly states that the two sums are equal. Hence if either sum is infinite, then both are.

Exercise 8.4 Prove the following limit relations:
(a) $\lim _{x \rightarrow 0} \frac{b^{x}-1}{x}=\log b \quad(b>0)$.
(b) $\lim _{x \rightarrow 0} \frac{\log (1+x)}{x}=1$.
(c) $\lim _{x \rightarrow 0}(1+x)^{\frac{1}{x}}=e$.
(d) $\lim _{n \rightarrow \infty}\left(1+\frac{x}{n}\right)^{n}=e^{x}$.

Solution. (a) Consider the function $f(x)=b^{x}=e^{x \log b}$. The limit we are considering is $f^{\prime}(0)$. By the chain rule

$$
f^{\prime}(x)=e^{x \log b} \log b
$$

Now take $x=0$.

(b) Let $y=\log (1+x)$, so that $x=e^{y}-1$. It is easy to justify the relation

$$
\lim _{x \rightarrow 0} \frac{\log (1+x)}{x}=\lim _{y \rightarrow 0} \frac{y}{e^{y}-1}=\frac{1}{\lim _{y \rightarrow 0} \frac{1}{\frac{e^{y}-1}{y}}}=1
$$

since $\lim _{y \rightarrow 0} \frac{e^{y}-1}{y}=E^{\prime}(0)$.

(c) Consider the function $(1+x)^{1 / x}=e^{\frac{\log (1+x)}{x}}$. By part $(b) \lim _{x \rightarrow 0}(1+x)^{\frac{1}{x}}=$ $e^{1}=e$.

(d) As above, we have $\left(1+\frac{x}{n}\right)^{n}=\left[\left(1+\frac{x}{n}\right)^{1 /(x / n)}\right]^{x}$, and by part (c) the limit of the expression inside the brackets is $e$.

Exercise 8.5 Find the following limits

(a) $\lim _{x \rightarrow 0} \frac{e-(1+x)^{1 / x}}{x}$.

(b) $\lim _{n \rightarrow \infty} \frac{n}{\log n}\left[n^{1 / n}-1\right]$.

(c) $\lim _{x \rightarrow 0} \frac{\tan x-x}{x(1-\cos x)}$.

(d) $\lim _{x \rightarrow 0} \frac{x-\sin x}{\tan x-x}$.

Solution. (a) This limit is $f^{\prime}(0)$, where $f(x)=(1+x)^{1 / x}$ (by part $(c)$ of the previous problem). Now for $x \neq 0$, we have

$$
f^{\prime}(x)=(1+x)^{1 / x}\left[\frac{(1+x) \log (1+x)-x}{x^{2}(x+1)}\right]
$$

Since we know that the limit of the first factor is $e$, we need only consider the limit inside the brackets. Since

$$
(1+x) \log (1+x)=\left(x-\frac{x^{2}}{2}+\cdots\right)+x\left(x-\frac{x^{2}}{2}+\cdots\right)
$$

we can cancel $x^{2}$ from the numerator and denominator of the expression in brackets, and we see that the limit of this expression is $\frac{1}{2}$. Hence the limit of $f^{\prime}(x)$ as $x \rightarrow 0$ exists and equals $\frac{e}{2}$. It then follows from the mean-value theorem that $f^{\prime}(0)$ equals this limit (see the corollary to Theorem 5.12).

(b) Write this expression as

$$
\frac{e^{\frac{\log n}{n}}-1}{\frac{\log n}{n}}
$$

Since $\frac{\log n}{n}$ tends to 0 as $n \rightarrow \infty$, this fraction tends to the derivative of $e^{x}$ at 0 , i.e., it tends to 1 .
(c) Write this expression as

$$
\frac{\sin x-x \cos x}{x \cos x(1-\cos x)}
$$

We can then use either Maclaurin series or L'Hospital's rule to prove that the limit is $\frac{2}{3}$.

(d) Write this expression as

$$
\frac{(x-\sin x) \cos x}{\sin x-x \cos x}
$$

and again either by Maclaurin series or L'Hospital's rule the limit is $\frac{1}{2}$.

Exercise 8.6 Suppose $f(x) f(y)=f(x+y)$ for all real $x$ and $y$.

(a) Assuming that $f$ is differentiable and not zero, prove that

$$
f(x)=e^{c x}
$$

where $c$ is a constant.

(b) Prove the same thing, assuming only that $f$ is continuous.

Solution. (a) Since $f$ is not 0 , it follows that $f(0)=1$ (take $x=y=0$ in the basic relation that defines $f$ ). It then follows that $f^{\prime}(x)=f(x) f^{\prime}(0)$, and hence that the function $g(x)=e^{-x f^{\prime}(0)} f(x)$ satisfies $g^{\prime}(0)=0$ for all $x$. Therefore $g(x)=g(0)=f(0)=1$ for all $x$, i.e., $f(x)=e^{c x}$, where $c=f^{\prime}(0)$.

(b) The relation $f(x) f(y)=f(x+y)$ shows that either $f(x)$ is always zero, or it is never zero. In the latter case, since $f$ is continuous, it cannot change sign, and therefore (since $f(0)=1$ ) it is always positive. Let $g(x)=\log [f(x)]$. Then $g(x+y)=g(x)+g(y)$, and $g$ is continuous. It suffices then to show that $g(x)=c x$ for some constant $c=g(1)$. To this end, we note that the additive property of $g$ implies that $g(0)=0, g(-x)=-g(x)$, and (by an easy induction) $g(n x)=n g(x)$ for all integers $n=0, \pm 1, \pm 2, \ldots$. Consider the set of $x$ such that $g(x)=g(1) x$. Obviously 0 and 1 belong to this set. If $a$ belongs to this set, so does $n a$ for any $n$, since $g(n a)=n g(a)=n g(1) a=g(1)(n a)$. Finally, if $a$ belongs to this set, so does $\frac{a}{n}, n=1,2, \ldots$, since $g(a)=g\left(n \frac{a}{n}\right)=n g\left(\frac{a}{n}\right)$. That is, $g\left(\frac{a}{n}\right)=\frac{1}{n} g(a)=\frac{1}{n} g(1) a=g(1) \frac{a}{n}$. It now follows that $r$ belongs to this set for all rational numbers $r$, that is, the two continuous functions $g(x)$ and $g(1) x$ have the same values at all rational numbers $r$. Since the rational numbers are dense, and the set of points at which two continuous functions are equal is a closed set, it follows that $g(x)^{\prime}=g(1) x$ for all $x$.

Exercise 8.7 If $0<x<\frac{\pi}{2}$, prove that

$$
\frac{2}{\pi}<\frac{\sin x}{x}<1
$$

Solution. To show the left-hand inequality, consider the function $f(x)=\sin x-$ $\frac{2 x}{\pi}$ on the interval $0 \leq x \leq \frac{\pi}{2}$. We have $f(0)=f\left(\frac{\pi}{2}\right)=0$. Since $f^{\prime \prime}(x)=$ $-\sin x \leq 0$, the function $f^{\prime}(x)$ is strictly decreasing on this interval. Therefore it has at most one zero on this interval; by Rolle's theorem, it has exactly one zero. Since $f^{\prime \prime}(x)<0$ at that point, the function $f(x)$ has a maximum at that point. Therefore $f(x)>0$ for $0<x<\frac{\pi}{2}$.

The proof of the right-hand inequality is similar, but easier. The function $g(x)=x-\sin x$ has derivative $1-\cos x$, which is nonnegative. Therefore $g(x)$ is strictly increasing, and so $g(x)>g(0)=0$ for all $x>0$ (the restriction $x<\frac{\pi}{2}$ is superfluous in this case).

Exercise 8.8 For $n=0,1,2, \ldots$, and $x$ real, prove that

$$
|\sin n x| \leq n|\sin x|
$$

Note that this inequality may be false for other values of $n$. For instance,

$$
\left|\sin \frac{1}{2} \pi\right|>\frac{1}{2}|\sin \pi| \text {. }
$$

Solution. The inequality is obvious if $n=0$ or $n=1$. Then by induction we have

$$
\begin{aligned}
|\sin n x| & =|\sin ((n-1) x+x)| \\
& =|\sin ((n-1) x) \cos x+\cos ((n-1) x) \sin x| \\
& \leq|\sin ((n-1) x)|+|\sin x| \\
& \leq(n-1)|\sin x|+|\sin x|=|n||\sin x| .
\end{aligned}
$$

A stronger remark can be made: If $c$ is not an integer, then $|\sin c \pi|>$ $|c||\sin \pi|$. Hence this inequality fails for $x=\pi$ unless $c$ is an integer.

Exercise 8.9 (a) Put $s_{N}=1+\left(\frac{1}{2}\right)+\cdots+(1 / N)$. Prove that

$$
\lim _{N \rightarrow \infty}\left(s_{N}-\log N\right)
$$

exists. (The limit, often denoted by $\gamma$, is called Euler's constant. Its numerical value is $0.5772 \ldots$. It is not known whether $\gamma$ is rational or not.)

(b) Roughly how large must $m$ be so that $N=10^{m}$ satisfies $s_{N}>100$ ?

Solution. (a) We observe that $\log (N+1)-\log N=\int_{N}^{N+1} \frac{1}{t} d t$, so that $\left(s_{N+1}-\right.$ $\log (N+1))-\left(s_{N}-\log N\right)=\frac{1}{N+1}-\int_{N}^{N+1} \frac{1}{t} d t<0$. Thus the sequence is a decreasing sequence. On the other hand, it consists of positive numbers, since
$\log N=\int_{1}^{N} \frac{1}{t} d t<1+\frac{1}{2}+\cdots+\frac{1}{N-1}<s_{N}$. It follows that the sequence must converge to a nonnegative number $\gamma$.

(b) The answer here depends on how "rough". an estimate is desired. We observe that $s_{10^{N+1}}-s_{10^{N}}$. lies between $9 \cdot 10^{N}\left(\frac{1}{10^{N+1}}\right)$ and $9 \cdot 10^{N}\left(\frac{1}{10^{N}}\right)$, i.e., between 0.9 and 9 . Hence by an easy induction $0.9 N<s_{10^{N}}<9 N$. Thus $m=112$ will certainly work, and $m$ must be at least 12 .

Exercise 8.10 Prove that $\sum 1 / p$ diverges; the sum extends over all primes.

(This shows that the primes form a fairly substantial subset of the positive integers.)

Hint: Given $N$, let $p_{1}, \ldots, p_{k}$ be those primes that divide at least one integer $\leq N$. Then

$$
\begin{aligned}
\sum_{n=1}^{N} \frac{1}{n} & \leq \prod_{j=1}^{k}\left(1+\frac{1}{p_{j}}+\frac{1}{p_{j}^{2}}+\cdots\right) \\
& =\prod_{j=1}^{k}\left(1-\frac{1}{p_{j}}\right)^{-1} \\
& \leq \exp \sum_{j=1}^{k} \frac{2}{p_{j}}
\end{aligned}
$$

The last inequality holds because

$$
(1-x)^{-1} \leq e^{2 x}
$$

if $0 \leq x \leq \frac{1}{2}$.

(There are many proofs of this result. See, for instance; the article by I. Niven in Amer. Math. Monthly, vol. 78, 1971, pp. 272-273, and the one by R. Bellman in Amer. Math. Monthly, vol. 50, 1943, pp. 318-319.)

Solution. We observe that the primes $p_{1}, \ldots, p_{k}$ form the set of all primes not greater than $N$. Each of them is at least 2, and therefore each integer from 1 to $N$ is a unique product of the form $p_{1}^{e_{1}} \cdots p_{k}^{e_{k}}$ for nonnegative integers $e_{j}$, $0 \leq e_{j} \leq \log _{2} N$. For simplicity let $m$ be the greatest integer in $\log _{2} N$. Then certainly

$$
\begin{aligned}
\sum_{n=1}^{N} \frac{1}{n} & \leq \sum_{e_{1}, \ldots, e_{k}=0}^{m} \frac{1}{p_{1}^{e_{1}} \cdots p_{k}^{e_{k}}} \\
& =\prod_{j=1}^{k}\left(1+\frac{1}{p_{j}}+\cdots+\frac{1}{p_{j}^{m}}\right)
\end{aligned}
$$

$$
\begin{aligned}
& =\prod_{j=1}^{k}\left(\frac{1-p_{j}^{-m-1}}{1-\frac{1}{p_{j}}}\right) \\
& \leq \prod_{j=1}^{k}\left(\frac{1}{1-\frac{1}{p_{j}}}\right) \\
& =\prod_{j=1}^{k}\left(1-\frac{1}{p_{j}}\right)^{-1} \\
& \leq \exp \left(\sum_{j=1}^{k} \frac{2}{p_{j}}\right) .
\end{aligned}
$$

To establish the inequality $(1-x)^{-1} \leq e^{2 x}$ on $\left[0, \frac{1}{2}\right]$, we simply observe that the function $f(x)=(1-x) e^{2 x}$ has derivative $(1-2 x) e^{2 x}$, which is positive on this interval. Hence the smallest value this function has on the interval is its value at $x=0$, which is 1 .

-We have now established the inequality

$$
\sum_{j=1}^{k} \frac{1}{p_{j}} \geq \frac{1}{2} \log \left(\sum_{n=1}^{N} \frac{1}{n}\right)
$$

for any integer $N$ less than $p_{k+1}$. Since the right-hand side of this inequality tends to $\infty$, so does the left.

Exercise 8.11 Suppose $f \in \mathcal{R}$ on $[0, A]$ for all $A<\infty$, and $f(x) \rightarrow 1$ as $x \rightarrow+\infty$. Prove that

$$
\lim _{t \rightarrow 0} \int_{0}^{\infty} e^{-t x} f(x) d x=1 \quad(t>0)
$$

Solution. We first observe that the improper integral converges absolutely for all $t>0$, since

$$
\int_{R}^{S} e^{-t x}|f(x)| d x \leq \frac{M}{t}\left(e^{-R t}-e^{-S t}\right) \rightarrow 0
$$

where $M=\sup _{x \geq R}|f(x)|$, as $R, S \rightarrow \infty$.

We also note that

$$
t \int_{0}^{\infty} e^{-t x} f(x) d x=\int_{0}^{\infty} e^{-u} f\left(\frac{u}{t}\right) d u
$$

and this last improper integral also converges for all $t>0$. Hence we have

$$
\begin{aligned}
\left|t \int_{0}^{\infty} e^{-t x} f(x) d x-1\right| & =\left|\int_{0}^{\infty} e^{-u} f\left(\frac{u}{t}\right) d u-1\right| \\
& \leq \int_{0}^{\infty} e^{-u}\left|f\left(\frac{u}{t}\right)-1\right| d x
\end{aligned}
$$

Since $f(x)$ has a limit at infinity and $f(x)$ is Riemann- integrable on $[0,1]$, it follows that $f(x) \leq K$ for some constant $K$ and all $x$. Thus for any $\eta>0$ we have

$$
\begin{aligned}
\left|t \int_{0}^{\infty} e^{-t x} f(x) d x-1\right| & \leq(K+1) \int_{0}^{\eta} e^{-u} d u+\int_{\eta}^{\infty}\left|f\left(\frac{u}{t}\right)-1\right| d u \\
& \leq \eta(K+1)+M_{\eta, t}
\end{aligned}
$$

where $M_{\eta, t}=\sup _{z \geq \frac{\eta}{t}}|f(z)-1|$. Hence, given $\varepsilon>0$ we take $\eta=\frac{\varepsilon}{2(K+1)}$. We then choose $X>0$ so large that $|f(z)-1|<\frac{\varepsilon}{2}$ if $z>X$, and we let $\delta=\frac{\eta}{X}$. It then follows that $M_{\eta, t}<\frac{\varepsilon}{2}$ if $0<t<\delta$.

Exercise 8.12 Suppose $0<\delta<\pi, f(x)=1$ if $|x| \leq \delta, f(x)=0$ if $\delta<|x|<\pi$, and $f(x+2 \pi)=f(x)$ for all $x$.

(a) Compute the Fourier coefficients of $f$.

(b) Conclude that

$$
\sum_{n=1}^{\infty} \frac{\sin (n \delta)}{n}=\frac{\pi-\delta}{2} \quad(0<\delta<\pi)
$$

(c) Deduce from Parseval's theorem that

$$
\sum_{n=1}^{\infty} \frac{\sin ^{2}(n \delta)}{n^{2} \delta}=\frac{\pi-\delta}{2}
$$

(d) Let $\delta \rightarrow 0$, and prove that

$$
\int_{0}^{\infty}\left(\frac{\sin x}{x}\right)^{2} d x=\frac{\pi}{2}
$$

(e) Put $\delta=\pi / 2$ in $(c)$. What do you get?

Solution. (a) Since $f(x)$ is an even real-valued function, it makes sense to use the real form of the Fourier series, since symmetry shows that $b_{n}=0$ for all $n$. Then $a_{0}=\frac{2 \delta}{\pi}$, and for $n \geq 1$ we have $a_{n}=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos n x d x=\frac{2}{\pi} \int_{0}^{\delta} \cos n x d x=$ $\frac{2 \sin n \delta}{\pi n}$.

(b) Since $f(x)$ satisfies the Lipschitz condition of Theorem 8.14 at $x=0$, it follows that the series actually converges to $f(0)$ at that point, i.e.,

$$
\frac{\delta}{\pi}+\frac{2}{\pi} \sum_{n=1}^{\infty} \frac{\sin (n \delta)}{n}=1
$$

so that

$$
\sum_{n=1}^{\infty} \frac{\sin (n \delta)}{n}=\frac{\pi-\delta}{2}
$$

(c) Parseval's theorem now implies that

$$
\frac{2 \delta}{\pi}=\frac{1}{\pi} \int_{-\delta}^{\delta}|f(x)|^{2} d x=\frac{1}{2}\left(\frac{2 \delta}{\pi}\right)^{2}+\sum_{n=1}^{\infty} \frac{4 \sin ^{2}(n \delta)}{\pi^{2} n^{2}}
$$

Now multiplying both sides by $\frac{\pi^{2}}{4 \delta}$ gives the required result.

(d) Let $R$ be any fixed number, $N$ any positive integer, and let $\delta_{N}=\frac{R}{N}$. As $N \rightarrow \infty$ we have $\sum_{n=1}^{N} \frac{\sin ^{2}\left(n \delta_{N}\right)}{n^{2} \delta_{N}} \rightarrow \int_{0}^{R}\left(\frac{\sin x}{x}\right)^{2} d x$, since the left- hand side of this equality is a Riemann sum for this integral. Note that

$$
\sum_{n=N+1}^{\infty} \frac{\sin ^{2}\left(n \delta_{N}\right)}{n^{2} \delta_{N}}<\frac{1}{N \delta_{N}}=\frac{1}{R}
$$

(The inequality results from the fact that $\sum_{n=k}^{\infty} \frac{1}{n^{2}}<\int_{k-1}^{\infty} \frac{1}{t^{2}} d t=\frac{1}{k-1}$.) Given $\varepsilon$, choose $R>\frac{\varepsilon}{3}$ such that

$$
\left|\int_{0}^{\infty}\left(\frac{\sin x}{x}\right)^{2} d x-\int_{0}^{S}\left(\frac{\sin x}{x}\right)^{2} d x\right|<\frac{\varepsilon}{3}
$$

if $S>R$. Then choose $N_{0}>\frac{3}{\varepsilon}$ so large that

$$
\left|\sum_{n=1}^{N} \frac{\sin ^{2}\left(n \delta_{N}\right)}{n^{2} \delta_{N}}-\int_{0}^{R}\left(\frac{\sin x}{x}\right)^{2} d x\right|<\frac{\varepsilon}{3}
$$

whenever $N>N_{0}$. Then for $N>N_{0}, \delta_{N}=\frac{R}{N}$ we have

$$
\left|\sum_{n=1}^{\infty} \frac{\sin ^{2}(n \delta)}{n^{2} \delta}-\int_{0}^{\infty}\left(\frac{\sin x}{x}\right)^{2} d x\right|<\varepsilon
$$

Consequently

$$
\int_{0}^{\infty}\left(\frac{\sin x}{x}\right)^{2} d x=\lim _{N \rightarrow \infty} \frac{\pi-\delta_{N}}{2}=\frac{\pi}{2}
$$

(e) Taking $\delta=\pi / 2$ yields

$$
\sum_{k=1}^{\infty} \frac{1}{(2 k-1)^{2}}=\frac{\pi^{2}}{8}
$$

Exercise 8.13 Put $f(x)=x$ if $0 \leq x<2 \pi$, and apply Parseval's theorem to conclude that

$$
\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}
$$

Solution. By computation we see that $a_{n}=0$ for $n>0$, and $a_{0}=2 \pi$. Computation shows that $b_{n}=\frac{-2}{n}$. Hence Parseval's relation gives

$$
\frac{8 \pi^{2}}{3}=\frac{1}{2}(2 \pi)^{2}+4 \sum_{n=1}^{\infty} \frac{1}{n^{2}}
$$

so that

$$
\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}
$$

There is another way of deriving this result. Since

$$
\sum_{n=1}^{\infty} \frac{1}{(2 n)^{2}}=\frac{1}{4} \sum_{n=1}^{\infty} \frac{1}{n^{2}}
$$

denoting this last sum by $X$, we find that

$$
X-\frac{1}{4} X=\sum_{k=1}^{\infty} \frac{1}{(2 k-1)^{2}}=\frac{\pi^{2}}{8}
$$

and hence, by part $(e)$ of the previous problem

$$
X=\frac{4}{3} \frac{\pi^{2}}{8}=\frac{\pi^{2}}{6}
$$

Exercise 8.14 If $f(x)=(\pi-|x|)^{2}$ on $[-\pi, \pi]$, prove that

$$
f(x)=\frac{\pi^{2}}{3}+\sum_{n=1}^{\infty} \frac{4}{n^{2}} \cos n x
$$

and deduce that

$$
\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}, \quad \sum_{n=1}^{\infty} \frac{1}{n^{4}}=\frac{\pi^{4}}{90}
$$

(A recent article by E. L. Stark contains many references to series of the form $\sum n^{-s}$, where $s$ is a positive integer. See Math. Mag., vol. 47, 1974, pp. 197-202:)

Solution. Since $f(x)$ is an even function, $b_{n}=0$ for all $n$. The $a_{n}$ 's are computed in a straightforward manner:

$$
a_{0}=\frac{2}{\pi} \int_{0}^{\pi} f(x) d x=\frac{2}{\pi} \int_{0}^{\pi}(\pi-x)^{2} d x=\frac{2}{\pi} \int_{0}^{\pi} x^{2} d x=\frac{2}{3} \pi^{2}
$$

and

$$
a_{n}=\frac{2}{\pi} \int_{0}^{\pi}(\pi-x)^{2} \cos n x d x=(-1)^{n} \frac{2}{\pi} \int_{0}^{\pi} x^{2} \cos n x d x
$$

so that, eventually, we find $a_{n}=\frac{4}{n^{2}}$.

This gives the stated Fourier series, and since $f(x)$ satisfies the Lipschitz condition of Theorem 8.14, the series converges to $f(x)$ at every point. Taking $x=0$ gives the first of the two desired equalities:

$$
\pi^{2}=f(0)=\frac{\pi^{2}}{3}+4 \sum_{n=1}^{\infty} \frac{1}{n^{2}}
$$

Parseval's theorem yields

$$
\frac{2 \pi^{4}}{5}=\frac{1}{\pi} \int_{-\pi}^{\pi}|f(x)|^{2} d x=\frac{2 \pi^{4}}{9}+16 \sum_{n=1}^{\infty} \frac{1}{n^{4}}
$$

which easily transforms to the desired relation.

Exercise 8.15 With $D_{n}$ as defined in (77), put

$$
K_{n}(x)=\frac{1}{N+1} \sum_{n=0}^{N} D_{n}(x)
$$

Prove that

$$
K_{N}(x)=\frac{1}{N+1} \frac{1-\cos (N+1) x}{1-\cos x}
$$

and that

(a) $K_{n} \geq 0$,

(b) $\frac{1}{2 \pi} \int_{-\pi}^{\pi} K_{N}(x) d x=1$.

(c) $K_{n}(x) \leq \frac{1}{N+1} \frac{2}{1-\cos \delta}$ if $0<\delta \leq|x| \leq \pi$.

If $s_{N}=s_{N}(f ; x)$ is the $N$ th partial sum of the Fourier series of $f$, consider the arithmetic means

$$
\sigma_{N}=\frac{s_{0}+s_{1}+\cdots+s_{N}}{N+1}
$$

Prove that

$$
\sigma_{N}(f ; x)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x-t) K_{N}(t) d t
$$

and hence prove Fej√©r's theorem:

If $f$ is continuous, with period $2 \pi$, then $\sigma_{N}(f ; x) \rightarrow f(x)$ uniformly on $[-\pi, \pi]$.

Hint: Use properties $(a),(b),(c)$ to proceed as in Theorem 7.26.

Solution. Using the formula $1-\cos \theta=2 \sin ^{2} \frac{1}{2} \theta$, and the formula $D_{n}(x)=$ $\frac{\sin \left(n+\frac{1}{2}\right) x}{\sin \frac{1}{2} x}$, we deduce that

$$
(1-\cos x) K_{N}(x)=\frac{1}{N+1} \sum_{n=0}^{N} \sin \frac{1}{2} x \sin \left(n+\frac{1}{2}\right) x
$$

Now, however, $\sin \alpha \sin \beta=\frac{1}{2} \cos (\alpha-\beta)-\cos (\alpha+\beta)$, so that

$$
\begin{aligned}
(1-\cos x) K_{N}(x)=\frac{1}{N+1} \sum_{n=0}^{N}(\cos (n x)-\cos ((n+1) x))= & \\
= & \frac{1}{N+1}(1-\cos (N+1) x)
\end{aligned}
$$

The formula is now established. Notice that it could also be written

$$
K_{N}(x)=\frac{1}{N+1}\left[\frac{\sin \left(\frac{N+1}{2} x\right)}{\sin \frac{1}{2} x}\right]^{2}
$$

(a) The nonnegativity of $K_{N}(x)$ is an immediate consequence of either of the formulas just written.

(b) It was established in the text that $\frac{1}{2 \pi} \int_{-\pi}^{\pi} D_{n}(x) d x=1$, and so the same result for $K_{N}(x)$, which is an average of the $D_{n}(x)$, must follow by routine computation.

(c) This inequality is an immediate consequence of the facts that $\cos (N+$ 1) $x \geq-1$ and that $\cos x$ is decreasing on $[0, \pi]$.

The formula for $\sigma_{N}(f ; x)$ is an immediate consequence of the definition of $\sigma_{N}(f ; x)$ and the corresponding formula for $s_{n}(f ; x)$.

Now let $M=\sup |f(x)|$, the supremum being taken over all $x$. By $(a)$ and (b) we have

$$
\begin{aligned}
\left|\sigma_{N}(x)-f(x)\right| \leq & \left|\frac{1}{2 \pi} \int_{-\pi}^{\pi}[f(x-t)-f(x)] K_{N}(t) d t\right| \\
\leq & \frac{1}{2 \pi} \int_{-\pi}^{\pi}|f(x-t)-f(x)| K_{N}(t) d t \\
\leq & \frac{1}{2 \pi} \int_{-\delta}^{\delta}|f(x-t)-f(x)| K_{N}(t) d t+ \\
& \quad+\frac{1}{\pi}(\pi-\delta) \frac{1}{N+1} \frac{2}{1-\cos \delta} 2 M \\
\leq & \sup _{|t| \leq \delta}|f(x-t)-f(x)|+\frac{Q_{\delta}}{N+1}
\end{aligned}
$$

where $Q_{\delta}=\frac{4 M(\pi-\delta)}{\pi(N+1)(1-\cos \delta)}$.

Given $\varepsilon>0$, we first choose $\delta>0$ so small that $\sup _{|t| \leq \delta}|f(x-t)-f(x)|<\frac{\varepsilon}{2}$ for all $x$. With this $\delta$ fixed, we then have $\left|\sigma_{N}(f ; x)-f(x)\right|<\varepsilon$ for all $N>\frac{2 Q_{\delta}}{\varepsilon}$ and all $x$.

Exercise 8.16 Prove a pointwise version of Fej√©r's Theorem:

If $f \in \mathcal{R}$ and $f(x+), f(x-)$ exist for some $x$, then

$$
\lim _{N \rightarrow \infty} \sigma_{N}(f ; x)=\frac{1}{2}[f(x+)+f(x-)] .
$$

Solution. We need only a slight modification of the argument just given, namely the formula

$$
\begin{aligned}
& \sigma_{N}(f ; x)-\frac{1}{2}[f(x+)+f(x-)]= \\
& =\frac{1}{2 \pi} \int_{0}^{\pi}[f(x-t)-f(x-)] K_{N}(t) d t+\frac{1}{2 \pi} \int_{-\pi}^{0}[f(x-t)-f(x+)] K_{N}(t) d t
\end{aligned}
$$

Each of these two integrals can be broken up into an integral over a halfneighborhood of 0 and an integral outside that neighborhood. The first of the integrals can be made small if the neighborhood is taken small enough (independently of $N$ ). With that neighborhood fixed, the second integral in each case can be made small if $N$ is large enough using the same inequalities just stated.

Exercise 8.17 Assume $f$ is bounded and monotonic on $[-\pi, \pi)$ with Fourier coefficients $c_{n}$, as given by (62).

(a) Use Exercise 17 of Chap. 6 to prove that $\left\{n c_{n}\right\}$ is a bounded sequence.

(b) Combine (a) with Exercise 16 and with Exercise 14(e) of Chap. 3, to conclude that

$$
\lim _{n \rightarrow \infty} s_{N}(f ; x)=\frac{1}{2}[f(x+)+f(x-)]
$$

for every $x$.

(c) Assume only that $f \in \mathcal{R}$ on $[-\pi, \pi]$ and that $f$ is monotonic in some segment $(\alpha, \beta) \subset[-\pi, \pi]$. Prove that the conclusion of $(b)$ holds for every $x \in(\alpha, \beta)$.

(This is an application of the localization theorem.)

Solution. (a) by Exercise 17 of Chap. 6 we have

$$
\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) e^{-i n x} d x=\frac{-1}{2 \pi n} \int_{-\pi}^{\pi} e^{-i n x} d f(x)
$$

from which it follows that

$$
\left|n c_{n}\right| \leq \frac{1}{2 \pi}[f(\pi)-f(-\pi)]
$$

(b) Since $f(x+)$ and $f(x-)$ exist at every point, it follows from the previous exercise that $\sigma_{N}(f ; x) \rightarrow \frac{1}{2}[f(x+)+f(x-)]$. Then Exercise $14(e)$ of Chap. 3 assures us that $s_{n}(f ; x)$ has the same limit.

(c) Let $g(x)=f(x)$ for $\alpha \leq x \leq \beta, g(x)=f(\alpha)$ for $0 \leq x \leq \alpha$ and $g(x)=f(\beta)$ for $\beta \leq x \leq 2 \pi$. Then $s_{N}(g ; x) \rightarrow \frac{1}{2}[g(x+)+g(x-)]$ for all $x$ by part $(b)$. Since $s_{N}(g ; x)-s_{N}(f ; x) \rightarrow 0$ for $\alpha<x<\beta$ by the Corollary to Theorem 8.14, it follows that $s_{N}(f ; x) \rightarrow \frac{1}{2}[g(x+)+g(x-)]=\frac{1}{2}[f(x+)+f(x-)]$ for these values of $x$.

Exercise 8.18 Define

$$
\begin{aligned}
& f(x)=x^{3}-\sin ^{2} x \tan x \\
& g(x)=2 x^{2}-\sin ^{2} x-x \tan x .
\end{aligned}
$$

Find out, for each of these two functions, whether it is positive or negative for all $x \in(0, \pi / 2)$, or whether it changes sign. Prove your answer.

Solution. Both functions tend to $-\infty$ as $x \rightarrow \frac{\pi}{2}$. Hence the only question is whether they ever become positive. We note that the derivative of the first function is $3 x^{2}-\sin ^{2} x-\tan ^{2} x$. By writing $\sin ^{2} x$ as $\frac{1}{2}-\frac{1}{2} \cos 2 x$ and making repeated use of the formula $\frac{d}{d x} \tan ^{k} x=k \tan ^{k-1} x+k \tan ^{k+1} x$, we find that the first six derivatives of this function vanish at 0 , and that the sixth derivative is

$$
-32 \sin 2 x-272 \tan x-1232 \tan ^{3} x-1104 \tan ^{5} x-144 \tan ^{7} x
$$

which is negative on $\left(0, \frac{\pi}{2}\right)$. Hence all of the first six derivatives are negative on this interval, and therefore the function itself is negative.

The same technique applies to the second function. All of its first five derivatives vanish at $x=0$ and the fifth is

$$
\begin{aligned}
-[16 \sin 2 x+16 x+ & 80 \tan x+136 x \tan ^{2} x+ \\
& \left.+200 \tan ^{3} x+240 x \tan ^{4} x+120 \tan ^{5} x+120 x \tan ^{6} x\right]
\end{aligned}
$$

which is negative on $\left(0, \frac{\pi}{2}\right)$. Hence this function is always negative on that interval.

Exercise 8.19 Suppose $f$ is a continuous function on $R^{1}, f(x+2 \pi)=f(x)$, and $\alpha / \pi$ is irrational. Prove that

$$
\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^{\infty} f(x+n \alpha)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(t) d t
$$

for every $x$. Hint: Do it first for $f(x)=e^{i k x}$.

Solution: Following the hint, we observe that both sides of the desired equality equal 1 trivially when $k=0$. In any other case the right-hand side is zero, and the left-hand side is

which tends to zero as $N \rightarrow \infty$.

$$
\lim _{N \rightarrow \infty} e^{i k x} \frac{1-e^{i(n+1) k \alpha}}{N\left(1-e^{i \alpha}\right)}
$$

Since both sides are linear functions of $f$, it now follows that the relation holds for all trigonometric polynomials. Finally, since both sides are bounded by the supremum of $f$, given $\varepsilon$, we can approximate $f$ uniformly within $\varepsilon$ by a trigonometric polynomial. It then follows that all the means on the left, for $N$ sufficiently large, are within $2 \varepsilon$ of the integral on the right. Since $\varepsilon$ is arbitrary, it follows that the limit on the left equals the integral on the right.

Exercise 8.20 The following simple computation yields a good approximation to Stirling's formula.

For $m=1,2,3, \ldots$, define

$$
f(x)=(m+1-x) \log m-(x-m) \log (m+1)
$$

if $m \leq x \leq m+1$, and define

$$
g(x)=\frac{x}{m}-1+\log m
$$

if $m-\frac{1}{2} \leq x<m+\frac{1}{2}$. Draw the graphs of $f$ and $g$. Note that $f(x) \leq \log x \leq g(x)$
if $x \geq 1$ and that

$$
\int_{1}^{n} f(x) d x=\log (n !)-\frac{1}{2} \log n>-\frac{1}{8}+\int_{1}^{n} g(x) d x
$$

Integrate $\log x$ over $[1, n]$. Conclude that

$$
\frac{7}{8}<\log (n !)-\left(n+\frac{1}{2}\right) \log n+n<1
$$

for $n=2,3,4, \ldots$ (Note: $\log \sqrt{2 \pi} \sim 0.918 \ldots)$ Thus

$$
e^{7 / 8}<\frac{n !}{(n / e)^{n} \sqrt{n}}<e
$$

Solution. We first draw the graphs of $f$ and $g$ in the range $x=1$ to $x=10$. We note that $f$ is merely the broken set of chords joining the points on the graph of $\log x$ at integer values of $x$, and $g$ is made up of segments of the tangents at these points ( $g$ is not continuous). Because the downward side of the graph of $\log x$ is convex, $f(x) \leq \log x \leq g(x)$ for all $x$. The estimate for the integral of $f$ is straightforward: The integral is the sum of the areas of one triangle and $n-2$ trapezoids with base 1 and parallel sides $\log k$ and $\log (k+1)(k=2, \ldots, n-1)$.

We find it equal to $\frac{1}{2} \log 1+\log 2+\log 3+\cdots+\log (n-1)+\frac{1}{2} \log n=\log (n !)-$ $\frac{1}{2} \log n$, as asserted. Meanwhile the integral of $g(x)$ is also a sum of trapezoids and two triangles and equals $\frac{1}{8}+\log (n !)-\frac{1}{2} \log n-\frac{1}{8 n}$. Hence we have

$\log (n !)-\frac{1}{2} \log n<\int_{1}^{n} \log x d x<\frac{1}{8}+\log (n !)-\frac{1}{2} \log n-\frac{1}{8 n}<\frac{1}{8}+\log (n !)-\frac{1}{2} \log n$.

Now straightforward computation reveals that

$$
\int_{1}^{n} \log x d x=(n \log n-n)-(1 \log 1-1)=n \log n-n+1
$$

The desired inequalities are now deduced by taking exponentials of the three expressions.

Exercise 8.21 Let

$$
L_{n}=\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left|D_{n}(t)\right| d t \quad(n=1,2,3, \ldots)
$$

Prove that there exists a constant $C>0$ such that

$$
L_{n}>C \log n \quad(n=1,2,3, \ldots)
$$

or, more precisely, that the sequence

$$
\left\{L_{n}-\frac{4}{\pi^{2}} \log n\right\}
$$

is bounded.

Solution. We observe that

$$
\begin{aligned}
L_{n}= & \frac{1}{\pi} \int_{0}^{\frac{2 \pi}{2 n+1}} \frac{\sin \left(n+\frac{1}{2}\right) t}{\sin \frac{1}{2} t} d t+ \\
& +\sum_{k^{t}=1}^{n-1} \frac{1}{\pi} \int_{\frac{2 \pi k}{2 n+1}}^{\frac{2 \pi(k+1)}{2 n+1}} \frac{(-1)^{k} \sin \left(n+\frac{1}{2}\right) t}{\sin \frac{1}{2} t} d t+\frac{1}{\pi} \int_{\frac{2 n \pi}{2 n+1}}^{\pi} \frac{(-1)^{n} \sin \left(n+\frac{1}{2}\right) t}{\sin \frac{1}{2} t} d t
\end{aligned}
$$

The substitution $u=\left(n+\frac{1}{2}\right) t$ changes the first and last terms into the sum

$$
\frac{1}{\pi} \int_{0}^{\pi} \frac{\sin u}{\left(n+\frac{1}{2}\right) \sin \left(\frac{u}{2 n+1}\right)} d u+\int_{n \pi}^{\left(n+\frac{1}{2}\right) \pi} \frac{(-1)^{n} \sin u}{\left(n+\frac{1}{2}\right) \sin \left(\frac{u}{2 n+1}\right)} d u
$$

The first of these terms tends to $\frac{1}{2 \pi} \int_{0}^{\pi} \sin u d u=\frac{1}{\pi}$ as $n \rightarrow \infty$. The second tends to 0 (for $u \in\left[n \pi,\left(n+\frac{1}{2}\right) \pi\right]$ we have $\sin \left(\frac{u}{2 n+1}\right) \geq \sin \frac{n \pi}{2 n+1}$, which tends to 1 as $n \rightarrow \infty)$.

Thus we find that

$$
\frac{1}{\pi}+\varepsilon_{n}+\sum_{k=1}^{n-1} \frac{1}{\pi \sin \left(\frac{\pi(k+1)}{2 n+1}\right)}\left|\int_{\frac{2 \pi k}{2 n+1}}^{\frac{2 \pi(k+1)}{2 n+1}} \sin \left(n+\frac{1}{2}\right) t d t\right|<L_{n}
$$

where $\varepsilon_{n} \rightarrow 0$ as $n \rightarrow \infty$.

If we take out the first two terms of the sum instead of just the first, we find similarly that

$$
\begin{aligned}
L_{n} & =\frac{1}{\pi} \int_{0}^{\frac{4 \pi}{2 n+1}} \frac{\left|\sin \left(n+\frac{1}{2}\right) t\right|}{\sin \frac{1}{2} t} d t+ \\
& +\sum_{k=2}^{n-1} \frac{1}{\pi} \int_{\frac{2 \pi k}{2 n+1}}^{\frac{2 \pi(k+1)}{2 n+1}} \frac{(-1)^{k} \sin \left(n+\frac{1}{2}\right) t}{\sin \frac{1}{2} t} d t+\frac{1}{\pi} \int_{\frac{2 n \pi}{2 n+1}}^{\pi} \frac{(-1)^{n} \sin \left(n+\frac{1}{2}\right) t}{\sin \frac{1}{2} t} d t .
\end{aligned}
$$

Again the substitution $u=\left(n+\frac{1}{2}\right) t$ changes the first and last terms into the sum

$$
\frac{1}{\pi} \int_{0}^{2 \pi} \frac{\sin u}{\left(n+\frac{1}{2}\right) \sin \left(\frac{u}{2 n+1}\right)} d u+\int_{n \pi}^{\left(n+\frac{1}{2}\right) \pi} \frac{(-1)^{n} \sin u}{\left(n+\frac{1}{2}\right) \sin \left(\frac{u}{2 n+1}\right)} d u
$$

The first of these terms tends to $\frac{1}{2 \pi} \int_{0}^{2 \pi}|\sin u| d u=\frac{2}{\pi}$ as $n \rightarrow \infty$, and once again the second tends to zero.

Thus we find that

$$
L_{n}<\frac{2}{\pi}+\eta_{n}+\sum_{k=2}^{n-1} \frac{1}{\pi \sin \left(\frac{\pi k}{2 n+1}\right)}\left|\int_{\frac{2 \pi k}{2 n+1}}^{\frac{2 \pi(k+1)}{2 n+1}} \sin \left(n+\frac{1}{2}\right) t d t\right|
$$

where $\eta_{n} \rightarrow 0$ as $n \rightarrow \infty$.

Once again, in each of the integrals under the sigma in the last two inequalities we make the substitution $u=\left(n+\frac{1}{2}\right) t$. When we do so, we have

$$
\frac{1}{\pi}+\varepsilon+\sum_{k=1}^{n-1} \frac{2}{\left(n+\frac{1}{2}\right) \pi \sin \left(\frac{\pi(k+1)}{2 n+1}\right)}<L_{n}<\frac{2}{\pi}+\eta_{n}+\sum_{k=2}^{n-1} \frac{2}{\left(n+\frac{1}{2}\right) \pi \sin \left(\frac{\pi k}{2 n+1}\right)}
$$

where $\varepsilon_{n} \rightarrow 0$ and $\eta_{n} \rightarrow 0$. It therefore follows that

$$
\frac{1}{\pi}+\varepsilon_{n}<L_{n}-\sum_{k=1}^{n-1} \frac{2}{\pi} \frac{1}{\left(n+\frac{1}{2}\right) \sin \left(\frac{\pi(k+1)}{2 n+1}\right)}<\frac{2}{\pi}+\eta_{n}-\frac{1}{\left(n+\frac{1}{2}\right) \sin \left(\frac{\pi n}{2 n+1}\right)}
$$

The extremes in these inequalities are both bounded. Hence we will be done if we can show that

$$
\frac{2}{\pi} \log n-\sum_{k=1}^{n-1} \frac{1}{\left(n+\frac{1}{2}\right) \sin \left(\frac{\pi(k+1)}{2 n+1}\right)}
$$

remains bounded. To do this, we use the fact that there is a constant $K$ such that

$$
\left|\frac{1}{\sin x}-\frac{1}{x}\right| \leq K x
$$

for $0<x \leq \frac{\pi}{2}$. This fact in turn is a consequence of the fact that, by L'Hospital's rule,

$$
\lim _{x \rightarrow 0} \frac{x-\sin x}{x^{2} \sin x}=-\frac{1}{2}
$$

We thus have

$$
\sum_{k=1}^{n-1} \frac{1}{\left(n+\frac{1}{2}\right) \sin \left(\frac{\pi(k+1)}{2 n+1}\right)}=E_{n}+\sum_{k=1}^{n-1} \frac{1}{\left(n+\frac{1}{2}\right)\left(\frac{\pi(k+1)}{2 n+1}\right)}
$$

where

$$
\begin{aligned}
\left|E_{n}\right| \leq K \frac{1}{n+\frac{1}{2}} & \sum_{k=1}^{n-1} \frac{\pi(k+1)}{2 n+1}= \\
& =\frac{2 K}{\pi(2 n+1)^{2}} \sum_{k=1}^{n} k+1=\frac{2 K}{\pi(2 n+1)^{2}}\left[\frac{(n+1)(n+2)}{2}-1\right]
\end{aligned}
$$

Since the right-hand side tends to $\frac{K}{4 \pi}$ as $n \rightarrow \infty$, we see that $E_{n}$ remains bounded as $n \rightarrow \infty$. We will be finished if we can show that

$$
\log n-\sum_{k=1}^{n-1} \frac{1}{k+1}
$$

remains bounded. But this was done in Exercise 9 above.

Exercise 8.22 If $\alpha$ is real and $-1<x<1$, prove Newton's binomial theorem

$$
(1+x)^{\alpha}=1+\sum_{n=1}^{\infty} \frac{\alpha(\alpha-1) \cdots(\alpha-n+1)}{n !} x^{n}
$$

Hint: Denote the right side by $f(x)$. Prove that the series converges. Prove that

$$
(1+x) f^{\prime}(x)=\alpha f(x)
$$

and solve this differential equation.

Show also that

$$
(1-x)^{-\alpha}=\sum_{n=0}^{\infty} \frac{\Gamma(n+\alpha)}{n ! \Gamma(\alpha)} x^{n}
$$

if $-1<x<1$ and $\alpha>0$.

Solution. Following the hint, we use the ratio test to establish that the radius of convergence of the power series that defines $f(x)$ is 1 . This amounts merely to the statement that

$$
\lim _{n \rightarrow \infty}\left|\frac{\alpha-n}{n+1}\right|=1
$$

The differential equation then results from termwise operations on the series and the fact that

$$
\frac{\alpha(\alpha-1) \cdots(\alpha-n)}{n !}+\frac{\alpha(\alpha-1) \cdots(\alpha-n+1)}{(n-1) !}=\alpha \frac{\alpha(\alpha-1) \cdots(\alpha-n+1)}{n !}
$$

Then, given that $f(0)=1 \neq 0$, it follows that for $x$ near 0 we have

$$
\frac{f^{\prime}(x)}{f(x)}=\frac{\alpha}{1+x}
$$

so that $\log f(x)$ and $\log (1+x)^{\alpha}$ have the same derivative, and hence differ by a constant, which turns out to be zero, since both equal 1 at $x=0$. It thus follows that $f(x)=(1+x)^{\alpha}$.

To prove the other relation, we merely observe that

$$
\left(-\alpha(-\alpha-1) \cdots(-\alpha-n+1)=(-1)^{n} \alpha(\alpha+1) \cdots(\alpha+n-1)=(-1)^{n} \frac{\Gamma(n+\alpha)}{\Gamma(\alpha)}\right.
$$

Exercise 8.23 Let $\gamma$ be a continuously differentiable closed curve in the complex plane with parameter interval $[a, b]$, and assume that $\gamma(t) \neq 0$ for every $t \in[a, b]$. Define the index of $\gamma$ to be

$$
\operatorname{Ind}(\gamma)=\frac{1}{2 \pi i} \int_{a}^{b} \frac{\gamma^{\prime}(t)}{\gamma(t)} d t
$$

Prove that Ind $(\gamma)$ is always an integer.

Hint: There exists $\varphi$ on $[a, b]$ with $\varphi^{\prime}=\gamma^{\prime} / \gamma, \varphi(a)=0$. Hence $\gamma \exp (-\varphi)$ is constant. Since $\gamma(a)=\gamma(b)$, it follows that $\exp (\varphi(a))=\exp (\varphi(b))=1$. Note that $\varphi(b)=2 \pi i \operatorname{Ind}(\gamma)$.

Compute Ind $(\gamma)$ when $\gamma(t)=e^{i n t}, a=0, b=2 \pi$.

Explain why Ind $(\gamma)$ is often called the winding number of $\gamma$ around 0 .

Solution. Again, following the hint leaves very little to do. We define

$$
\varphi(x)=\int_{a}^{x} \frac{\gamma^{\prime}(t)}{\gamma(t)} d t
$$

so that we automatically have $\varphi^{\prime}(t)=\frac{\gamma^{\prime}(t)}{\gamma(t)}$. The fact that $\gamma \exp (-\varphi)$ is constant is now a consequence of the chain rule. It then follows immediately that $\exp (\varphi(b))=1$, so that $\varphi(b)=2 \pi i n$ for some integer $n$.

Routine computation shows that Ind $(\gamma)=n$ if $\gamma(t)=e^{i n t}, 0 \leq t \leq 2 \pi$. Since this curve winds counterclockwise about 0 a total of $n$ times, the name winding number is appropriate.

Exercise 8.24 Let $\gamma$ be as in Exercise 23, and assume in addition that the range of $\gamma$ does not intersect the negative real axis. Prove that Ind $(\gamma)=0$. Hint: For $0 \leq c<\infty$, Ind $(\gamma+c)$ is a continuous integer-valued function of $c$. Also, Ind $(\gamma+c) \rightarrow 0$ as $c \rightarrow \infty$.

Solution. Following the hint, we observe that

$$
f(c)=\frac{1}{2 \pi i} \int_{a}^{b} \frac{\gamma^{\prime}(t)}{\gamma(t)+c} d t
$$

is a continuous function of $c$ on $[0, \infty)$, since

$$
\left|f\left(c_{1}\right)-f\left(c_{2}\right)\right|=\frac{1}{2 \pi}\left|\int_{a}^{b} \frac{\gamma^{\prime}(t)\left(c_{1}-c_{2}\right)}{\left(\gamma(t)+c_{1}\right)\left(\gamma(t)+c_{2}\right)} d t \leq K\right| c_{1}-c_{2} \mid
$$

where $K=\frac{1}{2 \pi r^{2}} \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t$ and $r$ is the supremum of the integrand for $c_{1}, c_{2} \geq$ 0 and $0 \leq t \leq 2 \pi$. (This supremum is finite, since the integrand tends to zero as either $c_{1}$ or $c_{2}$ tends to infinity.) Furthermore

$$
|f(c)| \leq \frac{1}{2 \pi c} \int_{a}^{b} \frac{\left|\gamma^{\prime}(t)\right|}{\left|1+\frac{\gamma(t)}{c}\right|} d t
$$

and this last expression tends to 0 as $c \rightarrow \infty$. It follows, since $f$ assumes only integer values, that $f(c) \equiv 0$. In particular $f(0)=\operatorname{Ind}(\gamma)=0$.

Exercise 8.25 Suppose $\gamma_{1}$ and $\gamma_{2}$ are curves as in Exercise 23, and

$$
\left|\gamma_{1}(t)-\gamma_{2}(t)\right|<\left|\gamma_{1}(t)\right| \quad(a \leq t \leq b)
$$

Prove that Ind $\left(\gamma_{1}\right)=\operatorname{Ind}\left(\gamma_{2}\right)$.

Hint: Put $\gamma=\gamma_{2} / \gamma_{1}$. Then $|1-\gamma|<1$. Hence Ind $(\gamma)=0$ by Exercise 24 . Also,

$$
\frac{\gamma^{\prime}}{\gamma}=\frac{\gamma_{2}^{\prime}}{\gamma_{2}}-\frac{\gamma_{1}^{\prime}}{\gamma_{1}}
$$

Solution. The hint leaves almost nothing to be done. The inequality established for $\gamma$ shows that the real part of $\gamma$ is always positive, so that the hypotheses of Exercise 24 are satisfied. The relation for $\frac{\gamma^{\prime}}{\gamma}$ is a routine computation, and shows in general that $\operatorname{Ind}(\gamma \delta)=\operatorname{Ind}(\gamma)+\operatorname{Ind}(\delta)$.

Exercise 8.26 Let $\gamma$ be a closed curve in the complex plane (not necessarily differentiable) with parameter interval $[0,2 \pi]$, such that $\gamma(t) \neq 0$ for every $t \in[0,2 \pi]$.

Choose $\delta>0$ such that $|\gamma(t)|>\delta$ for all $t \in[0,2 \pi]$. If $P_{1}$ and $P_{2}$ are trigonometric polynomials such that $\left|P_{i}(t)-\gamma(t)\right|<\delta / 4$ for all $t \in[0,2 \pi]$, (their existence is assured by Theorem 8.15), prove that

$$
\operatorname{Ind}\left(P_{1}\right)=\operatorname{Ind}\left(P_{2}\right)
$$

by applying Exercise 25.

Define this common value to be Ind $(\gamma)$.

Prove that the statements of Exercises 24 and 25 hold without any differentiability assumptions.

Solution. Since $\left|P_{1}(t)-P_{2}(t)\right|<\frac{\delta}{2}<\left|P_{1}(t)\right|$, (because $\left|P_{1}(t)\right| \geq|f(t)|-\mid f(t)-$ $\left.P_{1}(t) \mid>\frac{3 \delta}{4}\right)$, the equality of the indices follows from Exercise 25, as stated.

Exercise 24 remains valid, since if $\gamma(t)$ does not intersect the negative real axis, there is a positive number $\delta>0$ such that $|\gamma(t)-x| \geq \delta$ for all $x \leq 0$. Then if $\left|P_{j}(t)-\gamma(t)\right|<\delta$ for all $t \in[0,2 \pi]$, it follows that $P_{j}(t)$ also does not intersect the negative real axis, hence has winding number 0 .

Exercise 25 remains valid, since if $\left|\gamma_{1}(t)-\gamma_{2}(t)\right|<\left|\gamma_{1}(t)\right|$ for all $t$, we can let $\delta=\min _{t}\left|\gamma_{1}(t)\right|-\left|\gamma_{1}(t)-\gamma_{2}(t)\right|$. Then if $\left|P_{i}(t)-\gamma_{i}(t)\right|<\delta / 4$ for all $t$, it follows that $\left|P_{1}(t)-P_{2}(t)\right| \leq\left|\gamma_{1}(t)-\gamma_{2}(t)\right|+(\delta / 2)<\left|\gamma_{1}(t)\right|-(\delta / 4) \leq\left|P_{1}(t)\right|$, and so Ind $\left(P_{1}\right)=$ Ind $\left(P_{2}\right)$, by Exercise 25 .

Exercise 8.27 Let $f$ be a continuous complex function defined in the complex plane. Suppose there is a positive integer $n$ and a complex number $c \neq 0$ such that

$$
\lim _{|z| \rightarrow \infty} z^{-n} \gamma(z)=c
$$

Prove that $f(z)=0$ for at least one complex number $z$.

Note that this is a generalization of Theorem 8.8.

Hint: Assume $f(z) \neq 0$ for all $z$, define

$$
\gamma_{r}(t)=f\left(r e^{i t \theta}\right)
$$

for $0 \leq r<\infty, 0 \leq t \leq 2 \pi$, and prove the following statements about the curve $\gamma$.

(a) $\operatorname{Ind}\left(\gamma_{0}\right)=0$

(b) Ind $\left(\gamma_{r}\right)=n$ for all sufficiently large $r$.

(c) Ind $\left(\gamma_{r}\right)$ is a continuous function of $r$ on $[0, \infty)$.

[In $(b)$ and $(c)$, use the last part of Exercise 26.]

Show that $(a),(b)$, and $(c)$ are contradictory, since $n>0$.

Solution. (a) Since $\gamma_{0}(t)=f(0)$ for all $t$, we have $\gamma_{0}^{\prime}(t)=0$ for all $t$, and hence by definition $\operatorname{Ind}\left(\gamma_{0}\right)=0$.

(b) Choose $R$ so large that $\left|z^{-n} f(z)-c\right|<\frac{|c|}{2}$ whenever $|z|>R$. Then for all $r$ we have Ind $\left(\gamma_{r}\right)=\operatorname{Ind}\left(\gamma_{r 1}\right)+\operatorname{Ind}\left(\gamma_{r 2}\right)$, where $\gamma_{r_{1}}(t)=r^{n} e^{i n t}$ and

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-502.jpg?height=223&width=201&top_left_y=360&top_left_x=1035)

Figure 8.1: The Brouwer fixed-point theorem

$\gamma_{r 2}(t)=r^{-n} e^{-i n t} f\left(r e^{i t}\right)$. By Exercise 25 we have Ind $\left(\gamma_{r 2}\right)=0$ for $r>R$, and by direct computation we have $\operatorname{Ind}\left(\gamma_{r 1}\right)=n$ for all $r$.

(c) Fix $r_{0}>0$, and let $\varepsilon=\min _{0 \leq t \leq 2 \pi}\left|f\left(r_{0} e^{i t}\right)\right|$. Then choose $\delta \in\left(0, r_{0}\right)$ such that $\left|f\left(r_{0} e^{i t}\right)-f\left(r e^{i t}\right)\right|<\varepsilon$ if $\left|r-r_{0}\right|<\delta$. Then by Exercise 25 we again have $\operatorname{Ind}\left(\gamma_{r}\right)=\operatorname{Ind}\left(\gamma_{r_{0}}\right)$ for $\left|r-r_{0}\right|<\delta$. Hence Ind $\left(\gamma_{r}\right)$ is a locally constant function of $r$. By the connectivity of $[0, \infty)$, it follows that it is globally constant, which contradicts $(a)$ and $(b)$.

Exercise 8.28 Let $\bar{D}$ be the closed unit disc in the complex plane. (Thus $z \in \bar{D}$ if and only if $|z| \leq 1$.) Let $g$ be a continuous mapping of $\bar{D}$ into the unit circle $T$. (Thus $|g(z)|=1$ for every $z \in \bar{D}$.)

Prove that $g(z)=-z$ for at least one $z \in T$.

Hint: For $0 \leq r \leq 1,0 \leq t \leq 2 \pi$, put

$$
\gamma_{r}(t)=g\left(r e^{i t}\right)
$$

and put $\psi(t)=e^{-i t} \gamma_{1}(t)$. If $g(z) \neq-z$ for every $z \in T$, then $\psi(t) \neq-1$ for every $t \in[0,2 \pi]$. Hence Ind $(\psi)=0$, by Exercises 24 and 25 . It follows that Ind $\left(\gamma_{1}\right)=1$. But Ind $\left(\gamma_{0}\right)=0$. Derive a contradiction, as in Exercise 27 .

Solution. The hint tells us that $\psi(t)$ does not meet the negative real axis, hence has index 0 , by Exercise 24. Hence by Exercise 25, $\gamma_{1}$ has index 1. Again, since $\gamma_{0}=g(0) \neq 0($ since $g(0) \neq-0=0)$, it follows that Ind $\left(\gamma_{0}\right)=0$. But, as before, since $|g(z)|=1$ for all $z$, it follows that Ind $\left(\gamma_{r}\right)$ is locally constant and hence by the connectivity of $[0,1]$, globally constant. Thus, once again, we have a contradiction.

Exercise 8.29 Prove that every continuous mapping $f$ of $\bar{D}$ into $\bar{D}$ has a fixed point in $\bar{D}$.

(This is the 2-dimensional case of Brouwer's fixed-point theorem.)

Hint: Assume $f(z) \neq z$ for every $z \in \bar{D}$. Associate to each $z \in \bar{D}$ the point $g(z) \in T$ which lies on the ray that starts at $f(z)$ and passes through $z$. Then $g$ maps $\bar{D}$ into $T, g(z)=z$ if $z \in T$, and $g$ is continuous, because

$$
g(z)=z-s(z)[f(z)-z]
$$

where $s(z)$ is the unique nonnegative root of a certain quadratic equation whose coefficients are continuous functions of $f$ and $z$. Apply Exercise 28 .

Solution. The number $s=s(z)$ is a nonnegative real number because of the geometry of the situation (see figure). The quadratic equation in question is given by the relation $|g(z)|^{2}=1$, i.e.,

$$
|f(z)-z|^{2} s^{2}+2\left(|z|^{2}-\operatorname{Re}(\bar{z} f(z))\right) s+|z|^{2}-1=0
$$

It is well-known that a quadratic equation $a z^{2}+b z+c=0$ has one and only one nonnegative root if $a, b$, and $c$ are real and $a c<0$. We can write explicitly

$$
s(z)=\frac{|z|^{2}-\operatorname{Re}(\bar{z} f(z))+\sqrt{\left(|z|^{2}-\operatorname{Re}(\bar{z} f(z))^{2}+|f(z)-z|^{2}\left(1-|z|^{2}\right)\right.}}{|f(z)-z|^{2}} .
$$

which makes it clear that $s(z)$ is a continuous function of $z$. Hence $g(z)$ is continuous.

We now know that there must be a value at which $g(z)=-z$. But this is impossible, since $|g(z)|=1$ for all $z$ and $g(z)=z$ if $|z|=1$.

Exercise 8.30 Use Stirling's formula to prove that

$$
\lim _{x \rightarrow \infty} \frac{\Gamma(x+c)}{x^{c} \Gamma(x)}=1
$$

for every real constant $c$.

Solution. We need Stirling's formula in the form

$$
\lim _{z \rightarrow \infty} \frac{\Gamma(z)}{\left(\frac{z-1}{e}\right)^{z-1} \sqrt{2 \pi(z-1)}}=1
$$

Applying this result with $z=x+c$ and $z=x$, we get

$$
\begin{aligned}
\lim _{x \rightarrow \infty} & \frac{\Gamma(x+c)}{x^{c} \Gamma(x)}= \\
& =\lim _{x \rightarrow \infty} f(x) \cdot \frac{\Gamma(x+c)}{\left(\frac{x+c-1}{e}\right)^{x+c-1} \sqrt{2 \pi(x+c-1)}} \cdot \frac{\left(\frac{x-1}{e}\right)^{x-1} \sqrt{2 \pi(x-1)}}{\Gamma(x)}
\end{aligned}
$$

where

$f(x)=\frac{1}{x^{c}} \cdot \frac{\left(\frac{x+c-1}{e}\right)^{x+c-1}}{\left(\frac{x-1}{e}\right)^{x-1}} \cdot \sqrt{\frac{x+c-1}{x-1}}=\frac{\left(1+\frac{c-1}{x}\right)^{x}\left(1+\frac{c-1}{x}\right)^{c-1}}{e^{c}\left(1-\frac{1}{x}\right)^{x}\left(1-\frac{1}{x}\right)^{-1}} \cdot \sqrt{\frac{x+c-1}{x-1}}$.

Since $x^{x} \rightarrow 1$ as $x \rightarrow \infty$, it now follows that $\lim _{x \rightarrow \infty} f(x)=1$, which, combined with Stirling's formula, gives the desired result.

Exercise 8.31 In the proof of Theorem 7.26 it was shown that

$$
\int_{-1}^{1}\left(1-x^{2}\right)^{n} d x \geq \frac{4}{3 \sqrt{\pi}}
$$

for $n=1,2,3, \ldots$ Use Theorem 8.20 and Exercise 30 to show the more precise result

$$
\lim _{n \rightarrow \infty} \sqrt{n} \int_{-1}^{1}\left(1-x^{2}\right)^{n} d x=\sqrt{\pi}
$$

Solution: Let $u=x^{2}$ in the integral, so that $d x=\frac{1}{2} u^{-\frac{1}{2}} d u$. We then have.

$$
\sqrt{n} \int_{-1}^{1}\left(1-x^{2}\right)^{n} d x=\sqrt{n} \int_{0}^{1}(1-u)^{n} u^{-\frac{1}{2}} d u=\frac{\sqrt{n} \Gamma\left(n+\frac{1}{2}\right) \Gamma\left(\frac{1}{2}\right)}{\Gamma(n+1)}
$$

and taking $c=\frac{1}{2}$ in Exercise 30, we find that this last expression tends to $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$.

## Chapter 9

## Functions of Several Variables

Exercise 9.1 If $S$ is a nonempty subset of a vector space $X$, prove (as asserted in Sec. 9.1) that the span of $S$ is a vector space.

Solution. We need only verify that the span of $S$ is closed under the two vector space operations. All the other properties of a vector space hold in the span of $S$, since it is contained in a vector space in which they hold.

To that end, let $\mathbf{x}$ and $\mathbf{y}$ be elements of the span of $S$, and let $c$ be any real number. By definition there are elements $\mathbf{x}_{1}, \ldots, \mathbf{x}_{m}, \mathbf{y}_{1}, \ldots, \mathbf{y}_{n}$, and scalars $c_{1}, \ldots, c_{m}, d_{1}, \ldots, d_{n}$ such that $\mathbf{x}=c_{1} \mathbf{x}_{1}+\cdots+c_{m} \mathbf{x}_{m}$ and $\mathbf{y}=d_{1} \mathbf{y}_{1}+\cdots+d_{n} \mathbf{y}_{n}$ We then have

$$
\mathbf{x}+\mathbf{y}=c_{1} \mathbf{x}_{1}+\cdots+c_{m} \mathbf{x}_{m}+d_{1} \mathbf{y}_{1}+\cdots+d_{n} \mathbf{y}_{n}
$$

which is a finite linear combination of elements of $S$, hence belongs to the span of $S$. Likewise, by the distributive law,

$$
c \mathbf{x}=c\left(c_{1} \mathbf{x}_{1}+\cdots+c_{m} \mathbf{x}_{m}\right)=\left(c c_{1}\right) \mathbf{x}_{1}+\cdots+\left(c c_{m}\right) \mathbf{x}_{m}
$$

which belongs to the span of $S$.

Exercise 9.2 Prove (as asserted in Sec. 9.6) that $B A$ is linear if $A$ and $B$ are linear transformations. Prove also that $A^{-1}$ is linear and invertible.

Solution. Let $A: X \rightarrow Y$ and $B: Y \rightarrow Z$ be linear transformations, and let $\mathbf{x}$ and $\mathbf{y}$ be any elements of $A$ and $c$ any scalar. Then $B A: X \rightarrow Z$ satisfies

$$
\begin{aligned}
B A(\mathbf{x}+\mathbf{y}) & =B(A(\mathbf{x}+\mathbf{y})) \\
& =B(A(\mathbf{x})+A(\mathbf{y})) \\
& =B(A(x))+B(A(\mathbf{y})) \\
& =B A(\mathbf{x})+B A(\mathbf{y})
\end{aligned}
$$

Similarly,

$$
\begin{aligned}
B A(c \mathbf{x}) & =B(A(c \mathbf{x})) \\
& =B(c A(\mathbf{x})) \\
& =c B(A(\mathbf{x})) \\
& =c B A(\mathbf{x})
\end{aligned}
$$

If $A$ is a one-to-one mapping of $X$ onto $Y$, and $\mathrm{z}$ and $\mathrm{w}$ are any elements of $Y$, let $\mathbf{x}=A^{-1}(\mathbf{z})$ and $\mathbf{y}=A^{-1}(\mathbf{w})$. Then by definition $A(\mathbf{x})=\mathbf{z}$ and $A(\mathrm{y})=\mathrm{w}$. It therefore follows from the linearity of $A$ that $A(\mathrm{x}+\mathrm{y})=\mathrm{z}+\mathrm{w}$. Again, by definition, this means that $A^{-1}(\mathrm{z}+\mathrm{w})=\mathrm{x}+\mathrm{y}=A^{-1}(\mathrm{z})+A^{-1}(\mathrm{w})$, so that $A^{-1}$ preserves vector addition. Similarly, $A(c \mathbf{x})=c A(\mathbf{x})=c \mathbf{z}$, so that $A^{-1}(c \mathbf{z})=c \mathbf{x}=c A^{-1}(\mathbf{z})$, and hence $A^{-1}$ also preserves scalar multiplication.

Exercise 9.3 Assume $A \in L(X, Y)$ and $A \mathrm{x}=0$ only when $\mathrm{x}=0$. Prove that $A$ is then 1-1.

Solution. Suppose $A(\mathbf{x})=A(\mathbf{y})$. It then follows that $A(\mathbf{x}-\mathbf{y})=A(\mathbf{x})-A(\mathbf{y})=$ 0 . Hence by assumption $\mathbf{x}-\mathbf{y}=0$, and so $\mathbf{x}=\mathbf{y}$; therefore $A$ is one-to-one.

Exercise 9.4 Prove (as asserted in Sec. 9.30) that null spaces and ranges of linear transformations are vector spaces.

Solution. Let $N$ be the null space of the linear transformation $A: X \rightarrow Y$, let $\mathrm{x}$ and $\mathbf{y}$ be elements of $N$, and let $c$ be any scalar. By definition $A(\mathbf{x})=0=A(\mathbf{y})$, and $A(\mathrm{x}+\mathrm{y})=A(\mathrm{x})+A(\mathrm{y})=\mathbf{0}+\mathbf{0}=\mathbf{0}$, so that, by definition, $\mathrm{x}+\mathrm{y} \in N$. Likewise $A(c \mathbf{x})=c A(\mathbf{x})=c \mathbf{0}=\mathbf{0}$, and so $c \mathbf{x} \in N$. Therefore $N$ is a subspace of $X$.

Let $R$ be the range of $A$, let $\mathbf{z}$ and $\mathbf{w}$ be any elements of $R$, and let $c$ be any scalar. By definition, there exist vectors $\mathrm{x} \in X$ and $\mathrm{y} \in X$ such that $z=A(\mathrm{x})$ and $w=A(\mathbf{y})$. Then $A(\mathbf{x}+\mathbf{y})=A(\mathbf{x})+A(\mathbf{y})=\mathbf{z}+\mathbf{w}$, and hence $\mathbf{z}+\mathbf{w} \in R$. Likewise $A(c \mathbf{x})=c A(\mathbf{x})=c \mathbf{z}$, so that $c \mathbf{z} \in R$. Therefore $R$ is a subspace of $Y$.

Exercise 9.5 Prove that to every $A \in L\left(R^{n}, R^{1}\right)$ corresponds a unique $\mathrm{y} \in R^{n}$ such that $A \mathbf{x}=\mathbf{x} \cdot \mathbf{y}$. Prove also that $\|A\|=|\mathbf{y}|$.

Hint: Under certain conditions, equality holds in the Schwarz inequality.

Solution. Let $\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}$ be the standard basis of $R^{n}$, and let $\mathbf{y}=A\left(\mathbf{e}_{1}\right) \mathbf{e}_{1}+$ $\cdots+A\left(\mathbf{e}_{n}\right) \mathbf{e}_{n}$. Then for any $\mathbf{x}=c_{1} \mathbf{e}_{1}+\cdots+c_{n} \mathbf{e}_{n}$ we have

$$
\begin{aligned}
A(\mathbf{x}) & =c_{1} A\left(\mathbf{e}_{1}\right)+\cdots+c_{n} A\left(\mathbf{e}_{n}\right) \\
& =\mathbf{y} \cdot \mathbf{x}
\end{aligned}
$$

There can be at most one such $\mathbf{y}$, since if $A(\mathbf{x})=\mathbf{z} \cdot \mathbf{x}$, then $|\mathbf{y}-\mathbf{z}|^{2}=\mathbf{y} \cdot \mathbf{y}-$ $\mathbf{y}: \mathbf{z}-\mathbf{z} \cdot \mathbf{y}+\mathbf{z} \cdot \mathbf{z}=A(\mathbf{y})-A(\mathbf{y})-A(\mathbf{z})+A(\mathbf{z})=0$.

By the Schwarz inequality we have

$$
|A(\mathbf{x})|=|\mathbf{y} \cdot \mathbf{x}| \leq|\mathbf{y}||\mathbf{x}|
$$

for all $\mathbf{x}$, so that $\|A\| \leq|\mathbf{y}|$. On the other hand $A(\mathbf{y})=\mathbf{y} \cdot \mathbf{y}=|\mathbf{y}|^{2}$, so that $\|A\| \geq|\mathbf{y}|$.

Exercise 9.6 If $f(0,0)=0$ and

$$
f(x, y)=\frac{x y}{x^{2}+y^{2}} \quad \text { if }(x, y) \neq(0,0)
$$

prove that $\left(D_{1} f\right)(x, y)$ and $\left(D_{2} f\right)(x, y)$ exist at every point of $R^{2}$, although $f$ is not continuous at $(0,0)$.

Solution. At any point $(x, y)$ except $(0,0)$ the differentiability of $f(x, y)$ follows from the rules for differentiation and the principles of Chapter 5 . At $(0,0)$ it is a routine computation to verify that both partial derivatives equal zero:

$$
\left(D_{1} f\right)(0,0)=\lim _{h \rightarrow 0} \frac{f(h, 0)-f(0,0)}{h}=0
$$

However, $f(x, y)$ is not continuous at $\left(0,0\right.$, since $f(x, x)=\frac{1}{2}$ for all $x \neq 0$, and hence $\lim _{x \rightarrow 0} f(x, x)=\frac{1}{2} \neq f(0,0)$.

Exercise 9.7 Suppose that $f$ is a real-valued function defined in an open set $E \subset R^{n}$, and that the partial derivatives $D_{1} f, \ldots, D_{n} f$ are bounded in $E$. Prove that $f$ is continuous in $E$

Hint: Proceed as in the proof of Theorem 9.21.

Solution. Let $\varepsilon>0$ be given, and let $\mathbf{x}^{0}=\left(x_{1}^{0}, \ldots, x_{n}^{0}\right)$ be any point of $E$. First choose $\delta_{0}>0$ so that $\mathbf{y} \in E$ if $\left|\mathbf{y}-\mathbf{x}^{0}\right|<2 \delta_{0}$. Then, if $M=$ $\max _{\mathbf{x} \in E}\left(\left(D_{1} f\right)(\mathbf{x}), \ldots,\left(D_{n} f\right)(\mathbf{x})\right)$, choose $\delta=\min \left(\delta_{0}, \frac{\varepsilon}{(n+1) M}\right)$. It then follows that if $\left|\mathbf{y}-\mathbf{x}^{0}\right|<\delta$, we have

$$
\begin{aligned}
\left|f(\mathbf{y})-f\left(\mathbf{x}^{0}\right)\right|= & \left|f\left(y_{1}, \ldots, y_{n}\right)-f\left(x_{1}^{0}, \ldots, x_{n}^{0}\right)\right| \\
\leq & \left|f\left(y_{1}, y_{2}, \ldots, y_{n}\right)-f\left(x_{1}^{0}, y_{2}, \ldots, y_{n}\right)\right|+ \\
& \left.+\mid f\left(x_{1}^{0}, y_{2}\right), \ldots, y_{n}\right)-f\left(x_{1}^{0}, x_{2}^{0}, \ldots, y_{n}\right) \mid+\cdots \\
& \cdots+\left|f\left(x_{1}^{0}, x_{2}^{0}, \ldots, x_{n-1}^{0}, y_{n}\right)-f\left(x_{1}^{0}, x_{2}^{0}, \ldots, x_{n-1}^{0}, x_{n}^{0}\right)\right|,
\end{aligned}
$$

where the ellipsis indicates terms of the form

$$
\left|f\left(x_{1}^{0}, x_{2}^{0}, \ldots, x_{k-1}^{0}, y_{k}, y_{k+1}, \ldots, y_{n}\right)-f\left(x_{1}^{0}, x_{2}^{0}, \ldots, x_{k-1}^{0}, x_{k}^{0}, y_{k+1}, \ldots, y_{n}\right)\right|
$$

By the mean-value theorem there is a number $c_{k}$ between $x_{k}^{0}$ and $y_{k}$ such that this last difference equals

$$
\left|\left(D_{k} f\right)\left(x_{1}^{0}, x_{2}^{0}, \ldots, x_{k-1}^{0}, c_{k}, y_{k+1}, \ldots, y_{n}\right)\left(y_{k}-x_{k}^{0}\right)\right|
$$

which is at most $M \delta$. Since by definition $M \delta$ is at most $\frac{\varepsilon}{n+1}$, and there are only $n$ such terms, it follows that $\left|f\left(\mathbf{x}^{0}\right)-f(\mathbf{y})\right|<\varepsilon$. Thus $f$ is continuous.

Remark: We have actually shown that $f(x)$ satisfies a Lipschitz condition on any convex subset of $E$, i.e., that $|f(\mathbf{x})-f(\mathbf{y})| \leq n M|\mathbf{x}-\mathbf{y}|$ on each convex subset.

Exercise 9.8 Suppose that $f$ is a differentiable real function in an open set $E \subset R^{n}$, and that $f$ has a local maximum at a point $\mathbf{x} \in E$. Prove that $f^{\prime}(\mathbf{x})=0$.

Solution. Let $\mathrm{y}$ be any element of $R^{n}$, and consider the real-valued function $\varphi(t)=f(\mathbf{x}+t \mathbf{y})$, defined near $t=0$. This function is differentiable (by Theorem $\left.9.15 \varphi(t)=f^{\prime}(\mathbf{x}+t \mathbf{y})(\mathbf{y})\right)$. Since $\varphi(t)$ has a maximum at $t=0$, it follows that $\varphi^{\prime}(0)=0$, i.e., that $f^{\prime}(\mathbf{x})(\mathbf{y})=0$. Since $\mathrm{y}$ is arbitrary, it follows by definition of the zero linear transformation that $f^{\prime}(\mathbf{x})$ is the zero linear transformation.

Exercise 9.9 If $\mathrm{f}$ is a differentiable mapping of a connected open set $E \subset R^{n}$ into $R^{m}$, and if $\mathbf{f}^{\prime}(\mathbf{x})=\mathbf{0}$ for for every $\mathbf{x} \in E$, prove that $\mathbf{f}$ is constant in $E$.

Solution. The mean-value argument given in Exercise 7 above, applied to each component of $\mathbf{f}$, shows that $\mathbf{f}$ is locally constant (the partial derivatives are all zero). Hence, if $\mathbf{x}^{0}$ is any point of $E$, the set of $\mathbf{x}$ such that $\mathbf{f}(\mathbf{x})=\mathbf{f}\left(\mathbf{x}^{0}\right)$ is an open set. Since this set is also closed in $E$, and $E$ is connected, it follows that it must be all of $E$.

Exercise 9.10 If $f$ is a real function defined in a convex open set $E \subset R^{n}$, such that $\left(D_{1} f\right)(\mathbf{x})=0$ for every $\mathbf{x} \in E$, prove that $f(\mathbf{x})$ depends only on $x_{2}, \ldots, x_{n}$.

Show that the convexity of $E$ can be replaced by a weaker condition, but that some condition is required. For example, if $n=2$ and $E$ is shaped like a horseshoe, the statement may be false.

Solution. We need to show that $f\left(x_{1}^{0}, x_{2}, \ldots, x_{n}\right)=f\left(x_{1}^{1}, x_{2}, \ldots, x_{n}\right)$ whenever $\mathrm{x}^{0}=\left(x_{1}^{0}, x_{2}, \ldots, x_{n}\right)$ and $\mathbf{x}^{1}=\left(x_{1}^{1}, x_{2}, \ldots, x_{n}\right)$ both belong to $E$. Since $E$ is convex, the line segment joining $\mathbf{x}^{0}$ and $\mathbf{x}^{1}$ is contained in $E$. The meanvalue theorem applies on this line segment, showing that $f\left(\mathrm{x}^{0}\right)-f\left(\mathrm{x}^{1}\right)=\left(x_{1}^{0}-\right.$ $\left.x_{1}^{1}\right)\left(D_{1} f\right)(\mathbf{x})$ for some point $\mathbf{x}$ on this interval. Hence the result now follows from the hypothesis.

Note that convexity is needed only on each line segment through $E$ parallel to the $x_{1}$-axis. Thus if the intersection of $E$ with each line parallel to the $x_{1}$-axis is an interval and $\left(D_{1} f\right)(\mathbf{x})=0$ for all $\mathbf{x} \in E$, then $f$ is independent of $x_{1}$.

If we define $f(x, y)$ on all of $R^{2}$ except the nonnegative portion of the $y$-axis by specifying

$$
f(x, y)= \begin{cases}0 & \text { if } y<0 \text { or } x<0 \\ y^{2} & \text { if } y \geq 0 \text { and } x>0\end{cases}
$$

then $f(x, y)$ is continuously differentiable on its domain, $\left(D_{1} f\right)(x, y)=0$ everywhere on that domain, yet $f(-1,1)=0 \neq 1=f(1,1)$, so that $f$ is not independent of $x$.

Exercise 9.11 If $f$ and $g$ are differentiable real functions in $R^{n}$, prove that

$$
\nabla(f g)=f \nabla g+g \nabla f
$$

and that $\nabla(1 / f)=-f^{-2} \nabla f$ wherever $f \neq 0$.

Solution. This is a routine computation applied to the $i$ th component of the various quantities.

Exercise 9.12 Fix two real numbers $a$ and $b, 0<a<b$. Define a mapping $\mathbf{f}=\left(f_{1}, f_{2}, f_{3}\right)$ of $R^{2}$ into $R^{3}$ by

$$
\begin{aligned}
f_{1}(s, t) & =(b+a \cos s) \cos t \\
f_{2}(s, t) & =(b+a \cos s) \sin t \\
f_{3}(s, t) & =a \sin s
\end{aligned}
$$

Describe the range $K$ of $f$. (It is a certain compact subset of $R^{3}$.)

(a) Show that there are exactly 4 points $\mathbf{p} \in K$ such that

$$
\left(\nabla f_{1}\right)\left(\mathbf{f}^{-1}(\mathbf{p})\right)=\mathbf{0}
$$

Find these points.

(b) Determine the set of all $\mathbf{q} \in K$ such that

$$
\left(\nabla f_{3}\right)\left(\mathbf{f}^{-1}(\mathbf{q})\right)=\mathbf{0}
$$

(c) Show that one of the points $\mathbf{p}$ found in part $(a)$ corresponds to a local maximum of $f_{1}$, one corresponds to a local minimum, and that the other two are neither (they are so-called "saddle points").

Which of the points $\mathbf{q}$ found in part $(b)$ correspond to maxima or minima? (d) Let $\lambda$ be an irrational number, and define $\mathbf{g}(t)=\mathbf{f}(t, \lambda t)$. Prove that $\mathbf{g}$ is a 1-1 mapping of $R^{1}$ onto a dense subset of $K$. Prove that

$$
\left|\mathbf{g}^{\prime}(t)\right|^{2}=a^{2}+\lambda^{2}(b+a \cos t)^{2}
$$

Solution. The range $K$ is a torus obtained by moving a circle of radius $a$ with center on a circle of radius $b$, always keeping the planes of the two circles perpendicular and each plane passing through the center of the other circle. This can be seen by observing that in cylindrical coordinates the parametric equations say $r=b+a \cos s, z=a \sin x$, i.e., $(r-b)^{2}+z^{2}=a^{2}$, which, together with the equation $\theta=$ const, gives the equation of a circle with center at $(b, 0)$ and radius $a$ in the half-plane $\theta=$ const.

(a) The equation $\left(\nabla f_{1}\right)(s, t)=0$ says $-a \sin s \cos t=0$ and $-(b+a \cos s) \sin t=$ 0 . This second equation requires $t=k \pi$, and since these functions have period $2 \pi$ in both $s$ and $t$, we may as well assume $t=0$ or $t=\pi$. In that case the first equation implies $s=0$ or $s=\pi$. Hence the only points $\mathbf{p}$ satisfying this equation are the images of the points $(0,0),(0, \pi),(\pi, 0)$, and $(\pi, \pi)$, i.e., the points $(b+a, 0,0),(b-a, 0,0),(-b+a, 0,0)$, and $(-b-a, 0,0)$.

(b) The equation $\left(\nabla f_{3}\right)(s, t)=0$ says only that $a \cos s=0$, i.e., $s=\frac{\pi}{2}$ or $s=\frac{3 \pi}{2}$. The image of these two conditions consists of the two circles of radius $b$ about the $z$-axis in the planes $z= \pm a$.

(c) The point $(a+b, 0,0)$ is the maximum possible value of $f_{1}(s, t)$, and occurs only when $\cos s=1$ and $\cos t=1$. Likewise the point $(-a-b, 0,0)$ is the minimum possible value, and occurs only when $\cos s=1$ and $\cos t=-1$. The other two points, which occur when $s=0, t=\pi$ and when $s=\pi, t=0$, lie near points of both larger and smaller values of $f_{1}(s, t)$. For example, when $s=0$, the point $t=\pi$ is a minimum for the function $\varphi(t)=f_{1}(0, t)=b \cos t$; but when $t=\pi$, the point $s=0$ is a maximum of $\psi(s)=f_{1}(s, \pi)=-(b+a \cos s)$. Hence the point $(0, \pi)$ is neither a maximum nor a minimum for $f_{1}(s, t)$.

The points with $z=+a$ are obviously absolute maxima of $f_{3}(s, t)$, while those with $z=-a$ are the absolute minima.

(d) Suppose $\mathbf{g}\left(t_{1}\right)=\mathbf{g}\left(t_{2}\right)$. Then because $a \sin t_{1}=a \sin t_{2}$, and

$$
\sqrt{\left(f_{1}\left(t_{1}, \lambda t_{1}\right)\right)^{2}+\left(f_{2}\left(t_{1}, \lambda t_{1}\right)\right)^{2}}=\sqrt{\left(f_{1}\left(t_{2}, \lambda t_{2}\right)\right)^{2}+\left(f_{2}\left(t_{2}, \lambda t_{2}\right)\right)^{2}}
$$

(that is, $b+a \cos t_{1}=b+a \cos t_{2}$ ), we have $\sin t_{1}=\sin t_{2}$ and $\cos t_{1}=\cos t_{2}$. Therefore $\sin \left(t_{1}-t_{2}\right)=0$, which means $t_{2}=t_{1}+k \pi$ for some integer $k$. Because $\sin t_{1}=\sin t_{2}$, it follows that $k$ is an even integer, say $k=2 m$. It then follows, since $f_{i}\left(t_{1}, \lambda t_{1}\right)=f_{i}\left(t_{2}, \lambda t_{2}\right), \lambda=1,2$, that $\cos \lambda t_{1}=\cos \lambda t_{2}$ and $\sin \lambda t_{1}=\sin \lambda t_{2}$. This in turn implies that $\lambda t_{2}=\lambda t_{2}+2 r \pi$ for some integer $r$. Combining these two results, we find that $m \lambda=r$. Since $\lambda$ is irrational, this means that $m=0=r$, i.e., $t_{2}=t_{1}$. Thus $\mathbf{g}(t)$ is one-to-one.

To show that the range is dense in $K$, we need only show that the numbers $2 \pi n \lambda, n=0 \pm 1, \pm 2, \ldots$, are dense "modulo $2 \pi$," meaning that for any real number $\theta$ and any $\varepsilon>0$ there is are integers $m$ and $n$ such that $\mid 2 \pi n \lambda-2 \pi m-$ $\theta \mid<\varepsilon$. A proposition easily seen to be equivalent is that for any $\eta>0$ and any real number $c$ there exist integers $m$ and $n$ such that $|n \lambda-m-c|<\eta$. (This statement is obvious $(m=n=0)$ if $c=0$.) To prove that, fix an integer $r$ larger than $\frac{1}{\eta}$, and consider the numbers $0, \lambda-[\lambda], 2 \lambda-[2 \lambda], \ldots, r \lambda-[r \lambda]$. There are $r+1$ such numbers, all lying in the interval $[0,1)$. Hence two of them must
be closer than $\frac{1}{r}$ to each other, say $0<s \lambda-[s \lambda]-t \lambda+[t \lambda]<\frac{1}{r}$. In particular, the number $(s-t) \lambda$ lies within $\frac{1}{r}$ of an integer (namely $[s \lambda]-[t \lambda]$. Thus we have, say $(s-t) \lambda=k+\delta$, where $0<\delta<\frac{1}{r}$. Let $p$ be the unique integer such that $p \delta \leq c<(p+1) \delta$. We then have $p(s-t) \lambda=p k+p \delta$, and hence, taking $n=p(s-t)$ and $m=p k$, we find $|n \lambda-m-c|=|p \delta-c|<\delta<\frac{1}{r}<\eta$.

This being established, consider any point in $K$, say the point $\mathbf{p}=(b+$ $\left.\left.a \cos s_{0}\right) \cos t_{0},\left(b+a \cos s_{0}\right) \sin t_{0}, a \sin s_{0}\right)$, and let $\varepsilon>0$ be given. According to what was just established, there are integers $m, n$ such that $\mid 2 \pi m \lambda-2 \pi n-$ $\left(t_{0}-s_{0} \lambda\right) \mid<\frac{\varepsilon}{3 a+3 b}$. It then follows that

$$
\begin{aligned}
\left|\cos \left(\left(s_{0}+2 \pi m\right) \lambda\right)-\cos t_{0}\right| & =\left|\cos \left(\left(s_{0}+2 \pi m\right) \lambda-2 \pi n\right)-\cos t_{0}\right| \\
& \leq \frac{\varepsilon}{3 a+3 b}
\end{aligned}
$$

where we have used the inequality $|\cos u-\cos v| \leq|u-v|$, with $u=\left(s_{0}+\right.$ $2 \pi m) \lambda-2 \pi n$ and $v=t_{0}$. A similar inequality applies with sin in place of cos. It then follows that $\left|\mathbf{g}\left(s_{0}+2 \pi m\right)-\mathbf{p}\right| \leq \frac{2 \varepsilon}{3}<\varepsilon$. Therefore the range of $\mathbf{g}$ is dense in $K$.

The equation

$$
\left|\mathbf{g}^{\prime}(t)\right|^{2}=a^{2}+\lambda^{2}(b+a \cos t)^{2}
$$

is a routine, though tedious, computation.

Exercise 9.13 Suppose $\mathrm{f}$ is a differentiable mapping of $R^{1}$ into $R^{3}$ such that $|\mathbf{f}(t)|=1$ for every $t$. Prove that $\mathbf{f}^{\prime}(t) \cdot \mathbf{f}(t)=0$.

Interpret this result geometrically.

Solution. This result is obtained by merely differentiating the relation $\mathbf{f}(t) \cdot \mathbf{f}(t)=$ 1. Geometrically it asserts that the velocity vector of a point moving over a sphere is tangent to the sphere (perpendicular to the radius vector from the center of the sphere to the point).

Exercise 9.14 Define $f(0,0)=0$ and

$$
f(x, y)=\frac{x^{3}}{x^{2}+y^{2}} \quad \text { if }(x, y) \neq(0,0)
$$

(a) Prove that $D_{1} f$ and $D_{2} f$ are bounded functions in $R^{2}$. (Hence $f$ is continuous.)

(b) Let $\mathbf{u}$ be any unit vector in $R^{2}$. Show that the directional derivative $\left(D_{u} f\right)(0,0)$ exists, and that its absolute value is at most 1 .

(c) Let $\gamma$ be a differentiable mapping of $R^{1}$ into $R^{2}$ (in other words, $\gamma$ is a differentiable curve in $\left.R^{2}\right)$, with $\gamma(0)=(0,0)$ and $\left|\gamma^{\prime}(0)\right|>0$. Put $g(t)=f(\gamma(t))$ and prove that $g$ is differentiable for every $t \in R^{1}$.

If $\gamma \in \mathcal{C}^{\prime}$, prove that $g \in \mathcal{C}^{\prime}$.
(d) In spite of this, prove that $f$ is not differentiable at $(0,0)$.

Hint: Formula (40) fails.

Solution. (a) For $(x, y) \neq(0,0)$ we have

$$
D_{1} f(x, y)=\frac{x^{2}\left(x^{2}+3 y^{2}\right)}{\left(x^{2}+y^{2}\right)^{2}}, \quad D_{2} f(x, y)=-\frac{2 x^{3} y}{\left(x^{2}+y^{2}\right)^{2}}
$$

It follows that

$$
0 \leq D_{1} f(x, y) \leq \frac{3 x^{2}}{x^{2}+y^{2}} \leq 3
$$

and

$$
\left|D_{2} f(x, y)\right| \leq \frac{x^{2}}{x^{2}+y^{2}} \leq 1
$$

Also $D_{1} f(0,0)=\lim _{x \rightarrow 0} \frac{f(x, 0)-f(0,0)}{x}=\lim _{x \rightarrow 0} \frac{x-0}{x}=1$, and $D_{2} f(0,0)=$ $\lim _{y \rightarrow 0} \frac{f(0, y)-f(0,0)}{y}=\lim _{y \rightarrow 0} \frac{0}{y}=0$. Hence, as asserted, $f(x, y)$ is continuous. $\cos ^{3} \theta$.

(b) Let $\mathbf{u}=(\cos \theta, \sin \theta)$. Then $D_{\mathbf{u}} f(0,0)=\lim _{t \rightarrow 0} \frac{f(t \cos \theta, t \sin \theta)-f(0,0)}{t}=$

(c) Suppose $u(t)$ and $v(t)$ satisfy $u(0)=0=v(0), u^{\prime}(t)$ and $v^{\prime}(t)$ exist for every $t$, and $u^{\prime}(t)$ and $v^{\prime}(t)$ do not both vanish at the same value of $t$. Setting $g(t)=f(u(t), v(t))$, we find that $g(t)$ is obviously differentiable at any value of $t$ where $u(t)$ and $v(t)$ are not both zero. Now suppose $u\left(t_{0}\right)=v\left(t_{0}\right)=0$. Then, since one of $u(t)$ and $v(t)$ is one-to-one on a neighborhood of $t_{0}$, it follows that, for small non-zero values of $t-t_{0}$ we have $(u(t))^{2}+(v(t))^{2}>0$, and then

$$
\begin{aligned}
\frac{g(t)-g\left(t_{0}\right)}{t-t_{0}} & =\frac{f(u(t), v(t))-f\left(u\left(t_{0}\right), v\left(t_{0}\right)\right)}{t-t_{0}} \\
& =\frac{\left(\frac{u(t)-u\left(t_{0}\right)}{t-t_{0}}\right)^{3}}{\left(\frac{u(t)-u\left(t_{0}\right)}{t-t_{0}}\right)^{2}+\left(\frac{v(t)-v\left(t_{0}\right)}{t-t_{0}}\right)^{2}}
\end{aligned}
$$

so that

$$
g^{\prime}\left(t_{0}\right)=\lim _{t \rightarrow t_{0}} \frac{g(t)-g\left(t_{0}\right)}{t-t_{0}}=\frac{\left(u^{\prime}\left(t_{0}\right)\right)^{3}}{\left(u^{\prime}\left(t_{0}\right)\right)^{2}+\left(v^{\prime}\left(t_{0}\right)\right)^{2}}
$$

Thus $g(t)$ is differentiable. Observe that if $\gamma(t) \neq(0,0)$, then

$$
g^{\prime}(t)=\frac{(u(t))^{4} u^{\prime}(t)+3(u(t) v(t))^{2} u^{\prime}(t)-2(u(t))^{3} v(t) v^{\prime}(t)}{\left((u(t))^{2}+(v(t))^{2}\right)^{2}}
$$

The same argument used above to prove that $g^{\prime}\left(t_{0}\right)$ exists shows that

$$
\lim _{t \rightarrow t_{0}} g^{\prime}(t)=\frac{\left(u^{\prime}\left(t_{0}\right)\right)^{5}+\left(u^{\prime}\left(t_{0}\right)\right)^{3}\left(v^{\prime}\left(t_{0}\right)\right)^{2}}{\left(\left(u^{\prime}\left(t_{0}\right)\right)^{2}+\left(v^{\prime}\left(t_{0}\right)\right)^{2}\right)^{2}}=\frac{\left(u^{\prime}\left(t_{0}\right)\right)^{3}}{\left(u^{\prime}\left(t_{0}\right)\right)^{2}+\left(v^{\prime}\left(t_{0}\right)\right)^{2}}=g^{\prime}\left(t_{0}\right)
$$

so that $g^{\prime}$ is continuous at $t_{0}$ if $u^{\prime}$ and $v^{\prime}$ are. Continuity of $g^{\prime}$ at other points follows from the chain rule.

If $f$ is differentiable at $(0,0)$, we necessarily have

$$
f(x, y)=f(0,0)+\left[x D_{1} f(0,0)+y D_{2} f(0,0)\right]+\varepsilon(x, y)
$$

where

$$
\lim _{(x, y) \rightarrow(0,0)} \frac{\varepsilon(x, y)}{\sqrt{x^{2}+y^{2}}}=0
$$

Since $D_{1} f(0,0)=1$ and $D_{2} f(0,0)=0$, it follows that

$$
\varepsilon(x, y)=\frac{-x y^{2}}{x^{2}+y^{2}}
$$

and so we must have

$$
\lim _{(x, y) \rightarrow(0,0)} \frac{-x y^{2}}{\left(x^{2}+y^{2}\right)^{3 / 2}}=0
$$

But this is clearly not the case, as we see by taking $x=y$. (The limit is then $-2^{-3 / 2}$.)

Exercise 9.15 Define $f(0,0)=0$, and put

$$
f(x, y)=x^{2}+y^{2}-2 x^{2} y-\frac{4 x^{6} y^{2}}{\left(x^{4}+y^{2}\right)^{2}}
$$

if $(x, y) \neq(0,0)$.

(a) Prove, for all $(x, y) \in R^{2}$, that

$$
4 x^{4} y^{2} \leq\left(x^{4}+y^{2}\right)^{2}
$$

Conclude that $f$ is continuous.

(b) For $0 \leq \theta \leq 2 \pi,-\infty<t<\infty$, define

$$
g_{\theta}(t)=f(t \cos \theta, t \sin \theta)
$$

Show that $g_{\theta}(0)=0, g_{\theta}^{\prime}(0)=0, g_{\theta}^{\prime \prime}(0)=2$. Each $g_{\theta}$ has therefore a strict local minimum at $t=0$.

In other words, the restriction of $f$ to each line through $(0,0)$ has a strict local minimum at $(0,0)$.

(c) Show that $(0,0)$ is nevertheless not a local minimum for $f$, since $f\left(x, x^{2}\right)=$ $-x^{4}$.

Solution. (a) This inequality follows by squaring the inequality $2 x^{2}|y| \leq x^{4}+y^{2}$, which in turn is equivalent to the inequality $\left(x^{2}-|y|\right)^{2} \geq 0$. Then, since $f(x, y)$
is obviously continuous except at $(0,0)$, the continuity at the remaining point follows from the inequality

$$
|f(x, y)-f(0,0)| \leq 2 x^{2}+y^{2}+2 x^{2}|y|
$$

which is easily derived from the inequality just proved and the definition of $f(x, y)$.

(b) We observe that for $t \neq 0$ we have

$$
g_{\theta}(t)=t^{2}-2 t^{3} \cos ^{2} \theta \sin \theta-4 t^{4} \frac{\cos ^{6} \theta \sin ^{2} \theta}{\left(t^{2} \cos ^{4} \theta+\sin ^{2} \theta\right)^{2}}
$$

from which it is routine computation to show that $g_{\theta}(0)=0=g_{\theta}^{\prime}(0)$ and $g_{\theta}^{\prime \prime}(0)=2$.

(c) The assertion that $f\left(x, x^{2}\right)=-x^{4}$ is routine computation. It implies that $f(x, y)$ assumes negative values in any neighborhood of $(0,0)$, and hence that the $f(x, y)$ does not have a local minimum at $(0,0)$.

Exercise 9.16 Show that the continuity of $\mathrm{f}^{\prime}$ at the point $\mathbf{a}$ is needed in the inverse function theorem, even in the case $n=1$ : If

$$
f(t)=t+2 t^{2} \sin \left(\frac{1}{t}\right)
$$

for $t \neq 0$, and $f(0)=0$, then $f^{\prime}(0)=1, f^{\prime}$ is bounded in $(-1,1)$ but $f$ is not one-to-one in any neighborhood of 0 .

Solution. The assertion that $f^{\prime}(0)=1$ is proved by direct computation: $\frac{f(t)}{t}=$ $1+2 t \sin \left(\frac{1}{t}\right) \rightarrow 1$ as $t \rightarrow 0$. Since $f^{\prime}(t)=1+4 t \sin \left(\frac{1}{t}\right)-2 \cos \left(\frac{1}{t}\right)$ for $t \neq 0$, it follows that $\left|f^{\prime}(t)\right| \leq 7$ for all $t \in(-1,1)$. To show that $f$ is not one-to-one in any neighborhood of 0 , we observe that $f^{\prime}\left(\frac{1}{k \pi}\right)=1+2(-1)^{k}$, so that $f(t)$ is decreasing at $t=\frac{1}{k \pi}$ if $k$ is odd and increasing if $k$ is even. It follows that the minimum value of $f(t)$ on the interval $\left[\frac{1}{(2 k+1) \pi}, \frac{1}{2 k \pi}\right]$ is assumed at an interior point, so that $f(t)$ cannot be one-to-one on this interval.

Exercise 9.17 Let $\mathbf{f}=\left(f_{1}, f_{2}\right)$ be the mapping of $R^{2}$ into $R^{2}$ given by

$$
f_{1}(x, y)=e^{x} \cos y, \quad f_{2}(x, y)=e^{x} \sin y
$$

(a) What is the range of $f$ ?

(b) Show that the Jacobian of $\mathrm{f}$ is not zero at any point of $R^{2}$. Thus every point of $R^{2}$ has a neighborhood in which $\mathrm{f}$ is one-to-one. Nevertheless, $\mathrm{f}$ is not one-to-one on $R^{2}$.

(c) Put $\mathbf{a}=(0, \pi / 3), \mathbf{b}=f(\mathbf{a})$, let $\mathbf{g}$ be the continuous inverse of $\mathbf{f}$, defined in a neighborhood of $\mathbf{b}$, such that $\mathbf{g}(\mathbf{b})=\mathbf{a}$. Find an explicit formula for $\mathbf{g}$, compute $\mathbf{f}^{\prime}(\mathbf{a})$ and $\mathbf{g}^{\prime}(\mathbf{b})$, and verify the formula (52).
(d) What are the images under $f$ of lines parallel to the coordinate axes?

Solution. (a) The range of $\mathbf{f}$ is all of $R^{2}$ except the point $(0,0)$. Indeed if $(u, v) \neq(0,0)$, choose $y$ so that

$$
\cos y=\frac{u}{\sqrt{u^{2}+v^{2}}}, \quad \sin y=\frac{v}{\sqrt{u^{2}+v^{2}}}
$$

and let $x=\ln \sqrt{u^{2}+v^{2}}$, so that $e^{x}=\sqrt{u^{2}+v^{2}}$. It is then obvious from the equations defining $y$ and $x$ that $u=e^{x} \cos y$ and $v=e^{x} \sin y$. Hence every point except $(0,0)$ is in the range of $f$. The point $(0,0)$ is not in the range, since $u^{2}+v^{2}=e^{2 x}>0$ for any point $(u, v)=\mathbf{f}(x, y)$.

(b) The Jacobian of $\mathrm{f}(x, y)$ is $e^{2 x}$, which is never zero. However, since $\mathrm{f}(x, y+$ $2 \pi)=\mathbf{f}(x, y)$, it follows that $\mathbf{f}$ is not one-to-one.

(c) By our definition $\mathbf{b}=\left(\frac{1}{2}, \frac{\sqrt{3}}{2}\right)$. We can therefore take $y=\arctan \left(\frac{v}{u}\right)$ for $(u, v)$ near $\mathbf{b}$, the arctangent being between $-\frac{\pi}{2}$ and $\frac{\pi}{2}$. Thus we have $\mathbf{g}(u, v)=\left(\ln \sqrt{u^{2}+v^{2}}, \arctan \left(\frac{v}{u}\right)\right.$. We then have

$$
\mathbf{f}^{\prime}(x, y)=\left(\begin{array}{cc}
e^{x} \cos y & -\dot{e}^{x} \sin y \\
e^{x} \sin y & e^{x} \cos y
\end{array}\right), \quad \mathbf{g}^{\prime}(u, v)=\left(\begin{array}{cc}
\frac{u}{u^{2}+v^{2}} & \frac{v}{u^{2}+v^{2}} \\
\frac{-v}{u^{2}+v^{2}} & \frac{u}{u^{2}+v^{2}}
\end{array}\right)
$$

When we take $u=e^{x} \cos y$ and $v=e^{x} \sin y$, we find that

$$
\mathbf{g}^{\prime}(\mathbf{f}(x, y))=\left(\begin{array}{cc}
e^{-x} \cos y & e^{-x} \sin y \\
-e^{-x} \sin y & e^{-x} \cos y
\end{array}\right)
$$

It is then a routine computation to verify that $\mathbf{g}^{\prime}(\mathbf{f}(x, y)) \mathbf{f}^{\prime}(x, y)=\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)$.
Likewise we find

$$
\mathbf{f}^{\prime}(\mathbf{g}(u, v))=\left(\begin{array}{cc}
u & -v \\
v & u
\end{array}\right)
$$

and a routine computation shows that $\mathbf{f}^{\prime}(\mathbf{g}(u, v)) \mathbf{g}^{\prime}(u, v)=\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)$.

(d) The family of lines $x=c$ maps to the family of concentric circles $u^{2}+v^{2}=$ $e^{2 c}$. The lines $y=c$ map to half-lines $v=K u, u \geq 0$, where $K=\tan y$. (If $y$ is an odd multiple of $\frac{\pi}{2}$, the half-line is either the positive or negative $u$-axis.

Exercise 9.18 Answer analogous questions for the mapping defined by

$$
u=x^{2}-y^{2}, \quad v=2 x y .
$$

Solution. (a) the range of the mapping $\mathbf{f}(x, y)=\left(x^{2}-y^{2}, 2 x y\right)$ is the entire plane $R^{2}$. Indeed, every point $(u, v)$ except $(0,0)$ has two distinct preimages, one of which is

$$
x=\sqrt{\frac{\sqrt{u^{2}+v^{2}}+u}{2}}, \quad y=(\operatorname{sgn} v) \sqrt{\frac{\sqrt{u^{2}+v^{2}}-u}{2}} .
$$

(The other preimage is $-x,-y$, with this $x$ and this $y$.)

(b) The Jacobian of $\mathrm{f}$ vanishes only at $x=y=0$. Indeed,

$$
f^{\prime}(x, y)=\left(\begin{array}{cc}
2 x & -2 y \\
2 y & 2 x
\end{array}\right)
$$

Hence the Jacobian is $4\left(x^{2}+y^{2}\right)$.

(c) Taking $\mathbf{a}=(3,4)$, so that $\mathbf{b}=(-7,24)$, we can take, locally

$$
g(u, v)=\left(\sqrt{\frac{\sqrt{u^{2}+v^{2}}+u}{2}}, \sqrt{\frac{\sqrt{u^{2}+v^{2}}-u}{2}}\right)
$$

We then have

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-516.jpg?height=191&width=1294&top_left_y=1054&top_left_x=470)

Noting that the defining relations imply $u^{2}+v^{2}=\left(x^{2}+y^{2}\right)^{2}$, we see that

$$
\mathbf{g}^{\prime}(\mathbf{f}(x, y))=\left(\begin{array}{cc}
\frac{x}{2\left(x^{2}+y^{2}\right)} & \frac{y}{2\left(x^{2}+y^{2}\right)} \\
\frac{-y}{2\left(x^{2}+y^{2}\right)} & \frac{x}{2\left(x^{2}+y^{2}\right)}
\end{array}\right)
$$

from which we see easily that $\mathbf{g}^{\prime}(\mathbf{f}(x, y)) \mathbf{f}^{\prime}(x, y)=\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)$. The corresponding equality with $\mathbf{g}$ and $\mathbf{f}$ interchanged is likewise simple, though more cumbersome to write out.

Exercise 9.19 Show that the system of equations

$$
\begin{aligned}
3 x+y-z+u^{2} & =0 \\
x-y+2 z+u & =0 \\
2 x+2 y-3 z+2 u & =0
\end{aligned}
$$

can be solved for $x, y, u$ in terms of $z$; for $x, z, u$ in terms of $y$; for $y, z, u$ in terms of $x$; but not for $x, y, z$ in terms of $u$.

Solution. Adding the last two equations and subtracting the first yields $3 u-u^{2}=$ 0 , whence either $u=0$ or $u=3$. Hence unless $u$ has one of these two values, there are no solutions at all. Therefore the system cannot generally be solved for $x, y, z$ in terms of $u$. If one of these two equations holds, we can solve just the last two equations for any two of the variables $x, y, z$ in terms of the third. The remaining equation will then automatically be satisfied. For example,

$$
x=-\frac{z}{4}, y=\frac{7 z}{4}, u=0 ; \quad x=-\frac{9+z}{4}, y=\frac{3+7 z}{4}, u=3
$$

We could also have

$$
x=-\frac{y}{7}, z=\frac{4 y}{7}, u=0 ; \quad x=\frac{60+4 y}{7}, z=\frac{4 y-3}{7}, u=3 .
$$

Finally, we could also have

$$
y=-7 x, \quad z=-4 x, u=0 ; \quad y=\frac{7 x-60}{4}, z=9-4 x, u=3
$$

Note that the matrix of the derivative of the transformation $\mathrm{f}(x, y, z, u)=$ $\left(3 x+y-z+u^{2}, x-y+2 z+u, 2 x+2 y-3 z+2 u\right)$ is

$$
\mathbf{f}^{\prime}(x, y, z, u)=\left(\begin{array}{cccc}
3 & 1 & -1 & 2 u \\
1 & -1 & 2 & 1 \\
2 & 2 & -3 & 2
\end{array}\right)
$$

and any $3 \times 3$ submatrix containing the last column is invertible when $u=0$ or $u=3$. However, the first three columns of this matrix does not form an invertible matrix.

Exercise 9.20 Take $n=m=1$ in the implicit function theorem, and interpret the theorem (as well as its proof) graphically.

Solution. The theorem asserts that if $f(x, y)$ is continuously differentiable in a neighborhood of $\left(x_{0}, y_{0}\right), f\left(x_{0}, y_{0}\right)=0$, and $D_{2} f\left(x_{0}, y_{0}\right) \neq 0$, then there exist 1) an interval $\left.I=\left(x_{0}-\delta, x_{0}+\delta\right), 2\right)$ an interval $J=\left(y_{0}-\eta, y_{0}+\eta\right)$, and 3$)$ a continuously differentiable function $\varphi: I \rightarrow J$ such that for all $(x, y) \in I \times J$ the equation $f(x, y)=0$ holds if and only if $y=\varphi(x)$.

The proof amounts to the argument that, since $D_{2} f\left(x_{0}, y_{0}\right) \neq 0$ and $f$ is continuously differentiable, it must be that $D_{2} f(x, y) \neq 0$ for all $(x, y)$ near $\left(x_{0}, y_{0}\right)$. Hence the function $g(y)=f\left(x_{0}, y\right)$ is strictly monotonic near $y=y_{0}$. Therefore, since $g\left(y_{0}\right)=0$, there is a small interval $\left[y_{0}-\eta, y_{0}+\eta\right]$ such that $g\left(y_{0}-\eta\right)$ and $g\left(y_{0}+\eta\right)$ have opposite signs. By the continuity of $f(x, y)$, it follows that $f\left(x, y_{0}-\eta\right)$ has the same sign as $f\left(x_{0}, y_{0}-\eta\right)$ if $x$ is near $x_{0}$, and similarly $f\left(x, y_{0}+\eta\right)$ has the same sign as $f\left(x_{0}, y_{0}+\eta\right)$ for $x$ near $x_{0}$. That is, $f\left(x, y_{0}-\eta\right)$ and $f\left(x, y_{0}+\eta\right)$ have opposite signs if $x$ is near $x_{0}$. It follows that there is a point $\varphi(x) \in\left(y_{0}-\eta, y_{0}+\eta\right)$ such that $f(x, \varphi(x))=0$. By restricting the neighborhood so that $D_{2} f(x, y)$ is of constant sign, we assure that $g_{x}(y)=f(x, y)$ is monotonic on $\left[y_{0}-\eta, y_{0}+\eta\right]$ for each $x$ near $x_{0}$. It then follows that there can be at most one value of $y$ in $\left(y_{0}-\eta, y_{0}+\eta\right)$ satisfying the equation $f(x, y)=0$. That is, the function $\varphi(x)$ is unique. This proves all but the differentiability of $\varphi$.

The graphical interpretation is that, near a point on a smooth curve $f(x, y)=$ 0 where the tangent is not vertical $\left(D_{2} f\left(x_{0}, y_{0}\right) \neq 0\right)$ the curve intersects each vertical line exactly once.

Exercise 9.21 Define $f$ in $R^{2}$ by

$$
f(x, y)=2 x^{3}-3 x^{2}+2 y^{3}+3 y^{2}
$$

(a) Find the four points in $R^{2}$ at which the gradient of $f$ is zero. Show that $f$ has exactly one local maximum and one local minimum in $R^{2}$.

(b) Let $X$ be the set of all $(x, y) \in R^{2}$ at which $f(x, y)=0$. Find those points of $S$ that have no neighborhoods in which the equation $f(x, y)=0$ can be solved for $y$ in terms of $x$ (or for $x$ in terms of $y$ ). Describe $S$ as precisely as you can.

Solution. (a) We have $\nabla f(x, y)=6\left(x^{2}-x\right) \mathbf{i}+6\left(y^{2}+y\right) \mathbf{j}$. Hence $\nabla f(x, y)=\mathbf{0}$ precisely at the four points $(0,0),(1,0),(0,-1),(1,-1)$. Since the Hessian matrix of $f$ is

$$
\left(\begin{array}{cc}
12 x-2 & 0 \\
0 & 12 y+2
\end{array}\right)
$$

this matrix has a positive determinant when $x>\frac{1}{6}$ and $y>\frac{-1}{6}$ or when $x<\frac{1}{6}$ and $y<\frac{-1}{6}$. Thus $(1,0)$ and $(0,-1)$ are possible extrema. Since $12 x-2>0$ at $(1,0)$, that point is a minimum. Likewise $(0,-1)$ is a maximum.

(b) Since $f(x, y)=(x+y)\left[2 x^{2}-2 x y+2 y^{2}-3 x+3 y\right]$, the equation $f(x, y)=0$ has the real solution $y=-x$ for every real value of $x$. In addition, if $-\frac{1}{2} \leq x \leq \frac{3}{2}$, it has the real solutions

$$
y=\frac{2 x-3+\sqrt{9+12 x-12 x^{2}}}{4}, \quad y=\frac{2 x-3-\sqrt{9+12 x-12 x^{2}}}{4}
$$

According to the implicit function theorem, the only possible points near which there might not be a unique solution are for $y$ in terms of $x$ are those where $y=0$ or $y=-1$. The corresponding values of $x$ are $x=0$ and $x=\frac{3}{2}$ for $y=0$ and $x=1$ and $x=-\frac{1}{2}$ for $y=-1$.

We observe that both solutions $y=-x$ and $y=\frac{2 x-3+\sqrt{9+12 x-12 x^{2}}}{4}$ tend to 0 as $x \rightarrow 0$. Hence there is no unique solution for $y$ near $(0,0)$. As $x \uparrow \frac{3}{2}$, the quantity under the radical sign tends to zero, and hence these two solutions converge toward the common value $y=0$. Hence the point $\left(\frac{3}{2}, 0\right)$, is another point around which the solution for $y$ is not unique. The two radicals also tend to zero as $x \downarrow-\frac{1}{2}$, causing the two values of $y$ both to tend toward -1 , so that $\left(-\frac{1}{2},-1\right)$ is not a point of unique solvability. Finally, as $x \rightarrow 1$, the three $y$ values tend toward $-1, \frac{1}{2}$, and -1 . Since two of these values are identical, there is no unique solution around the point $(1,-1)$.

Finally, the three $x$-values corresponding to any $y$ are

$$
x=-y, x=\frac{2 y+3 \pm \sqrt{9-12 y-12 y^{2}}}{4}
$$

where the quantity under the radical is nonnegative in the range $-\frac{3}{2} \leq y \leq \frac{1}{2}$. The values where $D_{1} f(x, y)=0$ are $x=0$ and $x=1$, and the four points near which a solution for $x$ might not be unique are $(0,0),\left(0,-\frac{3}{2}\right),(1,-1)$, and
$\left(1, \frac{1}{2}\right)$. As $y$ tends to zero, two of these tend to zero. Hence $(0,0)$ is not a point of unique solvability for $x$ in terms of $y$. As $y$ tends to -1 , two of the $x$-values tend to 1 , so that $(1,-1)$ is not a point of unique solvability for $x$. Finally, as $y$ tends to $-\frac{3}{2}$ or $\frac{1}{2}$, the radical disappears, and so once again two of the $x$ values tend to the same value, namely $I$ as $y \rightarrow \frac{1}{2}$ and 0 as $y \rightarrow-\frac{3}{2}$. Thus these four points are not points of unique solvability for $x$.

In sum, the points near which the equation $f(x, y)=0$ does not define either $y$ as a function of $x$ or $x$ as a function of $y$ are $(0,0)$ and $(1,-1)$.

Exercise 9.22 Give a similar discussion for

$$
f(x, y)=2 x^{3}+6 x y^{2}-3 x^{2}+3 y^{2}
$$

Solution. The gradient is

$$
\nabla 6\left(x^{2}+y^{2}-x\right) \mathbf{i}+6(2 x y+y) \mathbf{j}
$$

As we see from solving the appropriate equations, this gradient vanishes at the points $(0,0)$ and $(1,0)$. The point $(0,0)$ is a saddle point, since $f(x, 0)$ is negative for $x<0$ and $f(0, y)$ is positive for $y$ near zero. The Hessian determinant is positive at $(1,0)$, and the upper left-hand entry is also; hence $(1,0)$ is a minimum.

Because the equation $f(x, y)=0$ can be written as

$$
(6 x+3) y^{2}=(3-2 x) x^{2}
$$

there will be real solutions $y$ if and only if $-\frac{1}{2}<x \leq \frac{3}{2}$. (When $x=-\frac{1}{2}$, the equation does not contain $y$.) In this range there are two distinct values of $y$ except for $x=0$ and $x=\frac{3}{2}$. Hence the two points on the locus of $f(x, y)=0$ at which the equation cannot be solved for $y$ are $(0,0)$ and $\left(\frac{3}{2}, 0\right)$.

Since the equation is cubic in $x$, its solvability is more complicated from this point of view. Every value of $y$ gives at least one value of $x$ (but those $x$ values always lie between $-\frac{1}{2}$ and $\left.\frac{3}{2}\right)$. To find the points where two of the three (complex) $x$-roots coincide, we observe that at such points $D_{1} f(x, y)=0$, and hence also $3 f(x, y)-x D_{1} f(x, y)=0$. This last equation says $x^{2}-4 x y^{2}+3 y^{2}=0$, i.e., $y^{2}=\frac{x^{2}}{4 x+3}$. Substituting this value of $y^{2}$ into $f(x, y)=0$, we get either $x=0$ and $y=0$ or

$$
x^{2}=\frac{3}{4}
$$

Since we have to have $-\frac{1}{2}<x$, we must have $x=\frac{\sqrt{3}}{2}$, and this gives $y^{2}=\frac{2 \sqrt{3}-3}{4}$. Hence the points near which $f(x, y)=0$ cannot be solved uniquely for $x^{4}$ are $(0,0)$ and $\left(\frac{\sqrt{3}}{2}, \pm \frac{\sqrt{2 \sqrt{3}-3}}{4}\right)$.

Exercise 9.23 Define $f$ in $R^{3}$ by

$$
f\left(x, y_{1}, y_{2}\right)=x^{2} y_{1}+e^{x}+y_{2}
$$

Show that $f(0,1,-1)=0,\left(D_{1} f\right)(0,1,-1) \neq 0$, and that there exists therefore a differentiable function $g$ in some neighborhood of $(1,-1)$ in $R^{2}$ such that $g(1,-1)=0$ and

$$
f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right)=0
$$

Find $\left(D_{1} g\right)(1,-1)$ and $\left(D_{2} g\right)(1,-1)$.

Solution. The proof that $f(0,1,-1)=0$ is a routine computation. We have $\left(D_{1} f\right)\left(x, y_{1}, y_{2}\right)=2 x y_{1}+e^{x}$, so that $\left(D_{1} f\right)(0,1,-1)=1 \neq 0$. To find the partial derivatives of $g$ we use the chain rule. Let $\psi\left(y_{1}, y_{2}\right)=f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right) \equiv 0$. Then

$$
0=D_{1} \psi\left(y_{1}, y_{2}\right)=D_{1} f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right) D_{1} g\left(y_{1}, y_{2}\right)+D_{2} f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right)
$$

so that

$$
0=\left(2 y_{1} g\left(y_{1}, y_{2}\right)+e^{g\left(y_{1}, y_{2}\right)}\right) D_{1} g\left(y_{1}, y_{2}\right)+\left(g\left(y_{1}, y_{2}\right)\right)^{2}
$$

- Similarly, setting

$$
0=D_{2} \psi\left(y_{1}, y_{2}\right)=D_{1} f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right) D_{2} g\left(y_{1}, y_{2}\right)+D_{3} f\left(g\left(y_{1}, y_{2}\right), y_{1}, y_{2}\right)
$$

we find

$$
0=\left(2 y_{1} g\left(y_{1}, y_{2}\right)+e^{g\left(y_{1}, y_{2}\right)}\right) D_{2} g\left(y_{1}, y_{2}\right)+1
$$

Taking $y_{1}=1, y_{2}=-1, g\left(y_{1}, y_{2}\right)=0$, we get

$$
D_{1} g(1,-1)=0, \quad D_{2} g(1,-1)=-1
$$

Exercise 9.24. For $(x, y) \neq(0,0)$, define $\mathbf{f}=\left(f_{1}, f_{2}\right)$ by

$$
f_{1}(x, y)=\frac{x^{2}-y^{2}}{x^{2}+y^{2}}, \quad f_{2}(x, y)=\frac{x y}{x^{2}+y^{2}}
$$

Compute the rank of $\mathbf{f}^{\prime}(x, y)$, and find the range of $\mathbf{f}$.

Solution. The matrix of $\mathrm{f}^{\prime}(x, y)$ is

$$
\left(\begin{array}{cc}
\frac{4 x y^{2}}{\left(x^{2}+y^{2}\right)^{2}} & \frac{-4 x^{2} y}{\left(x^{2}+y^{2}\right)^{2}} \\
\frac{y\left(y^{2}-x^{2}\right)}{\left(x^{2}+y^{2}\right)^{2}} & \frac{x\left(x^{2}-y^{2}\right)}{\left(x^{2}+y^{2}\right)^{2}}
\end{array}\right) \text {. }
$$

Its determinant is 0 at every point. Hence its rank is either 0 or $I$ at every point. Since the point $(0,0)$ is excluded from the domain, the rank is 1 at every point. The range must therefore be 1-dimensional, i.e., there is some non-trivial
relation connecting $f_{1}$ and $f_{2}$. Indeed, it is easy to verify that if $u=f_{1}(x, y)$ and $v=f_{2}(x, y)$, then

$$
u^{2}+4 v^{2}=1 \text {. }
$$

Thus the range of $f$ is a subset of this ellipse. In fact, it is all of this ellipse. The point $(1,0)$ is its own image, and the point $(-1,0)$ is the image of $(0,1)$. For any other point $(u, v)$ on this ellipse we have $-1<u<1$ and $v= \pm \frac{1}{2} \sqrt{1-u^{2}}$. The point $(u, v)$ is the image of the point $\left(1, \pm \sqrt{\frac{1-u}{1+u}}\right)$ (and, of course, many other points as well).

Exercise 9.25 Suppose $A \in L\left(R^{n}, R^{m}\right)$, let $r$ be the rank of $A$.

(a) Define $S$ as in the proof of Theorem 9.32. Show that $S A$ is a projection in $R^{n}$ whose nullspace is $\mathcal{N}(A)$ and whose range is $\mathcal{R}(S)$. Hint: By (68), SASA $=S A$. (b) Use $(a)$ to show that

$$
\operatorname{dim} \mathcal{N}(A)+\operatorname{dim} \mathcal{R}(A)=n
$$

Solution. We recall that $S$ is defined by first choosing a basis for the range of $A$, say $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$, then choosing vectors $\left\{\mathbf{z}_{1}, \ldots, \mathbf{z}_{r}\right\}$ such that $A \mathbf{z}_{i}=\mathbf{y}_{i}$ for $i=1,2, \ldots, r$. We then define $S \mathbf{y}_{i}=\mathbf{z}_{i}$ on the vectors $\mathbf{y}_{i}$ (and $S$ arbitrary on any set of vectors $\mathbf{y}_{r+1}, \ldots, \mathbf{y}_{m}$ that can be adjoined to $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$ so as to make a basis of $R^{m}$ ). Thus $S$ is a left inverse of the restriction of $A$ to the subspace spanned by $\mathbf{z}_{1}, \ldots, \mathbf{z}_{r}$. Since $A \mathbf{x}$ belongs to the range of $A$, it follows, as in (68), that $A S A \mathbf{x}=A \mathbf{x}$, from which we conclude that $S A S A \mathbf{x}=S A \mathbf{x}$, i.e., $S A$ is a projection. Then every vector $\mathrm{x}$ has the unique decomposition $\mathrm{x}=S A \mathrm{x}+(\mathrm{x}-S A \mathrm{x})$, where the first vector on the right belongs to the range of $S A$ and the second to the nullspace of this projection. The two subspaces have only the zero vector in common. Since $S$ is an isomorphism of the range of $A$, the range of $S A$ has the same dimension as the range of $A$. Since $A=A S A$, the nullspace of $S A$ is the same as the nullspace of $A$. Thus $n=\operatorname{dim} \mathcal{N}(S A)+$ $\operatorname{dim} \mathcal{R}(S A)=\operatorname{dim} \mathcal{N}(A)+\operatorname{dim} \mathcal{R}(A)$.

Exercise 9.26 Show that the existence (and even the continuity) of $D_{12} f$ does not imply the existence of $D_{1} f$. For example, let $f(x, y)=g(x)$, where $g$ is nowhere differentiable.

Solution. The second sentence in the exercise is its solution. Since $D_{2} f$ is identically zero, $D_{12} f$ is also identically zero, hence certainly continuous.

Exercise 9.27 Put $f(0,0)=0$, and

$$
f(x, y)=\frac{x y\left(x^{2}-y^{2}\right)}{x^{2}+y^{2}}
$$

if $(x, y) \neq(0,0)$. Prove that

(a) $f, D_{1} f$, and $D_{2} f$ are continuous in $R^{2}$;

(b) $D_{12} f$ and $D_{21} f$ exist at every point of $R^{2}$, and are continuous except at $(0,0)$;

(c) $\left(D_{12} f\right)(0,0)=1$, and $\left(D_{21} f\right)(0,0)=-1$.

Solution. (a) The continuity of $f$ is obvious at every point except $(0,0)$; at $(0,0)$ it follows from the inequality $|f(x, y)| \leq \frac{1}{2}\left(x^{2}+y^{2}\right)$. It is also clear that $D_{1} f(0,0)=0=D_{2} f(0,0)$. For $(x, y) \neq(0,0)$ we have $D_{1} f(x, y)=$ $\frac{x^{4} y+4 x^{2} y^{3}-y^{5}}{\left(x^{2}+y^{2}\right)^{2}}$ and $D_{2} f(x, y)=\frac{x^{5}-4 x^{3} y^{2}-x y^{4}}{\left(x^{2}+y^{2}\right)^{2}}$. The continuity of the partial derivatives at every point except $(0,0)$ is obvious. It is easy to see that these derivatives satisfy the inequalities $\left|D_{1} f(x, y)\right| \leq 2|y|$ and $\left|D_{2} f(x, y)\right| \leq$ $2|x|$, so that $D_{1} f$ and $D_{2} f$ are also continuous at $(0,0)$.

(b) Since $f(x, y)$ is a rational function with non-zero denominator for $(x, y) \neq$ $(0,0)$, it has continuous partial derivatives of all orders on this set.

(c) Since $D_{1} f(0, y)=-y$ and $D_{2} f(x, 0)=x$, it follows that $D_{21} f(0, y)=-1$ for all $y$ and $D_{12} f(x, 0)=1$ for all $x$.

Exercise 9.28 For $t \geq 0$ put

$$
\varphi(x, t)= \begin{cases}x & (0 \leq x \leq \sqrt{t}) \\ -x+2 \sqrt{t} & (\sqrt{t} \leq x \leq 2 \sqrt{t}) \\ 0 & \text { (otherwise) }\end{cases}
$$

and put $\varphi(x, t)=-\varphi(x,|t|)$ if $t<0$.

Show that $\varphi$ is continuous on $R^{2}$, and

$$
\left(D_{2} \varphi\right)(x, 0)=0
$$

for all $x$. Define

$$
f(t)=\int_{-1}^{1} \varphi(x, t) d x
$$

Show that $f(t)=t$ if $|t|<\frac{1}{4}$. Hence

$$
f^{\prime}(0) \neq \int_{-1}^{1}\left(D_{2} \varphi\right)(x, 0) d x
$$

Solution. This function is zero in the (closed) left half-plane of the $x t$-plane and on the positive $x$-axis. Since the functions by which it is defined are continuous, we need only verify that they agree on the boundary curves $x=\sqrt{t}$ and $x=2 \sqrt{t}$ in the first quadrant that separate the three different regions of definition. This is a routine computation.

Likewise the computation showing that $\left(D_{2} \varphi\right)(x, 0)=0$ is routine, since for each $x>0 \varphi(x, t)=0$ for $0 \leq t \leq \frac{1}{4} x^{2}$, while $\varphi(x, t)=0$ for all $t$ if $x \leq 0$.

If $0<t<\frac{1}{4}$, then

$$
\begin{aligned}
f(t) & =\int_{0}^{\sqrt{t}} x d x+\int_{\sqrt{t}}^{2 \sqrt{t}}-x+2 \sqrt{t} d t \\
& =\frac{1}{2} t-\frac{1}{2}(4 t-t)+2 \sqrt{t}(2 \sqrt{t}-\sqrt{t}) \\
& =\frac{t}{2}-\frac{3 t}{2}+4 t-2 t=t
\end{aligned}
$$

Obviously $f(0)=0$, and if $t<0$, then $f(t)=-f(-t)=t$. Therefore $f^{\prime}(0)=1$.
However

$$
\int_{-1}^{1}\left(D_{2} \varphi\right)(x, 0) d x=0
$$

Note: This result is possible only because $D_{2} \varphi(x, t)$ is not bounded on $[-1,1] \times$ $[-a, a]$ for any $a>0$. Also note that having -1 as the lower limit of the integral was a needless complication. The problem would have been more effective it the lower limit had been 0 .

Exercise 9.29 Let $E$ be an open set in $R^{n}$. The classes $\mathcal{C}^{\prime}(E)$ and $\mathcal{C}^{\prime \prime}(E)$ are defined in the text. By induction $\mathcal{C}^{(k)}(E)$ can be defined as follows for all positive integers $k$ : To say that $f \in \mathcal{C}^{(k)}(E)$ means that the partial derivatives $D_{1} f, \ldots, D_{n} f$ belong to $\mathcal{C}^{(k-1)}(E)$.

Assume $f \in \mathcal{C}^{(k)}(E)$, and show (by repeated application of Theorem 9.41) that the $k$ th-order derivative

$$
D_{i_{1} i_{2} \ldots i_{k}} f=D_{i_{1}} D_{i_{2}} \ldots D_{i_{k}} f
$$

is unchanged if the subscripts $i_{1}, \ldots, i_{k}$ are permuted.

For instance, if $n \geq 3$, then

$$
D_{1213} f=D_{3112} f
$$

for every $f \in \mathcal{C}^{(4)}$.

Solution. If the permutation leaves $i_{k}$ fixed, this follows from the result for $k-1$ applied to $D_{i_{k}} f$. To get the general result, we observe that by the case $\dot{k}=2$ we have $D_{i_{k-1} i_{k}} f=D_{i_{k} i_{k-1}} f$. Hence the result holds for any permutation that maps $i_{k-1}$ to $i_{k}$. But any permutation that maps $i_{j}$ to $i_{k}(j \neq k, k-$ 1) can be written as the composition of a permutation that maps $i_{j}$ to $i_{k-1}$, leaving $i_{k}$ fixed, followed by the interchange of $i_{k-1}$ and $i_{k}$, followed by a second permutation that leaves $i_{k}$ fixed. Therefore the result applies to all permutations whatsoever.

Exercise 9.30 Let $f \in \mathcal{C}^{(m)}(E)$, where $E$ is an open subset of $R^{n}$. Fix $\mathbf{a} \in E$, and suppose $\mathrm{x} \in R^{n}$ is so close to $\mathbf{0}$ that the points

$$
\mathbf{p}(t)=\mathbf{a}+t \mathbf{x}
$$

lie in $E$ whenever $0 \leq t \leq 1$. Define

$$
h(t)=f(\mathbf{p}(t))
$$

for all $t \in R^{1}$ for which $\mathbf{p}(t) \in E$.

(a) For $1 \leq k \leq m$, show (by repeated application of the chain rule) that

$$
h^{(k)}(t)=\sum\left(D_{i_{1} \ldots, i_{k}} f\right)(\mathbf{p}(t)) x_{i_{1}} \ldots x_{i_{k}}
$$

The sum extends over all ordered $k$-tuples $\left(i_{1}, \ldots, i_{k}\right)$ in which each $i_{j}$ is one of the integers $1, \ldots, n$.

(b) By Taylor's theorem (5.15)

$$
h(1)=\sum_{k=0}^{m-1} \frac{h^{(k)}(0)}{k !}+\frac{h^{(m)}(t)}{m !}
$$

for some $t \in(0,1)$. Use this to prove Taylor's theorem in $n$ variables by showing that the formula

$$
f(\mathbf{a}+\mathbf{x})=\sum_{k=0}^{m-1} \frac{1}{k !} \sum\left(D_{i_{1} \ldots i_{k}} f\right)(\mathbf{a}) x_{i_{1}} \ldots x_{i_{k}}+r(\mathbf{x})
$$

represents $f(\mathbf{a}+\mathbf{x})$ as the sum of its so-called "Taylor polynomial of degree $m-1$, , plus a remainder that satisfies

$$
\lim _{x \rightarrow 0} \frac{r(x)}{|x|^{m-1}}=0
$$

Each of the inner sums extends over all ordered $k$-tuples $\left(i_{1}, \ldots, i_{k}\right)$, as in part (a); as usual, the zero-order derivative of $f$ is simply $f$, so that the constant term of the Taylor polynomial of $f$ at $\mathbf{a}$ is $f(\mathbf{a})$.

(c) Exercise 29 shows that repetition occurs in the Taylor polynomial as written in part $(b)$. For instance $D_{113}$ occurs three times, as $D_{113}, D_{131}, D_{311}$. The sum of the corresponding three terms can be written in the form

$$
3\left(D_{1}^{2} D_{3} f\right)(\mathbf{a}) x_{1}^{2} x_{3}
$$

Prove (by calculating how often each derivative occurs) that the Taylor polynomial in (b) can be written in the form

$$
\sum \frac{\left(D_{1}^{s_{1}} \cdots D_{n}^{s_{n}} f\right)(\mathbf{a})}{s_{1} ! \cdots s_{n} !} x_{1}^{s_{1}} \cdots x_{n}^{s_{n}}
$$

Here the summation extends over all ordered $n$-tuples $\left(s_{1}, \ldots, s_{n}\right)$ such that each $s_{i}$ is a nonnegative integer, and $s_{1}+\cdots+s_{n} \leq m-1$.

Solution. (a) This formula is a simple application of the chain rule together with the fact that $D \mathrm{p}^{(i)}(t)=x_{i}$. The proof proceeds by induction on $k$.

(b) The formula is an immediate application of the fact that $\mathbf{p}(1)=\mathbf{a}+\mathbf{x}$, so that $h(1)=f(\mathbf{a}+\mathbf{x})$. The right-hand side is then an immediate application of the fact that $\mathbf{p}(0)=\mathbf{a}$. The only assertion that requires verification is that on the order of the remainder. The one-variable Taylor's theorem gives $r(\mathrm{x})=$ $\sum\left(D_{i_{1}, \ldots, i_{m}} f(\mathbf{p}(t)) x_{i_{1}} \cdots x_{i_{m}}\right.$ for some $t \in(0,1)$, so that $|r(\mathbf{x})| \leq K|\mathbf{x}|^{m}$ for some constant $K$. The assertion as to the order of $r$ follows from this fact.

(c) If $s_{1}+\cdots+s_{n} \leq m-1$, the number of terms having the derivative combination $D_{1}^{s_{1}} \cdots D_{n}^{s_{n}} f$ is $\left(\begin{array}{c}s_{1}+\cdots+s_{n} \\ s_{1}, \ldots, s_{n}\end{array}\right)=\frac{\left(s_{1}+\cdots+s_{n}\right) !}{s_{1} ! \cdots s_{n} !}$. Thus the $k$ ! that occurs in the one-variable Taylor's theorem is $\left(s_{1}+\cdots+s_{n}\right) ! ;$ and when the terms are consolidated, this factor cancels the numerator of the multinomial symbol, effectively being replaced by $s_{1} ! \cdots s_{n} !$.

Exercise 9.31 Suppose $f \in \mathcal{C}^{(3)}$ in some neighborhood of a point $\mathbf{a} \in R^{2}$, the gradient of $f$ is $\mathbf{0}$ at a, but not all second-order derivatives of $f$ are 0 at a. Show how one can then determine from the Taylor polynomial of $f$ at a (of degree 2 ) whether $f$ has a local maximum or a local minimum, or neither, at the point a.

Extend this to $R^{n}$ in place of $R^{2}$.

Solution. Let us simply do $R^{n}$ in the first place and save the trouble of doing $R^{2}$. According to Taylor's theorem

$$
f(\mathbf{a}+\mathbf{x})-f(\mathbf{a})=\frac{1}{2} \sum_{i_{1}, i_{2}}\left(D_{i_{1} i_{2}} f\right)(\mathbf{a}) x_{i_{1}} x_{i_{2}}+r(\mathbf{x})
$$

where $|\mathbf{x}|^{-2} r(\mathbf{x}) \rightarrow 0$ as $\mathbf{x} \rightarrow \mathbf{0}$. Note that the Taylor polynomial can be concisely written as $\frac{1}{2}\langle A \mathbf{x}, \mathbf{x}\rangle$, where $A$ is the $n \times n$ Hessian matrix whose $i, j$ entry is $D_{i j} f(\mathbf{a})$ and the angle brackets denote the inner product. If $A$ is positive-definite, i.e., if $\langle A \mathbf{x}, \mathbf{x}\rangle>0$ when $\mathbf{x} \neq \mathbf{0}$, there is a positive constant $c$ such that $\langle A \mathbf{x}, \mathbf{x}\rangle \geq c|\mathbf{x}|^{2}$. (The constant $c$ is the minimum value of $\langle A \mathrm{x}, \mathrm{x}\rangle$ on the unit sphere $|\mathbf{x}|=1$.) Hence if $\delta>0$ is chosen so that $|r(\mathbf{x})|<c|\mathbf{x}|^{2}$ when $0<|\mathbf{x}|<\delta$, we see that $f(\mathbf{a}+\mathbf{x})-f(\mathbf{a})>0$ if $0<|\mathbf{x}|<\delta$, i.e., $\mathbf{a}$ is a local minimum of $f$. Likewise if $A$ is negative-definite, then a is a local maximum of $f$.

It is well-known from linear algebra that a necessary and sufficient condition for positive-definiteness of the matrix $A$ is that the principal minors be positive, i.e., the $k \times k$-submatrix consisting of the elements in the first $k$ rows and columns of $A$ has a positive determinant. For negative-definiteness the corresponding criterion is that this minor have the same sign as $(-1)^{k}$.

There are no other resonably regular cases that guarantee a maximum or minimum. A nonnegative-definite or nonpositive-definite matrix may well fail
to guarantee a maximum or minimum, even in $R^{1}$. If the quadratic form $\langle A \mathrm{x}, \mathrm{x}\rangle$ assumes both signs, then the point $a$ is definitely not either a maximum or a minimum. (If $\langle A \mathbf{x}, \mathbf{x}\rangle>0$, then $f(\mathbf{a}+t \mathbf{x})-f(\mathbf{a})>0$ for small values of $t$, while if $\langle A \mathbf{x}, \mathbf{x}\rangle<0$, then $f(\mathbf{a}+t \mathbf{x})-f(\mathbf{a})<0$ for small values of $t$. $)$

## Chapter 10

## Integration of Differential Forms

Exercise 10.1 Let $H$ be a compact convex set in $R^{k}$ with nonempty interior. Let $f \in \mathcal{C}(H)$, put $f(\mathbf{x})=0$ in the complement of $H$ and define $\int_{H} f$ as in Definition 10.3.

Prove that $\int_{H} f$ is independent of the order in which the integrations are carried out.

Hint: Approximate $f$ by functions that are continuous on $R^{k}$ and whose supports are in $H$, as was done in Example 10.4.

Solution. We first give the definition of $\int_{H} f$, namely $\int_{I} f$, where $I$ is any $k$-cell containing $H$. This definition is unambiguous, since if $I$ and $J$ are both $k$-cells containing $H$, each of the single integrals carried out is an integral over the same line segment for both cells, namely the intersection of the path of integration with $H$.

There seems to be no way to avoid somehow proving that the boundary of $H$, denoted $\partial H$, has "measure zero." The definition of the integral as an iterated integral makes that problem slightly more difficult than it would be otherwise, although we can show how to avoid this approach in two dimensions. We shall reserve that discussion until after the proof, which is rather lengthy. The length of the proof is due to the fact that integrals are really defined only over parallelepipeds. The point of the exercise is to enlarge the class of sets over which one can integrate. Our challenge is to show that the boundary of $H$ can be enclosed in a finite set of parallelepipeds whose total volume can be arbitrarily small.

Our first job is to show that the hypersphere in $R^{k}$ has measure zero. As the proof of that fact involves some work with $(k-2)$-dimensional hyperspheres in $k$-dimensional space, we need to make several definitions in order to express these ideas properly.

First, for each real number $z$ and each positive number $r, S_{r}^{k-2}(z)$ denotes the $(k-2)$-dimensional hypersphere in $R^{k}$ having radius $r$ and center at the
point $(0,0, \ldots, 0, z)$, that is,

$$
S_{r}^{k-2}(z)=\left\{\mathbf{x}: x_{1}^{2}+\cdots+x_{k-1}^{2}=r^{2}, x_{k}=z\right\}
$$

When $z=0$, we shall write simply $S_{r}^{k-2}$ and identify this sphere with the same $(k-2)$-dimensional hypersphere in $R^{k-1}$. We observe that $S_{r}^{0}(z)$ consists of the two points $(r, z)$ and $(-r, z)$ in $R^{2}$. Another way of defining the set $S_{r}^{k-2}(z)$ is as the intersection of the $(k-1)$-dimensional sphere of radius $r$ centered at $(0,0, \ldots, 0, z)$ in $R^{k}$ with its equatorial hyperplane $P_{z}=\left\{\left(x_{1}, \ldots, x_{k}\right): x_{k}=z\right\}$.

Second, for each $\mathrm{a} \in R^{k}$ and each $\delta>0, I_{\mathrm{a}}^{k}(\delta)$ is the closed hypercube of side $\delta$ in $R^{k}$ whose "lower left" corner is a, that is

$$
I_{\mathbf{a}}^{k}(\delta)=\left\{\mathbf{x}: a_{j} \leq x_{j} \leq a_{j}+\delta, j=1, \ldots, k\right\}
$$

Third, the set of points $\left(m_{1}, \ldots, m_{k}\right) \in R^{k}$ having integer coordinates will be denoted $\mathcal{Z}^{k}$.

Fourth, for all real numbers $r$ and $\delta$ such that $0<\delta<r, C_{r, \delta}^{k}$ is the set of lattice points $\mathbf{m} \in \mathcal{Z}^{k}$ for which the closed hypercube of side $\delta$ with lower left corner $\delta \mathbf{m}$ intersects the hypersphere $S_{r}^{k-1}$ in $R^{k}$. That is,

$$
C_{r, \delta}^{k}=\left\{\mathbf{m}: I_{\delta \mathbf{m}}^{k}(\delta) \cap S_{r}^{k-1} \neq \emptyset\right\}
$$

Fifth, $N(r, \delta, k)$ is the number of points in $C_{r, \delta}^{k}(z)$, that is, the number of hypercubes of side $\delta$ with lower left hand corner at a point $\delta \mathbf{m}, \mathbf{m} \in \mathcal{Z}^{k}$, that intersect the hypersphere $S_{r}^{k-1}$. Our main goal in the first stage of the proof will be to prove the estimate $N(r, \delta, k) \leq 6^{k^{2}}\left(\frac{r}{\delta}\right)^{k-1}$. (A smaller constant than $12^{k^{2}}$ could easily be attained, but we have no need of any improvement, and this constant seems to be the one that makes the argument simplest.)

Sixth, and finally, $A_{r, \delta}^{k}$ is the union of all the hypercubes $I_{\delta \mathbf{m})}^{k}(\delta)$ that intersect the hypersphere $S_{r}^{k-1}$, that is, for which $\mathbf{m} \in C_{r, \delta}^{k}$. This set is a finite union of compact sets, hence is compact. Obviously it contains the hypersphere $S_{r}^{k-1}$. What is slightly less obvious is that its interior contains this sphere. In fact no point of the sphere can be a limit point of points exterior to $A_{r, \delta}^{k}$, since if $\left\{\mathbf{x}_{n}\right\}$ is a sequence of points such that each $\mathbf{x}_{n}$ belongs to a hypercube $I_{\delta \mathbf{m}_{n}}^{k}(\delta)$ not contained in $A_{r, \delta}^{k}$, and $\mathbf{x}_{n} \rightarrow \mathrm{x}$, some set $I_{\delta \mathrm{m}_{n_{0}}}^{k}(\delta)$ must occur infinitely often. (Any bounded neighborhood of $\mathbf{x}$ intersects only finitely many of these hypercubes.) Since $I_{\delta \mathrm{m}_{n_{0}}}^{k}(\delta)$ is closed, this implies that $\mathbf{x}$ belongs to $I_{\delta \mathrm{m}_{n_{0}}}^{k}(\delta)$. Since $I_{\delta \mathrm{m}_{n_{0}}}^{k}(\delta)$ is not contained in $A_{r, \delta}^{k}$ it follows that $\mathbf{x} \notin S_{r}^{k-1}$. Thus no sequence of points exterior to $A_{r, \delta}^{k}$ can approach a point of $S_{r}^{k-1}$. It follows that $S_{r}^{k-1}$ contains no points of the boundary of $A_{r, \delta}^{k}$ and is therefore contained in the
interior of this set.

With these definitions out of the way we can proceed to the proof, which we break into several stages, each broken into several steps, in order to make
navigating easier.

Stage 1. Establish that the sphere $S_{r}^{k-1}$ in $R^{k}$ has $k$-dimensional content 0 .

Step 1. Establish that $A_{r, \delta}^{k} \cup A_{r+\delta}^{k}$ contains the closed $k$-dimensional annulus consisting of the region between $S_{r}^{k-1}$ and $S_{r+\delta}^{k-1}$, that is, all the points $\mathbf{x} \in R^{k}$ such that $r \leq|\mathbf{x}| \leq r+\delta$.

To this end, let $\mathbf{x}$ belong to this annulus, so that $r \leq|\mathbf{x}| \leq r+\delta$. Since $S_{s}^{k-1} \subset A_{s, \delta}^{k}$ for each $s$, we can assume $r<|\mathbf{x}|<r+\delta$. Let $\mathbf{m}=\left(m_{1}, \ldots, m_{k}\right)$ be a lattice point in $\mathcal{Z}^{k}$ such that $\mathrm{x} \in I_{\delta \mathbf{m}}^{k}(\delta)$. Let $n_{j}=m_{j}$ if $m_{j} \geq 0$ and $n_{j}=m_{j}+1$ if $m_{j}<0$, so that $\left(\delta n_{1}, \ldots, \delta n_{k}\right)$ is a corner of $I_{\delta \mathbf{m}}(\delta)$. Since $\left|n_{j}\right|=\min \left(\left|m_{j}\right|,\left|m_{j}+1\right|\right)$ for all $j,(\delta \mathbf{n})$ is the unique point of $I_{\delta \mathbf{m}}^{k}(\delta)$ closest to the origin. In particular $|\delta \mathbf{n}| \leq|\mathbf{x}|$. The lattice point $\mathbf{n}^{\prime}=\left(n_{1}+\varepsilon_{1}\left(n_{1}\right), \ldots, n_{k}+\right.$ $\varepsilon_{k}\left(n_{k}\right)$ ), where $\varepsilon_{j}(t)$ is 1 . if $t \geq 0$ and -1 if $t<0$, is such that $\delta \mathbf{n}^{\prime}$ is the corner of $I_{\delta \mathbf{m}}(\delta)$ opposite to $\delta \mathbf{n}$ and is the unique point of $K_{\delta \mathbf{m}}(\delta)$ farthest from the origin. In particular $\left|\delta \mathbf{n}^{\prime}\right| \geq|\mathbf{x}|$.

We claim first that $\left|\mathbf{n}^{\prime}\right|-|\mathbf{n}| \geq 1$. Indeed, we have

$$
\begin{aligned}
\left|\mathbf{n}^{\prime}\right|^{2}=\left(n_{1}^{2}+\cdots+n_{k}^{2}\right)+2\left(n_{1} \varepsilon_{1}\left(n_{1}\right)\right. & +\cdots+n_{k} \varepsilon_{k}\left(n_{k}\right)+\left(\left(\varepsilon_{1}\left(n_{1}\right)\right)^{2}+\cdots+\left(\varepsilon_{k}\left(n_{k}\right)\right)^{2}\right) \\
= & \left(n_{1}^{2}+\cdots+n_{k}^{2}\right)+2\left(\left|n_{1}\right|+\cdots+\left|n_{k}\right|\right)+k
\end{aligned}
$$

and

$$
(|\mathbf{n}|+1)^{2}=\left(n_{1}^{2}+\cdots+n_{k}^{2}\right)+2 \sqrt{n_{1}^{2}+\cdots+n_{k}^{2}}+1
$$

so that the desired inequality follows from the two inequalities $k \geq 1$ and $\sqrt{n_{1}^{2}+\cdots+n_{k}^{2}} \leq\left|n_{1}\right|+\cdots+\left|n_{k}\right|$. This argument shows in general that, for any $\mathbf{m} \in \mathcal{Z}^{k}$, if $\mathbf{b}$ and $\mathbf{c}$ are the points in $I_{\delta \mathbf{m}}^{k}(\delta)$ of minimal and maximal absolute value respectively, then $|\mathbf{c}|-|\mathbf{b}| \geq \delta$.

From this we deduce a corollary: Let $r$ be any positive real number larger than $\delta$. If $r \leq s \leq r+\delta$ and $\mathbf{m} \in C_{s, \delta}^{k}$, then either $\mathbf{m} \in C_{r, \delta}^{k}$ or $\mathbf{m} \in C_{r+\delta, \delta}^{k}$. In plain words, if $I_{\delta \mathbf{m}}^{k}(\delta)$ meets $S_{s}^{k-1}$ for some $s \in[r, r+\delta]$, it must meet either
$S_{r}^{k-1}$ or $S_{r+\delta}^{k-1}$.

To prove this corollary, we note that the assumption $\mathbf{m} \in C_{s, \delta}^{k}$ says that there exists $\mathbf{x} \in R^{k}$ such that $\mathbf{x} \in S_{s}^{k-1} \cap I_{\delta \mathbf{m}}^{k}(\delta)$. Now suppose $\mathbf{m}$ belongs to neither of the sets $C_{r, \delta}^{k}$ and $C_{r+\delta, \delta}^{k}$. Then the set $I_{\delta \mathrm{m}}^{k}(\delta)$ contains no points of $S_{r}^{k-1}$. If $\mathbf{b}$ is the point of $I_{\delta \mathbf{m}}^{k}(\delta)$ of smallest norm, it follows that $|\mathbf{b}|>r$. (For $I_{\delta \mathrm{m}}^{k}(\delta)$ contains the point $\mathrm{x}$ of norm $s \geq r$. Since $I_{\delta \mathrm{m}}^{k}(\delta)$ is a connected set, if it contained a point of norm less than or equal to $r$ it would also contain a point of $S_{r}^{k-1}$.) Similarly, if the set $I_{\delta \mathbf{m}}^{k}(\delta)$ contains no points of $S_{r+\delta}^{k-1}$, then $|\mathrm{c}|<r+\delta$. But then it follows that $|\mathbf{c}|-|\mathbf{b}|<r+\delta-r=\delta$, contrary to what has been proved.

Another way of stating what was just proved is that if $\mathrm{x} \in R^{k}$ is such that $r \leq|\mathbf{x}| \leq r+\delta$, then $\mathbf{x} \in A_{r, \delta}^{k} \cup A_{r+\delta, \delta}^{k}$. That is, the union $A_{r, \delta}^{k} \cup A_{r+\delta, \delta}^{k}$ contains the entire annulus of points $\mathbf{x}$ such that $r \leq|\mathbf{x}| \leq r+\delta$. Step 1 of the proof is now complete.

Step 2. Assuming $k>1$, estimate the number of $k$-dimensional hypercubes $I_{\delta \mathbf{m}}^{k}(\delta)$ that intersect various zones on the $(k-1)$-sphere $S_{r}^{k-1}$ in $R^{k}$.

We divide the upper hemisphere, consisting of $\mathbf{x}$ such that $|\mathbf{x}|=r$ and $x_{k} \geq 0$
to half-open zones

$$
Z_{p}=\left\{\left(x_{1}, \ldots, x_{k-1}, x_{k}\right): x_{1}^{2}+\cdots+x_{k}^{2}=r, p \delta \leq x_{k}<(p+1) \delta\right\}
$$

for $0 \leq p \leq\left[\frac{r}{\delta}\right]-1$. Here $[a]$ denotes the integer part of $a$, that is, the integer $q$ such that $q \leq a<q+1$, and we assume $0<\delta<r$. Between the top zone $Z_{\left[\frac{r}{\delta}\right]-1}$ and the "north pole" (the point $(0,0, \ldots, r)$, there is a closed "cap" of height $Z_{\left[\frac{r}{\delta}\right]-1}$
for some $\eta \in[0, \delta)$. The hypercubes $I_{\delta \mathrm{m}}^{k}(\delta)$ (where $m_{k}=\left[\frac{r}{\delta}\right]-1$ or $m_{k}=\left[\frac{r}{\delta}\right]$ )
intersecting this cap must be handled separately from other zones.

We shall prove that the lattice points $\mathbf{m} \in \mathcal{Z}^{k}$ for which $m_{k}=p$ and $I_{\delta m}^{k}(\delta)$ intersects $Z_{p}$ are precisely those whose bottom face $x_{k}=p \delta$ intersects the half-closed $(k-1)$-dimensional annulus between $S_{s}^{k-2}(p \delta)$ and $S_{t}^{k-2}(p \delta)$ in the hyperplane $x_{k}=p \delta$. (This annulus is closed at $t$ and open at $s$, where
$s=\sqrt{r^{2}-((p+1) \delta)^{2}}$ and $t=\sqrt{r^{2}-(p \delta)^{2}}$.)

Indeed, this fact is nearly obvious, as the zone $Z_{p}$ is the union of the $(k-2)$ dimensional spheres $S_{\sqrt{r^{2}-u^{2}}}^{k-2}(u)$ for $p \delta \leq u<(p+1) \delta$. If $\left(x_{1}, \ldots, x_{k-1}, u\right) \in$ $I_{\delta \mathrm{m}}^{k}(\delta) \cap Z_{p}$ and $m_{k}=p$, then $p \delta \leq u<(p+1) \delta$ and $x_{1}^{2}+\cdots+x_{k-1}^{2}=$ $r^{2}-u^{2}$, so that $s<\sqrt{x_{1}^{2}+\cdots+x_{k-1}^{2}} \leq t$. Thus the point $\left(x_{1}, \ldots, x_{k-1}, \delta p\right)$ belongs to both $I_{\delta \mathrm{m}}^{k}(\delta)$ and to the annulus. Conversely, if $I_{\delta \mathrm{m}}^{k}(\delta)$ with $m_{k}=p$ intersects the annulus, then this hypercube contains a point $\left(x_{1}, \ldots, x_{k-1}, p \delta\right)$ with $s^{2}<x_{1}^{2}+\cdots+x_{k-1}^{2} \leq t^{2}$. Setting $u=\sqrt{r^{2}-\left(x_{1}^{2}+\cdots+x_{k-1}^{2}\right)}$, we have $p \delta \leq u<(p+1) \delta$, and therefore $\left(x_{1}, \ldots, x_{k-1}, u\right) \in Z_{p} \cap I_{\delta \mathrm{m}}^{k}(\delta)$.

To estimate the total number of hypercubes $I_{\delta \mathrm{m}}^{k}(\delta)$ that intersect the hypersphere $S_{r}^{k-1}$, we need an estimate of the number that intersect each zone $Z_{p}$. $I_{\left(\delta m_{1}, \ldots, \delta m_{k-1}, \delta p\right)}^{k}(\delta)$ also intersects $Z_{p}$. Hence we can get a (loose, but hence upper bound on the number of hyper $Z^{2}$ (loose, but safe) those for which $m_{k}=p$ and hypercubes $I_{\delta \mathrm{m}}^{k}(\delta)$ that intersect $Z_{p}$ by counting special, and the "northern cap" mentio. (The case of the bottom layer $Z_{0}$ is

The fact that a hypercube $I^{k}(\delta)$ inters above will be handled separately.) shows that we need only estimate of the widt $Z_{p}$ must intersect the annulus number $t-s=\sqrt{r^{2}-(p \delta)^{2}}-\sqrt{r^{2}-((p+1) \delta)^{2}}$. For that width we that the following simple result:

The proof of this inequality is straightforward:

$$
\sqrt{r^{2}-(p \delta)^{2}}-\sqrt{r^{2}-((p+1) \delta)^{2}} \leq \frac{(2 p+1) \delta^{2}}{\sqrt{r^{2}-(p \delta)^{2}}}
$$

$$
\begin{aligned}
\sqrt{r^{2}-(p \delta)^{2}}-\sqrt{r^{2}-((p+1) \delta)^{2}} & =\frac{\left(r^{2}-(p \delta)^{2}\right)-\left(r^{2}-((p+1) \delta)^{2}\right)}{\sqrt{r^{2}-(p \delta)^{2}}+\sqrt{r^{2}-((p+1) \delta)^{2}}} \\
& \leq \frac{(2 p+1) \delta^{2}}{\sqrt{r^{2}-(p \delta)^{2}}}
\end{aligned}
$$

Now let $j$ be the largest integer not larger than $\sqrt{\left(\frac{r}{\delta}\right)^{2}-(p+1)^{2}}$ and $l$ the smallest integer not smaller than $\sqrt{\left(\frac{r}{\delta}\right)^{2}-p^{2}}$. We have just shown that

$$
0<l-j<2+\frac{(2 p+1) \delta}{\sqrt{r^{2}-(p \delta)^{2}}}=2+\frac{2 p+1}{\sqrt{\left(\frac{r}{\delta}\right)^{2}-p^{2}}}
$$

It is this inequality that provides the required estimate of the width of the annulus corresponding to the zone $Z_{p}$.

Step 3. Prove the estimate $N_{r, \delta}^{k} \leq 6^{k^{2}}\left(\frac{r}{\delta}\right)^{k-1}$.

We first do the case $k=1$. This case is very straightforward. If $I_{(\delta m, z)}^{1}(\delta) \cap$ $S_{r}^{0}$, then either $m \delta \leq r \leq(m+1) \delta$ or $m \delta \leq-r \leq(m+1) \delta$. If $r=k \delta$ for some integer $k$, there are four values of $m$ for which one of these two sets of inequalities hold, namely $-k-1,-k, k-1$, and $k$. Otherwise there are only two such integers $m$, namely $\left[\frac{r}{\delta}\right]$ and $\left[\frac{-r}{\delta}\right]$. Thus we actually have $N_{r, \delta}^{0} \leq 4<6^{1}\left(\frac{r}{\delta}\right)^{0}$.

We now proceed by induction, supposing the theorem proved for dimensions less than $k$, and we assume $k \geq 2$. We consider all the lattice points $\mathbf{m} \in R^{k}$ such that $m_{k}=p$ and $I_{\delta \mathbf{m}}^{k}(\delta)$ intersects $Z_{p}$. From what we have shown above in Step 1 and Step 2, if $\mathbf{m}$ has this property, then the bottom face of $I_{\delta \mathbf{m}}^{k}(\delta)$ intersects one of the spheres $S_{s \delta}^{k-2}(p \delta)$, where $s$ is an integer such that $j \leq s \leq l$. The number of such $\mathrm{m}$ is at most $N_{s \delta, \delta}^{k-1}$ and hence is at most $6^{(k-1)^{2}} s^{k-2}$, which is certainly no larger than

$$
6^{(k-1)^{2}}\left(1+\sqrt{\left(\frac{r}{\delta}\right)^{2}-p^{2}}\right)^{k-2}
$$

Because of our estimate of $l-j$, we see that the total number of $\mathbf{m}$ for which $m_{k}=p$ and $I_{\delta \mathrm{m}}^{k}(\delta)$ intersects $Z_{p}$ is at most

$$
6^{(k-1)^{2}}\left(2+\frac{2 p+1}{\sqrt{\left(\frac{r}{\delta}\right)^{2}-p^{2}}}\right)\left(2^{k-2}+2^{k-2}\left(\left(\frac{r}{\delta}\right)^{2}-p^{2}\right)^{\frac{k-2}{2}}\right)
$$

where we have used the inequality $(1+t)^{q} \leq 2^{q}+(2 t)^{q}$ with $q=k-2$. We expand this last product into a sum of six terms:

$$
\begin{aligned}
6^{(k-1)^{2}}\left(2^{k-1}\right. & +2^{k-1}\left(\left(\frac{r}{\delta}\right)^{2}-p^{2}\right)^{\frac{k-2}{2}}+2^{k-1} \frac{p}{\sqrt{\left(\frac{r}{\delta}\right)^{2}-p^{2}}} \\
& \left.+\frac{2^{k-2}}{\sqrt{\left(\frac{r}{\delta}\right)^{2}-p^{2}}}+2^{k-1} p\left(\left(\frac{r}{\delta}\right)^{2}-p^{2}\right)^{\frac{k-3}{2}}+2^{k-2}\left(\left(\frac{r}{\delta}\right)^{2}-p^{2}\right)^{\frac{k-3}{2}}\right) \\
= & 6^{(k-1)^{2}}\left(I_{1}(p)+I_{2}(p)+I_{3}(p)+I_{4}(p)+I_{5}(p)+I_{6}(p)\right)
\end{aligned}
$$

We need to estimate the sum of each of these terms over $p$ from 0 to $\left[\frac{r}{\delta}\right]-1$.

For $I_{1}(p)$ we have the simple estimate

$$
\sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} I_{1}(p)=2^{k-1}\left[\frac{r}{\delta}\right] \leq 2^{k-1}\left(\frac{r}{\delta}\right)<2^{k-1}\left(\frac{r}{\delta}\right)^{k-1}
$$

since $k>1$ and $r>\delta$.

For $I_{2}(p)$ we have

$$
\begin{aligned}
\sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} I_{2}(p)=2^{k-1}\left(\frac{r}{\delta}\right)^{k-2} \sum_{p=0}^{\left[\frac{r}{\delta}\right]-1}\left(1-\frac{p \delta}{r}\right)^{\frac{k-2}{2}} & <2^{k-1}\left(\frac{r}{\delta}\right)^{k-2}\left(\frac{r}{\delta}\right)=2^{k-1}\left(\frac{r}{\delta}\right)^{k-1}
\end{aligned}
$$

Here we have used the fact that $0<1-\left(\frac{p \delta}{r}\right)^{2} \leq 1$ for the values of $p$ in the range of summation, as we shall do twice more below.

For $I_{3}(p)$ we have

$$
\begin{aligned}
\sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} I_{3}(p) & =2^{k-1}\left(\frac{\delta}{r}\right) \sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} \frac{p}{\sqrt{1-\left(\frac{p \delta}{r}\right)^{2}}} \\
& \leq 2^{k-1}\left(\frac{\delta}{r}\right) \int_{1}^{\frac{r}{\delta}} \frac{x}{\sqrt{1-\left(\frac{\delta x}{r}\right)^{2}}} d x \\
& =2^{k-1}\left(\frac{r}{\delta}\right) \int_{\frac{\delta}{r}}^{1} \frac{y}{\sqrt{1-y^{2}}} d y \\
& <2^{k-1}\left(\frac{r}{\delta}\right) \int_{0}^{1} \frac{y}{\sqrt{1-y^{2}}} d y \\
& =2^{k-1} \frac{r}{\delta} \leq 2^{k-1}\left(\frac{r}{\delta}\right)^{k-1} .
\end{aligned}
$$

Since $I_{4}(p) \leq \frac{1}{2} I_{3}(p)$ for $p=1,2, \ldots,\left[\frac{r}{\delta}\right]-1$, it is clear that

$$
\sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} I_{4}(p)<2^{k-2}+2^{k-2}\left(\frac{r}{\delta}\right)^{k-1}<2^{k-1}\left(\frac{r}{\delta}\right)^{k-1}
$$

For $I_{5}(p)$ we have

$$
\begin{aligned}
\sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} I_{5}(p)=2^{k-1}\left(\frac{r}{\delta}\right)^{k-3} \sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} p\left(1-\frac{p \delta}{r}\right)^{\frac{k-3}{2}} & <2^{k-2}\left(\frac{r}{\delta}\right)^{k-3}\left(\frac{r}{\delta}\right)^{2}=2^{k-2}\left(\frac{r}{\delta}\right)^{k-1}
\end{aligned}
$$

For $I_{6}(p)$ we have

$$
\begin{aligned}
\sum_{p=0}^{\left[\frac{r}{\delta}\right]-1} I_{6}(p)=2^{k-2}\left(\frac{r}{\delta}\right)^{k-3} \sum_{p=0}^{\left[\frac{r}{\delta}\right]-1}\left(1-\frac{p \delta}{r}\right)^{\frac{k-3}{2}} & \\
& <2^{k-2}\left(\frac{r}{\delta}\right)^{k-3}\left(\frac{r}{\delta}\right)<2^{k-2}\left(\frac{r}{\delta}\right)^{k-1}
\end{aligned}
$$

Adding all these estimates, we find a sum that is at most

$$
6^{(k-1)^{2}}\left(2^{k-1}+2^{k-1}+2^{k-1}+2^{k-2}+2^{k-2}+2^{k-2}\right)\left(\frac{r}{\delta}\right)^{k-1} \leq 6^{(k-1)^{2}+1} 2^{k-1}\left(\frac{r}{\delta}\right)^{k-1} .
$$

If we wish to count the total number of hypercubes $I_{\delta \mathrm{m}}^{k}(\delta)$ that intersect the zones $Z_{1}, \ldots, Z_{\left[\frac{r}{\delta}\right]-1}$, we recall our previous observation that such a hypercube can intersect $Z_{p}$ only if $m_{k}=p$ or $m_{k}=p-1$, and if a hypercube $I_{\delta \mathrm{m}}^{k}(\delta)$ with $m_{k}=p-1$ intersects $Z_{p}$, then so does the hypercube $I_{\delta\left(\mathbf{m}+\mathbf{e}_{k}\right)}^{k}(\delta)$, and the latter has already been counted. Hence in estimating the total number of hypercubes that intersect one of these zones we are more than safe in simply doubling the estimate we have already obtained.

As for the zone $Z_{0}$, any hypercube $I_{\delta \mathrm{m}}^{k}(\delta)$ with $m_{k}=-1$ that intersects its bottom edge (the hypersphere $S_{r}^{k-2}$ ) also meets the reflection of $Z_{0}$ through the plane $x_{k}=0$, and herice the reflection of that hypercube has already been counted among those that meet $Z_{0}$. When we double our count to include the hypercubes meeting the "southern" hemisphere, all these hypercubes will automatically be counted. Thus it remains only to estimate the hypercubes that meet the "northern arctic zone," then double the count.

Hence we now consider the cap at the top of the hemisphere, whose boundary is the $(k-2)$-dimensional hypersphere

$$
S_{s}^{k-2}\left(\left[\frac{r}{\delta}\right] \delta\right)
$$

where

$$
s=\delta \sqrt{\left(\frac{r}{\delta}\right)^{2}-\left[\frac{r}{\delta}\right]^{2}}
$$

If $\mathbf{m}$ is such that $I_{\delta \mathrm{m}}^{k}(\delta)$ meets this set, then we must have $\left[\frac{r}{\delta}\right]-1 \leq m_{k} \leq\left[\frac{r}{\delta}\right]$, so that there are only two possible values for $m_{k}$. As for $m_{j}, j<k$, we certainly have $-\frac{s}{\delta}-1 \leq m_{j} \leq \frac{s}{\delta}$, so that there are at most $2^{k}\left(\frac{s}{\delta}+1\right)^{k-1}$ such hypercubes not already counted.

Since $\left[\frac{r}{\delta}\right] \leq \frac{r}{\delta}<\left[\frac{r}{\delta}\right]+1$, we easily find that

$$
s \leq 2 \sqrt{r \delta}
$$

so that $\frac{s}{\delta}+1 \leq 2 \sqrt{\frac{r}{\delta}}+1$. Once more using the inequality $(1+t)^{q} \leq 2^{q}\left(1+t^{q}\right)$ for positive $t$, with $q=k-1$, we find that the number of hypercubes meeting the northern polar cap is at most $2^{2 k-1}\left(1+\left(\frac{r}{\delta}\right)^{\frac{k-1}{2}}\right)$, which is less than $2^{2 k}\left(\frac{r}{\delta}\right)^{k-1}$.

Adding the numbers up and then doubling to count the hypercubes that meet the southern hemisphere, we find that the hypersphere $S_{r}^{k-1}$ meets at most

$$
\left[6^{(k-1)^{2}+1} 2^{k}+2^{2 k+1}\right)\left(\frac{r}{\delta}\right)^{k-1} .
$$

The constant coefficient here is less than $6^{k^{2}}$. This is directly computable for $k=2$ and $k=3$. Indeed, the quantity $2^{2 k+1}$ is less than $6^{k}$, while $2^{k^{\prime}}<6^{\frac{k}{2}}$. The extremely weak inequality $a+b<a b$, which is valid for positive integers $a$ and $b$ both larger than 1 , then implies that for $k \geq 4$ the coefficient is at most

$$
6^{(k-1)^{2}+1+(k / 2)+k}=6^{k^{2}-k / 2+2} \leq 6^{k^{2}} .
$$

Stage 1 in the proof is now complete. We have shown that the $(k-1)$-sphere $S_{r}^{k-1}$ intersects at most $6^{k^{2}}\left(\frac{r}{\delta}\right)^{k-1}$ hypercubes from the family $I_{\delta \mathbf{m}}^{k}(\delta)$. As the volume of each hypercube is $\delta^{k}$, it follows that the $(k-1)$-sphere is contained in a finite union of cubes of total $k$-dimensional volume $6^{k^{2}} r^{k-1} \delta$. Since $\delta$ is an arbitrary positive number, the $k$-dimensional volume of $S_{r}^{k-1}$ is zero.

We now move on to the second stage of the proof.

Stage 2. Given any convex set $H$ in $R^{k}$ with non-empty interior, construct a homeomorphism $\mathbf{T}$ of $R^{k}$ onto itself that maps $S^{k-1}=S_{1}^{k-1}$ to $\partial H$, the inside of the unit ball to the interior of $H$, and the outside to the exterior of $H$, and satisfies a Lipschitz condition on a neighborhood of $S^{k-1}$. To get this result we need some more background work on general convex sets in $R^{k}$.

Let $C$ be a bounded convex set in $R^{k}$, and let $\mathbf{z}$ be an interior point of $C$. For each point $\mathbf{x}$ on the unit sphere $S^{k-1}$ in $R^{k}$, let $\psi(\mathbf{x})$ be the distance from $\mathbf{z}$ to the complement of $C$ in the direction of $\mathbf{x}$, that is,

$$
\psi(\mathbf{x})=\inf \{t>0: \mathbf{z}+t \mathbf{x} \notin C\}=\sup \{t>0: \mathbf{z}+t \mathbf{x} \in C\}
$$

Step 1. Prove that the function $\psi: S^{k-1} \rightarrow(0,+\infty)$ is continuous, in fact, that it satisfies a Lipschitz condition: for some constant $K,|\psi(\mathbf{x})-\psi(\mathbf{y})| \leq K|\mathbf{x}-\mathbf{y}|$.

There exist positive numbers $a$ and $b$ such that $a \leq \psi(\mathbf{x}) \leq b$ for all $\mathbf{x} \in S^{k-1}$. Indeed we can let $a$ be the radius of the largest open ball about $z$ that is contained in $C$ and $b$ the radius of the smallest closed ball about $\mathbf{z}$ containing

The result now follows from a lemma. Let $0 \leq s \leq \psi(\mathbf{x})$. Then $C$ contains the open ball of radius $\left(1-\frac{s}{\psi(\mathbf{x})}\right) a$ about
$\mathbf{z}+s \mathbf{x}$.

Proof: If $s=\psi(\mathbf{x})$, this ball is empty, and if $s=0$ the assertion is merely the definition of $a$. Hence assume $0<s<\psi(\mathbf{x})$. Now suppose $\mid \mathbf{y}-(\mathbf{z}+$ $s \mathbf{x}) \mid<\left(1-\frac{s}{\psi(\mathbf{x})}\right) a$. Let $t=1-\frac{s}{\psi(\mathbf{x})}$, so that $0<t<1$, and let $r=$ $\frac{1}{2} \min \left(a-\frac{|\mathbf{y}-(\mathbf{z}+s \mathbf{x})|}{t}, \frac{s}{t}\right)$, so that $r>0$. Let $\mathbf{w}=\frac{\mathbf{y}-(\mathbf{z}+s \mathbf{x})}{t}+r \mathbf{x}$,
and let $u=\frac{s-t r}{1-t}$. We claim that $\mathbf{z}+\mathbf{w} \in C$ and that $\mathbf{z}+u \mathbf{x} \in C$, so that $\mathbf{y}=t(\mathbf{z}+\mathbf{w})+(1-t)(\mathbf{z}+u \mathbf{x}) \in C$.

The first claim will follow if we show that $|\mathbf{w}|<a$. In fact

$$
\begin{aligned}
|\mathbf{w}| & \leq \frac{|\mathbf{y}-(\mathbf{z}+s \mathbf{x})|}{t}+r(\text { since }|\mathbf{x}|=1) \\
& \leq \frac{|\mathbf{y}-(\mathbf{z}+s \mathbf{x})|}{t}+\frac{1}{2}\left(a-\frac{|\mathbf{y}-(\mathbf{z}+s \mathbf{x})| \mid}{t}\right) \\
& <\frac{|\mathbf{y}-(\mathbf{z}+s \mathbf{x})|}{t}+a-\frac{|\mathbf{y}-(\mathbf{z}+s \mathbf{x})|}{t}=a .
\end{aligned}
$$

The second claim will follow if we prove $0<u<\psi(\mathbf{x})$. In fact $u=(s-$ $t r) \cdot \frac{1}{1-t}=(s-t r) \cdot \frac{\psi(\mathbf{x})}{s}=\left(1-\frac{t r}{s}\right) \psi(\mathbf{x})<\psi(\mathbf{x})$. Since $r \leq \frac{s}{2 t}$, we have $u \geq \frac{1}{2} \psi(\mathbf{x})>0$.

Finally, the last claim is a routine computation:

$$
\begin{aligned}
t(\mathbf{z}+\mathbf{w})+(1-t)(\mathbf{z}+u \mathbf{x}) & =\mathbf{z}+t \mathbf{w}+(1-t) u \mathbf{x} \\
& =\mathbf{z}+\mathbf{y}-(\mathbf{z}+s \mathbf{x})+t r \mathbf{x}+(1-t) u \mathbf{x} \\
& =\mathbf{y}+(t r-s+(1-t) u) \mathbf{x} \\
& =\mathbf{y} \text { (since }(1-t) u=s-t r)
\end{aligned}
$$

The lemma is now proved.

Taking $\mathbf{y}=\mathbf{z}+s \mathbf{v}$ in this lemma (where $|\mathbf{v}|=1$ ), we see that $\mathbf{y} \in C$ (and hence $\psi(\mathbf{v}) \geq s)$ if

$$
|\mathbf{v}-\mathbf{x}|<\left(\frac{1}{s}-\frac{1}{\psi(\mathbf{x})}\right) a
$$

Now let $t>\psi(\mathbf{x}),|\mathbf{v}|=1$, and $|\mathbf{v}-\mathbf{x}|<\left(\frac{1}{\psi(\mathbf{x})}-\frac{1}{t}\right) a$. Choose $t^{\prime} \in(\psi(\mathbf{x}), t)$ such that

$$
|\mathbf{v}-\mathbf{x}|<\left(\frac{1}{t^{\prime}}-\frac{1}{t}\right) a
$$

If $\psi(\mathbf{v}) \geq t$, we have a fortiori $\psi(\mathbf{v})>t^{\prime}$ and

$$
|\mathrm{x}-\mathrm{v}|<\left(\frac{1}{t^{\prime}}-\frac{1}{\psi(\mathbf{v})}\right) a
$$

which, as already shown, implies $\psi(\mathbf{x}) \geq t^{\prime}$, contradicting the choice of $t^{\prime}$. Therefore $\psi(\mathbf{v})<t$.

To summarize, if $s<\psi(\mathbf{x})<t$, then $s \leq \psi(\mathbf{v})<t$ provided

$$
|\mathbf{x}-\mathbf{v}|<\min \left(\left(\frac{1}{s}-\frac{1}{\psi(\mathbf{x})}\right) a,\left(\frac{1}{\psi(\mathbf{x})}-\frac{1}{t}\right) a\right)
$$

This proves that $\psi$ is continuous. Specializing to the case where $s=\psi(\mathbf{x})-\varepsilon$ and $t=\psi(\mathbf{x})+\varepsilon$, we see that $|\psi(\mathbf{x})-\psi(\mathbf{v})| \leq \varepsilon$ provided $|\mathbf{x}-\mathbf{v}|<\frac{a \varepsilon}{\psi(\mathbf{x})(\psi(\mathbf{x})+\varepsilon)}$.

Again, a fortiori,

$$
|\mathbf{x}-\mathbf{v}|<\frac{a \varepsilon}{b(b+\varepsilon)} \Rightarrow|\psi(\mathbf{x})-\psi(\mathbf{v})| \leq \varepsilon
$$

We now claim that

$$
|\mathrm{x}-\mathrm{v}| \leq \frac{a \varepsilon}{2 b^{2}} \Rightarrow|\psi(\mathbf{x})-\psi(\mathrm{v})| \leq \varepsilon
$$

This follows from the previous statement and the continuity of $\psi$ (together with the fact that a closed ball on the sphere $S$ is the closure of the open ball with the same center and radius) if $\varepsilon \leq b-a$. If $\varepsilon>b-a$, the second inequality automatically holds because $\psi(\mathrm{x})$ and $\psi(\mathbf{v})$ differ by at most $b-a$. Specializing to equality in the hypothesis, we deduce the Lipschitz inequality

$$
|\psi(\mathbf{x})-\psi(\mathbf{v})| \leq \frac{2 b^{2}}{a}|\mathbf{x}-\mathbf{v}|
$$

We remark that the statement that $\mathbf{y}$ belongs to the interior, boundary, and exterior of $C$ is equivalent to $|\mathbf{y}-\mathbf{z}|<\psi\left(\frac{\mathbf{y}-\mathbf{z}}{|\mathbf{y}-\mathbf{z}|}\right),|\mathbf{y}-\mathbf{z}|=\psi\left(\frac{\mathbf{y}-\mathbf{z}}{|\mathbf{y}-\mathbf{z}|}\right)$, or $|\mathbf{y}-\mathbf{z}|>\psi\left(\frac{\mathbf{y}-\mathbf{z}}{|\mathbf{y}-\mathbf{z}|}\right)$.

Step 2. Use the function $\psi(\mathbf{x})$ to define a homeomorphism of $R^{k}$ onto itself that maps $S^{k-1}$ to $\partial H$ and is Lipschitz in a neighborhood of $S^{k-1}$. Such a homeomorphism $\mathbf{T}(\mathbf{x})$ is defined for all $\mathbf{x} \in R^{k}$ as follows. We set $\mathbf{T}(\mathbf{0})=\mathbf{z}$ and

$$
\mathbf{T}(\mathbf{x})=\mathbf{z}+\psi\left(\frac{\mathbf{x}}{|\mathbf{x}|}\right) \mathbf{x}
$$

if $\mathbf{x} \neq \mathbf{0}$. Since $|\mathbf{T}(\mathbf{x})-\mathbf{T}(\mathbf{0})| \leq M|\mathbf{x}|$, where $M=\sup \{\psi(\mathbf{y}):|\mathbf{y}|=1\}$, it is clear that $\mathbf{T}$ is continuous at $\mathbf{0}$. At all other points it is a composition of continuous functions, hence continuous. Since $\frac{\mathbf{T}(\mathbf{x})-\mathbf{z}}{|\mathbf{T}(\mathbf{x})-\mathbf{z}|}=\frac{\mathbf{x}}{|\mathbf{x}|}$, we have the continuous inverse function

$$
\mathbf{x}=\frac{\mathbf{T}(\mathbf{x})-\mathbf{z}}{\psi((\mathbf{T}(\mathbf{x})-\mathbf{z}) /|\mathbf{T}(\mathbf{x})-\mathbf{z}|)}
$$

That is, for $\mathbf{y} \neq \mathbf{z}$,

$$
\mathbf{T}^{-1}(\mathbf{y})=\frac{\mathbf{y}-\mathbf{z}}{\psi((\mathbf{y}-\mathbf{z}) /|\mathbf{y}-\mathbf{z}|)}
$$

which is not $\mathbf{0}$. Thus the mapping is one-to-one and onto.

The mapping also satisfies a Lipschitz condition on the exterior of each ball about $\mathbf{0}$; that is, on the set $E_{\eta}=\{\mathbf{x}:|\mathbf{x}| \geq \eta\}$ for each $\eta>0$. To see this we observe that for any $\mathbf{x}$ and $\mathbf{y}$ in this set,

$$
|\mathbf{T}(\mathbf{x})-\mathbf{T}(\mathbf{y})|=\left|\psi\left(\frac{\mathbf{x}}{|\mathbf{x}|}\right) \mathbf{x}-\psi\left(\frac{\mathbf{y}}{|\mathbf{y}|}\right) \mathbf{y}\right|
$$

$$
\begin{aligned}
& \leq\left|\psi\left(\frac{\mathbf{x}}{|\mathbf{x}|}\right)-\psi\left(\frac{\mathbf{y}}{|\mathbf{y}|}\right)\right||\mathbf{y}|+\psi\left(\frac{\mathbf{x}}{|\mathbf{x}|}\right) \mathbf{x}-\mathbf{y} \mid \\
& \leq K\left|\frac{\mathbf{x}}{|\mathbf{x}|}-\frac{\mathbf{y}}{|\mathbf{y}|}\right|+M|\mathbf{x}-\mathbf{y}| \\
& \leq\left(\frac{2 K}{\eta}+M\right)|\mathbf{x}-\mathbf{y}|
\end{aligned}
$$

Here we have used the fact that

$$
\begin{aligned}
\left|\frac{x}{|x|}-\frac{y}{|y|}\right| & =\frac{1}{|x||y|}|| y|x-| x|y| \\
& =\frac{1}{|x||y|}|(|y|-|x|) x+| x|(x-y)| \\
& \leq \frac{1}{|y|}|| y|-| x||+\frac{1}{|y|}|x-y| \\
& =\frac{2}{|y|}|y-x|
\end{aligned}
$$

The statements about the images of the inside of the unit ball, the unit sphere, and the outside are now obvious. For example, as remarked above, $\mathbf{y} \in \partial H$ if and only if $|\mathbf{y}-\mathbf{z}|=\psi\left(\frac{\mathbf{y}-\mathbf{z}}{|\mathbf{y}-\mathbf{z}|}\right)$. But this is equivalent to the statement that $\mathbf{T}^{-1}(\mathbf{y})=\frac{\mathbf{y}-\mathbf{z}}{|\mathbf{y}-\mathbf{z}|}$, which says precisely that $\mathbf{T}^{-1}(\mathbf{y})$ belongs to the unit sphere.

We have now finished Stage 2 of the proof and are ready for the third and final stage.

Stage 3. For each $\delta>0$, approximate a function $f(\mathrm{x})$ that is continuous on $H$ by a function $f_{\delta}(\mathrm{x})$ that is continuous on all of $R^{k}$ and such that the iterated integrals of $f$ and $f_{\delta}$ differ by at most a fixed multiple of $\delta$ no matter what order they are taken in.

To that end, we first let $\delta \in(0,1 / \sqrt{k})$ be given. According to what was proved in Stage 1, the hypersphere $S^{k-1}$ is contained in the interior of the set of hypercubes $I_{\delta \mathbf{m}}^{k}(\delta)$ that intersect it, and there are at most $6^{k^{2}} \delta^{1-k}$ of these hypercubes. In each hypercube $I_{\delta \mathbf{m}}^{k}(\delta)$ from this family we choose and keep fixed one point $\mathbf{x}_{\mathrm{m}}$ belonging to $S^{k-1}$. The image of these hypercubes under $\mathbf{T}$ is a compact set containing $\partial H$ in its interior, and each of them is contained in a hypercube of side at most $2 L \sqrt{k} \delta$ centered at $\mathbf{T}\left(\mathbf{x}_{\mathbf{m}}\right) \in \partial H$, where $L$ is the Lipschitz constant for the mapping $\mathbf{T}$ on the set $E_{1-\delta}$, so that the total volume of these hypercubes is at most $6^{k^{2}}(2 L \sqrt{k})^{k} \delta$. Let $c>0$ be the distance from $H$ to the complement of the union of these hypercubes.

We define $f_{\delta}(\mathbf{x})$ as a continuous function that equals $f(\mathbf{x})$ for $\mathbf{x} \in H$, while for $\mathbf{x}$ not in the interior of $H$ we set $f_{\delta}(\mathbf{x})=\max \left(0,1-\frac{d(\mathbf{x}, H)}{c}\right) f(\theta(\mathbf{x}))$. Here $\theta(\mathbf{x})$ is the unique point of $H$ closest to $\mathbf{x}$ and $d(\mathbf{x}, H)$ is the distance from $\mathbf{x}$ to $H$. On the boundary of $H$, where we have apparently given two definitions of $f_{\delta}$ we have $d(\mathbf{x}, H)=0$, so that the two definitions are consistent. Hence the piecewisedefined function will be continuous if each of the pieces is. The piece defined
on $H$ is continuous by assumption, so that we need only concern ourselves with the second definition. It is well-known that $d(\mathbf{x}, H)$ is a continuous function of $\mathbf{x}$. It is somewhat less obvious that $\theta(\mathbf{x})$ is continuous, so that we must prove that fact.

First we show that there is a unique point $\theta(\mathbf{x})$ in $H$ closest to $\mathbf{x}$. This is obvious if $\mathbf{x} \in H$, so we assume $\mathbf{x} \notin H$. Let $c=\min \{|\mathbf{x}-\mathbf{z}|: \mathbf{z} \in H\}$, and suppose $\mathbf{z}$ and $\mathbf{w}$ are two points of $H$ such that $|\mathbf{x}-\mathbf{z}|=c=|\mathbf{x}-\mathbf{w}|$. Then the point $\mathbf{w}+t(\mathbf{z}-\mathbf{w})$ belongs to $H$ for $0 \leq t \leq 1$, and so the quadratic function $\left.|\mathrm{x}-\mathrm{w}-t(\mathbf{z}-\mathbf{w})|^{2}=|\mathbf{x}-\mathbf{w}|^{2}-2 t(\mathrm{x}-\mathrm{w}) \cdot(\mathbf{z}-\mathbf{w})\right)+t^{2}|\mathbf{z}-\mathbf{w}|^{2}$ has its minimum value $c$ on $[0,1]$ at both endpoints. But this is impossible for a non-constant quadratic function whose leading coefficient is positive. Hence the function is constant, that is, $\mathbf{z}=\mathbf{w}$. Now suppose $\mathbf{x}_{n} \rightarrow \mathbf{x}$. We claim $\theta\left(\mathbf{x}_{n}\right) \rightarrow \theta(\mathbf{x})$.

Since $H$ is compact, we can pass to a subsequence if necessary and assume that $\theta\left(\mathbf{x}_{n}\right) \rightarrow \mathbf{z}$ for some point $\mathbf{z} \in H$. Certainly $\left|\mathbf{x}_{n}-\theta\left(\mathbf{x}_{n}\right)\right| \rightarrow|\mathbf{x}-\mathbf{z}|$. But $\left|\mathbf{x}_{n}-\theta\left(\mathbf{x}_{n}\right)\right|=d\left(\mathbf{x}_{n}, H\right) \rightarrow d(\mathbf{x}, H)$, so that $|\mathbf{x}-\mathbf{z}|=d(\mathbf{x}, H)=|\mathbf{x}-\theta(\mathbf{x})|$. As $H$ contains only one point satisfying this equality, we must have $\mathbf{z}=\theta(\mathbf{x})$. Thus $\theta(\mathrm{x})$ is a continuous function, and therefore $f_{\delta}(\mathbf{x})$ is continuous on all of $R^{k}$.

It is now clear that $|f(\mathbf{x})|$ and $\left|f_{\delta}(\mathbf{x})\right|$ have the same maximum value, say $J$, and that $f$ and $f_{\delta}$ differ only on the finite set of hypercubes covering $\partial H$. The iterated integrals of the two functions, taken in any order, over this finite set of hypercubes differ by at most $6^{k^{2}} J L(2 \sqrt{k})^{k} \delta$. Thus the iterated integral of $f$ differs from the iterated integral of $f_{\delta}$ by at most this amount, and since all the iterated integrals of $f_{\delta}$ are equal, it follows that any two iterated integrals of $f$ differ by arbitrarily small amounts, hence are equal.

The proof is, at long last, complete.

Because this proof is so long and involved, it may be worthwhile to look at an alternative proof that works only for the case $k=2$ and does not generalize to higher dimensions. To this end, let $k=2$. we define two functions $m(x)$ and $M(x)$, as follows: The domain of both functions is the projection of $H$ on the $x$-axis, that is, the set $\Pi(H)$ consisting of $x$ such that there exists $y$ for which $(x, y) \in H$. By definition $m(x)$ is the minimal $y$ for which $(x, y) \in H$, and $M(x)$ is the maximal $y$ for which $(x, y) \in H$. We claim that these functions are continuous on $\Pi(H)$. Indeed, suppose $\left(x^{(n)}, y^{(n)}\right) \in H$ and $x^{(n)} \rightarrow x$. Without loss of generality we can assume that $x^{(n)}>x$ for all $x$. (By passing to a subsequence if necessary, we can have either $x^{(n)}<x$ for all $n$ or $x^{(n)}>x$ or $x^{(n)}=x$ for all $n$. The last case is trivial, and the other two cases are handled by identical arguments.). Some subsequence of $M\left(x^{(n)}\right)$ converges to a value $z$. Since $\left(x^{(n)}, M\left(x^{(n)}\right)\right) \in H$, and $H$ is closed, it follows that $(x, z)$ belongs to $H$. It is clear then that the assumption $z>M(x)$ contradicts the definition of $M(x)$ as the maximal number $y$ for which $(x, y) \in H$. Hence it suffices to prove that $z \geq M(x)$. This will certainly be the case if $M\left(x^{(n)} \geq M(x)\right.$ for all $n$. Hence assume that $n_{0}$ is an index for which $M\left(x^{\left(n_{0}\right)}\right)<M(x)$. Now $x^{\left(n_{0}\right)}>x$, since if the two were equal, $M\left(x^{\left(n_{0}\right)}\right)$ would equal $M(x)$. We observe that if $t \in[0,1]$, then the point $\left(t x^{\left(n_{0}\right)}+(1-t) x, t M\left(x^{\left(n_{0}\right)}\right)+(1-t) M(x)\right)$ belongs
to $H$. In particular, taking $t=\frac{x^{(n)}-x}{x^{\left(n_{0}\right)}-x}$, we find that $t x^{\left(n_{0}\right)}+(1-t) x=x^{(n)}$. It therefore follows that $M\left(x^{(n)}\right) \geq \frac{x^{(n)}-x}{x^{\left(n_{0}\right)}-x} M\left(x^{\left(n_{0}\right)}+\frac{x^{\left(n_{0}\right)}-x^{(n)}}{x^{\left(n_{0}\right)}-x} M(x) \rightarrow M(x)\right.$. Therefore $z \geq M(x)$.

(It is this part of the argument that does not generalize to $R^{3}$, as shown by the the convex set

$$
H=\left\{(1-t, t y, t z): 0 \leq t \leq 1,-1 \leq y \leq 1, y^{2} \leq z \leq 1\right\}
$$

On this set, if we define $M(y, z)=\sup \{x:(x, y, z) \in H\}$, we have $M\left(s, s^{2}\right)=0$ for $s \neq 0$, but $M(0,0)=1$.)

It now follows that $M(x)$ is continuous on $H$, and the proof that $m(x)$ is continuous is similar.

Now let $H$ be a convex closed set in $R^{2}$ containing an interior point. For each $\delta>0$, we let $H_{\delta}$, be the $\delta$-neighborhood of $H$, that is, the set of points whose distance from $H$ is at most $\delta$. It is clear that $H_{\delta}$ is a convex set containing $H$ in its interior. If $f$ is a continuous function on $H$, we extend $f$ to a function $f_{\delta}$ defined on all of $R^{2}$, as above.

By our definition

$$
\int_{H} f(x, y) d y d x=\int_{a}^{b} \int_{m(x)}^{M(x)} f(x, y) d y d x
$$

where $[a, b]$ is the projection of $H$ on the $x$-axis and for each $x \in[a, b]$

$$
m(x)=\min \{t:(x, t) \in H\}
$$

and

$$
M(x, y)=\max \{t:(x, t) \in H\}
$$

We intend to show that the when these integrals are evaluated, the resulting value is the limit of the same integrals evaluated for $f_{\delta}$, and of course the same for the integrals in reverse order. Hence these two iterated integrals are equal.

To that end, let $A$ be the maximal value of $|f(x, y)|$, which is also the maximal value of $\left|f_{\delta}(x, y)\right|$. As we have set $f(x, y)=0$ on the complement of $H$, the two functions $f(x, y)$ and $f_{\delta}(x, y)$ differ only on the set $H_{\delta} \backslash H$, and by no more than $A$ at any point.

Let $P_{\delta}=[a-\lambda(\delta), b+\mu(\delta)]$ be the projection of $H_{\delta}$ on the $x$-axis. We claim that $\lambda(\delta)$ and $\mu(\delta)$ both tend to zero as $\delta$ tends to zero. For certainly $\lambda(\delta)$ decreases as $\delta$ decreases. Let its limit be $c$. There is a point $\left(a-\lambda(\delta), y_{\delta}\right) \in H_{\delta}$ for each $\delta>0$. If $y$ is a limit point of $y_{\delta}$ as $\delta \rightarrow 0$, then, since $\left(a-\lambda(\eta), y_{\eta}\right) \in$ $H_{\eta} \subset H_{\delta}$ for $\eta<\delta$ and $H_{\delta}$ is closed, it follows that $(a-c, y) \in H_{\delta}$ for all $\delta>0$, and therefore, since $\cap_{\delta>0} H_{\delta}=H$, that $(a-c, y) \in H$. By definition of $a$, it then follows that $a \leq a-c \leq a$, and so $c=0$. The proof that $\mu \rightarrow 0$ is similar.

Let $m_{\delta}(x)$ and $M_{\delta}(x)$ be the functions corresponding to $m(x)$ and $M(x)$ for $H_{\delta}$. For $x \in[a, b]$ we have $m_{\delta}(x)<m(x) \leq M(x)<M_{\delta}(x)$. An argument similar to the one just given shows that $m(x)-m_{\delta}(x)$ and $M_{\delta}(x)-M(x)$ tend
monotonically to zero for each $x \in[a, b]$. Since these are continuous functions, this convergence is uniform. let $\varphi(\delta)=\max _{x \in[a, b]}\left\{m(x)-m_{\delta}(x), M_{\delta}(x)-M(x)\right\}$, so that $\varphi(\delta) \rightarrow 0$ as $\delta \rightarrow 0$.

Now the double integrals that we wish to evaluate are

$$
\int_{a}^{b} \int_{m(x)}^{M(x)} f(x, y) d y d x
$$

and

$$
\int_{a-\lambda(\delta)}^{b+\mu(\delta)} \int_{m_{\delta}(x)}^{M_{\delta}(x)} f_{\delta}(x, y) d y d x
$$

Now fix a number $N$ larger than twice the absolute value of any coordinate of any point in $H_{1}$, and assume $\delta<\min (1, N)$. We observe that the difference between the two integrals is

$$
\begin{aligned}
\int_{a-\lambda(\delta)}^{a} \int_{-N}^{N} f_{\delta}(x, y) d y d x & +\int_{a}^{b} \int_{m_{\delta}(x)}^{m(x)} f_{\delta}(x, y) d y d x \\
& +\int_{a}^{b} \int_{M(x)}^{M_{\delta}(x)} f_{\delta}(x, y) d y d x+\int_{b}^{b+\mu(\delta)} \int_{-N}^{N} f_{\delta}(x, y) d y d x
\end{aligned}
$$

This expression is assuredly not larger than

$$
2 A N(\lambda(\delta)+\varphi(\delta)+\mu(\delta))
$$

and hence it tends to zero as $\delta \rightarrow 0$. The same is true of the integral in the reverse order, and for the same reasons. Since the integral of $f_{\delta}$ is the same in either order, it follows that the integral of $f$ is also the same in either order.

Exercise 10.2 For $i=1,2,3, \ldots$, let $\varphi_{i} \in \mathcal{C}\left(R^{1}\right)$ have support in $\left(2^{-i}, 2^{1-i}\right)$, such that $\int \varphi_{i}=1$. Put

$$
f(x, y)=\sum_{i=1}^{\infty}\left[\varphi_{i}(x)-\varphi_{i+1}(x)\right] \varphi_{i}(y)
$$

The $f$ has compact support in $R^{2}, f$ is continuous except at $(0,0)$, and

$$
\int d y \int f(x, y) d x=0 \text { but } \int d x \int f(x, y) d y=1
$$

Observe that $f$ is unbounded in every neighborhood of $(0,0)$.

Solution: The computation is straightforward:

$$
\int f(x, y) d x=\sum_{i=1}^{\infty} \varphi_{i}(y)[1-1]=0
$$

$$
\int f(x, y) d y=\sum_{i=1}^{\infty}\left[\varphi_{i}(x)-\varphi_{i+1}(x)\right]=\varphi_{1}(x)
$$

To justify the first of these, we observe that the sum is finite for each fixed $y$, since $\varphi_{i}(y)=0$ for $i>-\log _{2}(y)$ if $y>0$. Likewise the second sum is finite for each fixed $x$. The result now follows. The function must be unbounded, since the integral of $\varphi_{i}$ must be 1 , even though the support of that function has length $2^{-i}$.

Exercise $\mathbf{1 0 . 3}(a)$ If $\mathbf{F}$ is as in Theorem 10.7, put $\mathbf{A}=\mathbf{F}^{\prime}(0), \mathbf{F}_{1}(\mathbf{x})=$ $\mathbf{A}^{-1} \mathbf{F}(\mathbf{x})$. Then $\mathbf{F}_{1}^{\prime}(\mathbf{0})=I$. Show that

$$
\mathbf{F}_{1}(\mathbf{x})=\mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x})
$$

in some neighborhood of 0 , for certain primitive mappings $\mathbf{G}_{1}, \ldots, \mathbf{G}_{n}$. This gives another version of Theorem 10.7:

$$
\mathbf{F}(\mathbf{x})=\mathbf{F}^{\prime}(\mathbf{0}) \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x})
$$

(b) Prove that the mapping $(x, y) \rightarrow(y, x)$ of $R^{2}$ onto $R^{2}$ is not the composition of any two primitive mappings, in any neighborhood of the origin. (This shows that the flips $B_{1}$ cannot be omitted from the statement of Theorem 10.7.)

Solution: (a) According to the proof of Theorem 10.7, the flips are needed only to interchange $m$ and $k$, where $k$ is the first index not less than $m$ for which $D_{m} \alpha_{k}(\mathbf{0}) \neq 0$. Here

$$
\mathbf{F}_{m}^{\prime}(\mathbf{0}) \mathbf{e}_{m}=\sum_{i=m}^{n}\left(D_{m} \alpha_{i}\right)(\mathbf{0}) \mathbf{e}_{i}
$$

But in that proof $\mathbf{F}_{1}=\mathbf{F}$, and since in the present case $\mathbf{F}^{\prime}(\mathbf{0})$ is the identity, $B_{1}$ is the identity. But then the definition of $\mathbf{G}_{1}(\mathbf{x})$ as

$$
\mathbf{G}_{1}(\mathbf{x})=\mathbf{x}+\left[\alpha_{1}(\mathbf{x})-x_{1}\right] \mathbf{e}_{1}
$$

implies that $\mathbf{G}_{1}^{\prime}(\mathbf{0})$ is also the identity. Suppose we know that $B_{j}, \mathbf{F}_{m}^{\prime}(\mathbf{0})$, and $\mathrm{G}_{j}^{\prime}(0)$ are all equal to the identity for $j \leq m$. Then the inductive definition of $\mathbf{F}_{m+1}$ as $\mathbf{F}_{m+1}(\mathbf{y})=\mathbf{F}_{m} \circ \mathbf{G}_{m}^{-1}(\mathbf{y})$ implies that $\mathbf{F}_{m+1}^{\prime}(\mathbf{0})$ is also the identity, from which it then follows that $\mathbf{F}_{m+1}^{\prime}(\mathbf{0}), B_{m+1}$, and $\mathbf{G}_{m+1}^{\prime}(\mathbf{0})$ are all equal to the identity. Thus the decomposition of $\mathbf{F}_{1}$ involves no flips, as asserted.

(b) If this map were a composition of two primitive maps, its derivative at $(0,0)$ would be the product of two matrices of the form

$$
\left(\begin{array}{ll}
a & b \\
0 & 1
\end{array}\right)\left(\begin{array}{ll}
1 & 0 \\
c & d
\end{array}\right)=\left(\begin{array}{cc}
a+b c & b d \\
c & d
\end{array}\right)
$$

Since this matrix must be $\left(\begin{array}{ll}0 & 1 \\ 1 & 0\end{array}\right)$, it follows that $c=1, d=0$. But then the second column of the product of the two matrices is zero, which is a contradiction.

Exercise 10.4 For $(x, y) \in R^{2}$, define

$$
\mathbf{F}(x, y)=\left(e^{x} \cos y-1, e^{x} \sin y\right)
$$

Prove that $\mathbf{F}=\mathbf{G}_{2} \circ \mathbf{G}_{1}$, where

$$
\begin{aligned}
\mathbf{G}_{1}(x, y) & =\left(e^{x} \cos y-1, y\right) \\
\mathbf{G}_{2}(x, y) & =(u,(1+u) \tan v)
\end{aligned}
$$

are primitive in some neighborhood of $(0,0)$.

Compute the Jacobians of $\mathbf{G}_{1}, \mathbf{G}_{2}$, and $\mathbf{F}$ at $(0 ; 0)$. Define

$$
\mathbf{H}_{2}(x, y)=\left(x, e^{x} \sin y\right)
$$

and find

$$
\mathbf{H}_{1}(u, v)=(h(u, v), v)
$$

so that $\mathbf{F}=\mathrm{H}_{1} \circ \mathbf{H}_{2}$ in some neighborhood of $(0,0)$.

Solution: The equation $\mathbf{F}=\mathbf{G}_{2} \circ \mathbf{G}_{1}$ is a routine computation, and the fact that $\mathbf{G}_{1}$ and $\mathbf{G}_{2}$ are primitive is immediate.

The Jacobians of $\mathbf{G}_{1}$ and $\mathbf{G}_{2}$ are

$$
\mathbf{G}_{1}^{\prime}(x, y)=\left(\begin{array}{cc}
e^{x} \cos y & -e^{x} \sin y \\
0 & 1
\end{array}\right), \quad \mathbf{G}_{2}^{\prime}(u, v)=\left(\begin{array}{cc}
1 & 0 \\
\tan v & (1+u) \sec ^{2} v
\end{array}\right)
$$

so that each of them equals the identity at $(0,0)$. It therefore follows that $\mathbf{F}^{\prime}(0,0)=I$ also.

If we take $h(u, v)=\left(\sqrt{e^{2 u}-v^{2}}-1, v\right)$, the primitive mapping $\mathbf{H}_{1}(u, v)=$ $(h(u, v), v)$ will yield $\mathbf{H}_{1} \circ \mathbf{H}_{2}=\mathbf{F}$.

Exercise 10.5 Formulate and prove an analogue of Theorem 10.8, in which $K$ is a compact subset of an arbitrary metric space. (Replace the functions $\varphi_{i}$ that occur in the proof of Theorem 10.8 by functions of the type constructed in Exercise 22 of Chap. 4.)

Solution: We are given a compact set $K$ in a metric space $X$ (say with metric d) and a cover of $K$ by open sets $V_{i}, i=1,2, \ldots, n$. (We may as well assume a finite number of sets, since we can find a finite subcover of any infinite cover.)

We need to construct continuous functions $\psi_{i}, i=1,2, \ldots, n$ such that $0 \leq \psi_{i}(x) \leq 1$ for all $i$ and all $x \in X$, the support of $\psi_{i}(x)$ is contained in $V_{i}$, and $\sum_{i=1}^{n} \psi_{i}(x)=1$ for all $x \in K$.

To do this, let $\eta>0$ be a Lebesgue number for the covering of $K$ by the sets $V_{i}$, that is such that the $\eta$-neighborhood of every point $x \in K$ is contained in some $V_{i}$. Let $\varepsilon \in(0, \eta)$, and let $U_{i}$ be the set of points whose distance from the complement of $V_{i}$ is larger than $\varepsilon$ and $W_{i}$ the set of points whose distance
from the complement of $V_{i}$ is larger than $\frac{\varepsilon}{2}$. Since the distance from $x$ to the complement of $V_{i}$ is a continuous function of $x$, it follows that $U_{i}$ and $W_{i}$ are open sets. It is obvious that the closure of $U_{i}$ is contained in $W_{i}$ and the closure of $W_{i}$ is contained in $V_{i}$. We note that $K \subset \bigcup_{i=1}^{n} U_{i}$. For if $x \in K$ there exists $V_{i}$ such that the $\eta$-neighborhood of $x$ is contained in $V_{i}$, and hence the distance from $x$ to the complement of that $V_{i}$ is at least $\eta$.

Now let $A_{i}$ be the closure of $U_{i}$, and $B_{i}$ the complement of $W_{i}$. Define

$$
\varphi_{i}(x)=\frac{d\left(x, B_{i}\right)}{d\left(x, A_{i}\right)+d\left(x, B_{i}\right)}
$$

Then $\varphi_{i}(x)$ is 1 on $A_{i}$ (and hence certainly on $U_{i}$ ) and 0 on $B_{i}, \varphi_{i}(x)$ is continuous, and $0 \leq \varphi_{i}(x) \leq 1$ for all $x$. Since the support of $\varphi_{i}(x)$ is the closure of $W_{i}$, it is contained in $V_{i}$. Since $\varphi_{i}(x)>0$ for $x$ in $W_{i}$, the $\operatorname{sum} \varphi(x)=\sum_{i=1}^{n} \varphi_{i}(x)$ is positive on the open set $W=\bigcup_{i=1}^{n} W_{i}$, which contains $K$. Now let $L$ be the complement of $W$, and define a continuous function $\psi(x)$ by

$$
\psi(x)=\frac{d(x, L)}{d(x, K)+d(x, L)}
$$

so that $0 \leq \psi(x) \leq 1$ for all $x, \psi(x)=1$ if $x \in K$, and $\psi(x)=0$ if $x \in L$. If we now define $\psi_{i}(x)=0$ for $x \notin W$ and

$$
\psi_{i}(x)=\frac{\varphi_{i}(x) \psi(x)}{\varphi(x)}
$$

then $\psi_{i}(x)$ is continuous on the entire space. Its restriction to $L$ is continuous. If we can show that its restriction to the closure of $W$ is continuous, we shall be done. But it is obvious that it is continuous on $W$ itself, and so we need only show that it is continuous at a point of $\partial W$. Hence let $x_{n} \rightarrow x \in \partial W$. Since $\varphi_{i}(x) / \varphi(x)$ is bounded, and $\psi\left(x_{n}\right) \rightarrow 0$, it follows that $\psi_{i}\left(x_{n}\right) \rightarrow 0=\psi_{i}(x)$, and hence $\psi_{i}$ is continuous at $x$.

The construction is now complete.

Exercise 10.6 Strengthen the conclusion of Theorem 10.8 by showing that the functions $\psi_{i}$ can be made differentiable, and even infinitely differentiable. (Use Exercise 1 of Chap. 8 in the construction of the auxiliary functions $\varphi_{i}$.)

Solution: The function $\varphi_{i}(\mathbf{x})$ is required to have only three properties: 1) $\varphi_{i}(\mathbf{x})=1$ for $\left.\left|\mathbf{x}-\mathbf{a}_{i}\right| \leq r_{i} ; 2\right) \varphi_{i}(\mathbf{x})=0$ for $\left.\left|\mathbf{x}-\mathbf{a}_{i}\right| \geq s_{i} ; 3\right) 0=\leq \varphi_{i}(\mathbf{x}) \leq 1$ for all $x$. These properties can be achieved with an infinitely differentiable function $\varphi_{i}(\mathbf{x})$. To construct such a function, we go to the function $f(t)$ in Exercise 1 of Chapter 8 , namely

$$
f(t)=e^{-\frac{1}{t^{2}}}
$$

for $t \neq 0$ and $f(0)=0=f^{(n)}(0)$ for all positive integers $n, f^{(n)}(t)$ being the $n$th derivative of $f(t)$. It was established in that exercise that $f(t)$ is infinitely differentiable, and it is obvious that $f(t)$ is strictly increasing for nonnegative values of $t$.

Let

$$
g(t)=\frac{f(f(1)-f(t))}{f(f(1))}
$$

Then it is obvious that $g(t)$ is an infinitely differentiable function that decreases from 1 to 0 as $x$ increases from 0 to 1 . If we show that $g^{(n)}(0)=0=$ $g^{(n)}(1)$ for all positive integers $n$, it will follow that the function

$$
h(t)= \begin{cases}1, & t \leq 0 \\ g(t), & 0 \leq t \leq 1 \\ 0, & 1 \leq t\end{cases}
$$

is also a $\mathcal{C}^{\infty}$ function, and we can then take

$$
\varphi(\mathbf{x})=h\left(\frac{|\mathbf{x}|^{2}-r_{i}^{2}}{s_{i}^{2}-r_{i}^{2}}\right)
$$

But it is easy to prove these properties by showing inductively that for all integers $j$ and $k$ with $0 \leq j \leq n-k$ and $1 \leq k \leq n$ there exist infinitely differentiable functions $\theta_{j, k, n}(t)$ such that

$$
g^{(n)}(t)=\sum_{\substack{0 \leq j \leq n-k \\ 1 \leq k \leq n}} \theta_{j, k, n}(t) f^{(k)}(f(1)-f(t)) f^{(n-k-j+1)}(t)
$$

In fact the chain rule shows that

$$
\theta_{0,1,1}(t)=-\frac{1}{f(f(1))}
$$

Then, assuming there exist such functions $\theta_{j, k, n}(t)$, we find

$$
\begin{aligned}
g^{(n+1)}(t)= & \sum_{\substack{0 \leq j \leq n-k \\
1 \leq k \leq n}}\left\{\theta_{j, k, n}^{\prime}(t) f^{(k)}(f(1)-f(t)) f^{(n-k-j+1)}(t)\right. \\
& +\theta_{j, k, n}(t)\left(-f^{\prime}(t)\right) f^{(k+1)}(f(1)-f(t)) f^{n-k-j+1}(t) \\
& \left.+\theta_{j, k, n}(t) f^{(k)}(f(1)-f(t)) f^{(n-k-j+2)} .\right\}
\end{aligned}
$$

Each term in this expression contains a factor $f^{(s)}(f(1)-f(t)) f^{(n+1-s-r+1)}(t)$ with $0 \leq r \leq n+1-s, 1 \leq s \leq n+1$ and with a coefficient that is infinitely differentiable. Thus when suitably rearranged, this sum has the appropriate form

$$
g^{(n+1)}(t)=\sum_{\substack{0 \leq j \leq n+1-k \\ 1 \leq k \leq n+1}} \theta_{j, k, n+1}(t) f^{(k)}(f(1)-f(t)) f^{(n-k-j+1)}(t)
$$

with infinitely differentiable functions $\theta_{j, k, n+1}$. Since each term contains a factor $f^{(k)}(f(1)-f(t)) f^{(l)}(t)$ with $k \geq 1$, it follows that each term vanishes when $t=0$ or $t=1$, and hence that $g^{(n)}(1)=0=g^{(n)}(0)$ for $n=1,2, \ldots$

Exercise 10.7 (a) Show that the simplex $Q^{k}$ is the smallest convex subset of $R^{k}$ that contains $\mathbf{0}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{k}$.

(b) Show that affine mappings take convex sets to convex sets.

Solution: (a) By definition $Q^{k}=\left\{\mathbf{x}: x_{1}+\cdots x_{k} \leq 1, x_{j} \geq 0, j=1, \ldots, k\right\}$. It is obvious that $Q^{k}$ contains all the points $0, \mathrm{e}_{1}, \ldots, \mathrm{e}_{k}$. It is nearly obvious that $Q^{k}$ is convex. Indeed, if $\mathbf{x}$ and $\mathbf{y}$ are points of $Q^{k}$ and $0<t<1$, then $t \mathbf{x}+(1-t) \mathbf{y}=\mathbf{z}$, where $z_{j}=t x_{j}+(1-t) y_{j}$. Since $x_{j} \geq 0$ and $y_{j} \geq 0$ and $0<t<1$, it is clear that $z_{j} \geq 0$. Simple algebra shows that $z_{1}+\cdots+z_{k} \leq$ $t+(1-t)=1$, so that $\mathbf{z} \in Q^{k}$ also. Thus $Q^{k}$ is convex.

Now let $C$ be any convex set containing these points, and let $\mathbf{x} \in Q^{k}$. We need to show that $\mathrm{x} \in C$. We shall show by induction that the point $x_{1} \mathbf{e}_{1}+\cdots+x_{j} \mathbf{e}_{j}$ is in $C$ whenever $x_{1} \geq 0, \ldots, x_{j} \geq 0$ and $x_{1}+\cdots+x_{j} \leq 1$. If $j=1$, this is obvious, since $x_{1} \mathbf{e}_{1}=x_{1} \mathbf{e}_{1}+\left(1-x_{1}\right) \mathbf{0}$ and by assumption $0 \leq x_{1} \leq 1$.

Suppose the theorem is true for $j$, and let $c=x_{1}+\cdots+x_{j+1} \leq 1, x_{1} \geq$ $0, \ldots, x_{j+1} \geq 0$. If $c=0$, the point $x_{1} \mathbf{e}_{1}+\cdots+x_{j+1} \mathbf{e}_{j+1}$ is 0 , and hence belongs to $C$. Therefore we assume $c>0$. Since $\mathbf{e}_{j+1} \in C$, we need only consider the case $x_{j+1}<1$. By the induction assumption, taking $x_{l}^{\prime}=\frac{x_{l}}{1-x_{j+1}}$ for $l=1, \ldots, j$, we find that the point $\mathbf{y}=x_{1}^{\prime} \mathbf{e}_{1}+\cdots+x_{j}^{\prime} \mathbf{e}_{j}$ belongs to $C$, and therefore the point $\left(1-x_{j+1}\right) \mathbf{y}+x_{j+1} \mathbf{e}_{j+1}=x_{1} \mathbf{e}_{1}+\cdots+x_{j} \mathbf{e}_{j}+x_{j+1} \mathbf{e}_{j+1}$ does also.

(b) Let $\mathbf{A}(\mathbf{x})$ be an affine mapping, that is, $\mathbf{A}(\mathbf{x})=\mathbf{x}_{0}+\mathbf{T}(\mathbf{x})$, where $\mathbf{T}(\mathbf{x})$ is a linear transformation, let $C$ be any convex set, and let $\mathbf{u} \in \mathbf{A}(C), \mathbf{v} \in \mathbf{A}(C)$. We need to show that $t \mathbf{u}+(1-t) \mathbf{v} \in \mathbf{A}(C)$ for all $t \in(0,1)$. But this is trivial, since if $\mathbf{u}=\mathbf{A}(\mathbf{x})$ and $\mathbf{v}=\mathbf{A}(\mathbf{y})$, then $t \mathbf{u}+(1-t) \mathbf{v}=\mathbf{A}(t \mathbf{x}+(1-t) \mathbf{y})$ and $t \mathbf{x}+(1-t) \mathbf{y} \in C$.

Exercise 10.8 Let $H$ be the parallelogram in $R^{2}$ whose vertices are $(1,1)$, $(3,2),(4,5),(2,4)$. Find the affine map $T$ which sends $(0,0)$ to $(1,1),(1,0)$ to $(3,2),(0,1)$ to $(2,4)$. Show that $J_{T}=5$. Use $T$ to convert the integral

$$
\alpha=\int_{H} e^{x-y} d x d y
$$

to an integral over $I^{2}$ and thus compute $\alpha$.

Solution: Clearly the constant term in an affine mapping is the image of $(0,0)$, which in the present case is to be $(1,1)$. Thus we are looking for a linear transformation $L$ such that $(3,2)=(1,1)+L(1,0)$ and $(2,4)=(1,1)+L(0,1)$,
which is to say $L(1,0)=(2,1)$ and $L(0,1)=(1,3)$. Obviously $L(x, y)=$ $(2 x+y, x+3 y)$. Then $J_{T}=2 \cdot 3-1 \cdot 1=5$. The inverse of $T$ is given by $T^{-1}(u, v)=L^{-1}((u, v)-(1,1))$. Simple algebra then reveals that

$$
T^{-1}(u, v)=\left(\frac{-2+3 u-v}{5}, \frac{-1-u+2 v}{5}\right)
$$

The parallelogram $H$ is the image of the unit square $S$ under $T$, and so

$$
\alpha=\int_{T(S)} e^{x-y} d x d y=\int_{S} e^{T^{-1}(u, v)}\left|J_{T^{-1}}\right| d u d v
$$

Thus

$$
\begin{aligned}
\alpha & =\int_{0}^{1} \int_{0}^{1} e^{\frac{-1+4 u-3 v}{5}} \frac{1}{5} d u d v \\
& =\frac{e^{-\frac{1}{5}}}{5} \int_{0}^{1} e^{\frac{4 u}{5}} d u \int_{0}^{1} e^{\frac{-3 v}{5}} d v \\
& =e^{-\frac{1}{5}} \cdot \frac{5}{4} \cdot\left(e^{\frac{4}{5}}-1\right) \cdot\left(\frac{-5}{3}\right) \cdot\left(e^{\frac{-3}{5}}-1\right)
\end{aligned}
$$

Exercise 10.9 Define $(x, y)=T(r, \theta)$ on the rectangle

$$
0 \leq r \leq a, \quad 0 \leq \theta \leq 2 \pi
$$

by the equations

$$
x=r \cos \theta, \quad y=r \sin \theta
$$

Show that $T$ maps this rectangle onto the closed disc $D$ with center at $(0,0)$ and radius $a$, that $T$ is one-to-one on the interior of the rectangle, and that $J_{T}(r, \theta)=r$. If $f \in \mathcal{C}(D)$, prove the formula for integration in polar coordinates:

$$
\int_{D} f(x, y) d x d y=\int_{0}^{a} \int_{0}^{2 \pi} f(T(r, \theta)) r d r d \theta
$$

Hint: Let $D_{0}$ be the interior of $D$, minus the interval from $(0,0)$ to $(0, a)$. As it stands, Theorem 10.9 applies to continuous functions whose support lies in $D_{0}$. To remove this restriction, proceed as in Example 10.4.

Solution: The simple geometry of this transformation allows a fairly straightforward proof. Let $\varepsilon \in(0, \min (\pi, a / 2))$. Let $H_{\varepsilon}=\{(r, \theta): \varepsilon \leq r \leq a-\varepsilon, \varepsilon \leq \theta \leq$ $2 \pi-\varepsilon\}$. The transformation $T$ is one-to-one on $H_{\varepsilon}$. Let $\varphi_{\varepsilon}(x, y)$ be a continuous function on all of $R^{2}$ such that $\varphi_{\varepsilon}(x, y)=1$ for $(x, y) \in T\left(H_{\varepsilon}\right), \varphi_{\varepsilon}(x, y)=0$ for $(x, y) \notin T\left(H_{\varepsilon / 2}\right)$ and $0 \leq \varphi_{\varepsilon}(x, y) \leq 1$. Define $f_{\varepsilon}(x, y)=f(x, y) \varphi_{\varepsilon}(x, y)$ for $(x, y) \in D$ and $f_{\varepsilon}(x, y)=0$ for $(x, y) \notin D$. Then $f_{\varepsilon}(x, y)=f(x, y)$ except for $(x, y) \in D \backslash T\left(H_{\varepsilon}\right)$. Hence $f_{\varepsilon}(T(x, y))=f(x, y)$ on $[0, a] \times[0,2 \pi] \backslash H_{\varepsilon}$. Let
$M$ be the maximum of $|f(x, y)|$ on $D$. Since the support of $f_{\varepsilon}$ is contained in $T\left(H_{\varepsilon / 2}\right)$, which in turn is contained in $D_{0}$, we certainly have

$$
\int_{R^{2}} f_{\varepsilon}(x, y) d x d y=\int_{R^{2}} f_{\varepsilon}(r \cos \theta, r \sin \theta) r d r d \theta
$$

We need to see how much each of these integrals differs from the corresponding integral of $f$. We first look at $f_{\varepsilon}(x, y)$. In evaluating its integral we can confine ourselves to the square $-a \leq x \leq a,-a \leq y \leq a$, since $D$ is contained in that square. We first exclude the three intervals $-a \leq y \leq-a+\varepsilon$, $\min (-\varepsilon,-a \sin \varepsilon) \leq y \leq \max (\varepsilon, a \sin \varepsilon)$, and $a-\varepsilon \leq y \leq a$. When $y$ is not in these intervals, we have $\varepsilon^{2} \leq y^{2} \leq(a-\varepsilon)^{2}$, and $f(x, y)$ and $f_{\varepsilon}(x, y)$ can differ only on the two intervals where $\sqrt{(a-\varepsilon)^{2}-y^{2}} \leq|x| \leq \sqrt{a^{2}-y^{2}}$, each of which has length

$$
\begin{aligned}
\sqrt{a^{2}-y^{2}}-\sqrt{(a-\varepsilon)^{2}-y^{2}} & =\frac{\left(a^{2}-y^{2}\right)-\left((a-\varepsilon)^{2}-y^{2}\right)}{\sqrt{a^{2}-y^{2}}+\sqrt{(a-\varepsilon)^{2}-y^{2}}} \\
& \leq \frac{2 a \varepsilon}{\sqrt{a^{2}-y^{2}}} \leq \frac{2 a \varepsilon}{\sqrt{2 a \varepsilon+\varepsilon^{2}}} \leq \frac{2 a}{\sqrt{2 a+\varepsilon}} \sqrt{\varepsilon} \leq \sqrt{2 a \varepsilon}
\end{aligned}
$$

Since the maximum possible difference between $f(x, y)$ and $f_{\varepsilon}(x, y)$ is $M$, we see that

$$
\left|\int f(x, y) d x-\int f_{\varepsilon}(x, y) d x\right| \leq 2 M \sqrt{2 a \varepsilon}
$$

if $y$ is not in one of the three excluded intervals.

If $y$ is in one of the three excluded intervals, since $f$ and $f_{\varepsilon}$ can differ by at most $M$, we have

$$
\left|\int f(x, y) d x-\int f_{\varepsilon}(x, y) d x\right| \leq 2 M a
$$

Since the total length of the excluded $y$-intervals is at most $(2 a+3) \varepsilon$, and the total length of the interval over which $y$ varies is at most $2 a$, we see that

$$
\left|\iint f(x, y) d x d y-\iint f_{\varepsilon}(x, y) d x d y\right| \leq 4 M a \sqrt{2 a \varepsilon}+2 M a(2 a+3) \varepsilon
$$

Thus this approximation can be made arbitrarily good by taking $\varepsilon$ sufficiently small.

As for the integral with respect to $r, \theta$, we observe that we can confine ourselves to the rectangle $0 \leq r \leq a, 0 \leq \theta \leq 2 \pi$, and that $f_{\varepsilon}(r \cos \theta, r \sin \theta)=$ $f(r \cos \theta, r \sin \theta)$ for $\varepsilon \leq \dot{r} \leq a-\varepsilon$ and $\varepsilon \leq \theta \leq 2 \pi-\varepsilon$. Thus, excluding the intervals $0 \leq \theta \leq \varepsilon$, and $2 \pi-\varepsilon \leq \theta \leq 2 \pi$, we find that for $\theta$ not in these intervals $f(r \cos \theta, r \sin \theta)$ and $f_{\varepsilon}(r \cos \theta, r \sin \theta)$ can differ (by at most $M$ ) only on the two intervals $0 \leq r \leq \varepsilon$ and $a-\varepsilon \leq r \leq a$. Hence as before, if $\theta$ is not in one of these two intervals, then

$$
\left|\int f(r \cos \theta, r \sin \theta) r d r-\int f_{\varepsilon}(r \cos \theta, r \sin \theta) r d r\right| \leq 2 M a \varepsilon
$$

On the other hand, if $\theta$ is in one of these two intervals, we have

$$
\left|\int f(r \cos \theta, r \sin \theta) r d r-\int f_{\varepsilon}(r \cos \theta, r \sin \theta) r d r\right| \leq M a^{2}
$$

Since the exceptional intervals have total length $2 \varepsilon$ and the total length of the $\theta$ interval is $2 \pi$, we see that

$$
\left|\iint f(r \cos \theta, r \sin \theta) r d r d \theta-\iint f_{\varepsilon}(r \cos \theta, r \sin \theta) r d r d \theta\right| \leq 4 \pi M a \varepsilon+2 M a^{2} \varepsilon
$$

Hence these two integrals also can be made arbitrarily close together by choosing $\varepsilon$ sufficiently small. Since the two integrals of $f_{\varepsilon}$ are equal for each $\varepsilon>0$, it follows that the other two are also equal.

Exercise 10.10 Let $a \rightarrow \infty$ in Exercise 9, and prove that

$$
\int_{R^{2}} f(x, y) d x d y=\int_{0}^{\infty} \int_{0}^{2 \pi} f(T(r, \theta)) r d \theta d r
$$

for continuous functions $f$ that decrease sufficiently rapidly as $|x|+|y| \rightarrow \infty$. (Find a more precise formulation.) Apply this to

$$
f(x, y)=\exp \left(-x^{2}-y^{2}\right)
$$

to derive formula (101) of Chap. 8 .

Solution: Without striving for ultimate generality, we shall assume that there are positive numbers $K$ and $\delta$ such that $|f(x, y)| \leq K\left(x^{2}+y^{2}\right)^{-1-\delta}$ for all $(x, y) \neq(0,0)$. (Such an estimate holds for $(x, y)$ ranging over any bounded set merely because $f(x, y)$ is continuous.) Let $D_{a}=\left\{(x, y): 0 \leq x^{2}+y^{2} \leq a^{2}\right\}$ and $S_{a}=\{(x, y):|x| \leq a,|y| \leq a\}$. Since both $D_{a}$ and $S_{a}$ are convex sets, the functions $g_{a}(x, y)=\chi_{D_{a}}(x, y) f(x, y)$ and $h_{a}(x, y)=\chi_{S_{a}}(x, y) f(x, y)$ are both integrable over $R^{2}$. We shall show that

$$
\lim _{a \rightarrow \infty} \int_{R^{2}} g_{a}(x, y) d x d y=\int_{R^{2}} f(x, y) d x d y=\lim _{a \rightarrow \infty} \int_{R^{2}} h_{a}(x, y) d x d y
$$

Our job is simpler if we first show that

$$
\lim _{a \rightarrow \infty}\left(\int_{R^{2}} g_{a}(x, y) d x d y-\int_{R^{2}} h_{a}(x, y) d x d y\right)=0
$$

As before, we let $M=\sup \{|f(x, y)|\}$. Since $g_{a}(x, y)=h_{a}(x, y)$ except for $(x, y) \in S_{a} \backslash D_{a}$, and on this set $g_{a}(x, y)=0$ and $\left|h_{a}(x, y)\right| \leq K a^{-2-2 \delta}$, the maximum possible difference in these two integrals is $4 \mathrm{Ka}^{-2 \delta}$, which does indeed tend to zero as $a \rightarrow \infty$.

It now suffices to show only the second of the two equalities given above, i.e., that

$$
\int_{R^{2}} f(x, y) d x d y=\lim _{a \rightarrow \infty} \int_{R^{2}} h_{a}(x, y) d x d y
$$

To that end, we fix $y$. We then have, if $|y| \geq a$, so that $h_{a}(x, y)=0$,

$$
\begin{aligned}
\int_{-\infty}^{\infty} f(x, y) & -h_{a}(x, y) d x \leq K \int_{-\infty}^{\infty} \frac{1}{\left(x^{2}+y^{2}\right)^{1+\delta}} d x \\
\leq & \int_{-\infty}^{-|y|} \frac{K}{\left(x^{2}\right)^{1+\delta}} d x+\int_{-|y|}^{|y|} \frac{K}{\left(y^{2}\right)^{1+\delta}} d x+\int_{|y|}^{\infty} \frac{K}{\left(x^{2}\right)^{1+\delta}} d x \\
& \leq \frac{2 K|y|^{-1-2 \delta}}{1+2 \delta}+2 K|y|^{-1-2 \delta} \leq 4 K|y|^{-1-2 \delta}
\end{aligned}
$$

If $|y| \leq a$, we note that $f(x, y)=h_{a}(x, y)$ for $-a \leq x \leq a$, and so

$$
\begin{aligned}
\int_{-\infty}^{\infty} f(x, y)-h_{a}(x, y) d x \leq \int_{-\infty}^{-a} \frac{K}{\left(x^{2}\right)^{1+\delta}} d x+\int_{a}^{\infty} & \frac{K}{\left(x^{2}\right)^{1+\delta}} d x \\
& \leq \frac{2 K a^{-1-2 \delta}}{1+2 \delta} \leq 2 K a^{-1-2 \delta} .
\end{aligned}
$$

Applying these two inequalities we find that

$$
\begin{aligned}
\left|\int_{R^{2}} f(x, y)-h_{a}(x, y) d x d y\right| \leq 4 K \int_{-\infty}^{-a}|y|^{-1-2 \delta} d y+4 K a^{-2 \delta}+ & \\
& +4 K \int_{a}^{\infty} y^{-1-2 \delta} d y \leq 4 K\left(1+\frac{1}{\delta}\right) a^{-2 \delta}
\end{aligned}
$$

The desired formula is now proved by merely remarking that

$$
\int_{R^{2}} h_{a}(x, y) d x d y=\int_{0}^{a} \int_{0}^{2 \pi} f(r \cos \theta, r \sin \theta) r d \theta d r
$$

The fact that the limit on the right-hand side exists as $a \rightarrow \infty$ follows from the fact that the limit on the left-hand side does, but can also be proved directly, since $|f(r \cos \theta, r \sin \theta) r| \leq K r^{-1-2 \delta}$.

Applying this formula with $f(x, y)=e^{-x^{2}-y^{2}}$, we find that

$$
\begin{array}{r}
\left(\int_{-\infty}^{\infty} e^{-t^{2}} d t\right)^{2}=\int_{-\infty} \infty \int_{-\infty}^{\infty} e^{-x^{2}} e^{-y^{2}} d x d y=\int_{0}^{\infty} \int_{0}^{2 \pi} e^{-r^{2}} r d \theta d r= \\
=2 \pi \int_{0}^{\infty} e^{-r^{2}} r d r=\pi \int_{0}^{\infty} e^{-u} d u=\pi
\end{array}
$$

In other words,

$$
\int_{-\infty}^{\infty} e^{-t^{2}} d t=\sqrt{\pi}
$$

Exercise 10.11 Define $(u, v)=T(s, t)$ on the strip

$$
0<s<\infty, \quad 0<t<1
$$

by setting $u=s-s t, v=s t$. Show that $T$ is a 1-1 mapping of the strip onto the positive quadrant $Q$ in $R^{2}$. Show that $J_{T}(s, t)=s$.

For $x>0, y>0$ integrate

$$
u^{x-1} e^{-u} v^{y-1} e^{-v}
$$

over $Q$, use Theorem 10.9 to convert the integral to one over the strip, and derive formula (96) of Chap. 8 in this way.

(For this application, Theorem 10.9 has to be extended so as to cover certain improper integrals. Provide this extension.)

Solution: It is easy to compute the inverse of $T$, namely

$$
s=u+v, \quad t=\frac{v}{u+v}
$$

and this inverse is defined on the entire $(u, v)$-plane with the line $v=-u$ removed. It is obvious that $v$ is positive if and only if $s$ and $t$ have the same sign, and that $u$ is positive if and only if $s$ and $1-t$ have the same sign.

Thus if $u$ and $v$ are both positive, then $t$ and $1-t$ have the same sign, which happens if and only if $0<t<1$. In this case $s$ must also be positive. Conversely, the equations that give $s$ and $t$ show that if $u$ and $v$ are both positive, then $s>0$ and $0<t<1$. The Jacobian matrix of $T$ is

$$
\left(\begin{array}{cc}
1-t & -s \\
t & s
\end{array}\right)
$$

so that $J_{T}(s, t)=s$.

The integral of $u^{x-1} e^{-u} v^{y-1} e^{-v}$ over the quadrant is

$$
\int_{0}^{\infty} u^{x-1} e^{-u} d u \int_{0}^{\infty} v^{y-1} e^{-v} d v=\Gamma(x) \Gamma(y)
$$

According to Theorem 10.9

$$
\int_{0}^{\infty} \int_{0}^{1} f(s-s t, s t) s d t d s=\int_{0}^{\infty} \int_{0}^{\infty} f(u, v) d u d v
$$

for any function $f(u, v)$ having compact support contained in the open quadrant. Assuming this theorem remains valid for the particular function we have in mind, we get

$$
\begin{aligned}
\Gamma(x) \Gamma(y)=\int_{0}^{\infty} s^{x+y-1} e^{-s} d s \int_{0}^{1} t^{y-1}(1-t)^{x-1} d t= & \\
= & =\Gamma(x+y) \int_{0}^{1} t^{x-1}(1-t)^{y-1} d t
\end{aligned}
$$

which is indeed formula (96) of Chapter 8.

Thus we need only justify the use of Theorem 10.9 with the function $f$ in the unbounded regions. To do this, we first show that Theorem 10.9 applies to the function $f(u, v) \varphi_{\delta}(u, v)$, where $\varphi_{\delta}(u, v)$ is the characteristic function of the set

$$
E_{\delta}=\left\{(u, v): \delta \leq u \leq \delta^{-1}, \delta \leq v \leq \delta^{-1}\right\}
$$

Since this function is positive on $E_{\delta}$, it is easy to modify it and make it into a continuous nonnegative function $f_{\eta}$ that vanishes outside the set $E_{\delta-\eta}$, for $\eta<\delta$ and this can be done without increasing its maximal value. Theorem 10.9 applies to $f_{\eta}$, and it is easy to see that the integral of $f_{\eta}$ on both sides of the formula tends to the integral of $f \varphi_{\delta}$ as $\eta \rightarrow \delta$. (Indeed, there is a constant $\varepsilon$ such that $T^{-1}(u, v)=(s, t)$ lies in the strip $\varepsilon \leq t \leq 1-\varepsilon$ whenever $(u, v) \in E_{\delta-\eta}$ and $\eta \leq \frac{\delta}{2}$. In that case, for each fixed $t$, the distance between the rightmost points $(s, t)$ in $T^{-1}\left(E_{\delta-\eta}\right)$ and in $T^{-1}\left(E_{\delta}\right)$ is at most $\frac{\delta-\eta}{\varepsilon}$. A similar statement applies to the leftmost points in the two regions, showing that the usual argument applies: The integrals of $f$ and $f \varphi_{\delta}$ over each horizontal line differ by at most $\frac{2 M(\delta-\eta)}{\varepsilon}$, except for a small range of $t$ whose length tends to zero with $\delta-\eta$, on which the difference is bounded. It then follows that both of the integrals of $f_{\eta}$ tend toward the corresponding integrals of $f \varphi_{\delta}$

It then remains only to prove that the integral of $f \varphi_{\delta}$ tends to the integral of $f$ on both sides of the formula. Since these integrals increase as $\delta$ decreases, there is no question that the limit exists, and we need only show that in both cases the limit is the integral in the formula. This is nearly immediate in the case of the integral over the quadrant. As for the integral over the strip, the set $T^{-1}\left(E_{\delta}\right)$ contains the region $\delta^{1 / 2} \leq s \leq \frac{1}{\delta-\delta^{3 / 2}}, \delta^{1 / 2} \leq t \leq 1-\delta^{1 / 2}$. For these inequalities imply that $\delta \leq s t \leq \frac{1}{\delta}$, and since $1-t$ satisfies the same inequalities as $t$, we also have $\delta \leq s(1-t) \leq \frac{1}{\delta}$. The integral of $f(s-s t, s t) s$ over the two strips $0 \leq t \leq \sqrt{\delta}$ and $1-\sqrt{\delta} \leq t \leq 1$ tends to zero with $\delta$, and for each $t$ with $\sqrt{\delta} \leq t \leq 1-\sqrt{\delta}$ the integral

$$
\int_{\sqrt{\delta}}^{1 /\left(\delta-\delta^{3 / 2}\right)} f(s-s t, s t) s d s
$$

differs from the integral from 0 to $\infty$ by less than

$$
t^{1-x}(1-t)^{1-y}\left(\int_{0}^{\sqrt{\delta}} s^{x+y-1} d s+\int_{1 /\left(\delta-\delta^{3 / 2}\right)}^{\infty} s^{x+y-1} e^{-s} d s\right)
$$

The first of these integrals is explicitly calculable and tends to zero as $\delta \rightarrow 0$. In the second we use the fact that $e^{-s}<\frac{n !}{s^{n}}$ for all $s>0$ and take $n \geq x+y+1$. It then follows that the integral of $f(s-s t ; s t) \varphi_{\delta}(s-s t, s t) s$ over each of these horizontal line differs from the integral of $f(s-s t, s t) s$ by an amount that tends to zero uniformly for $\sqrt{\delta} \leq t \leq 1-\sqrt{\delta}$.

The proof is now complete.

Exercise 10.12 Let $I^{k}$ be the set of all $\mathbf{u}=\left(u_{1}, \ldots, u_{k}\right) \in R^{k}$ with $0 \leq u_{i} \leq 1$ for all $i$; let $Q^{k}$ be the set of all $\mathrm{x}=\left(x_{1}, \ldots, x_{k}\right) \in R^{k}$ with $x_{i} \geq 0, \sum x_{k} \leq 1$. ( $I^{k}$ is the unit cube; $Q^{k}$ is the standard simplex in $R^{k}$.) Define $\mathbf{x}=T(\mathbf{u})$ by

$$
\begin{aligned}
& x_{1}=u_{1} \\
& x_{2}=\left(1-u_{1}\right) u_{2} \\
& \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\
& x_{k}=\left(1-u_{1}\right) \cdots\left(1-u_{k-1}\right) u_{k} .
\end{aligned}
$$

Show that

$$
\sum_{i=1}^{k} x_{i}=1-\prod_{i=1}^{k}\left(1-u_{i}\right)
$$

Show that $T$ maps $I^{k}$ onto $Q^{k}$, that $T$ is $1-1$ in the interior of $I^{k}$, and that its inverse $S$ is defined in the interior of $Q^{k}$ by $u_{1}=x_{1}$ and

$$
u_{i}=\frac{x_{i}}{1-x_{1}-\cdots-x_{i-1}}
$$

for $i=2, \ldots, k$. Show that

$$
J_{T}(\mathbf{u})=\left(1-u_{1}\right)^{k-1}\left(1-u_{2}\right)^{k-2} \cdots\left(1-u_{k-1}\right)
$$

and

$$
J_{S}(\mathbf{x})=\left[\left(1-x_{1}\right)\left(1-x_{1}-x_{2}\right) \cdots\left(1-x_{1}-\cdots-x_{k-1}\right)\right]^{-1}
$$

Solution: The first identity is easily proved by induction on $k$. It is obvious for $k=1$, and

$$
\begin{aligned}
\sum_{i=1}^{k+1} x_{i} & =x_{k+1}+\sum_{i=1}^{k} x_{k} \\
& =\left(1-u_{1}\right) \cdots\left(1-u_{k}\right) u_{k+1}+1-\left(1-u_{1}\right) \cdots\left(1-u_{k}\right) \\
& =1-\left(1-u_{1}\right) \cdots\left(1-u_{k}\right)\left(1-u_{k+1}\right)
\end{aligned}
$$

The defining formulas and the formula just proved show that $\mathrm{x} \in Q^{k}$ whenever $\mathbf{u} \in I^{k}$. In the process of showing that $T$ is onto, we shall prove the inverse formula. Let $\mathrm{x} \in Q^{k}$, and assume for the moment that $\sum_{i=1}^{k-1} x_{i}<1$. Then all of the equations given as inverse equations are defined. We need only show that the defining equations yield $\mathbf{x}$ when applied to the left-hand sides of these equations. Certainly we do have $x_{1}=u_{1}$. Suppose that $x_{r}=\left(1-u_{1}\right) \cdots\left(1-u_{r-1}\right) u_{r}$ for $r<j$. For the moment assume $u_{r} \neq 0$.

$$
\begin{aligned}
& \left(1-u_{1}\right) \cdots\left(1-u_{r-1}\right)\left(1-u_{r}\right) u_{r+1}=x_{r}\left(1-\frac{1}{u_{r}}\right) u_{r+1}= \\
& =x_{r} \cdot \frac{1-x_{1}-\cdots-x_{r}}{x_{r}} \cdot \frac{x_{r+1}}{1-x_{1}-\cdots-x_{r}}=x_{r+1} .
\end{aligned}
$$

If $u_{l} \neq 0$, but $u_{j}=0$ for $l<j \leq r$, then $x_{j}=0$ also for these values, and $u_{r+1}=\frac{x_{r+1}}{1-x_{1}-\cdots-x_{l}}$. We then have

$$
\begin{aligned}
x_{r+1} & =\left(1-u_{1}\right) \cdots\left(1-u_{l}\right) u_{r+1} \\
& =x_{l}\left(1-\frac{1}{u_{l}}\right) u_{r+1} \\
& =x_{l} \cdot \frac{1-x_{1}-\cdots-x_{l}}{x_{l}} \cdot \frac{x_{r+1}}{1-x_{1}-\cdots-x_{l}} \\
& =x_{r+1} .
\end{aligned}
$$

Finally, if $u_{1}=u_{2}=\cdots=u_{r}=0$, we have simply $u_{r+1}=x_{r+1}$ in both sets of equations. Thus in all cases the point $\mathbf{u} \in I^{k}$ is a preimage of the point $\mathrm{x} \in Q^{k}$.

It remains only to consider the case when $\sum_{i=1}^{r} x_{i}=1$ for some $r<k$. For these points $x_{r+1}=\cdots=x_{k}=0$.

To find preimages of these points, let $r$ be the first index for which $\sum_{i=1}^{r} x_{i}=1$. If $r=1$, we have $x_{2}=\cdots=x_{k}=0$, and this point is its own preimage. In general the preimage of the point $\mathbf{x}$ for which $x_{r+1}=\cdots=x_{k}=0$ is $\mathbf{u}$, where $u_{1}, \ldots, u_{r}$ are given by the formulas for $S$. The formulas imply $u_{r}=1$. The values of $u_{r+1}, \cdots, u_{k}$ are then arbitrary, since the formulas that define $T$ will automatically make the remaining $x_{i}$ equal to zero.

The Jacobian matrix is a triangular matrix whose diagonal consists of the entries $1,\left(1-u_{1}\right),\left(1-u_{1}\right)\left(1-u_{2}\right), \ldots,\left(1-u_{1}\right) \cdots\left(1-u_{k-1}\right)$, and this fact yields the formula for $J_{T}(\mathbf{u})$ immediately.

Likewise, the Jacobian of $S$ is triangular and has diagonal entries $1, \frac{1}{1-x_{1}}$, $\frac{1}{1-x_{1}-x_{2}}, \ldots, \frac{1}{1-x_{1}-x_{2} \ldots-x_{k-1}}$, from which again the formula for $J_{\mathcal{S}}(\mathbf{x})$ is immediate.

Exercise 10.13 Let $r_{1}, \ldots, r_{k}$ be nonnegative integers, and prove that

$$
\int_{Q^{k}} x_{1}^{r_{1}} \cdots x_{k}^{r_{k}} d x=\frac{r_{1} ! \cdots r_{k} !}{\left(k+r_{1}+\cdots+r_{k}\right) !}
$$

Hint: Use Exercise 12, Theorems 10.9 and 8.20. $1 / k !$.

Note that the special case $r_{1}=\cdots=r_{k}=0$ shows that the volume of $Q^{k}$ is Solution: Following the hint, we rewrite the integral in terms of $\mathbf{u}$, getting

$$
\begin{aligned}
\int_{I^{k}} u_{1}^{r_{1}} \cdots u_{k}^{r_{k}}\left(1-u_{1}\right)^{r_{2}+\cdots+r_{k}}\left(1-u_{2}\right)^{r_{3}+\cdots+r_{k}} \cdots & \\
& \left(1-u_{k-1}\right)^{r_{k}}\left(1-u_{1}\right)^{k-1}\left(1-u_{2}\right)^{k-2} \cdots\left(1-u_{k-1}\right) d u_{1} \cdots d u_{k}
\end{aligned}
$$

This integral is the product

$$
\prod_{i=1}^{k} \int_{0}^{1} u_{i}^{r_{i}}\left(1-u_{i}\right)^{k-i+r_{i+1} \cdots+r_{k}} d u_{i}
$$

which by formula (96) of Chapter 8 (just proved in Exercise 11 above) equals the product

$$
\prod_{i=1}^{k} \frac{\Gamma\left(r_{i}+1\right) \Gamma\left(k+1-i+r_{i+1} \cdots+r_{k}\right)}{\Gamma\left(k+2-i+r_{i}+r_{i+1} \cdots+r_{k}\right)}
$$

When this product is evaluated, the numerator $\Gamma\left(k+1-i+r_{i+1} \cdots+r_{k}\right)$ in each factor cancels the denominator $\Gamma\left(k+2-(i+1)+r_{i+1} \cdots+r_{k}\right)$ in the next factor. Thus the product "telescopes" to the product of the factors $\Gamma\left(r_{i}+1\right)$ in the numerators divided by the first denominator $\Gamma\left(k+1+r_{1}+\cdots+r_{k}\right)$. Considering that $\Gamma(n+1)=n$ ! for integers $n$, we therefore get the required formula.

Theoretically we ought to be worried about the fact that $T$ is not 1-1 on the entire cube $I^{k}$. This problem, however, is handled by the same reasoning used in Exercises 9, 10, and 11, and need not be repeated.

Exercise 10.14 Prove formula (46).

Solution: Formula (46) asserts that $\prod_{p<q} \operatorname{sgn}\left(j_{q}-j_{p}\right)$ is -1 if the permutation $j_{1}, \ldots, j_{k}$ is odd and 1 if the permutation is even. We observe that this product is $(-1)^{k}$, where $k$ is the number of pairs $\left(j_{p}, j_{q}\right)$ for which $j_{p}>j_{q}$. Since $\operatorname{sgn}\left(j_{q}-j_{p}\right)=1$ if $j_{p}<j_{q}$ and $\operatorname{sgn}\left(j_{q}-j_{p}\right)=-1$ if $j_{p}>j_{q}$, we need to show that the parity of $k$ is the same as the parity of the number of interchanges that will be used in converting this permutation to the identity. (As a corollary, that parity will be the same, no matter what particular sequence of interchanges is used to get to the identity.) This equality is obvious if the permutation is the identity to begin with. Suppose then that $j_{m}>j_{n}$ and $m<n$. The elements $j_{i}, m<i<n$ are of three kinds: Set $A$, those for which $j_{i}<j_{n}$; set $B$, those for which $j_{n}<j_{i}<j_{m}$; and set $C$, those for which $j_{m}<j_{i}$. Before $j_{m}$ and $j_{n}$ are interchanged, there is one out-of-order pair $\left(j_{m}, j_{i}\right)$ for each $j_{i} \in A$, one out-of-order pair $\left(j_{i}, j_{n}\right)$ for each $j_{i} \in C$, and two out-of-order pairs $\left(j_{m}, j_{i}\right)$ and $\left(j_{i}, j_{n}\right)$ for each $j_{i} \in B$. After the switch there is one out-of-order pair $\left(j_{n}, j_{i}\right)$ for each $j_{i} \in A$, and one pair $\left(j_{i}, j_{m}\right)$ for each $j_{i} \in C$. There are no pairs involving any $j_{i} \in B$. Hence, when an out-of-order pair $\left(j_{m}, j_{n}\right)$ is put in the right order by interchanging its elements, the number of out-of-order pairs decreases by $2|B|+1$, where $B$ is the set of elements $j_{i}$ between $j_{m}$ and $j_{n}$ that are in the wrong order relative to both $j_{m}$ and $j_{n}$ and $|B|$ is the number of elements in $B$.

Of course the number would increase by an odd number if we foolishly interchanged a pair that were not out-of-order relative to each other. (The number would increase by $2|B|+1$, where $|B|$ is the number of elements between them that were in the correct order relative to both elements of the interchanged pair.) In any case, each interchange of two elements changes the number of inversions (out-of-order pairs) by an odd number, so that an odd number of interchanges, starting from the identity, will result in an odd number of inversions, and an even number of interchanges will result in an even number of inversions.

Exercise 10.15 If $\omega$ and $\lambda$ are $k$ - and $m$-forms, respectively, prove that

$$
\omega \wedge \lambda=(-1)^{k m} \lambda \wedge \omega
$$

Solution: Because of the associative and distributive laws, it suffices to prove this in the case when $\omega=f d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}}$ and $\lambda=g d x_{i_{k+1}} \wedge \cdots \wedge d x_{i_{k+m}}$. In that case

$$
\omega \wedge \lambda=f g d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}} \wedge d x_{i_{k+1}} \wedge \cdots \wedge d x_{i_{k+m}}
$$

For each $j=1,2, \ldots, k$ exactly $m$ interchanges of adjacent basic one-forms will move $d x_{i_{k+1-j}}$ to the position just right of $d x_{i_{k+m}}$, if these moves are made in increasing order of $j$. Thus a total of $k m$ interchanges will exactly reverse $\lambda$ and $\omega$. The result now follows from the alternating property of the wedge product on basis elements.

Exercise 10.16 If $k \geq 2$ and $\sigma=\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \ldots, \mathbf{p}_{k}\right]$ is an oriented affine $k$ simplex, prove that $\partial^{2} \sigma=0$, directly from the definition of the boundary operator $\partial$. Deduce from this that $\partial^{2} \Psi=0$ for every chain $\Psi$.

Hint: For orientation, do it first for $k=2, k=3$. In general, if $i<j$, let $\sigma_{i j}$ be the $(k-2)$-simplex obtained by deleting $\mathbf{p}_{i}$ and $\mathbf{p}_{j}$ from $\sigma$. Show that each $\sigma_{i j}$ occurs twice in $\partial^{2} \sigma$ with opposite sign.

Solution. For $k=2$ we have

$$
\partial \sigma=\left[\mathbf{p}_{1}, \mathbf{p}_{2}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{2}\right]+\left[\mathbf{p}_{0}, \mathbf{p}_{1}\right]
$$

so that

$$
\partial^{2} \sigma=\left(\mathbf{p}_{2}-\mathbf{p}_{1}\right)-\left(\mathbf{p}_{2}-\mathbf{p}_{0}\right)+\left(\mathbf{p}_{1}-\mathbf{p}_{0}\right)=0
$$

For $k=3$ we have

$$
\partial \sigma=\left[\mathbf{p}_{1}, \mathbf{p}_{2}, \mathbf{p}_{3}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{2}, \mathbf{p}_{3}\right]+\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \mathbf{p}_{3}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{1}, \mathbf{p}_{2}\right]
$$

so that

$$
\begin{aligned}
\partial^{2} \sigma= & \left(\left[\mathbf{p}_{2}, \mathbf{p}_{3}\right]-\left[\mathbf{p}_{1}, \mathbf{p}_{3}\right]+\left[\mathbf{p}_{1}, \mathbf{p}_{2}\right]\right) \\
& -\left(\left[\mathbf{p}_{2}, \mathbf{p}_{3}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{3}\right]+\left[\mathbf{p}_{0}, \mathbf{p}_{2}\right]\right) \\
& +\left(\left[\mathbf{p}_{1}, \mathbf{p}_{3}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{3}\right]+\left[\mathbf{p}_{0}, \mathbf{p}_{1}\right]\right) \\
& -\left(\left[\mathbf{p}_{1}, \mathbf{p}_{2}\right]-\left[\mathbf{p}_{0}, \mathbf{p}_{2}\right]+\left[\mathbf{p}_{0}, \mathbf{p}_{1}\right]\right) \\
= & 0 .
\end{aligned}
$$

In general the order in which $\mathbf{p}_{i}$ and $\mathbf{p}_{j}$ are omitted from $\sigma$ determines the sign that $\sigma_{i j}$ will have. If $\mathbf{p}_{j}$ is omitted first, the resulting $(k-1)$-simplex $\sigma_{j}=$ $\left[\mathbf{p}_{0}, \ldots, \mathbf{p}_{j-1}, \mathbf{p}_{j+1}, \ldots, \mathbf{p}_{k}\right]$ will acquire the sign $(-1)^{j}$. If $\mathbf{p}_{i}$ is then omitted, the resulting $(k-2)$-simplex will acquire a factor of $(-1)^{i}$, resulting in $(-1)^{i+j} \sigma_{i j}$.

However, if $\mathbf{p}_{i}$ is omitted first, $\mathbf{p}_{j}$ will move forward one position in the resulting $(k-1)$-simplex $\sigma_{i}$, and when it is subsequently omitted, a factor of
$(-1)^{j-1}$ will be affixed, resulting in $(-1)^{i+j-1} \sigma_{i j}$. Hence the two occurrences of $\sigma_{i j}$ in the second boundary will cancel each other.

The linearity of the boundary operator, operating on a base of simplexes, then shows that $\partial^{2}$ is the zero operator on all chains.

Exercise 10.17 Put $J^{2}=\tau_{1}+\tau_{2}$, where

$$
\tau_{1}=\left[\mathbf{0}, \mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right], \quad \tau_{2}=-\left[\mathbf{0}, \mathbf{e}_{2}, \mathbf{e}_{2}+\mathbf{e}_{1}\right]
$$

Explain why it is reasonable to call $J^{2}$ the positively oriented unit square in $R^{2}$. Show that $\partial J^{2}$ is the sum of 4 oriented affine simplexes. Find these. What is $\partial\left(\tau_{1}-\tau_{2}\right)$ ?

Solution: Although $J^{2}$ is really a collection of two affine mappings, the ranges of these mappings cover the unit square, the diagonal from $(0,0)$ to $(1,1)$ being covered twice with opposite orientations in the two mappings. In both cases, the sense of orientation is such that the cross product of the last two vertices of the simplex is $\mathbf{e}_{3}$, which is a reasonable definition of the positive orientation on the unit square.

By routine computation,

$$
\begin{aligned}
\partial J^{2}= & \left(\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]\right. \\
& \left.-\left[0, \mathbf{e}_{1}+\mathbf{e}_{2}\right]+\left[0, \mathbf{e}_{1}\right]\right)-\left(\left[\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]-\left[0, \mathbf{e}_{1}+\mathbf{e}_{2}\right]+\left[0, \mathbf{e}_{2}\right]\right) \\
= & {\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]+\left[\mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{2}\right]+\left[\mathbf{e}_{2}, \mathbf{0}\right]+\left[0, \mathbf{e}_{1}\right] . }
\end{aligned}
$$

Again, by routine computation,

$$
\begin{aligned}
\partial\left(\tau_{1}-\tau_{2}\right)= & \left(\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]-\left[\mathbf{0}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]+\left[\mathbf{0}, \mathbf{e}_{1}\right]\right) \\
& \quad+\left(\left[\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]-\left[0, \mathbf{e}_{1}+\mathbf{e}_{2}\right]+\left[\mathbf{0}, \mathbf{e}_{2}\right]\right) \\
= & {\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]-\left[\mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{2}\right]-\left[\mathbf{e}_{2}, \mathbf{0}\right]+\left[\mathbf{0}, \mathbf{e}_{1}\right]-2\left[0, \mathbf{e}_{1}+\mathbf{e}_{2}\right] }
\end{aligned}
$$

Exercise 10.18 Consider the oriented affine 3-simplex

$$
\sigma_{1}=\left[\mathbf{0}, \mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]
$$

in $R^{3}$. Show that $\sigma_{1}$ (regarded as a linear transformation) has determinant 1 . Thus $\sigma_{1}$ is positively oriented.

Let $\sigma_{2}, \ldots, \sigma_{6}$ be five other oriented simplexes, obtained as follows: There are five permutations $\left(i_{1}, i_{2}, i_{3}\right)$ of $(1,2,3)$ distinct from $(1,2,3)$. Associate with each $\left(i_{1}, i_{2}, i_{3}\right)$ the simplex

$$
s\left(i_{1}, i_{2}, i_{3}\right)\left[\mathbf{0}, \mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]
$$

where $s$ is the sign that occurs in the definition of the determinant. (This is how $\tau_{2}$ was obtained from $\tau_{1}$ in Exercise 17.)

Show that $\sigma_{2}, \ldots, \sigma_{6}$ are positively oriented.

Put $J^{3}=\sigma_{1}+\cdots+\sigma_{6}$. Then $J^{3}$ may be called the positively oriented unit cube in $R^{3}$.

Show that $\partial J^{3}$ is the sum of 12 oriented affine 2 -simplexes. (These 12 triangles cover the surface of the unit cube $I_{3}$.)

Show that $\mathrm{x}=\left(x_{1}, x_{2}, x_{3}\right)$ is in the range of $\sigma_{1}$ if and only if $0 \leq x_{3} \leq x_{2} \leq$ $x_{1} \leq 1$.

Show that the ranges of $\sigma_{1}, \ldots, \sigma_{6}$ have disjoint interiors, and that their union covers $I^{3}$. (Compare with Exercise 13 ; note that $3 !=6$.)

Solution. We first show that each of these simplexes is positively oriented. To that end, it is convenient to refer to the simplex $\left[0, \mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]$ corresponding to the permutation $\left(i_{1}, i_{2}, i_{3}\right)$ as $\sigma^{\left(i_{1}, i_{2}, i_{3}\right)}$.

The simplex $\sigma^{\left(i_{1}, i_{2}, i_{3}\right)}$, regarded as a linear transformation, maps $(x, y, z)$ to $(x+y+z) \mathrm{e}_{i_{1}}+(y+z) \mathrm{e}_{i_{2}}+z \mathrm{e}_{i_{3}}$. Its matrix therefore has $\left(\begin{array}{lll}1 & 1 & 1\end{array}\right)$ as row $i_{1},\left(\begin{array}{lll}0 & 1 & 1\end{array}\right)$ as row $i_{2}$, and $\left(\begin{array}{lll}0 & 0 & 1\end{array}\right)$ as row $i_{3}$. By interchanging rows in correspondence with the interchanges needed to convert the permutation $\left(i_{1}, i_{2}, i_{3}\right)$ to the identity, we can convert this matrix to an upper-triangular matrix with l's on the main diagonal. The determinant of the matrix is therefore $s\left(i_{1}, i_{2}, i_{3}\right)$, so that the simplex $s\left(i_{1}, i_{2}, i_{3}\right)\left[\mathbf{0}, \mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]$ is positively oriented.

The boundary of $\sigma^{\left(i_{1}, i_{2}, i_{3}\right)}$ consists of four terms, two of which $\left(\left[\mathrm{e}_{i_{1}}, \mathrm{e}_{i_{1}}+\right.\right.$ $\left.\mathbf{e}_{i_{2}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]$ and $\left.-\left[0, \mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}\right]\right)$ are not shared with any other $\sigma^{(i)}$. The other two terms $\left(-\left[0, \mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]\right.$ and $\left[0, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]$ are shared with $\sigma^{\left(i_{1}, i_{3}, i_{2}\right)}$ and $\sigma^{\left(i_{2}, i_{1}, i_{3}\right)}$ respectively. As these two permutations each differ from $\left(i_{1}, i_{2}, i_{3}\right)$ by a single interchange, the sign of each of these terms will be opposite in its two occurrences, and hence they will cancel out. Thus the boundary of $J^{3}$ will consist of a total of 12 oriented affine 2-simplexes.

A point $\mathrm{x}=\left(x_{1}, x_{2}, x_{3}\right)$ is in the range of $\sigma_{1}$ if and only if there are numbers $r, s, t \in[0,1]$ such that $r+s+t \leq 1$ and $\mathbf{x}=r \mathbf{e}_{1}+s\left(\mathbf{e}_{1}+\mathbf{e}_{2}\right)+t\left(\mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right)$, that is, $x_{1}=r+s+t, x_{2}=s+t$, and $x_{3}=t$. If such numbers $r, s, t$ exist, obviously $0 \leq x_{3} \leq x_{2} \leq x_{1} \leq 1$. Conversely, if these conditions hold, there will be such numbers $r, s, t$, namely $t=x_{3}, s=x_{2}-x_{3}$, and $r=x_{1}-x_{2}$.

The interior of the range of $\sigma^{\left(i_{1}, i_{2}, i_{3}\right)}$ is the set of all $\mathrm{x}=\left(x_{1}, x_{2}, x_{3}\right)$ such that $0<x_{i_{3}}<x_{i_{2}}<x_{i_{1}}<1$. For the range of this simplex is the set of $\mathbf{x}$ for which each of these inequalities or the corresponding equality holds. If equality holds in any of them, the point can be approached by points outside the range, as one can easily see. That the union covers $I^{3}$ is also obvious. Indeed, the characterization of the range of $\sigma_{1}$ applies to all $\sigma^{\left(i_{1}, i_{2}, i_{3}\right)}$ and shows that this range is contained in $I^{3}$. Thus we need only show the reverse inclusion.

If $\mathbf{x}=\left(x_{1}, x_{2}, x_{3}\right) \in I^{3}$, let $i_{1}$ be the smallest subscript $i$ for which $x_{i}=$ $\max \left\{x_{1}, x_{2}, x_{3}\right\}$. Let $i_{2}$ be the first subscript for which $x_{i}=\max \left(\left\{x_{1}, x_{2}, x_{3}\right\} \backslash\right.$ $\left.\left\{x_{i_{1}}\right\}\right)$. Finally, let $i_{3}$ be such that $\left\{x_{i_{3}}\right\}=\left\{x_{1}, x_{2}, x_{3}\right\} \backslash\left\{x_{i_{1}}, x_{i_{2}}\right\}$. By construction $0 \leq x_{i_{3}} \leq x_{i_{2}} \leq x_{i_{1}} \leq 1$, and so, by the argument given above, $\mathbf{x}$ belongs to the range of $\sigma^{\left(i_{1}, i_{2}, i_{3}\right)}$. Symmetry shows that all of these simplexes have the same volume, which must therefore be $1 / 6$. (Remember that we showed back in Exercise 1 that the boundary of a convex set in $R^{k}$ has $k$-dimensional volume 0 ,
so that the volume of each of these sets equals the volume of its interior. As the interiors are disjoint, the sum of their volumes is at most 1. Since the simplexes together cover $I^{3}$, the sum of their volumes is at least 1 . Therefore it is exactly 1.)

Exercise 10.19 Let $J^{2}$ and $J^{3}$ be as in Exercise 17 and 18. Define

$$
\begin{aligned}
& B_{01}(u, v)=(0, u, v), \quad B_{11}(u, v)=(1, u, v) \\
& B_{02}(u, v)=(u, 0, v), \quad B_{12}(u, v)=(u, 1, v) \\
& B_{03}(u, v)=(u, v, 0), \quad B_{13}(u, v)=(u, v, 1) .
\end{aligned}
$$

These are affine and map $R^{2}$ into $R^{3}$.

Put $\beta_{r i}=B_{r i}\left(J^{2}\right)$, for $r=0,1, i=1,2,3$. Each $\beta_{r i}$ is an affine-oriented 2-chain. (See Sec. 10.30.) Verify that

$$
\partial J^{3}=\sum_{i=1}^{3}(-1)^{i}\left(\beta_{0 i}-\beta_{1 i}\right)
$$

in agreement with Exercise 18.

Solution. Although we did not spell it out in our solution of Exercise 18, the boundary of $J^{3}$ is the 2 -chain

$$
\sum_{i_{1}, i_{2}, i_{3}} s\left(i_{1}, i_{2}, i_{3}\right)\left(\left[\mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}+\mathbf{e}_{i_{3}}\right]-\left[\mathbf{0}, \mathbf{e}_{i_{1}}, \mathbf{e}_{i_{1}}+\mathbf{e}_{i_{2}}\right]\right)
$$

This sum can be rearranged as a sum of three terms, each of which consists of four terms. For example, the terms in the sum for which $i_{=} 1$ can be written as

$$
\begin{aligned}
& \left(\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]\right. \\
& \left.\quad-\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{3}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]\right)-\left(\left[0, \mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]-\left[0, \mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{3}\right]\right)
\end{aligned}
$$

For $i_{1}=2$ we get a similar set of four terms, namely,

$$
\begin{aligned}
\left(-\left[\mathbf{e}_{2}, \mathbf{e}_{2}+\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]+\left[\mathbf{e}_{2}, \mathbf{e}_{2}\right.\right. & \left.\left.+\mathbf{e}_{3}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]\right) \\
& +\left(\left[0, \mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}\right]-\left[0, \mathbf{e}_{2}, \mathbf{e}_{2}+\mathbf{e}_{3}\right]\right)
\end{aligned}
$$

Finally, for $i_{1}=3$ we have

$$
\begin{aligned}
\left(\left[\mathbf{e}_{3}, \mathbf{e}_{3}+\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]-\left[\mathbf{e}_{3}, \mathbf{e}_{3}\right.\right. & \left.\left.+\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]\right) \\
& -\left(\left[0, \mathbf{e}_{3}, \mathbf{e}_{3}+\mathbf{e}_{1}\right]-\left[0, \mathbf{e}_{3}, \mathbf{e}_{3}+\mathbf{e}_{2}\right]\right)
\end{aligned}
$$

Now consider the 2-chain $\beta_{01}$. According to the notation of Eq. (88), it is $B_{01}\left(\tau_{1}\right)+B_{01}\left(\tau_{2}\right)$. Letting $(u, v)=\tau_{1}(x, y)=(x+y) \mathbf{e}_{1}+y \mathbf{e}_{2}$, and then $(u, v)=\tau_{2}(x, y)=(x+y) \mathbf{e}_{2}+y \mathbf{e}_{1}$ (and keeping in mind the orientation assigned to $\left.\tau_{2}\right)$, we see that $\beta_{01}(x, y)=B_{01}(x+y, y)-B_{01}(y, x+y)=(0, x+y, y)-$
$(0, y, x+y)=\left[0, \mathbf{e}_{2}, \mathbf{e}_{2}+\mathbf{e}_{3}\right]-\left[0, \mathbf{e}_{3}, \mathbf{e}_{3}+\mathbf{e}_{2}\right]$. Notice that these two terms occur in the expression for $\partial J^{3}$, in the groupings for $i_{1}=2$ and $i_{1}=3$ respectively, but each occurs with the opposite sign. Hence these terms can be accounted for in $\partial J^{3}$ by being grouped together and written as $-\beta_{01}$. Similarly when we look at $\beta_{11}$, we find that it is the 2 -chain whose points are $(1, x+y, y)-(1, y, x+y)$, which is $\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{2}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]-\left[\mathbf{e}_{1}, \mathbf{e}_{1}+\mathbf{e}_{3}, \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}\right]$. Again these terms occur in the expression for $e_{1}$, this time with exactly the same signs, so that they can be accounted for by grouping them and writing them as the term $\beta_{11}$. Thus four of the twelve simplexes in $\partial J^{3}$ are accounted for by the expression $(-1)^{1}\left(\beta_{01}-\beta_{11}\right)$. The other 8 simplexes are accounted for similarly.

Exercise 10.20 State conditions under which the formula

$$
\int_{\Phi} f d \omega=\int_{\partial \Phi} f \omega-\int_{\Phi}(d f) \wedge \omega
$$

is valid, and show that it generalizes the formula for integration by parts.

Hint: $d(f \omega)=(d f) \wedge \omega+f d \omega$.

Solution. Given the formula in the hint, we need only invoke Stokes' Theorem. For any chain $\Phi$ satisfying the hypotheses of that theorem we shall have

$$
\int_{\Phi} d(f \omega)=\int_{\partial \Phi} f \omega
$$

which is precisely the given theorem. The ordinary formula for integration by parts follows by considering a 0 -form $f g$.

Exercise 10.21 As in Example 10.36, consider the 1-form

$$
\eta=\frac{x d y-y d x}{x^{2}+y^{2}}
$$

in $R^{2} \backslash\{0\}$. (a) Carry out the computation that leads to formula (113), and prove that
$d \eta=0$.

(b) Let $\gamma(t)=(r \cos t, r \sin t)$, for some $r>0$, and let $\Gamma$ be a $\mathcal{C}^{\prime \prime}$-curve in $R^{2} \backslash\{\boldsymbol{0}\}$, with parameter interval $[0,2 \pi]$, with $\Gamma(0)=\Gamma(2 \pi)$, such that the intervals $[\gamma(t), \Gamma(t)]$ do not contain 0 for any $t \in[0,2 \pi]$. Prove that

$$
\int_{\Gamma} \eta=2 \pi
$$

Hint: For $0 \leq t \leq 2 \pi, 0 \leq u \leq 1$, define

$$
\Phi(t, u)=(1-u) \Gamma(t)+u \gamma(t)
$$

then $\Phi$ is a 2-surface in $R^{2} \backslash\{0\}$ whose parameter domain is the indicated rectangle. Because of cancellations (as in Example 10.32),

$$
\partial \Phi=\Gamma-\gamma
$$

Use Stokes" theorem to deduce that

$$
\int_{\Gamma} \eta=\int_{\dot{\gamma}} \eta
$$

because $d \eta=0$.

(c) Take $\Gamma(t)=(a \cos t, b \sin t)$ where $a>0, b>0$ are fixed. Use part $(b)$ to show that

$$
\int_{0}^{2 \pi} \frac{a b}{a^{2} \cos ^{2} t+b^{2} \sin ^{2} t} d t=2 \pi
$$

(d) Show that

$$
\eta=d\left(\arctan \frac{y}{x}\right)
$$

in any convex open set in which $x \neq 0$, and that

$$
\eta=d\left(-\arctan \frac{x}{y}\right)
$$

in any convex open set in which $y \neq 0$.

Explain why this justifies the notation $\eta=d \theta$, in spite of the fact that $\eta$ is not exact in $R^{2} \backslash\{\mathbf{0}\}$.

(e) Show that $(b)$ can be derived from $(d)$.

(f) If $\Gamma$ is any closed $\mathcal{C}^{\prime}$-curve in $R^{2} \backslash\{0\}$, prove that

$$
\frac{1}{2 \pi} \int_{\Gamma} \eta=\operatorname{Ind}(\Gamma)
$$

(See Exercise 23 of Chap. 8 for the definition of the index of a curve.)

Solution. (a) By the rules for computing line integrals, given that $x=r \cos t$ and $y=r \sin t$,

$$
\int_{\gamma} \eta=\int_{0}^{2 \pi} \frac{(r \cos t)(r \cos t) d t-(r \sin t)(-r \sin t) d t}{r^{2} \cos ^{2} t+r^{2} \sin ^{2} t}=\int_{0}^{2 \pi} d t=2 \pi
$$

(b) Let $\Gamma(t)=(X(t), Y(t))$ and $\gamma(t)=(x(t), y(t))$. Following the hint, observing that the hypothesis that the interval from $\Gamma(t)$ to $\gamma(t)$ does not pass through 0 , we find that $\Phi(t, u)$ is indeed a 2-surface in $R^{2} \backslash\{0\}$; and making it into a singular 2-chain by regarding the domain as an affine 2-chain, as in Exercise 17, we find by Stokes' theorem that

$$
\begin{aligned}
0 & =\int_{\Phi} d \eta=\int_{\partial \Phi} \eta \\
& =-\int_{\Gamma} \eta+\int_{\gamma} \eta+\int_{\delta} \eta-\int_{\varepsilon} \eta
\end{aligned}
$$

where $\delta$ is the curve $\delta(u)=\Phi(2 \pi, u)=(1-u) \Gamma(2 \pi)+u \gamma(2 \pi)$. and $\varepsilon$ is the curve $\varepsilon(u)=\Phi(0, u)=(1-u) \Gamma(0)+u \gamma(0)$. Since $\delta$ and $\varepsilon$ are the same curve, the last two terms in this expression cancel each other, yielding the required result.

(c) We need only verify that $\Phi(t, u) \neq \mathbf{0}=(0,0)$. But this is clear: If $((1-$ $u) a+u r) \cos t=0$, then $t=\frac{\pi}{2}$ or $t=\frac{3 \pi}{2}$, since $(1-u) a+u r \geq \min (a, r)>0$. But this means that $((1-u) b+u r) \sin t \neq 0$, since $t$ is not a multiple of $\pi$. The result now follows.

(d) It is a routine computation that the differential of $\arctan \frac{y}{x}$ is $\eta$ in the entire right or left half-plane, and similarly for $\pi-\arctan \frac{x}{y}$, which is after all arccot $\frac{x}{y}$, which in turn is $\arctan \frac{y}{x}$ wherever both functions are defined. Thus locally we have $\eta=d \theta$, even though $\theta$ is not defined globally in $R^{2} \backslash\{0\}$.

(e) Break the integral over $\gamma$ into five parts: $0 \leq t \leq \frac{\pi}{4}, \frac{\pi}{4} \leq t \leq \frac{3 \pi}{4}, \frac{3 \pi}{4} \leq$ $t \leq \frac{5 \pi}{4}, \frac{5 \pi}{4} \leq t \leq \frac{7 \pi}{4}, \frac{7 \pi}{4} \leq t \leq 2 \pi$. In the first, third, and fifth parts we have $\eta=d\left(\arctan \frac{y}{x}\right)$, and in the second and fourth we have $\eta=d\left(-\arctan \frac{x}{y}\right)$. Now in the first, third, and fifth parts, $\frac{y}{x}=\frac{\sin t}{\cos t}=\tan t$, so that either $t=\arctan \frac{y}{x}$ or $t=\pi+\arctan \frac{y}{x}$ on these arcs. In either case the integral over the these parts is just the difference in $t$ at the endpoints. Hence these three integrals contribute $\frac{\pi}{4}+\frac{\pi}{2}+\frac{\pi}{4}=\pi$ to the integral. On the other parts $\frac{x}{y}=\cot t=\tan \left(\frac{\pi}{2}-t\right)$. Hence, once again, $\arctan \frac{x}{y}$ is either $\frac{\pi}{2}-t$ or $\frac{3 \pi}{2}-t$. In either case, these two integrals contribute $\frac{\pi}{2}+\frac{\pi}{2}=\pi$ to the integral, and provides the result of $(b)$.

$(f)$ The definition of Ind $(\Gamma)$ is defined by regarding $\Gamma(t)$ as a curve $X(t)+Y(t) i$ in the complex plane, in which case

$$
\text { Ind }(\Gamma)=\frac{1}{2 \pi i} \int_{0}^{2 \pi} \frac{\Gamma^{\prime}(t)}{\Gamma(t)} d t=\frac{1}{2 \pi i} \int_{0}^{2 \pi} \frac{(X(t)-Y(t) i)\left(X^{\prime}(t)+Y^{\prime}(t) i\right)}{\left.(X(t))^{2}+Y(t)\right)^{2}} d t
$$

Since we know the imaginary part is zero, we consider only the real part, which
is

$$
\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{X(t) Y^{\prime}(t)-Y(t) X^{\prime}(t)}{\left.(X(t))^{2}+Y(t)\right)^{2}} d t=\frac{1}{2 \pi} \int_{\Gamma} \eta
$$

(Incidentally, it follows from Stokes' theorem that the imaginary part of this complex integral is zero, since it is

$$
-\frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{X(t) X^{\prime}(t)+Y(t) Y^{\prime}(t)}{(X(t))^{2}+(Y(t))^{2}} d t=-\frac{1}{4 \pi} \int_{\Gamma} d \zeta=-\frac{1}{4 \pi} \int_{\partial \Gamma} \zeta
$$

where $\zeta(x, y)=\ln \left(x^{2}+y^{2}\right)$. This last integral is zero, since $\Gamma$ is a closed curve.)

Exercise 10.22 As in Example 10.37, define $\zeta$ in $R^{3}-0$ by

$$
\zeta=\frac{x d y \wedge d z+y d z \wedge d x+z d x \wedge d y}{r^{3}}
$$

where $r=\left(x^{2}+y^{2}+z^{2}\right)^{1 / 2}$, let $D$ be the rectangle given by $0 \leq u \leq \pi$, $0 \leq v \leq 2 \pi$, and let $\Sigma$ be the 2 -surface in $R^{3}$, with parameter domain $D$, given by

$$
x=\sin u \cos v, \quad y=\sin u \sin v, \quad z=\cos u
$$

(a) Prove that $d \zeta=0$ in $R^{3} \backslash \mathbf{0}$.

(b) Let $S$ denote the restriction of $\Sigma$ to a parameter domain $E \subset D$. Prove that

$$
\int_{S} \zeta=\int_{E} \sin u d u d v=A(S)
$$

where $A$ denotes area, as in Sec. 10.43. Note that this contains (115) as a special case.

(c) Suppose $g, h_{1}, h_{2}, h_{3}$, are $\mathcal{C}^{\prime \prime}$-functions on $[0,1], g>0$. Let $(x, y, z)=\Phi(s, t)$ define a 2 -surface $\Phi$, with parameter domain $I^{2}$, by

$$
x=g(t) h_{1}(s), \quad y=g(t) h_{2}(s), \quad z=g(t) h_{3}(s)
$$

Prove that

$$
\int_{\Phi} \zeta=0
$$

directly from (35).

Note the shape of the range of $\Phi:$ For fixed $s, \Phi(s, t)$ runs over an interval on a line through $\mathbf{0}$. The range of $\Phi$ thus lies in a "cone" with vertex at the origin.

(d) Let $E$ be a closed rectangle in $D$, with edges parallel to those of $D$. Suppose $f \in \mathcal{C}^{\prime \prime}(D), f>0$. Let $\Omega$ be the 2-surface with parameter domain $E$, defined by

$$
\Omega(u, v)=f(u, v) \Sigma(u, v)
$$

Define $S$ as in (b) and prove that

$$
\int_{\Omega} \zeta=\int_{S} \zeta=A(S)
$$

(Since $S$ is the "radial projection" of $\Omega$ onto the unit sphere, this result makes it reasonable to call $\int_{\Omega} \zeta$ the "solid angle" subtended by the range of $\Omega$ at the origin.)

Hint: Consider the 3-surface $\Psi$ given by

$$
\Psi(t, u, v)=[1-t+t f(u, v)] \Sigma(u, v)
$$

where $(u, v) \in E, 0 \leq t \leq 1$. For fixed $v$, the mapping $(t, u) \rightarrow \Psi(t, u, v)$ is a 2 -surface $\Phi$ to which $(c)$ can be applied to show that $\int_{\Phi} \zeta=0$. The same thing holds when $u$ is fixed. By $(a)$ and Stokes' theorem,

$$
\int_{\partial \Psi} \zeta=\int_{\Psi} d \zeta=0
$$

(e) Put $\lambda=-(z / r) \eta$, where

$$
\eta=\frac{x d y-y d x}{x^{2}+y^{2}}
$$

as in Exercise 21. Then $\lambda$ is a 1-form in the open set $V \subset R^{3}$ in which $x^{2}+y^{2}>0$. Show that $\zeta$ is exact in $V$ by showing that

$$
\zeta=d \lambda .
$$

$(f)$ Derive $(d)$ from $(e)$, without using $(c)$.

Hint: To begin with, assume $0<u<\pi$ on $E$. By $(e)$,

$$
\int_{\Omega} \zeta=\int_{\partial \Omega} \lambda \text { and } \int_{S} \zeta=\int_{\partial S} \lambda
$$

Show that the two integrals of $\lambda$ are equal, by using part $(d)$ of Exercise 21 , and by noting that $z / r$ is the same at $\Sigma(u, v)$ as at $\Omega(u, v)$.

(g) Is $\zeta$ exact in the complement of every line through the origin?

Solution. (a) We note that, since $\frac{\partial r}{\partial x}=x r^{-1}$, we have

$$
\frac{\partial}{\partial x} \frac{x}{r^{3}}=r^{-3}-3 x^{2} r^{-5}=r^{-5}\left(r^{2}-3 x^{2}\right) .
$$

By symmetry we have analogous relations for the partial derivatives of $y r^{-3}$ and $z r^{-3}$ with respect to $y$ and $z$ respectively. Since $d x \wedge d y \wedge d z=d y \wedge d z \wedge d x=$ $d z \wedge d x \wedge d y$, we find that

$$
d \zeta=r^{-5}\left(r^{2}-3 x^{2}+r^{2}-3 y^{2}+r^{2}-3 z^{2}\right) d x \wedge d y \wedge d z=0
$$

(b) Since $r(\Sigma(u, v))=1$, we have only to note that the differentials pull back to $D$ as $d y \wedge d z=\frac{\partial(y, z)}{\partial(u, v)} d u \wedge d v=\sin ^{2} u \cos v d u \wedge d v, d z \wedge d x=\sin ^{2} u \sin v d u \wedge d v$ and $d x \wedge d y=\sin u \cos u d u \wedge d v$. The integrand then pulls back as $\left(\sin ^{3} u+\right.$ $\left.\sin u \cos ^{2} u\right) d u \wedge d v=\sin u d u \wedge d v$. The reference to Sec. 10.43 must be a misprint for Sec. 10.46 .

(c) For the application to be made in part (d) below we actually need to allow the function $g(t)$ to depend on $s$ also. Thus we consider $g(s, t)$ instead of $g(t)$. Using only the definition (35) for the integral, we need to get the pullbacks of the wedge products to the parameter domain $[0,1] \times[0,1]$. Since $d x=\frac{\partial g}{\partial t} h_{1}(s) d t+\left(g(s, t) h_{1}^{\prime}(s)+h_{1}(s) \frac{\partial g}{\partial s} d s\right.$, with similar expressions for $d y$ and $d z$, we find that $d y \wedge d z=g(s, t) \frac{\partial g}{\partial t}\left(h_{3}(s) h_{2}^{\prime}(s)-h_{3}^{\prime}(s) h_{2}(s)\right) d s \wedge d t, d z \wedge d x=$ $g(s, t) \frac{\partial g}{\partial t}\left(h_{3}^{\prime}(s) h_{1}(s)-h_{3}(s) h_{1}^{\prime}(s)\right) d s \wedge d t$, and $d x \wedge d y=g(s, t) \frac{\partial g}{\partial t}\left(h_{1}^{\prime}(s) h_{2}(s)-\right.$ $\left.h_{1}(s) h_{2}^{\prime}(s)\right) d s \wedge d t$. Thus, assuming $h_{1}(t), h_{2}(t)$, and $h_{3}(t)$ do not vanish simultaneously, we have

$$
\int_{\Phi} \zeta=\int_{0}^{1} \int_{0}^{1} \frac{\frac{\partial g}{\partial t}}{g(s, t)} \frac{\left(h_{1}(s) h_{2}(s) h_{3}(s)\right)^{\prime}-\left(h_{1}(s) h_{2}(s) h_{3}(s)\right)^{\prime}}{\left(h_{1}(s)\right)^{1}+\left(h_{2}(s)\right)^{2}+\left(h_{3}(s)\right)^{2}} d s d t=0
$$

(d) Using part $(c)$, as amended, we note that $\partial \Psi$ consists of six mappings $\Psi(1, u, v)=\Omega(u, v), \Psi(0, u)=,S(u, v), \Psi(t, b, v), \Psi(t, a, v), \Psi(t, u, d)$, and $\Psi(t, u, c)$, whete $E=[a, b] \times[c, d]$. By part $(c)$ the integrals over each of the last 4 surfaces are all zero. Since $d \zeta=0$, Stokes' theorem implies that

$$
\int_{\Omega} \zeta-\int_{S} \zeta=0
$$

(e) By straightforward computation,

$$
\begin{aligned}
d \lambda & =-d(z / r) \wedge \eta-(z / r) d \eta \\
& =\frac{x z d x+y z d y+\left(z^{2}-r^{2}\right) d z}{r^{3}} \wedge \eta \\
& =\frac{\left(x^{2} z+y^{2} z\right) d x \wedge d y}{r^{3}\left(x^{2}+y^{2}\right)}-\frac{x d z \wedge d y-y d z \wedge d x}{r^{3}} \\
& =\zeta
\end{aligned}
$$

$(f)$ Again by Stokes' theorem we must have

$$
\int_{\Omega} \zeta=\int_{\Omega} d \lambda=\int_{\partial \Omega} \lambda
$$

But $\eta$ is independent of $z$, and $z / r$ is the same for both $S(u, v)$ and $\Omega(u, v)$. Therefore

$$
\int_{\partial \Omega} \lambda=\int_{\partial S} \lambda
$$

$(g)$ Yes, $\zeta$ is exact on the complement of every line through the origin. Indeed, for every line through the origin there is a rotation $T$ that maps that line to the $z$-axis. By Theorem 10.22, part $(c)$ we have $d\left(\lambda_{T}\right)=(d \lambda)_{T}=\zeta_{T}$. However, $\zeta_{T}=\zeta$, as one can easily compute. Indeed, since $r$ is invariant under $T$, we need only show that $x d y \wedge d z+y d z \wedge d x+z d x \wedge d y$ is rotation-invariant. To that end, suppose $(u, v, w)=T(x, y, z)$, say $u=t_{11} x+t_{12} y+t_{13} z$, so that $d u=t_{11} d x+t_{12} d y+t_{13} d z$, etc. We then have $d v \wedge d w=\left(t_{22} t_{33}-\right.$ $\left.t_{32} t_{23}\right) d y \wedge d z+\left(t_{23} t_{31}-t_{33} t_{21}\right) d z \wedge d x+\left(t_{21} t_{32}-t_{31} t_{22}\right) d x \wedge d y$, etc. and so $u d v \wedge d w+v d w \wedge d u+w d u \wedge d v$ works out (after tedious computation) to precisely $x d y \wedge d z+y d z \wedge d x+z d x \wedge d y$.

Exercise 10.23 Fix $n$. Define $r_{k}=\left(x_{1}^{2}+\cdots+x_{k}^{2}\right)^{1 / 2}$ for $1 \leq k \leq n$, let $E_{k}$ be the set of all $\mathrm{x} \in R^{n}$ at which $r_{k}>0$, and let $\omega_{k}$ be the $(k-1)$-form defined in $E_{k}$ by

$$
\omega_{k}=\left(r_{k}\right)^{-k} \sum_{i=1}^{k}(-1)^{i-1} x_{i} d x_{1} \wedge \cdots \wedge d x_{i-1} \wedge d x_{i+1} \wedge \cdots \wedge d x_{k}
$$

that

Note that $\omega_{2}=\eta, \omega_{3}=\zeta$ in the notation of Exercises 21 and 22. Note also

$$
E_{1} \subset E_{2} \subset \cdots \subset E_{n}=R^{n} \backslash\{\mathbf{0}\}
$$

(a) Prove that $d \omega_{k}=0$ in $E_{k}$.

(b) For $k=2, \ldots, n$, prove that $\omega_{k}$ is exact in $E_{k-1}$, by showing that

$$
\omega_{k}=d\left(f_{k} \omega_{k-1}\right)=\left(d f_{k}\right) \wedge \omega_{k-1}
$$

where $f_{k}(\mathbf{x})=(-1)^{k} g_{k}\left(x_{k} / r_{k}\right)$ and

$$
g_{k}(t)=\int_{-1}^{t}(1-s)^{(k-3) / 2} d s \quad(-1<t<1)
$$

Hint: $f_{k}$ satisfies the differential equations

$$
\mathbf{x} \cdot\left(\nabla f_{k}\right)(\mathbf{x})=0
$$

and

(c) Is $\omega_{n}$ exact in $E_{n}$ ?

$$
\left(D_{k} f_{k}\right)(\mathbf{x})=\frac{(-1)^{k}\left(r_{k-1}\right)^{k-1}}{\left(r_{k}\right)^{k}}
$$

$(d)$ Note that $(b)$ is a generalization of part (e) of Exercise 22. Try to extend some of the other assertions of Exercises 21 and 22 to $\omega_{n}$, for arbitrary $n$.

Solution. (a) Computation shows that $d\left(\sum_{i=1}^{k}(-1)^{i-1} x_{i} d x_{1} \wedge \cdots \wedge d x_{i-1} \wedge d x_{i+1} \wedge\right.$ $\left.\cdots \wedge d x_{k}\right)=k d x_{1} \wedge \cdots \wedge d x_{k}$, and $\frac{\partial r_{k}}{\partial x_{j}}=\frac{x_{j}}{r_{k}}$ for $j \leq k$, so that $d\left(r_{k}\right)=$ $-k\left(r_{k}\right)^{-k-2} \sum_{j=1}^{k} x_{j} d x_{j}$, we find that

$$
\begin{aligned}
d \omega_{k}=k\left(r_{k}\right)^{-k} d x_{1} \wedge \cdots \wedge d x_{k}-k\left(r_{k}\right)^{-k-2} \sum_{j=1}^{k} x_{j}^{2} d x_{1} \wedge \cdots \wedge d x_{k}= \\
=k\left(r_{k}\right)^{-k-2}\left(r_{k}^{2}-\sum_{j=1}^{k} x_{j}^{2}\right) d x_{1} \wedge \cdots \wedge d x_{k}=0
\end{aligned}
$$

This argument shows, incidentally, that $d \omega_{k}=0$ in $E_{n}=R^{n} \backslash\{\mathbf{0}\}$.

(b) We compute that

$$
\begin{aligned}
d f_{k} & =(-1)^{k}\left(1-x_{k}^{2} / r_{k}^{2}\right)^{(k-3) / 2}\left(\left(r_{k}^{-1}-x_{k}^{2} r_{k}^{-3}\right) d x_{k}-\sum_{i=1}^{k-1} x_{k} x_{i} r_{k}^{-3} d x_{i}\right) \\
& =(-1)^{k}\left(r_{k-1} / r_{k}\right)^{k-3}\left(\left(r_{k}^{-3} r_{k-1}^{2}\right) d x_{k}-r_{k}^{-3} \sum_{i=1}^{k-1} x_{i} x_{k}\right) d x_{i} \\
& =(-1)^{k}\left(r_{k}\right)^{-k}\left(r_{k-1}^{k-1} d x_{k}-r_{k-1}^{k-3} \sum_{i=1}^{k-1} x_{i} x_{k} d x_{i}\right) .
\end{aligned}
$$

Hence, since $\left(d f_{k}\right) \wedge \omega_{k-1}=(-1)^{k-2} \omega_{k-1} \wedge\left(d f_{k}\right)$, the first term in this last expression contributes

$$
\left(r_{k}\right)^{-k} \sum_{i=1}^{k-1}(-1)^{i-1} x_{i} d x_{i} \wedge \cdots \wedge d x_{i-1} \wedge d x_{i+1} \wedge \cdots \wedge d x_{k-1} \wedge d x_{k}
$$

to the wedge product. As this contribution is all of $\omega_{k}$ except the last term $r_{k}^{-k}(-1)^{k-1} x_{k} d x_{1} \wedge \cdots \wedge d x_{k-1}$, we must endeavor to show that the contribution of the remaining terms amounts to this expression. Since any term containing a repeated factor $d x_{j}$ is zero, we see that the rest of the expression is

$$
\begin{aligned}
(-1)^{k-1} x_{k}\left(r_{k}\right)^{-k}\left(r_{k-1}\right)^{k-3}\left(\sum_{i=1}^{k-1} x_{i} d x_{i}\right) \wedge\left(r_{k-1}\right)^{k-1} \times & \\
& \times \sum_{i=1}^{k-1}(-1)^{i-1} x_{i} d x_{1} \wedge \cdots \wedge d x_{i-1} \wedge d x_{i+1} \wedge \cdots \wedge d x_{k-1}
\end{aligned}
$$

which is easily seen to be the same as

$$
(-1)^{k-1} r_{k}^{-k} x_{k} r_{k-1}^{-2} \sum_{i=1}^{k-1} x_{i}^{2} d x_{1} \wedge \cdots \wedge d x_{k-1}=(-1)^{k-1} r_{k}^{-k} x_{k} d x_{1} \wedge \cdots \wedge d x_{k-1}
$$

exactly as required. Thus we have computed this result by "brute force," arrogantly ignoring the hint.

For the benefit of those who wish to use the hint, here is an alternative approach. The wedge product $\left(d f_{k}\right) \wedge \omega_{k-1}$ is the sum of $D_{k} f_{k}(\mathbf{x}) d x_{k} \wedge \omega_{k-1}$ and

$$
\begin{aligned}
r_{k-1}^{-k-1}\left(\sum_{i=1}^{k-1} x_{i} D_{i} f(\mathbf{x})\right) & d x_{1} \wedge \cdots \wedge d x_{k}= \\
= & r_{k-1}^{-k-1}\left(\mathbf{x} \cdot\left(\nabla f_{k}\right)(\mathbf{x})-x_{k} D_{k} f_{k}(\mathbf{x})\right) d x_{1} \wedge \cdots \wedge d x_{k}
\end{aligned}
$$

and hence, by the first differential equation, equals

$$
D_{k} f_{k}(\mathbf{x}) d x_{k} \wedge \omega_{k-1}-r_{k-1}^{-k-1} x_{k} D_{k} f_{k}(\mathbf{x}) d x_{1} \wedge \cdots d x_{k-1}
$$

so that the second equation yields the result immediately. The two differential equations themselves are routine computations.

(c) No, $\omega_{n}$ is not exact in $E_{n}$ for any $n$, since its integral over the $(n-1)$-sphere equals $\frac{2 \pi^{n / 2}}{\Gamma\left(\frac{n}{2}\right)}$, as will be shown below in the answer to part $(d)$. (If it were exact, say the differential of $\lambda$, this integral would equal thte integral of $\lambda$ over the boundary of the $(n-1)$-sphere, which is the $0(n-2)$-chain.)

(d) We can parameterize the $(n-1)$-sphere $\Sigma^{n-1}$ by the mapping $T_{n}$ defined by

$$
x_{1}=\sin t_{1} \sin t_{2} \cdots \sin t_{n-1},
$$

$$
\begin{aligned}
x_{2} & =\cos t_{1} \sin t_{2} \cdots \sin t_{n-1} \\
x_{3} & =\cos t_{2} \sin t_{3} \cdots \sin t_{n-1} \\
\cdots & \cdots \cdots \\
x_{n-1} & =\cos t_{n-2} \sin t_{n-1} \\
x_{n} & =\cos t_{n-1}
\end{aligned}
$$

where $0 \leq t_{1} \leq 2 \pi$ and $0 \leq t_{j} \leq \pi$ for $2 \leq j \leq n-1$ That is, the domain of $T_{n}$ is the parallelepiped $D=[0,2 \pi] \times[0, \pi]^{n-2}$. This is known to be true for $n=2$ and $n=3$, and follows easily by induction on $n$. Suppose, for example, we know it is true for $n-1$, and suppose $x_{1}^{2}+\cdots+x_{n}^{2}=1$. If $x_{n}= \pm 1$, we can take $t_{n-1}=0$ or $\pi$, and the values of the other angles can be anything. If $-1<x_{n}<1$, there is precisely one angle $t_{n-1} \in(0, \pi)$ such that $x_{n}=\cos t_{n-1}$. But then the point $\left(x_{1} / \sin t_{n-1}, \ldots, x_{n-1} / \sin t_{n-1}\right)$ belongs to $\Sigma^{n-2}$, and hence, by induction, can be written as

$$
\begin{aligned}
x_{1} / \sin t_{n-1} & =\sin t_{1} \cdots \sin t_{n-2}, \\
x_{2} / \sin t_{n-1} & =\cos t_{1} \cdots \sin t_{n-2}, \\
\cdots & \cdots \\
x_{n-2} / \sin t_{n-1} & =\cos t_{n-3} \sin t_{n-2} \\
x_{n-1} / \sin t_{n-1} & =\cos t_{n-2} .
\end{aligned}
$$

This completes the induction. Observe that the angle $t_{1}$ requires the entire range $[0,2 \pi]$. That is, all points on the unit circle in $R^{2}$ can be written as $(\cos t, \sin t)$ only if $t$ is allowed to range from 0 to $2 \pi$. Otherwise put, the $(n-1)$-sphere is parameterized by $n-2$ latitude angles and one longitude angle.

We can easily show by induction that the pullback of $\omega_{n}$ is

$$
\left(\omega_{n}\right)_{T_{n}}=(-1)^{n-1} \sin t_{2} \sin ^{2} t_{3} \cdots \sin ^{n-2} t_{n-1} d t_{1} \wedge \cdots \wedge d t_{n-1}
$$

To make the induction work, we need to distinguish the $x_{i}$ 's in various numbers of dimensions; hence let the transformation $T_{n}$ be defined by giving its components $x_{i}^{(n)}, i \leq n$, by the equations

$$
\begin{aligned}
x_{1}^{(n)} & =\sin t_{1} \sin t_{2} \cdots \sin t_{n-1} \\
x_{2}^{(n)} & =\cos t_{1} \sin t_{2} \cdots \sin t_{n-1} \\
x_{3}^{(n)} & =\cos t_{2} \sin t_{3} \cdots \sin t_{n-1}, \\
\cdots & \cdots \cdots \\
x_{n-1}^{(n)} & =\cos t_{n-2} \sin t_{n-1}, \\
x_{n}^{(n)} & =\cos t_{n-1},
\end{aligned}
$$

Thus we have $x_{n}^{n}=\cos t_{n-1}$ and $x_{j}^{(n)}=x_{j}^{(n-1)} \sin t_{n-1}$ for $j<n$. Suppose we have proved that

$$
\left(\sum_{i=1}^{n-1}(-1)^{i-1} x_{i}^{(n-1)} d x_{1}^{(n-1)} \wedge \cdots \wedge d x_{i-1}^{(n-1)} \wedge d x_{i+1}^{(n-1)} \wedge \cdots \wedge d x_{n-1}^{(n-1)}\right)_{T_{n-1}}=
$$

$$
=(-1)^{n-2}\left(\sin t_{1} \sin ^{2} t_{2} \cdots \sin ^{n-3} t_{n-2} d t_{1} \wedge \therefore \wedge d t_{n-2}\right.
$$

We observe that the Jacobian matrix of the transformation $T_{n}$ is the $n \times$ $(n-1)$ matrix

$$
\frac{\partial\left(x_{1}^{(n)}, \ldots, x_{n}^{(n)}\right)}{\partial\left(t_{1}, \ldots, t_{n-1}\right)}=\left(\begin{array}{cccc}
\frac{\partial x_{1}^{(n-1)}}{\partial t_{1}} \sin t_{n-1} & \cdots & \frac{\partial x_{1}^{(n-1)}}{\partial t_{n-2}} \sin t_{n-1} & x_{1}^{(n-1)} \cos t_{n-1} \\
\frac{\partial x_{2}^{(n-1)}}{\partial t_{1}} \sin t_{n-1} & \ldots & \frac{\partial x_{2}^{(n-1)}}{\partial t_{n-2}} \sin t_{n-1} & x_{2}^{(n-1)} \cos t_{n-1} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
\frac{\partial x_{n-1}^{(n-1)}}{\partial t_{1}} \sin t_{n-1} & \ldots & \frac{\partial x_{n-1}^{(n-1)}}{\partial t_{n-2}} \sin t_{n-1} & x_{n-1}^{(n-1)} \cos t_{n-1} \\
0 & \ldots & 0 & -\sin t_{n-1}
\end{array}\right)
$$

It follows immediately, when we expand the determinant of the first $n-1$ rows along the last column, that

$$
\begin{aligned}
& \left(d x_{1}^{(n)} \wedge \cdots \wedge d x_{n-1}^{(n)}\right)_{T_{n}}=\sin ^{n-2} t_{n-1} \cos t_{n-1} \times \\
& \times \sum_{i=1}^{n-1}(-1)^{n-1+i} x_{i}^{(n-1)} \frac{\partial\left(x_{1}^{(n-1)}, \ldots, x_{n-1}^{(n-1)}\right)}{\partial\left(t_{1}, \ldots, t_{n-2}\right)} d t_{1} \wedge \cdots \wedge d t_{n-1} r= \\
& =(-1)^{n} \sin ^{n-2} t_{n-1} \cos t_{n-1}\left(\sum_{i=1}^{n-1}(-1)^{i-1} x_{i}^{(n-1)} d x_{1}^{(n-1)} \wedge \cdots\right. \\
& \left.\quad \cdots \wedge d x_{i-1}^{(n-1)} \wedge d x_{i+1}^{(n-1)} \wedge \cdots \wedge d x_{n-1}^{(n-1)}\right)_{T_{n-1}} \wedge d t_{n-1} \\
& \quad=\sin ^{n-2} t_{n-1} \cos t_{n-1}\left(\sin t_{2} \sin ^{2} t_{3} \cdots \sin ^{n-3} t_{n-2}\right) d t_{1} \wedge \cdots \wedge d t_{n-1} .
\end{aligned}
$$

Hence

$$
\begin{aligned}
& (-1)^{n-1}\left(x_{n}^{(n)} d x_{1}^{(n)} \wedge \cdots \wedge d x_{n-1}^{(n)}\right)_{T_{n}}= \\
& \left.\quad=(-1)^{n-1} \cos ^{2} t_{n-1} \sin t_{2} \sin ^{2} t_{3} \cdots \sin ^{n-2} t_{n-1}\right) d t_{1} \wedge \cdots \wedge d t_{n-1}
\end{aligned}
$$

Next, omitting row $i(i<n)$ and expanding the resulting determinant along the last row, we find that

$$
\begin{aligned}
& \left(d x_{1}^{(n)} \wedge \cdots \wedge d x_{i-1}^{(n)} \wedge d x_{i+1}^{(n)} \wedge \cdots \wedge d x_{n}^{(n)}\right)_{T_{n}}= \\
= & -\sin ^{n-1} t_{n-1}\left(d x_{1}^{(n-1)} \wedge \cdots \wedge d x_{i-1}^{(n-1)} \wedge d x_{i+1}^{(n-1)} \wedge \cdots \wedge d x_{n-1}^{(n-1)}\right)_{T_{n-1}} \wedge d t_{n-1}
\end{aligned}
$$

so that

$$
\begin{aligned}
&\left(\sum_{i=1}^{n-1}(-1)^{i-1} x_{i}^{(n)} d x_{1}^{(n)} \wedge \cdots \wedge d x_{i-1}^{n} \wedge d x_{i+1}^{(n)} \wedge \cdots \wedge d x_{n}^{(n)}\right)_{T_{n}}= \\
&=-\sin ^{n} t_{n-1}\left(\sum_{i=1}^{n-1}(-1)^{i-1} x_{i}^{(n-1)} d x_{1}^{(n-1)} \wedge \cdots \wedge d x_{i-1}^{(n-1)} \wedge d x_{i+1}^{(n-1)} \wedge \cdots\right. \\
&\left.\cdots \wedge d x_{n-1}^{(n-1)}\right)_{T_{n-1}} \wedge d t_{n-1}
\end{aligned}
$$

and again by induction this is

$$
(-1)^{n-1} \sin ^{2} t_{n-1}\left(\sin t_{2} \sin ^{2} t_{3} \cdots \sin ^{n-2} t_{n-1}\right) d t_{1} \cdots d t_{n-1}
$$

Combining these results we find that

$$
\begin{aligned}
\left(\sum_{i=1}^{n}(-1)^{i-1} x_{i}^{(n)} d x_{1}^{(n)}\right. & \left.\wedge \cdots \wedge d x_{i-1}^{(n)} \wedge d x_{i+1}^{(n)} \wedge \cdots \wedge d x_{n}^{n)}\right)_{T_{n}}= \\
= & (-1)^{n-1} \sin t_{2} \sin ^{2} t_{3} \cdots \sin ^{n-2} t_{n-1} d t_{1} \wedge \cdots \wedge d t_{n-1}
\end{aligned}
$$

The induction is now complete.

Except for the unimportant factor of -1 , this formula gives results consistent with the known results for the area of the $(n-1)$-sphere, namely a total area of

$$
A_{n-1}=\frac{2 \pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}\right)}
$$

This is easily verified for $n=2$ and $n=3$. In general

$$
\begin{aligned}
& A_{n-1}=A_{n-2} \int_{0}^{\pi} \sin ^{n-2} t_{n-1} d t_{n-1}= \\
& 2 A_{n-2} \int_{0}^{\frac{\pi}{2}} \sin ^{n-2} s d s=2 A_{n-2} \frac{\Gamma\left(\frac{1}{2}\right) \Gamma\left(\frac{n-1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)} .
\end{aligned}
$$

It easily follows, since $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$, that the formula for the surface area of $\Sigma^{(n-1)}$ is valid for all $n$.

Similarly we can show the analog of part (c) of Exercise 22, namely that

$$
\int_{\Phi} \omega_{n}=0
$$

for any $(n-1)$-dimensional surface given by a mapping of the form

$$
\begin{aligned}
\Phi\left(s_{1}, \ldots, \dot{s}_{n-2}, t\right)=\left(g\left(s_{1}, \ldots, s_{n-2}, t\right) h_{1}\left(s_{1}, \ldots, s_{n-2}\right), \ldots\right. \\
\left.\ldots, g\left(s_{1}, \ldots, s_{n-2}, t\right) h_{n}\left(s_{1}, \ldots, s_{n-2}\right)\right)
\end{aligned}
$$

Indeed, the pullback of $\omega_{n}$ is

$$
\begin{aligned}
& \left(\omega_{n}\right)_{\Phi}=
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2023_11_05_13727d40d8f1718df899g-569.jpg?height=184&width=1434&top_left_y=2201&top_left_x=427)

But this determinant is zero, since the first and last columns are proportional.

We can now prove that if $f\left(t_{1}, \ldots, t_{n-1}\right)>0$ and

$$
\Omega\left(t_{1}, \ldots, t_{n-1}\right)=f\left(t_{1}, \ldots, t_{n-1}\right) \Sigma^{(n-1)}\left(t_{1}, \ldots, t_{n-1}\right)
$$

then

$$
\int_{\Omega} \omega_{n}=\int_{S} \omega_{n}= \pm A_{n-1}(S)
$$

To do so, we consider the $n$-surface in $R^{n}$ given by

$$
\Psi\left(t_{1}, \ldots, t_{n-1}, t\right)=\left[1-t+t f\left(t_{1}, \ldots, t_{n-1}\right)\right] \Sigma^{(n-1)}\left(t_{1}, \ldots, t_{n-1}\right)
$$

for $0 \leq t \leq 1$ and $t_{1}, \ldots, t_{n}$ ranging over a parallelepiped contained in the interior of $D$ with boundary faces parallel to those of $D$. For each fixed $t_{j}$, this $\Psi$ is an $(n-1)$-surface of the form just considered, and hence the integral of $\omega_{n}$ over it is zero. This applies in particular to the faces of the closed parallelepiped $E$. Since $\int_{\partial \Psi} \omega_{n}=\int_{\Psi} d \omega_{n}=0$, it then follows that, up to a factor of \pm 1 ,

$$
\int_{\Omega} \omega_{n}=\int_{S} \omega_{n}=A_{n-1}(S)
$$

Finally, as in Exercise $22, \omega_{n}$ is exact in the complement of every $(n-2)$ hyperplane through the origin, since there is a rotation that maps the complement of that hyperplane to $E_{n-1}$, while $\omega_{n}$ is rotation-invariant.

Exercise 10.24 Let $\omega=\sigma a_{i}(\mathbf{x}) d x_{i}$ be a 1-form of class $\mathcal{C}^{\prime \prime}$ in a convex open set $E \subset R^{n}$. Assume $d \omega=0$ and prove that $\omega$ is exact in $E$ by completing the
following outline: Fix $\mathbf{p} \in E$. Define

$$
f(\mathbf{x})=\int_{[\mathbf{p}, \mathbf{x}]} \omega \quad(\mathrm{x} \in E)
$$

Apply Stokes'.theorem to affine-oriented 2-simplexes $[\mathbf{p}, \mathbf{x}, \mathbf{y}]$ in $E$. Deduce that

$$
f(\mathbf{y})-f(\mathbf{x})=\sum_{i=1}^{n}\left(y_{i}-x_{i}\right) \int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t
$$

for $\mathrm{x} \in E, \mathrm{y} \in E$. Hence $D_{i} f(\mathbf{x})=a_{i}(\mathbf{x})$.

Solution. Because $d \omega=0$, the integral of $\omega$ over the boundary of the oriented 2 -simplex $[\mathbf{p}, \mathbf{x}, \mathbf{y}]$ is zero. That is

$$
\int_{[\mathbf{x}, \mathbf{y}]} \omega-\int_{[\mathbf{p}, \mathbf{y}]} \omega+\int_{[\mathbf{p}, \mathbf{x}]} \omega=0
$$

which can be rewritten as

$$
f(\mathbf{y})-f(\mathbf{x})=\int_{[\mathbf{x}, \mathbf{y}]} \omega=\sum_{i=1}^{n}\left(y_{i}-x_{i}\right) \int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t
$$

Differentiating with respect to $y_{i}$, we find

$$
D_{i} f(\mathbf{y})=\int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t+\sum_{j=1}^{n}\left(y_{j}-x_{j}\right) \int_{0}^{1} t D_{i} a_{j}((1-t) \mathbf{x}+t \mathbf{y}) d t
$$

The fact that $d \omega=0$ says that $D_{i} a_{j}=D_{j} a_{i}$, so that we have

$$
\begin{aligned}
D_{i} f(\mathbf{y})= & \int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t+\int_{0}^{1} \sum_{j=1}^{n} t\left(y_{j}-x_{j}\right) D_{j} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t \\
= & \int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t+\int_{0}^{1} t \frac{d}{d t} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t \\
= & \int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t+\left.t a_{i}((1-t) \mathbf{x}+t \mathbf{y})\right|_{0} ^{1} \\
& \quad-\int_{0}^{1} a_{i}((1-t) \mathbf{x}+t \mathbf{y}) d t \\
= & a_{i}(\mathbf{y}) .
\end{aligned}
$$

Thus $\omega=d f$.

Exercise 10.25 Assume that $\omega$ is a 1-form in an open set $E \subset R^{n}$ such that

$$
\int_{\gamma} \omega=0
$$

for every closed curve $\gamma$ in $E$ of class $\mathcal{C}^{\prime}$. Prove that $\omega$ is exact in $E$, by imitating part of the argument sketched in Exercise 24.

Solution. We first observe that Stokes' theorem and the argument of Theorem 10.15 show that $d \omega=0$ in $E$. (Theorem 10.15 actually shows that if some component of $d \omega$ is nonzero at some point of $E$, then there is a 2-surface $\Phi$ in $E$ whose domain is a 2-cell in $R^{2}$ for which $\int_{\Phi} d \omega \neq 0$. Then by Stokes' theorem, $\int_{\partial \Phi} \omega \neq 0$ also, contradicting the assumption of the problem.

In each connected component $E_{\alpha}$ of $E$, we choose a fixed point $\mathbf{x}_{\alpha}$. There is a ball of some positive radius $r_{\alpha}$ centered at $x_{\alpha}$ and contained in $E$. Let this ball be $B_{\alpha}$. Exercise 24 shows that there is a function $f(\mathbf{x})$ such that $\omega(\mathbf{x})=d f(\mathbf{x})$ inside $B_{\alpha}$. By subtracting a constant from $f$ we can assume that $f\left(\mathbf{x}_{\alpha}\right)=0$.

Now consider the set $S$ of all points $\mathbf{x} \in E_{\alpha}$ having the property that there exist a connected open set $F_{\mathrm{x}}$ containing $\mathrm{x}$ and $\mathrm{x}_{\alpha}$ and a function $f_{\mathrm{x}}$ defined on $F_{\mathbf{x}}$ such that $d f_{\mathbf{x}}=\omega$ on $F_{\mathbf{x}}$ and $f_{\mathbf{x}_{\alpha}}=0$. It is clear that $S$ is an open connected subset of $E_{\alpha}$, being the union of all the connected open sets $F_{\mathbf{x}}$, which have the common point $\mathrm{x}_{\alpha}$. It is also clear that there is a function $f$ defined on $S$ such that $d f=\omega$ on $S$. In fact we can define $f(\mathbf{x})=f_{\mathbf{x}}(\mathbf{x})$, and this definition is unambiguous, since if $f_{\mathbf{x}}$ and $f_{\mathbf{y}}$ are both defined at $\mathbf{z}$, then

$$
f_{\mathbf{x}}(\mathbf{z})=\int_{\gamma} d f_{\mathbf{x}}=\int_{\gamma} \omega=\int_{\delta} \omega=\int_{\delta} d f_{\mathbf{y}}=f_{\mathbf{y}}(\mathbf{z})
$$

Here $\gamma$ is a path in $E_{\mathbf{x}}$ from $\mathbf{x}_{\alpha}$ to $\mathbf{z}$, and $\delta$ is a path in $E_{\mathbf{y}}$ from $\mathbf{x}_{\alpha}$ to $\mathbf{z}$. The path $\gamma-\delta$ lies in $E$ and is a closed loop, so that

$$
\int_{\gamma-\delta} \omega=0
$$

We need only show that $S=E_{\alpha}$. But if not, then $E_{\alpha}$ contains a boundary point $\mathrm{x} \in S$. Some open ball $B$ about $\mathrm{x}$ is contained in $E$, and this open ball contains a point $\mathbf{y} \in S$. But then there exists a function $g$ such that $d g=\omega$ in $B$, and subtracting a constant makes it possible to ensure that $g(\mathbf{y})=f_{\mathbf{y}}(\mathbf{y})=f(\mathbf{y})$. We claim that $g(\mathbf{z})=f(\mathbf{z})$ on the entire set $S \cap B$. In fact this argument merely repeats the argument just given to show that $f$ is unambiguously defined. It then follows that $\mathbf{y}$ is contained in the connected open set $S \cap B$ and that the function $h$ defined to be $f$ on $S$ and $g$ on $B$ has the property that $d h=\omega$ on $S \cap B$. By definition, this means $\mathbf{y} \in S$, which contradicts the assumption that $\mathrm{y}$ is a boundary point of $S$. Therefore $S=E_{\alpha}$.

Thus we can find a primitive for $\omega$ on each connected component of $E$. These primitives can be pieced together to provide a single primitive for $\omega$ on $E$.

Exercise 10.26 Assume $\omega$ is a 1 -form in $R^{3} \backslash\{0\}$, of class $\mathcal{C}^{\prime}$ and $d \omega=0$. Prove that $\omega$ is exact in $R^{3} \backslash\{0\}$.

Hint: Every closed continuously differentiable curve in $R^{3} \backslash\{0\}$ is the boundary of a 2-surface in $R^{3} \backslash\{0\}$. Apply Stokes' theorem and Exercise 25.

Solution. Given the assumption in the hint, the solution is easy. By Exercise 25 we need only show that the integral of $\omega$ over every closed curve is zero. By the assertion in the hint, this closed curve is the boundary of a two-surface. By Stokes' theorem, the integral of $\omega$ over the curve equals the integral. of $d \omega$ over the 2 -surface.

To prove the claim that every continuously differentiable curve in $R^{3} \backslash\{\mathbf{0}\}$ is the boundary of a two-surface, we may assume that the curve is of the form $\mathbf{x}(t), 0 \leq t \leq 1$ and $\mathbf{x}(0)=\mathbf{x}(1)$. Let $\mathbf{x}(t)=(x(t), y(t), z(t))$. We shall show first of all that there is some line through the origin in $R^{3}$ that does not intersect the curve.

To that end, we observe that the intersection of a sphere of radius $\rho$ in $R^{3}$ with a ball of radius $r(r \leq 2 \rho)$ about a point of the sphere is a spherical cap whose area is $\pi r^{2}$. (Note that this result is independent of $\rho$. It is a remarkable fact, whose proof is a routine computation.) Since the area of the whole sphere is $4 \pi \rho^{2}$, it follows that half of any given hemisphere cannot be covered by fewer than $\rho^{2} / r^{2}$ such spherical caps. Now, since $\mathbf{x}(t) \neq 0$ and $\mathbf{x}^{\prime}(t)$ is continuous, it follows that $\mathbf{v}(t)=\mathbf{x}(t) /|\mathbf{x}(t)|$ is a Lipschitz function, that is, there exists a constant $M$ such that $|\mathbf{v}(s)-\mathbf{v}(t)| \leq M|t-s|$ for all $s$ and $t$. In particular the image of each interval $[k / n,(k+1) / n]$ is contained in a spherical cap of radius $M / n$. Thus the complete curve is contained in a set of $n$ spherical caps of radius at most $M / n$. But to cover the half of any given hemisphere of the unit sphere
requires at least $\frac{n^{2}}{M^{2}}$ such caps. Hence, if $n>M^{2}$, the projection of the curve $\mathbf{x}(t)$ on the unit sphere is contained in a set of spherical caps covering less than half of the upper hemisphere and less than half of the lower hemisphere. Hence there are two antipodal points $x_{0}$ and $-x_{0}$ on the unit sphere not in its image. That means there is at least one line through the origin that the curve does not intersect.

This line through the origin gives us a sense of positive rotation from $\mathbf{x}(t)$ to $\mathbf{x}\left(t+\frac{1}{2}\right)$ for each $t \in\left[0, \frac{1}{2}\right]$. We can then construct a $\mathcal{C}^{\prime}$-curve $\gamma_{t}(s)$ in $R^{3} \backslash\{\mathbf{0}\}$ that goes from $\mathbf{x}(t)$ to $\mathbf{x}\left(t+\frac{1}{2}\right)$ by letting cylindrical coordinates vary linearly with respect to $s$. To be specific, we can assume without loss of generality that the line is the $z$-axis. In that case, the radial coordinate $r(t)=\sqrt{x^{2}(t)+y^{2}(t)}$ is never zero and is a continuously differentiable function of position. We choose $\theta(t)$ as the cylindrical polar coordinate of $\mathbf{x}(t)$ in a continuously differentiable manner for $0 \leq t \leq 1$. (This is possible by piecing together sections of this function over sufficiently small intervals.) We then define $\gamma(s, t)=(x(s, t), y(s, t), z(s, t))$ for $0 \leq s \leq 1,0 \leq t \leq 1 / 2$ by

$$
\begin{aligned}
& x(t, u)=(1-u) r(t) \cos ((1-u) \theta(t))+u r(1-t) \cos (u \theta(1-t))) \\
& y(t, u)=(1-u) r(t) \sin ((1-u) \theta(t))+u r(1-t) \sin (u \theta(1-t))) \\
& z(t, u)=(1-u) z(t)+u z(1-t)
\end{aligned}
$$

We let the boundary of this cell be $\delta_{1}+\delta_{2}+\delta_{3}+\delta_{4}$. Here $\delta_{1}$ is $\gamma(t, 0), 0 \leq t \leq 1 / 2$, which is just $\mathbf{x}(t)$ over the same interval; $\delta_{2}$ is $\gamma(1 / 2, u)$, which is the "line segment" from $x(1 / 2)$ to $x(1 / 2)$, whose range is just a point, and hence counts as 0 when regarded as a 1 -chain; $\delta_{3}$ is $\gamma(1 / 2-t, 1)$ which is just $\mathbf{x}\left(t+\frac{1}{2}\right)$, so that $\delta_{1}+\delta_{3}$ represents $\mathbf{x}(t)$ as $t$ goes from 0 to 1 . Finally $\delta_{4}$ is $\gamma(0, u)$, which is the line segment from $\mathbf{x}(1)$ to $\mathbf{x}(0)$, and since $\mathbf{x}$ is a closed curve, these two points are the same. Hence once again $\delta_{4}$ counts as 0 when regarded as a 1-chain. Thus the boundary of $\gamma$ is indeed the curve $\mathbf{x}$.

Exercise 10.27 Let $E$ be an open 3-cell in $R^{3}$, with edges parallel to the coordinate axes. Suppose $(a, b, c) \in E, f_{t} \in \mathcal{C}^{\prime}(E)$ for $i=1,2,3$,

$$
\omega=f_{1} d y \wedge d z+f_{2} d z \wedge d x+f_{3} d x \wedge d y
$$

and assume that $d \omega=0$ in $E$. Define

$$
\lambda=g_{1} d x+g_{2} d y
$$

where

$$
\begin{aligned}
g_{1}(x, y, z) & =\int_{c}^{z} f_{2}(x, y, s) d s-\int_{b}^{y} f_{3}(x, t, c) d t \\
g_{2}(x, y, z) & =-\int_{c}^{z} f_{1}(x, y, s) d s
\end{aligned}
$$

for $(x, y, z) \in E$. Prove that $d \lambda=\omega$ in $E$.

Evaluate these integrals when $\omega=\zeta$ and thus find the form $\lambda$ that occurs in part (e) of Exercise 22.

Solution. Since

$$
d \lambda=-\frac{\partial g_{2}}{\partial z} d y \wedge d z+\frac{\partial g_{1}}{\partial z} d z \wedge d x+\left(\frac{\partial g_{2}}{\partial x}-\frac{\partial g_{1}}{\partial y}\right) d x \wedge d y
$$

we need only show that

$$
\begin{aligned}
\frac{\partial g_{2}}{\partial z} & =-f_{1} \\
\frac{\partial g_{1}}{\partial z} & =f_{2} \\
\frac{\partial g_{2}}{\partial x}-\frac{\partial g_{1}}{\partial y} & =f_{3}
\end{aligned}
$$

The first two equations are immediate. As for the third, direct computation shows that.

$D_{1} g_{2}(x, y, z)-D_{2} g_{1}(x, y, z)=\int_{c}^{z}-\left(D_{1} f_{1}(x, y, s)+D_{2} f_{2}(x, y, s)\right) d s+f_{3}(x, y, c)$.

Now the assumption that $d \omega=0$ says that

$$
D_{1} f_{1}(x, y, s)+D_{2} f_{2}(x, y, s)=-D_{3} f_{3}(x, y, s)
$$

Substituting this value into the last expression and evaluating the integral using the fundamental theorem of calculus yields the result $d \lambda=\omega$.

Taking

we get

$$
\begin{aligned}
f_{1}(x, y, z) & =\frac{x}{\left(x^{2}+y^{2}+z^{2}\right)^{3 / 2}} \\
f_{2}(x, y, z) & =\frac{y}{\left(x^{2}+y^{2}+z^{2}\right)^{3 / 2}} \\
f_{3}(x, y, z) & =\frac{z}{\left(x^{2}+y^{2}+z^{2}\right)^{3 / 2}}
\end{aligned}
$$

$$
\begin{array}{r}
g_{2}(x, y, z)=-\int_{c}^{z} \frac{x}{\left(x^{2}+y^{2}+s^{2}\right)^{3 / 2}} d s= \\
\quad=\frac{1}{x^{2}+y^{2}}\left(\frac{c x}{\sqrt{x^{2}+y^{2}+c^{2}}}-\frac{z x}{\sqrt{x^{2}+y^{2}+z^{2}}}\right) \\
g_{1}(x, y, z)=\int_{c}^{z} \frac{y}{\left(x^{2}+y^{2}+s^{2}\right)^{3 / 2}} d s-\int_{b}^{y} \frac{c}{\left(x^{2}+t^{2}+c^{2}\right)^{3 / 2}} d t \\
=\frac{1}{x^{2}+y^{2}}\left(\frac{y z}{\sqrt{x^{2}+y^{2}+z^{2}}}-\frac{y c}{\sqrt{x^{2}+y^{2}+c^{2}}}\right) \\
\quad-\frac{1}{x^{2}+c^{2}}\left(\frac{c y}{\sqrt{x^{2}+y^{2}+c^{2}}}-\frac{b c}{\sqrt{x^{2}+b^{2}+c^{2}}}\right)
\end{array}
$$

It is a routine computation to verify that these functions do indeed provide a primitive for $\omega$.

Exercise 10.28 Fix $b>a>0$, define

$$
\Phi(r, \theta)=(r \cos \theta, r \sin \theta)
$$

for $a \leq r \leq b, 0 \leq \theta \leq 2 \pi$. (The range of $\Phi$ is an annulus in $R^{2}$.) Put $\omega=x^{3} d y$, and compute both

$$
\int_{\Phi} d \omega \text { and } \int_{\partial \Phi} \omega
$$

to verify that they are equal.

Solution. Since $d \omega=3 x^{2} d x \wedge d y$, we have $(d \omega)_{\Phi}=-r d r \wedge d \theta$, and

$$
\int_{\Phi} d \omega=-\int_{a}^{b} \int_{0}^{2 \pi} 3 r^{3} \cos ^{2} \theta d \theta d r=\frac{3 \pi}{4}\left(a^{4}-b^{4}\right)
$$

For the integral over the boundary we have $d y=r \cos \theta d \theta+\sin \theta d r$, and we get

$$
\int_{0}^{2 \pi}\left(a^{4}-b^{4}\right) \cos ^{4} \theta d \theta=\frac{3 \pi}{4}\left(a^{4}-b^{4}\right)
$$

Exercise 10.29 Prove the existence of a function $\alpha$ with the properties needed in the proof of Theorem 10.38, and prove that the resulting function $F$ is of class $\mathcal{C}^{\prime}$. (Both assertions become trivial if $E$ is an open cell or an open ball, since $\alpha$ can then be taken to be a constant. Refer to Theorem 9.42.)

Solution. We are given a convex open set $V \subseteq R^{p}$ whose projection on $R^{p-1}$ is the convex open set $U$. We need to show that there is a continuously differentiable function $\alpha: U \rightarrow R$ whose graph is contained in $V$. If $V$ is a cell or an open ball, there exists a section $x_{p}=c$ of it (many sections, if it is a cell), whose projection is $U$, and we can simply define $\alpha(\mathbf{y})=c$ for all $\mathbf{y} \in U$.

Now write $V$ as a countable union of open balls $V=\bigcup_{i=1}^{\infty} B_{i}$. Also write $V$ as the union of an increasing sequence of compact sets $K_{n}$ such that $K_{n} \subseteq$ $\operatorname{int}\left(K_{n+1}\right)$.

We claim that, as in Theorem 10.8, there exist continuous functions $\psi_{i}$ such that the support of $\psi_{i}$ is contained in the projection of $B_{i}$ on $R^{p-1}, 0 \leq \psi_{i}(\mathbf{y}) \leq$ 1 for all $\mathbf{y}$, and $\sum_{i=1} \psi_{i}(\mathbf{y})=1$ for all $\mathbf{y} \in U$. Moreover, this sum is locally finite, that is, each point $\mathbf{y}$ has a neighborhood $U_{\mathbf{y}}$ such that the set of indices $i$ for which $\psi_{i}(\mathbf{z}) \neq 0$ for some $\mathbf{z} \in U_{\mathbf{y}}$ is finite.

To construct such functions, for each $\mathrm{x} \in V$, let $i(\mathrm{x})$ be the smallest index $r$ such that $\mathrm{x} \in B_{i}$. Then, as in the proof of Theorem 10.8, for each $\mathrm{x} \in K_{1}$, choose open balls $B(\mathbf{x})$ and $W(\mathbf{x})$ centered at $\mathbf{x}$ such that

$$
\overline{B(\mathbf{x})} \subset W(\mathbf{x}) \subset \overline{W(\mathbf{x})} \subset B_{i(\mathbf{x})}
$$

Since $K_{1}$ is compact, there are points $\mathbf{x}_{11}, \ldots, \mathbf{x}_{1 N_{1}}$ such that

$$
K_{1} \subseteq B\left(\mathrm{x}_{11}\right) \cup \cdots \cup B\left(\mathrm{x}_{1 N_{1}}\right)
$$

For later convenience we define $L_{1}=K_{1}$.

Now let $L_{2}=K_{2} \backslash \bigcup_{j=1}^{N_{2}} B\left(\mathrm{x}_{1 j}\right)$. For each $\mathrm{x} \in L_{2}$ there are open balls $B(\mathrm{x})$ and $W(\mathbf{x})$ centered at $\mathbf{x}$ such that

$$
\overline{B(\mathbf{x})} \subset W(\mathbf{x}) \subset \overline{W(\mathbf{x})} \subset B_{i(\mathbf{x})} \backslash K_{1} .
$$

Since $L_{2}$ is compact, we choose a finite set of points $\mathbf{x}_{21}, \ldots, \mathbf{x}_{2 N_{2}}$ such that

$$
L_{2} \subseteq B\left(\mathbf{x}_{21}\right) \cup \cdots \cup B\left(\mathbf{x}_{2 N_{2}}\right)
$$

Notice that $K_{2} \subset \bigcup_{k=1}^{2} \bigcup_{j=1}^{N_{k}} B\left(\mathbf{x}_{k j}\right)$.

Now suppose we have chosen a (possibly empty) collection of open balls $B\left(\mathrm{x}_{k j}\right)$ and $W\left(\mathrm{x}_{k j}\right), 1 \leq j \leq N_{k}, 1 \leq k \leq r$, centered at $\mathrm{x}_{k j} \in L_{k}=K_{k} \backslash$ $\bigcup_{i=1}^{k-1} \bigcup_{j=1}^{N_{i}} B\left(\mathbf{x}_{i j}\right)$, and such that

and

$$
\overline{B\left(\mathbf{x}_{k j}\right)} \subset W\left(\mathbf{x}_{k j}\right) \subset \overline{W\left(\mathbf{x}_{k j}\right)} \subset B_{i\left(\mathbf{x}_{k j}\right)} \backslash K_{k-1}
$$

and

$$
K_{r} \backslash \bigcup_{k=1}^{r-1} \bigcup_{j=1}^{N_{k}} B\left(\mathrm{x}_{k j}\right) \subset \bigcup_{j=1}^{N_{r}} B\left(\mathrm{x}_{r j}\right)
$$

$$
K_{s} \subset \bigcup_{k=1}^{s} \bigcup_{j=1}^{N_{k}} B\left(\mathbf{x}_{k j}\right)
$$

for $1 \leq s \leq r-1$. It then follows from the last two relationships that the last one also holds with $s=r$. By then considering the compact set $L_{r+1}=$ $K_{r+1} \backslash \bigcup_{k=1}^{r} \bigcup_{j=1}^{N_{k}} B\left(\mathrm{x}_{k j}\right)$ and repeating the argument, we can assume that the sets $B\left(\mathbf{x}_{k j}\right)$ and $W\left(\mathbf{x}_{k j}\right)$ with these properties have been chosen for all $k$ and all $j$,
$1 \leq k<\infty, 1 \leq j \leq N_{k}$. It follows in particular that

$$
V=\bigcup_{n=1}^{\infty} K_{n} \subset \bigcup_{k=1}^{\infty} \bigcup_{j=1}^{N_{k}} B\left(\mathbf{x}_{k j}\right)
$$

Now let $\widetilde{K}_{k}, \widetilde{B}\left(\mathbf{x}_{k j}\right)$, and $\widetilde{W}\left(\mathbf{x}_{k j}\right)$ be respectively the projections on $R^{p-1}$ of $K_{k}, B\left(\mathbf{x}_{k j}\right)$, and $W\left(\mathbf{x}_{k j}\right)$, and let

$$
\widetilde{L}_{r}=\widetilde{K}_{k} \backslash{ }_{k=1}^{r-1} \bigcup_{j=1}^{N_{k}} \widetilde{B}\left(\mathbf{x}_{k j}\right)
$$

We then choose functions $\varphi_{j k}$ as smooth as we like such that $\varphi_{k j}(\mathbf{y})=1$ on $\widetilde{B}\left(\mathbf{x}_{k j}\right)$ (and hence also on $\left.\widetilde{B}\left(\mathbf{x}_{k j}\right)\right), \varphi_{k j}(\mathbf{y})=0$ outside $\widetilde{W}\left(\mathbf{x}_{k j}\right)$, and $0 \leq$ $\varphi_{k j}(\mathbf{y}) \leq 1$ on $R^{p-1}$. Let $\varphi_{j}(\mathbf{y})=\varphi_{1 j}(\mathbf{y})$ for $1 \leq j \leq N_{1}$ and $\varphi_{j}(\mathbf{y})=$
$\varphi_{k, j-\left(N_{1}+\cdots+N_{k-1}\right)}(\mathbf{y})$ for $N_{1}+\cdots+N_{k-1}<j \leq N_{1}+\cdots+N_{k}, 2 \leq k<\infty$. We define $\mathbf{x}_{j}$ analogously. Let $\mathbf{x}_{j}=\left(\mathbf{y}_{j}, c_{j}\right)$.

We then proceed to define $\psi_{1}(\mathbf{y})=\varphi_{1}(\mathrm{y})$ and

$$
\psi_{j+1}(\mathbf{y})=\left(1-\varphi_{1}(\mathbf{y})\right) \cdots\left(1-\varphi_{j}(\mathbf{y})\right) \varphi_{j+1}(\mathbf{y})
$$

for $j=1,2, \ldots$, as in Theorem 10.8. It is obvious that the support of $\psi_{j}$ is contained in the closure of $\widetilde{W}\left(\mathbf{x}_{j}\right)$ and hence in $\widetilde{B}_{i\left(\mathbf{x}_{j}\right)} \backslash \widetilde{K}_{k-1} \subseteq U \backslash \widetilde{K}_{k-1}$ when $N_{1}+\cdots+N_{k-1}<j \leq N_{1}+\cdots+N_{k}$.

Now by the choice of the sets $K_{n}$, if $\mathrm{y} \in U$, there is some $n$ such that $\mathbf{y} \in \widetilde{K}_{n} \subset \operatorname{int}\left(\widetilde{K}_{n+1}\right)$, and hence $\psi_{j}(\mathbf{y})=0$ on the open neighborhood $\operatorname{int} \widetilde{K}_{n+1}$ of $\mathbf{y}$ if $j>N_{1}+\cdots+N_{n+1}$. Therefore the sum and product

$$
\sum_{j=1}^{\infty} \psi_{j}(\mathbf{y})=1-\prod_{i=1}^{\infty}\left[1-\varphi_{i}(\mathbf{y})\right]
$$

are both locally finite at each point. (Local finiteness of the product means all but a finite number of factors equal 1 on a neighborhood of each point.) However, if $y \in U$, then $\mathbf{y} \in \widetilde{B}\left(\mathbf{x}_{j}\right)$ for some $j$, and so $\varphi_{j}(\mathbf{y})=1$, from which it then follows that

$$
\sum_{j=1}^{\infty} \psi_{j}(\mathbf{y})=1
$$

for all $\mathbf{y} \in U$.

Since we have defined $c_{j}$ so that $\mathbf{x}_{j}=\left(\mathbf{y}_{j}, c_{j}\right)$, it follows that the projection of the $c_{j}$-section of $B\left(\mathbf{x}_{j}\right)$ on $R^{p-1}$, which we denote $C_{j}$, is the same as the projection of $B\left(\mathbf{x}_{j}\right)$ on this subspace. That is, it is $\widetilde{B}\left(\mathbf{x}_{j}\right)$. We can now let $\alpha(\mathbf{y})=\sum_{j=1}^{\infty} c_{j} \psi_{1}(\mathbf{y})$. For then at each $\mathbf{y} \in U$ there is a finite integer $n$ such that

$$
(\mathbf{y}, \alpha(\mathbf{y}))=\psi_{1}(\mathbf{y})\left(\mathbf{y}, c_{1}\right)+\cdots+\psi_{n}(\mathbf{y})\left(\mathbf{y}, c_{n}\right)
$$

Since $\psi_{k}(\mathbf{y})=0$ if $\mathbf{y} \notin C_{k}$ and $\left(\mathbf{y}, c_{k}\right) \in B_{k} \subset V$ if $\mathbf{y} \in C_{k}$, it follows that $(\mathbf{y}, \alpha(\mathbf{y}))$ is a weighted average of points in $V$, hence belongs to $V$ for all $\mathbf{y} \in U$.

Exercise 10.30 If $\mathbf{N}$ is the vector given by (135), prove that

$$
\operatorname{det}\left[\begin{array}{lll}
\alpha_{1} & \beta_{1} & \alpha_{2} \beta_{3}-\alpha_{3} \beta_{2} \\
\alpha_{2} & \beta_{2} & \alpha_{3} \beta_{1}-\alpha_{1} \beta_{3} \\
\alpha_{3} & \beta_{3} & \alpha_{2} \beta_{2}-\alpha_{2} \beta_{1}
\end{array}\right]=|\mathbf{N}|^{2}
$$

Also, verify Eq. (137).

Solution. The equation in the problem is a straightforward computation, and amounts merely to expanding the determinant along the last column. Likewise Eq. (137), which merely asserts that a cross product is perpendicular to each
of the factors, is routine. The two inner products in the equation can be obtained by replacing the last column of this determinant by either $\left(\alpha_{1}, \alpha_{2}, \alpha_{3}\right)$ or $\left(\beta_{1}, \beta_{2}, \beta_{3}\right)$. In each case, the result is a determinant with two equal columns, which is therefore zero.

Exercise 10.31 Let $E \subset R^{3}$ be open, suppose $g \in \mathcal{C}^{\prime \prime}(E), h \in \mathcal{C}^{\prime \prime}(E)$, and consider the vector field

$$
\mathbf{F}=g \nabla h .
$$

(a) Prove that

$$
\nabla \cdot \mathbf{F}=g \nabla^{2} h+(\nabla g) \cdot(\nabla h)
$$

where $\nabla^{2} h=\nabla \cdot(\nabla h)=\sum \partial^{2} h / \partial x_{i}^{2}$ is the so-called "Laplacian" of $h$. (b) If $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial \Omega$ (as in Theorem $10.51)$, prove that

$$
\int_{\Omega}\left[g \nabla^{2} h+(\nabla g) \cdot(\nabla h)\right] d V=\int_{\partial \Omega} g \frac{\partial h}{\partial n} d A
$$

where (as is customary) we have written $\partial h / \partial n$ in place of $(\nabla h) \cdot \mathbf{n}$. (Thus $\partial h / \partial n$ is the directional derivative of $h$ in the direction of the outward normal to $\partial \Omega$, the so-called normal derivative of $h$.) Interchange $g$ and $h$, subtract the resulting formula from the first one, to obtain

$$
\int_{\Omega}\left(g \nabla^{2} h-h \nabla^{2} g\right) d V=\int_{\partial \Omega}\left(g \frac{\partial h}{\partial n}-h \frac{\partial g}{\partial n}\right) d A .
$$

These two formulas are usually called Green's identities.

(c) Assume that $h$ is harmonic in $E$; this means that $\nabla^{2} h=0$. Take $g=1$ and conclude that

$$
\int_{\partial \Omega} \frac{\partial h}{\partial n} d A=0
$$

Take $g=h$, and conclude that $h=0$ in $\Omega$ if $h=0$ on $\partial \Omega$.

(d) Show that Green's identities are also valid in $R^{2}$.

Solution. Part $(a)$ is simply the product rule for derivatives.

The main equation in part $(b)$ is simply the divergence theorem applied to $\mathbf{F}$. Green's identities then follow by completely routine computation.

(c) Taking $g=1$ forces $\partial g / \partial n=0$ and $\nabla^{2} g=0$. Since $\nabla^{2} h=0$ by the assumption that $h$ is harmonic, the result follows. For the other assertion of this part we have to go back to the main equation before taking $g=h$. When we do, we actually get a slightly stronger assertion: $\nabla h=0$ in $\Omega$, and so $h$ is constant on each component of $\Omega$, if either $h=0$ or $\partial h / \partial n=0$ on all of $\partial \Omega$. When $h=0$ on $\partial \Omega$, obviously the constant value of $h$ must be 0 .
(d) The "two-dimensional" divergence theorem is simply Green's theorem. That is, the assertion that

$$
\int_{\Omega} \nabla \cdot \mathbf{F}=\int_{\partial \Omega} \mathbf{k} \times \mathbf{F}
$$

follows upon applying Green's theorem to the one-form $\omega=-F_{2} d x+F_{1} d y$ corresponding to the vector field $\mathbf{k} \times \mathbf{F}=-F_{2} \mathbf{i}+F_{1} \mathbf{j}$. Because the dot and cross operations can be interchanged in the scalar triple product, integrating $\mathbf{k} \times \mathbf{F}$ along a curve, that is, taking the product $\mathbf{k} \times \mathbf{F} \cdot \mathbf{r}$, where $\mathbf{r}$ is the tangent to the curve, and then integrating, is the same as integrating $\mathbf{F} \cdot \mathbf{k} \times \mathbf{r}$, which is the normal component of $\mathbf{F}$. All the same identities now follow.

Exercise 10.32 Fix $\delta, 0<\delta<1$. Let $D$ be the set of all $(\theta, t) \in R^{2}$ such that $0 \leq \theta \leq \pi,-\delta \leq t \leq \delta$. Let $\Phi$ be the 2 -surface in $R^{3}$ with parameter domain $D$ given by

$$
\begin{aligned}
& x=(1-t \sin \theta) \cos 2 \theta \\
& y=(1-t \sin \theta) \sin 2 \theta \\
& z=t \cos \theta
\end{aligned}
$$

where $(x, y, z)=\Phi(\theta, t)$. note that $\Phi(\pi, t)=\Phi(0,-t)$ and that $\Phi$ is one-to-one on the rest of $D$.

The range $M=\Phi(D)$ is known as a M√∂bius band. It is the simplest example of a nonorientable surface.

Prove the various assertions made in the following description: Put $\mathbf{p}_{1}=$ $(0,-\delta), \mathbf{p}_{2}=(\pi,-\delta), \mathbf{p}_{3}=(\pi, \delta), \mathbf{p}_{4}=(0, \delta), \mathbf{p}_{5}-\mathbf{p}_{1}$. Put $\gamma_{i}=\left[\mathbf{p}_{i}, \mathbf{p}_{i-1}\right]$, $i=1,2 \ldots, 4$, and put $\Gamma_{i}=\Phi \circ \gamma_{i}$. Then

$$
\partial \Phi=\Gamma_{1}+\Gamma_{2}+\Gamma_{3}+\Gamma_{4}
$$

Put $\mathbf{z}=(1,0,-\delta), \mathbf{b}=(1,0, \delta)$. Then

$$
\Phi\left(\mathbf{p}_{1}\right)=\Phi\left(\mathbf{p}_{3}\right)=\mathbf{a}, \quad \Phi\left(\mathbf{p}_{2}\right)=\Phi\left(\mathbf{p}_{4}\right)=\mathbf{b}
$$

and $\partial \Phi$ can be described as follows.

$\Gamma_{1}$ spirals up from $\mathbf{a}$ to $\mathbf{b}$; its projection into the $(x, y)$-plane has winding number +1 around the origin. (See Exercise 23, Chap. 8).

$\Gamma_{2}=[\mathbf{b}, \mathbf{a}]$.

$\Gamma_{3}$ spirals up from $\mathbf{a}$ to $\mathbf{b}$; its projection into the $(x, y)$-plane has winding number -1 around the origin.

$\Gamma_{4}=[\mathrm{b}, \mathrm{a}]$.

Thus $\partial \Phi=\Gamma_{1}+\Gamma_{3}+2 \Gamma_{2}$.

If we go from $\mathbf{a}$ to $\mathbf{b}$ along $\Gamma_{1}$ and continue along the "edge" of $M$ until we return to a, the curve traced out is

$$
\Gamma=\Gamma_{1}-\Gamma_{3}
$$

which may also be represented on the parameter interval $[0,2 \pi]$ by the equations

$$
\begin{aligned}
x & =(1+\delta \sin \theta) \cos 2 \theta \\
y & =(1+\delta \sin \theta) \sin 2 \theta \\
z & =-\delta \cos \theta
\end{aligned}
$$

It should be emphasized that $\Gamma \neq \partial \Phi$ : Let $\eta$ be the 1-form discussed in Exercises 21 and 22. Since $d \eta=0$, Stokes' theorem shows that

$$
\int_{\partial \Phi} \eta=0
$$

But although $\Gamma$ is the "geometric" boundary of $M$, we have

$$
\int_{\Gamma} \eta=4 \pi
$$

In order to avoid this possible source of confusion, Stokes' formula (Theorem $10.50)$ is frequently stated only for orientable surfaces $\Phi$.

Solution.. The claim about the boundary $\partial \Phi$ follows immediately from the definition of a boundary. The domain $D$ is a cell whose boundary is $\gamma_{1}+\gamma_{2}+$ $\gamma_{3}+\gamma_{4}$, so that by definition $\partial \Phi=\Phi\left(\gamma_{1}\right)+\Phi\left(\gamma_{2}\right)+\Phi\left(\gamma_{3}\right)+\Phi\left(\gamma_{4}\right)$.

The claims that $\Phi\left(\mathbf{p}_{1}\right)=\Phi\left(\mathbf{p}_{3}\right)=\mathbf{a}$ and $\Phi\left(\mathbf{p}_{2}\right)=\Phi\left(\mathbf{p}_{4}\right)=\mathbf{b}$ are routine computations.

The description of $\Gamma_{1}$ follows from the fact that $\gamma_{1}$ can be described as the set $(\theta,-\delta), 0 \leq \theta \leq \pi$, so that the projection of $\Gamma_{1}$ in the $(x, y)$-plane is the set of all points $(x(\theta), y(\theta))$, where

$$
\begin{aligned}
& x(\theta)=(1+\delta \sin \theta) \cos 2 \theta \\
& y(\theta)=(1+\delta \sin \theta) \sin 2 \theta
\end{aligned}
$$

Regarding the pair $(x(\theta), y(\theta))$ as the complex number $z(\theta)=x(\theta)+i y(\theta)=$ $(1+\delta \sin \theta)(\cos 2 \theta+i \sin 2 \theta)$, and using the definition of the winding number,

$$
n=\frac{1}{2 \pi i} \int_{0}^{\pi} \frac{z^{\prime}(\theta)}{z(\theta)} d \theta
$$

Now, $z^{\prime}(\theta)=2(1+\delta \sin \theta)(-\sin 2 \theta+i \cos 2 \theta)+\delta \cos \theta(\cos 2 \theta+i \sin 2 \theta)$, so that we get

$$
n=\frac{1}{\pi i}\left(\int_{0}^{\pi} \frac{-\sin 2 \theta+i \cos 2 \theta}{\cos 2 \theta+i \sin 2 \theta} d \theta+\delta \int_{0}^{\pi} \frac{\cos \theta}{1+\delta \sin \theta} d \theta\right) .
$$

But $-\sin 2 \theta+i \cos 2 \theta=i(\cos 2 \theta+i \sin 2 \theta)$, so that the first integral is just $\pi i$, and that term contributes +1 to the winding number. The second integral is just $\ln (1+\delta \sin \theta)$, and since this function has the value 0 at both $\theta=0$ and $\theta=\pi$, it contributes nothing.

As for $\Gamma_{2}$, since $\theta=\pi$, it is given by $(x(t), y(t), z(t)),-\delta \leq t \leq \delta$, where $x(t)=1, y(t)=0, z(t)=-t$. It therefore describes the line segment from $\mathbf{b}$ to a as $t$ goes from $-\delta$ to $\delta$.

The descriptions of $\Gamma_{3}$ and $\Gamma_{4}$ are justified exactly as was just done for $\Gamma_{1}$ and $\Gamma_{2}$.

As both $\Gamma_{1}$ and $\Gamma_{3}$ spiral upward from $a$ to $b$, it is manifest that $\Gamma_{1}-\Gamma_{3}$ represents a spiral that goes from $\mathbf{a}$ to $\mathbf{b}$ and back again. It is also easy to see that this spiral does not intersect itself, as the ranges of $\Gamma_{1}$ and $\Gamma_{3}$ meet only in $\mathbf{a}$ and $\mathbf{b}$. For suppose $\theta$ and $\varphi$ are such that $\Gamma_{1}(\theta)=\Gamma_{3}(\varphi)$. This means in particular that $-\delta \cos \theta=\delta \cos \varphi$, and so $\theta=\pi-\varphi$. It then follows that $(1+\delta \sin \theta) \cos 2 \theta=(1-\delta \sin \theta) \cos 2 \theta$, so that either $\cos 2 \theta=0$ or $\sin \theta=0$. Since we also have $(1+\delta \sin \theta) \sin 2 \theta=-(1-\delta \sin \theta) \sin 2 \theta$, the possibility that $\cos 2 \theta=0$ is ruled out, and so $\sin \theta=0$, i.e., $\theta=0$ or $\theta=\pi$, meaning the point in common is either $\mathbf{a}$ or $\mathbf{b}$, as asserted.

As for the description of $\Gamma_{1}-\Gamma_{3}$, it is clear that the mapping $T(\theta)$ given by the equations

$$
\begin{aligned}
x & =(1+\delta \sin \theta) \cos 2 \theta \\
y & =(1+\delta \sin \theta) \sin 2 \theta \\
z & =-\delta \cos \theta
\end{aligned}
$$

has the property that $T(\theta+\pi)$ is given by the equations

$$
\begin{aligned}
x & =(1-\delta \sin \theta) \cos 2 \theta \\
y & =(1-\delta \sin \theta) \sin 2 \theta \\
z & =\delta \cos \theta
\end{aligned}
$$

Hence it equals describes $\Gamma_{1}(-\delta, \theta)$ on the interval $[0, \pi]$ and $-\Gamma_{3}(\delta, \theta)$ (since $\Gamma_{3}$ is given by the latter formulas, but is traversed with $\theta$ decreasing from $\pi$ to 0 ).

Since $x^{2}+y^{2}=(1-t \sin \theta)^{2} \geq(1-\delta)^{2}>0$ on all of $M$, it follows that $\eta$ is defined on $M$. On $\Gamma_{1}$ and $\Gamma_{3}$ we have $\eta=2 d \theta$, so that

$$
\int_{\Gamma} \eta=4 \pi
$$

## Chapter 11

## The Lebesgue Theory

Exercise 11.1 If $f \geq 0$ and $\int_{E} f d \mu=0$, prove that $f(x)=0$ almost everywhere on $E$. Hint: Let $E_{n}$ be the subset of $E$ on which $f(x)>1 / n$. Write $A=\cup E_{n}$. Then $\mu(A)=0$ if and only if $\mu\left(E_{n}\right)=0$ for every $n$.

Solution. The assertion in the hint is immediate. If $\mu(A)=0$, then $\mu\left(E_{n}\right)=0$ also, since $E_{n} \subseteq A$. Conversely, letting $F_{n}=E_{n} \backslash \sum_{k=1}^{n-1} E_{k}$, we have $F_{n} \subset E_{n}$, $F_{m} \cap F_{n}=\emptyset$ if $m \neq n$, and $\cup F_{n}=\cup E_{n}=A$. Hence if $\mu\left(E_{n}\right)=0$, then $\mu\left(F_{n}\right)=0$ also, and therefore $\mu(A)=0$ by the countable additivity of $\mu$.

Given the hint, the solution is immediate, since $A$ is the subset of $E$ on which $f(x)>0$. If $\mu\left(F_{n}\right)>0$ for any $n$, then $\int_{E} f d \mu \geq \int_{F_{n}} f d \mu \geq \mu\left(F_{n}\right) / n>0$.

Exercise 11.2 If $\int_{A} f d \mu=0$ for every measurable subset $A$ of a measurable set $E$, then $f(x)=0$ almost everywhere on $E$.

Solution. The hypothesis applies in particular if $A$ is the set on which $f(x)>0$. Since $\chi_{A} f \geq 0$, the preceding exercise shows that $\mu(A)=0$. Likewise, taking. $B$ as the set on which $-f(x)>0$, we find that $\mu(B)=0$. Hence $f(x)=0$ for almost every $x$.

Exercise 11.3 If $\left\{f_{n}\right\}$ is a sequence of measurable functions, prove that the set of points $x$ at which $\left\{f_{n}(x)\right\}$ converges is measurable.

Solution. This set can be written as

$$
\bigcap_{n=1}^{\infty} \bigcup_{m=1}^{\infty} \bigcap_{k=m}^{\infty} \bigcap_{l=m}^{\infty}\left\{x:\left|f_{k}(x)-f_{l}(x)\right|<\frac{1}{n}\right\}
$$

For this set is the set of $x$ such that for every $n$ there exists $m$ such that $\left|f_{k}(x)-f_{l}(x)\right|<1 / n$ for all $k \geq m, l \geq m$. That is precisely the Cauchy criterion for convergence.

Exercise 11.4 If $f \in \mathcal{L}(\mu)$ on $E$ and $g$ is bounded and measurable on $E$, then $f g \in \mathcal{L}(\mu)$ on $E$.

Solution. This follows immediately from the dominated convergence theorem and the fact that $|g(x)| \leq M$ for some constant $M$. (Take $f_{n}(x)=g_{n}(x) f(x)$ for all $n$, where $g_{n}(x)$ is a sequence of simple functions converging to $g(x)$ almost everywhere. We can assume $\left|g_{n}(x)\right| \leq M$ and let the dominating function be $M|f(x)|$.

Exercise 11.5 Put

$$
\begin{aligned}
& g(x)= \begin{cases}0 & \left(0 \leq x \leq \frac{1}{2}\right) \\
1 & \left(\frac{1}{2}<x \leq 1\right)\end{cases} \\
& f_{2 k}(x)=g(x) \quad(0 \leq x \leq 1) \\
& f_{2 k+1}(x)=g(1-x) \quad(0 \leq x \leq 1)
\end{aligned}
$$

Show that

$$
\liminf _{n \rightarrow \infty} f_{n}(x)=0 \quad(0 \leq x \leq 1)
$$

but

$$
\int_{0}^{1} f_{n}(x) d x=\frac{1}{2}
$$

[Compare with (77).]

Solution. Since for each $x \in\left[0, \frac{1}{2}\right]$ we have $f_{2 k}(x)=0$ for all $k$, it follows that the inferior limit at such an $x$ is zero. The same is true for $x \in\left[\frac{1}{2}, 1\right]$, since $f_{2 k+1}(x)=0$ for all these $x$. The value of the integral is immediate, since each $f_{n}(x)$ is a step function.

Lemma.

The point of this exercise is that strict inequality can easily occur in Fatou's

Exercise 11.6 Let

$$
f_{n}(x)= \begin{cases}\frac{1}{n} & (|x| \leq n) \\ 0 & (|x|>n)\end{cases}
$$

Then $f_{n}(x) \rightarrow 0$ uniformly on $R^{1}$, but

$$
\int_{-\infty}^{\infty} f_{n} d x=2 \quad(n=1,2,3, \ldots)
$$

(We write $\int_{-\infty}^{\infty}$ in place of $\int_{R^{1}}$.) Thus uniform convergence does not imply dominated convergence in the sense of Theorem 11.32. However, on sets of finite measure, uniformly convergent sequences of bounded functions do satisfy Theorem 11.32.

Solution. The uniform convergence to zero is obvious, since $0 \leq f_{n}(x) \leq 1 / n$ for all $x$ and all $n$.

Again, since $f_{n}(x)$ is a step function, the value of the integral is immediate.

Exercise 11.7 Find a necessary and sufficient condition that $f \in \mathcal{R}(\alpha)$ on $[a, b]$. Hint: Consider Example 11.6(b) and Theorem 11.33.

A bounded function $f$ belongs to $\mathcal{R}(\alpha)$ on $[a, b]$ if and only if the following two conditions hold:

(i) $f$ is right-continuous wherever $\alpha$ is not right-continuous and left-continuous wherever $\alpha$ is not left-continuous (that is, one of $f$ and $\alpha$ is right-continuous at each point and one is left-continuous);

(ii) the set of points where $\alpha$ is continuous and $f$ is not continuous is a set of zero $\alpha$-variation. That is, this set has $\mu_{\alpha}$-measure zero, where $\mu_{\alpha}$ is the regular Borel measure generated by the function $\alpha$, as in Example 11.6(b).

To prove this fact, all we have to do is copy the proof of Theorem 11.33, mutatis mutandis, specifically, replacing $d x$ by $d \alpha$ and $\Delta x$ by $\Delta \alpha$ at every stage. It will follow as a corollary of the proof that if $f \in \mathcal{R}(\alpha)$, then $f \in \mathcal{L}\left(\mu_{\alpha}\right)$ and

$$
\int_{[a, b]} f d \mu_{\alpha}=\mathcal{R} \int_{a}^{b} f(x) d \alpha(x)
$$

In modifying the proof we need to clear out just one case in order to make the changes run smoothly. To that end, we note that if $f$ and $\alpha$ both have a onesided discontinuity from the same side and at the same point, it is impossible for $f$ to belong to $\mathcal{R}(\alpha)$. Indeed, suppose $p$ is a common right-sided discontinuity of both $f$ and $\alpha$. For any partition $P$ we have $x_{k} \leq p<x_{k+1}$ for some index $k$, and then

$$
U(P, f, \alpha)-L(P, f, \alpha) \geq 0 \cdot(\alpha(p+)-\alpha(p)>0
$$

where $o$ is the limit of $\sup _{p \leq x<p+h} f(x)-\inf _{p \leq x<p+h} f(x)$ as $h \downarrow 0$. (The function $f(x)$ is right-continuous at $p$ if and only if $o=0$.)

Note that if $f$ and $\alpha$ are discontinuous from opposite sides at a point, it is quite possible that $f \in \mathcal{R}(\alpha)$. For example, let $f(x)=\chi_{[0,1 / 2]}(x)$ and $\alpha(x)=$ $\chi_{[1 / 2, \infty)}(x)$. Then for any partition $P$ of $[0,1]$ containing $1 / 2$, we have $x_{k}=\frac{1}{2}$ for some $k$, and

$$
U(P, f, \alpha)-L(P, f, \alpha)=1-1=0
$$

(It is for this reason that I define $\mathcal{R}(\alpha)$ differently in my courses. I require that for each $\varepsilon>0$ there must exist $\delta>0$ such that $U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon$ for all partitions $P$ such that $\max _{1 \leq k \leq n}\left(x_{k}-x_{k-1}\right)<\delta$. When this is done, conditions (i) and $(i i)$ are no longer sufficient for $f$ to belong to $\mathcal{R}(\alpha)$, and the theory is somewhat simpler. Except for special considerations at discontinuities of $\alpha$, however, the results are the same in both theories.)

We now suppose that condition $(i)$ holds and prove the necessity of condition (ii). To avoid having to single out the endpoints in what follows, we simply extend $\alpha$ outside the interval $[a, b]$ by specifying $\alpha(x)=\alpha(b)$ f $\odot$ r $x>b$ and $\alpha(x)=\alpha(a)$ for $x<a$.

Suppose that $f$ is in $\mathcal{R}(\alpha)$. Let $\left\{P_{k}\right\}$ be a sequence of partitions such that $P_{k+1}$ is a refinement of $P_{k}$, the distance between adjacent points of $P_{k}$ is less than $\frac{1}{k}, P_{k}$ contains all points $x$ at which $\alpha(x+)-\alpha(x-)>\frac{1}{k}$, and

$$
U\left(P_{k}, f, \alpha\right) \rightarrow \mathcal{R} \int f d \alpha, \quad L\left(P_{k}, f, \alpha\right) \rightarrow \mathcal{R} \int f d \alpha
$$

We note that every discontinuity of $\alpha$ belongs to some partition $P_{k}$. Assume $P_{k}$ consists of the points $a=x_{k, 0}<x_{k, 1}<\cdots<x_{k, n_{k}}=b$.

As in the proof of Theorem 11.33, we define $U_{k}(x)=M_{i}$ and $L_{k}(x)=m_{i}$ for $x_{k, i-1}<x \leq x_{k, i}, 1 \leq i \leq n_{k}$. For definiteness we define $U_{k}(a)=M_{1}$ and $L_{k}(a)=m_{1}$. Then by definition of the upper and lower sume, the definition of $\mu_{\alpha}((a, b])$, and the definition of the integral of a simple function,

$$
\begin{aligned}
& \int_{[a, b]} U_{k} d \mu_{\alpha}=U\left(P_{k}, f, \alpha\right)+M_{1}(\alpha(a+)-\alpha(a)), \\
& \int_{[a, b]} L_{k} d \mu_{\alpha}=L\left(P_{k}, f, \alpha\right)+m_{1}(\alpha(a+)-\alpha(a)) .
\end{aligned}
$$

By condition (i), either $M_{1}-m_{1} \rightarrow 0$ as $k \rightarrow \infty$ or $\alpha(a+)=\alpha(a)$. It then follows that the monotonic sequences $L_{k}$ and $U_{k}$ have limits $L$ and $U$ that are measurable, and either

$$
\int_{[a, b]} L d \mu_{\alpha}=\mathcal{R} \int f d \alpha, \quad \int_{[a, b]} U d \mu_{\alpha}=\mathcal{R} \int f d \alpha
$$

(when $\alpha(a+)=\alpha(a)$ ) or

$$
\begin{aligned}
& \int_{[a, b]} L d \mu_{\alpha}=\mathcal{R} \int f d \alpha+f(a)(\alpha(a+)-\alpha(a)) \\
& \int_{[a, b]} U d \mu_{\alpha}=\mathcal{R} \int f d \alpha+f(a)(\alpha(a+)-\alpha(a))
\end{aligned}
$$

(when $\alpha(a+)>\alpha(a)$ ).

- If these two integrals are the same, it follows that $U(x)=L(x)$ almost everywhere with respect to the measure $\mu_{\alpha}$. If $x$ is not a point of any partition $P_{k}$ and $U(x)=L(x)$, then $f$ is continuous at $x$. As for points of the partition, they are either points of discontinuity of $\alpha$ or points $x$ such that $\mu_{\alpha}(\{x\})=0$. Since there are only countably many points in all the partitions, the partition points $x$ for which $\mu_{\alpha}(\{x\})=0$ form a set of measure zero. Thus the set of discontinuities of $f(x)$ can be written as the union $A \cup B$, where $A$ consists of points where $f$ is continuous from only one side and $\alpha$ is discontinuous from
that side (these points are all among the points of partition), and $B$ consists of the points of discontinuity of $f$ where $\alpha$ is continuous. We have just shown that $B$ is of zero $\alpha$-variation, as claimed.

Conversely, if $f$ satisfies these two conditions, we note that $U(x)=L(x)$ at all points where $f(x)$ is continuous. Hence if the discontinuities of $f(x)$ other than one-sided discontinuities at points where $\alpha$ is continuous from the side on which $f$ is discontinuous form a set of zero $\alpha$-variation, then $U\left(P_{k}, f, \alpha\right)-L\left(P_{k}, f, \alpha\right) \rightarrow$ 0 .

Exercise 11.8 If $f \in \mathcal{R}$ on $[a, b]$ and if $F(x)=\int_{a}^{x} f(t) d t$, prove that $F^{\prime}(x)=$ $f(x)$ almost everywhere on $[a, b]$.

Solution. We know by Theorem 6.20 that $F^{\prime}(x)=f(x)$ at every point where $f(x)$ is continuous. Theorem 11.33 shows that if $f$ is Riemann-integrable on $[a, b]$, then it is continuous almost everywhere. Hence the result follows.

Exercise 11.9 Prove that the function $F$ given by $(96)$ is continuous on $[a, b]$.

Solution. The function $F$ is the one in the preceding exercise. Its continuity follows from the dominated convergence theorem; taking $|f|$ as the "dominating" function and letting $f_{n}=\chi_{\left[a, x_{n}\right]} f$ where $\left\{x_{n}\right\}$ is any sequence of numbers converging to $x$, so that $f_{n}$ converges pointwise to $\chi_{[a, x]} f$ except possibly at the point $x$, which is a set of measure 0 . The dominated convergence theorem then guarantees that $F\left(x_{n}\right) \rightarrow F(x)$. Since the sequence $\left\{x_{n}\right\}$ is arbitrary, it follows that $F$ is continuous at $x$, as in Theorem 4.2.

Exercise 11.10 If $\mu(X)<+\infty$ and $f \in \mathcal{L}^{2}(\mu)$ on $X$, prove that $f \in \mathcal{L}(\mu)$ on $X$. If

$$
\mu(X)=+\infty
$$

this is false. For instance, if

$$
f(x)=\frac{1}{1+|x|}
$$

then $f \in \mathcal{L}^{2}$ on $R^{1}$, but $f \notin \mathcal{L}$ on $R^{1}$.

Solution. This follows from Theorems 11.27 and 11.29 , if we let $A=\{x$ : $|f(x)| \leq 1\}$ and $B=\{x:|f(x)|>1\}$. We can then write

$$
|f| \leq \chi_{A}+\chi_{B} \cdot|f|^{2} .
$$

and $\chi_{A}$ is integrable by Theorem $11.23(a)$.

As for the counterexample, we have

$$
|f|^{2} \leq \chi_{[-1,1]}+\chi_{[1, \infty)} \cdot \frac{1}{x^{2}}
$$

which implies that $f \in \mathcal{L}^{2}$, and

$$
f(x) \geq \chi_{[0, n]}(x) \frac{1}{1+x},
$$

so that

$$
\int f d x \geq \int_{0}^{n} \frac{1}{1+x} d x=\ln (1+n) \rightarrow \infty
$$

Hence $f \notin \mathcal{L}$.

Exercise 11.11 If $f, g \in \mathcal{L}(\mu)$ on $X$, define the distance between $f$ and $g$ by

$$
\int_{X}|f-g| d \mu
$$

Prove that $\mathcal{L}(\mu)$ is a complete metric space.

Solution. We have to regard functions equal almost everywhere as the same function. Given that, it does follow that if $d(f, g)=0$, then $f=g$. The fact that $d(f, g)=d(g, f)$ is immediate from the definition and the triangle inequality follows from simply integrating the triangle inequality for the values of the functions. Hence $\mathcal{L}$ is a metric space.

To prove that it is complete, we merely repeat the reasoning of Theorem 11.42, replacing $\mathcal{L}^{2}$ by $\mathcal{L}$ and taking the function $g(x)$ to be identically equal to 1. When this is done, every step in the proof of Theorem 11.42 follows for $\mathcal{L}$.

Exercise 11.12 Suppose

(a) $\mid f(x, y) \leq 1$ if $0 \leq x \leq 1,0 \leq y \leq 1$,

(b) for fixed $x, f(x, y)$ is a continuous function of $y$,

(c) for fixed $y, f(x, y)$ is a continuous function of $x$.

Put

$$
g(x)=\int_{0}^{1} f(x, y) d y \quad(0 \leq x \leq 1)
$$

Is $g$ continuous?

Solution. Yes, $g(x)$ is continuous. Let $x_{n} \rightarrow x$. Then by $(c), f\left(x_{n}, y\right) \rightarrow f(x, y)$ for each $y \in[0,1]$, in particular for almost every $y$. Since $\left|f\left(x_{n}, y\right)\right| \leq 1$ for all $x_{n}$ and $y$ by assumption $(a)$, and the set $[0,1]$ has finite measure, it follows from the dominated convergence theorem that $g\left(x_{n}\right) \rightarrow g(x)$.

Note that property $(b)$ was used only to guarantee that $g(x)$ is actually defined. Thus the word continuous could be replaced by integrable in this condition.

Exercise 11.13 Consider the functions

$$
f_{n}(x)=\sin n x \quad(n=1,2,3, \ldots,-\pi \leq x \leq \pi)
$$

as points of $\mathcal{L}^{2}$. Prove that the set of these points is closed and bounded, but not compact.

Solution. We compute by brute force that

$$
\left\|f_{m}-f_{n}\right\|^{2}= \begin{cases}0, & \text { if } m=n \\ 2 \pi, & \text { if } m \neq n\end{cases}
$$

Further, it is easy to see that $\left\|f_{n}\right\|^{2}=\pi$. Hence the set $\left\{f_{n}\right\}$ is bounded and has no limit points. (The $\sqrt{\frac{\pi}{2}}$-neighborhood of any point contains at most one point of this set.) Having no limit points, it contains all of its limit points and is therefore closed. Being infinite, if it were compact, it would have a limit point. Therefore it is not compact.

Exercise 11.14 Prove that a complex function $f$ is measurable if and only if $f^{-1}(V)$ is measurable for every open set $V$ in the plane.

Solution. By definition $f=u+i v$, where $u$ and $v$ are real-valued, is measurable if and only if $u$ and $v$ are.

Suppose $f$ is measurable (that is, $u$ and $v$ are measurable). Let $V$ be any open set in the plane and $(x, y) \in V$. Then there exists $\delta>0$ such that the square $S(x, y)=(x-\delta, x+\delta) \times(y-\delta, y+\delta)$ is contained in $V$. The union of these open squares is all of $V$, and there is a countable set of points $\left(x_{n}, y_{n}\right) \in V$ such that $\bigcup_{n=1}^{\infty} S\left(x_{n}, y_{n}\right)=V$. (This is proved by appealing to Exercise 23 of Chapter 2.) But then

$$
f^{-1}(V)=\bigcup_{n=1}^{\infty} f^{-1}\left(S\left(x_{n}, y_{n}\right)\right)=\bigcup_{n=1}^{\infty} u^{-1}\left(x_{n}-\delta, x_{n}+\delta\right) \cap v^{-1}\left(y_{n}-\delta, y_{n}+\delta\right)
$$

It follows that $f^{-1}(V)$ is measurable.

Conversely if $f^{-1}(V)$ is measurable for every open set in the plane, then in particular this set is measurable if $V=(a, b) \times R^{1}$ (where $f^{-1}(V)=u^{-1}((a, b))$ ) or $V=R^{1} \times(a, b)$ (where $f^{-1}(V)=v^{-1}((a, b))$ ), and hence both $u$ and $v$ are measurable. By definition, that means that $f$ is measurable.

Exercise 11.15 Let $\mathcal{R}$ be the ring of all elementary subsets of $(0,1]$. If $0<$ $a \leq b \leq 1$, define

$$
\phi([a, b])=\phi([a, b))=\phi((a, b])=\phi((a, b))=b-a
$$

but define

$$
\phi((0, b))=\phi((0, b])=1+b
$$

if $0<b \leq 1$. Show that this gives an additive set function $\phi$ on $\mathcal{R}$, which is not regular and which cannot be extended to a countably additive set function on a $\sigma$-ring.

Solution. In brief, since an elementary set $A$ is a finite disjoint union of intervals, $\phi(A)$ is the sum of the lengths of those intervals if 0 is not the endpoint of any interval in $A$ and 1 larger than the sum of the lengths of the intervals if 0 is one of the endpoints. In particular $\phi(A)<1$ if $A$ is a closed set, since 0 cannot be the endpoint of any closed set that is a finite union of intervals in $(0,1]$.

(This alternate definition is independent of the particular way in which the set $A$ is represented as a finite disjoint union of intervals, since if $A=\bigcup_{i=1}^{m} I_{i}=$ $\bigcup_{j=1}^{n} J_{j}$, where each of the collections $\left\{I_{i}\right\}$ and $\left\{J_{j}\right\}$ is a set of pairwise disjoint intervals, one can easily verify that

$$
\left|I_{i}\right|=\sum_{j=1}^{n}\left|I_{i} \cap J_{j}\right|, \quad\left|J_{j}\right|=\sum_{i=1}^{m}\left|I_{i} \cap J_{j}\right|
$$

so that $\sum_{i=1}^{m}\left|I_{i}\right|=\sum_{j=1}^{n}\left|J_{j}\right|=\sum_{i, j}\left|I_{i} \cap J_{j}\right|$. HEre $|I|$ is the length of the interval $I$.)

If two elementary sets $A$ and $B$ are disjoint, at most one of them can have the point 0 as the endpoint of one of its intervals. Then $\phi(A \cup B)$ is the sum of the lengths of the intervals in $A \cup B$ if neither set contains an interval having 0 as the endpoint, and 1 larger than this sum if one of them does contain an interval with 0 as endpoint. In either case $\phi(A \cup B)=\phi(A)+\phi(B)$ when $A \cap B=\emptyset$. Thus the function $\phi$ is additive.

The function $\phi$ is not regular, however, since there is no closed subset of $(0, c]$ that can approximate $(0, c]$ if $c<1$. For $\phi((0, c])=1+c$, but $\phi(A) \leq 1$ if $A$ is closed.

The function $\phi$ also cannot be extended to a countably additive set function on a $\sigma$-ring, since

$$
\left(0, \frac{1}{2}\right]=\bigcup_{n=1}^{\infty}\left(\frac{1}{2^{n+1}}, \frac{1}{2^{n}}\right)
$$

and

$$
\phi\left(\left(0, \frac{1}{2}\right]\right)=\frac{3}{2}, \quad \sum_{n=1}^{\infty} \phi\left(\left(\frac{1}{2^{n+1}}, \frac{1}{2^{n}}\right]\right)=\frac{1}{2}
$$

Exercise 11.16 Suppose $\left\{n_{k}\right\}$ is an increasing sequence of positive integers and $E$ is the set of all $x \in(-\pi, \pi)$ at which $\left\{\sin n_{k} x\right\}$ converges. Prove that $m(E)=0$. Hint: For every $A \subset E$,

$$
\int_{A} \sin n_{k} x d x=0
$$

and

$$
2 \int_{A}\left(\sin n_{k} x\right)^{2} d x=\int_{A}\left(1-\cos 2 n_{k} x\right) d x \rightarrow m(A) \text { as } k \rightarrow \infty
$$

Solution. The two statements in the hint follow from the Riemann-Lebesgue lemma (or from Bessel's inequality applied to the Fourier series of $\chi_{A}$, if you wish). Let $f(x)$ be the limit of $\sin n_{k} x$ on the set $E$. Then, since termwise integration is justified by the dominated convergence theorem, we have

$$
\int_{A}\left[(f(x))^{2}-\frac{1}{2}\right] d x=0
$$

for all $A$. Hence, by Exercise 2 above, $f(x)= \pm \frac{1}{\sqrt{2}}$ almost everywhere on $E$. If we let $A$ be the set of points of $E$ at which $f(x)=\frac{1}{\sqrt{2}}$, we find that $\int_{A} f(x) d x=0$, and so by Exercise $1, f(x)=0$ almost everywhere on $A$. Since in fact $f(x) \neq 0$ on $A$, it follows that $A$ has measure 0 . Similarly the set where $f(x)=-\frac{1}{\sqrt{2}}$ has measure 0 .

Exercise 11.17 Suppose $E \subset(-\pi, \pi), m(E)>0, \delta>0$. Use the Bessel inequality to prove that there are at most finitely many integers $n$ such that
$\sin n x \geq \delta$ for all $x \in E$.

Solution. For any integer with this property we have

$$
\int_{E} \sin n x d x \geq \delta \mu(E)
$$

and the Bessel inequality implies that this inequality can hold for only a finite number of $n$. (The integral is the imaginary part of the Fourier coefficient of the $\mathcal{L}^{2}$-function $\chi_{E}$.)

Exercise 11.18 Suppose $f \in \mathcal{L}^{2}(\mu), g \in L^{2}(\mu)$. Prove that

$$
\left|\int f \bar{g} d \mu\right|^{2}=\int|f|^{2} d \mu \int|g|^{2} d \mu
$$

if and only if there is a constant $c$ such that $g(x)=c f(x)$ almost everywhere. (Compare Theorem 11.35.)

Solution. There is a slight mistake in the statement of the problem, since equality certainly holds if $f(x)$ is identically zero, whether $g(x)$ equals zero or not. We must either assume that $f(x)$ is not identically zero, or allow the possibility that $f(x)=c g(x)$.

Equality can hold if $g(x)=0$ almost everywhere, and in that case $c=0$ in the relation $g(x)=c f(x)$. Hence assume now that $\int|g|^{2} d \mu>0$. The inequality

$$
0 \leq \int(|f|+\lambda|g|)^{2} d \mu
$$

which holds for real values of $\lambda$, is equivalent to the inequality

$$
-2 \lambda \int|f g| d \mu \leq \int|f|^{2} d \mu+\lambda^{2} \int|g|^{2} d \mu
$$

In this inequality take $\lambda=-\sqrt{\frac{\int|f|^{2} d \mu}{\int|g|^{2} d \mu}}$. The result is

$$
2 \frac{\int|f g| d \mu \sqrt{f|f|^{2} d \mu}}{\sqrt{\int|g|^{2} d \mu}} \leq 2 \int|f|^{2} d \mu
$$

which is equivalent to

$$
\left[\int|f g| d \mu\right]^{2} \leq \int|f|^{2} d \mu \int|g|^{2} d \mu \text {. }
$$

Hence the equality in the problem can hold only if equality holds in this last equality, which, since it implies that

$$
\int(|f|+\lambda|g|)^{2} d \mu=0
$$

implies that $|f|=-\lambda|g|$ almost everywhere. In particular $f$ vanishes almost everywhere that $g$ vanishes. In addition, the equality in the hypothesis of the problem requires that

$$
\left|\int f \bar{g} d \mu\right|=\int|f g| d \mu
$$

If both sides of this last equality are zero, then at almost every point either $f(x)=0$ or $g(x)=0$. Since $|f|=-\lambda|g|$, it then follows that in fact either both functions vanish identically, a case we have already discussed, or $\lambda=0$, in which case only $f$ vanishes identically. In either case we do have the kind of linear dependence specified in the amended statement of the problem.

Hence assume that neither side of this equality is zero. Let $\omega$ be the complex number

$$
\omega=\frac{\overline{\int f \bar{g} d \mu}}{\left|\int f \bar{g} d \mu\right|}
$$

so that $|\omega|=1$. We note that

$$
\int \omega f \bar{g} d \mu=\omega \int f \bar{g} d \mu=\left|\int f \bar{g} c \mu\right| \leq \int|f \bar{g}| d \mu=\int|\omega f \bar{g}| d \mu
$$

This means that the real parts of the two integrals on the extremes here are equal, and the imaginary parts of both are zero. Taking just the real parts, since $\operatorname{Re}(\omega f \bar{g}) \leq|\omega f \bar{g}|$, this implies that the real part of $\omega f \bar{g}$ is equal to $|f g|=$ $-\lambda g g$ almost everywhere, and therefore that the imaginary part is zero almost everywhere. But then, almost everywhere where $g$ does not vanish, we can cancel $\bar{g}$ from the equality, getting $f=-\lambda \bar{\omega} g$ wherever $g$ does not vanish. Since this equality also holds almost everywhere where $g$ does vanish, we are done.

