\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}

\title{NUMERICAL SEQUENCES AND SERIES }

\author{}
\date{}


\begin{document}
\maketitle
As the title indicates, this chapter will deal primarily with sequences and series of complex numbers. The basic facts about convergence, however, are just as easily explained in a more general setting. The first three sections will therefore be concerned with sequences in euclidean spaces, or even in metric spaces.

\section{CONVERGENT SEQUENCES}
3.1 Definition A sequence $\left\{p_{n}\right\}$ in a metric space $X$ is said to converge if there is a point $p \in X$ with the following property: For every $\varepsilon>0$ there is an integer $N$ such that $n \geq N$ implies that $d\left(p_{n}, p\right)<\varepsilon$. (Here $d$ denotes the distance in $X$.) In this case we also say that $\left\{p_{n}\right\}$ converges to $p$, or that $p$ is the limit of $\left\{p_{n}\right\}$ [see Theorem 3.2(b)], and we write $p_{n} \rightarrow p$, or

$$
\lim _{n \rightarrow \infty} p_{n}=p
$$

If $\left\{p_{n}\right\}$ does not converge, it is said to diverge.

It might be well to point out that our definition of "convergent sequence" depends not only on $\left\{p_{n}\right\}$ but also on $X$; for instance, the sequence $\{1 / n\}$ converges in $R^{1}$ (to 0 ), but fails to converge in the set of all positive real numbers [with $d(x, y)=|x-y|$ ]. In cases of possible ambiguity, we can be more precise and specify "convergent in $X$ " rather than "convergent."

We recall that the set of all points $p_{n}(n=1,2,3, \ldots)$ is the range of $\left\{p_{n}\right\}$. The range of a sequence may be a finite set, or it may be infinite. The sequence $\left\{p_{n}\right\}$ is said to be bounded if its range is bounded.

As examples, consider the following sequences of complex numbers (that is, $X=R^{2}$ ):

(a) If $s_{n}=1 / n$, then $\lim _{n \rightarrow \infty} s_{n}=0$; the range is infinite, and the sequence is bounded.

(b) If $s_{n}=n^{2}$, the sequence $\left\{s_{n}\right\}$ is unbounded, is divergent, and has infinite range.

(c) If $s_{n}=1+\left[(-1)^{n} / n\right]$, the sequence $\left\{s_{n}\right\}$ converges to 1 , is bounded, and has infinite range.

(d) If $s_{n}=i^{n}$, the sequence $\left\{s_{n}\right\}$ is divergent, is bounded, and has finite range.

(e) If $s_{n}=1(n=1,2,3, \ldots)$, then $\left\{s_{n}\right\}$ converges to 1 , is bounded, and has finite range.

We now summarize some important properties of convergent sequences in metric spaces.

\subsection{Theorem Let $\left\{p_{n}\right\}$ be a sequence in a metric space $X$.}
(a) $\left\{p_{n}\right\}$ converges to $p \in X$ if and only if every neighborhood of $p$ contains $p_{n}$ for all but finitely many $n$.

(b) If $p \in X, p^{\prime} \in X$, and if $\left\{p_{n}\right\}$ converges to $p$ and to $p^{\prime}$, then $p^{\prime}=p$.

(c) If $\left\{p_{n}\right\}$ converges, then $\left\{p_{n}\right\}$ is bounded.

(d) If $E \subset X$ and if $p$ is a limit point of $E$, then there is a sequence $\left\{p_{n}\right\}$ in $E$ such that $p=\lim _{n \rightarrow \infty} p_{n}$.

Proof (a) Suppose $p_{n} \rightarrow p$ and let $V$ be a neighborhood of $p$. For some $\varepsilon>0$, the conditions $d(q, p)<\varepsilon, q \in X$ imply $q \in V$. Corresponding to this $\varepsilon$, there exists $N$ such that $n \geq N$ implies $d\left(p_{n}, p\right)<\varepsilon$. Thus $n \geq N$ implies $p_{n} \in V$.

Conversely, suppose every neighborhood of $p$ contains all but finitely many of the $p_{n}$. Fix $\varepsilon>0$, and let $V$ be the set of all $q \in X$ such that $d(p, q)<\varepsilon$. By assumption, there exists $N$ (corresponding to this $V$ ) such that $p_{n} \in V$ if $n \geq N$. Thus $d\left(p_{n}, p\right)<\varepsilon$ if $n \geq N$; hence $p_{n} \rightarrow p$.
(b) Let $\varepsilon>0$ be given. There exist integers $N, N^{\prime}$ such that

$$
\begin{aligned}
& n \geq N \quad \text { implies } \quad d\left(p_{n}, p\right)<\frac{\varepsilon}{2}, \\
& n \geq N^{\prime} \quad \text { implies } \quad d\left(p_{n}, p^{\prime}\right)<\frac{\varepsilon}{2}
\end{aligned}
$$

Hence if $n \geq \max \left(N, N^{\prime}\right)$, we have

$$
d\left(p, p^{\prime}\right) \leq d\left(p, p_{n}\right)+d\left(p_{n}, p^{\prime}\right)<\varepsilon .
$$

Since $\varepsilon$ was arbitrary, we conclude that $d\left(p, p^{\prime}\right)=0$.

(c) Suppose $p_{n} \rightarrow p$. There is an integer $N$ such that $n>N$ implies $d\left(p_{n}, p\right)<1$. Put

$$
r=\max \left\{1, d\left(p_{1}, p\right), \ldots, d\left(p_{N}, p\right)\right\}
$$

Then $d\left(p_{n}, p\right) \leq r$ for $n=1,2,3, \ldots$

(d) For each positive integer $n$, there is a point $p_{n} \in E$ such that $d\left(p_{n}, p\right)<1 / n$. Given $\varepsilon>0$, choose $N$ so that $N \varepsilon>1$. If $n>N$, it follows that $d\left(p_{n}, p\right)<\varepsilon$. Hence $p_{n} \rightarrow p$.

This completes the proof.

For sequences in $R^{k}$ we can study the relation between convergence, on the one hand, and the algebraic operations on the other. We first consider sequences of complex numbers.

3.3 Theorem Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then
(a) $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$
(b) $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;
(c) $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$;
(d) $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.

\section{Proof}
(a) Given $\varepsilon>0$, there exist integers $N_{1}, N_{2}$ such that

$$
\begin{array}{ll}
n \geq N_{1} \quad \text { implies } & \left|s_{n}-s\right|<\frac{\varepsilon}{2}, \\
n \geq N_{2} \quad \text { implies } & \left|t_{n}-t\right|<\frac{\varepsilon}{2}
\end{array}
$$

If $N=\max \left(N_{1}, N_{2}\right)$, then $n \geq N$ implies

$$
\left|\left(s_{n}+t_{n}\right)-(s+t)\right| \leq\left|s_{n}-s\right|+\left|t_{n}-t\right|<\varepsilon
$$

This proves $(a)$. The proof of $(b)$ is trivial.

(c) We use the identity

$$
s_{n} t_{n}-s t=\left(s_{n}-s\right)\left(t_{n}-t\right)+s\left(t_{n}-t\right)+t\left(s_{n}-s\right)
$$

Given $\varepsilon>0$, there are integers $N_{1}, N_{2}$ such that

$$
\begin{aligned}
& n \geq N_{1} \quad \text { implies } \quad\left|s_{n}-s\right|<\sqrt{\varepsilon} \\
& n \geq N_{2} \text { implies }\left|t_{n}-t\right|<\sqrt{\varepsilon} \text {. }
\end{aligned}
$$

If we take $N=\max \left(N_{1}, N_{2}\right), n \geq N$ implies

$$
\left|\left(s_{n}-s\right)\left(t_{n}-t\right)\right|<\varepsilon
$$

so that

$$
\lim _{n \rightarrow \infty}\left(s_{n}-s\right)\left(t_{n}-t\right)=0
$$

We now apply $(a)$ and $(b)$ to (1), and conclude that

$$
\lim _{n \rightarrow \infty}\left(s_{n} t_{n}-s t\right)=0
$$

(d) Choosing $m$ such that $\left|s_{n}-s\right|<\frac{1}{2}|s|$ if $n \geq m$, we see that

$$
\left|s_{n}\right|>\frac{1}{2}|s| \quad(n \geq m) .
$$

Given $\varepsilon>0$, there is an integer $N>m$ such that $n \geq N$ implies

$$
\left|s_{n}-s\right|<\frac{1}{2}|s|^{2} \varepsilon
$$

Hence, for $n \geq N$,

$$
\left|\frac{1}{s_{n}}-\frac{1}{s}\right|=\left|\frac{s_{n}-s}{s_{n} s}\right|<\frac{2}{|s|^{2}}\left|s_{n}-s\right|<\varepsilon
$$

\subsection{Theorem}
(a) Suppose $\mathbf{x}_{n} \in R^{k}(n=1,2,3, \ldots)$ and

$$
\mathbf{x}_{n}=\left(\alpha_{1, n}, \ldots, \alpha_{k, n}\right)
$$

Then $\left\{\mathbf{x}_{n}\right\}$ converges to $\mathbf{x}=\left(\alpha_{1}, \ldots, \alpha_{k}\right)$ if and only if

$$
\lim _{n \rightarrow \infty} \alpha_{J, n}=\alpha_{J} \quad(1 \leq j \leq k)
$$

(b) Suppose $\left\{\mathbf{x}_{n}\right\},\left\{\mathbf{y}_{n}\right\}$ are sequences in $R^{k},\left\{\beta_{n}\right\}$ is a sequence of real numbers, and $\mathbf{x}_{n} \rightarrow \mathbf{x}, \mathbf{y}_{n} \rightarrow \mathbf{y}, \beta_{n} \rightarrow \beta$. Then

$$
\lim _{n \rightarrow \infty}\left(\mathbf{x}_{n}+\mathbf{y}_{n}\right)=\mathbf{x}+\mathbf{y}, \quad \lim _{n \rightarrow \infty} \mathbf{x}_{n} \cdot \mathbf{y}_{n}=\mathbf{x} \cdot \mathbf{y}, \quad \lim _{n \rightarrow \infty} \beta_{n} \mathbf{x}_{n}=\beta \mathbf{x}
$$

Proof

(a) If $\mathbf{x}_{n} \rightarrow \mathbf{x}$, the inequalities

$$
\left|\alpha_{j, n}-\alpha_{j}\right| \leq\left|\mathbf{x}_{n}-\mathbf{x}\right|,
$$

which follow immediately from the definition of the norm in $R^{k}$, show that (2) holds.

Conversely, if (2) holds, then to each $\varepsilon>0$ there corresponds an integer $N$ such that $n \geq N$ implies

$$
\left|\alpha_{j, n}-\alpha_{j}\right|<\frac{\varepsilon}{\sqrt{k}} \quad(1 \leq j \leq k)
$$

Hence $n \geq N$ implies

$$
\left|\mathbf{x}_{n}-\mathbf{x}\right|=\left\{\sum_{j=1}^{k}\left|\alpha_{j, n}-\alpha_{j}\right|^{2}\right\}^{1 / 2}<\varepsilon,
$$

so that $\mathbf{x}_{n} \rightarrow \mathbf{x}$. This proves $(a)$.

Part (b) follows from $(a)$ and Theorem 3.3.

\section{SUBSEQUENCES}
3.5 Definition Given a sequence $\left\{p_{n}\right\}$, consider a sequence $\left\{n_{k}\right\}$ of positive integers, such that $n_{1}<n_{2}<n_{3}<\cdots$. Then the sequence $\left\{p_{n}\right\}$ is called a subsequence of $\left\{p_{n}\right\}$. If $\left\{p_{n}\right\}$ converges, its limit is called a subsequential limit of $\left\{p_{n}\right\}$.

It is clear that $\left\{p_{n}\right\}$ converges to $p$ if and only if every subsequence of $\left\{p_{n}\right\}$ converges to $p$. We leave the details of the proof to the reader.

\subsection{Theorem}
(a) If $\left\{p_{n}\right\}$ is a sequence in a compact metric space $X$, then some subsequence of $\left\{p_{n}\right\}$ converges to a point of $X$.

(b) Every bounded sequence in $R^{k}$ contains a convergent subsequence.

\section{Proof}
(a) Let $E$ be the range of $\left\{p_{n}\right\}$. If $E$ is finite then there is a $p \in E$ and a sequence $\left\{n_{l}\right\}$ with $n_{1}<n_{2}<n_{3}<\cdots$, such that

$$
p_{n_{1}}=p_{n_{2}}=\cdots=p .
$$

The subsequence $\left\{p_{n_{t}}\right\}$ so obtained converges evidently to $p$.

If $E$ is infinite, Theorem 2.37 shows that $E$ has a limit point $p \in X$. Choose $n_{1}$ so that $d\left(p, p_{n_{1}}\right)<1$. Having chosen $n_{1}, \ldots, n_{i-1}$, we see from Theorem 2.20 that there is an integer $n_{l}>n_{i-1}$ such that $d\left(p, p_{n}\right)<1 / i$. Then $\left\{p_{n_{1}}\right\}$ converges to $p$.

(b) This follows from (a), since Theorem 2.41 implies that every bounded subset of $R^{k}$ lies in a compact subset of $R^{k}$.

3.7 Theorem The subsequential limits of a sequence $\left\{p_{n}\right\}$ in a metric space $X$ form a closed subset of $X$.

Proof Let $E^{*}$ be the set of all subsequential limits of $\left\{p_{n}\right\}$ and let $q$ be a limit point of $E^{*}$. We have to show that $q \in E^{*}$.

Choose $n_{1}$ so that $p_{n_{1}} \neq q$. (If no such $n_{1}$ exists, then $E^{*}$ has only one point, and there is nothing to prove.) Put $\delta=d\left(q, p_{n_{1}}\right)$. Suppose $n_{1}, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E^{*}$, there is an $x \in E^{*}$ with $d(x, q)<2^{-1} \delta$. Since $x \in E^{*}$, there is an $n_{l}>n_{i-1}$ such that $d\left(x, p_{n_{1}}\right)<2^{-i} \delta$. Thus

$$
d\left(q, p_{n}\right) \leq 2^{1-i} \delta
$$

for $i=1,2,3, \ldots$. This says that $\left\{p_{n}\right\}$ converges to $q$. Hence $q \in E^{*}$.

\section{CAUCHY SEQUENCES}
3.8 Definition A sequence $\left\{p_{n}\right\}$ in a metric space $X$ is said to be a Cauchy sequence if for every $\varepsilon>0$ there is an integer $N$ such that $d\left(p_{n}, p_{m}\right)<\varepsilon$ if $n \geq N$ and $m \geq N$.

In our discussion of Cauchy sequences, as well as in other situations which will arise later, the following geometric concept will be useful.

3.9 Definition Let $E$ be a nonempty subset of a metric space $X$, and let $S$ be the set of all real numbers of the form $d(p, q)$, with $p \in E$ and $q \in E$. The sup of $S$ is called the diameter of $E$.

If $\left\{p_{n}\right\}$ is a sequence in $X$ and if $E_{N}$ consists of the points $p_{N}, p_{N+1}, p_{N+2}, \ldots$, it is clear from the two preceding definitions that $\left\{p_{n}\right\}$ is a Cauchy sequence if and only if

$$
\lim _{N \rightarrow \infty} \operatorname{diam} E_{N}=0
$$

\subsection{Theorem}
(a) If $\bar{E}$ is the closure of a set $E$ in a metric space $X$, then

$$
\operatorname{diam} \bar{E}=\operatorname{diam} E .
$$

(b) If $K_{n}$ is a sequence of compact sets in $X$ such that $K_{n} \supset K_{n+1}$ $(n=1,2,3, \ldots)$ and if

$$
\lim _{n \rightarrow \infty} \operatorname{diam} K_{n}=0 \text {, }
$$

then $\bigcap_{1}^{\infty} K_{n}$ consists of exactly one point.

Proof

(a) Since $E \subset \bar{E}$, it is clear that

$$
\operatorname{diam} E \leq \operatorname{diam} \bar{E} .
$$

Fix $\varepsilon>0$, and choose $p \in \bar{E}, q \in \bar{E}$. By the definition of $\bar{E}$, there are points $p^{\prime}, q^{\prime}$, in $E$ such that $d\left(p, p^{\prime}\right)<\varepsilon, d\left(q, q^{\prime}\right)<\varepsilon$. Hence

$$
\begin{aligned}
d(p, q) & \leq d\left(p, p^{\prime}\right)+d\left(p^{\prime} q^{\prime}\right)+d\left(q^{\prime}, q\right) \\
& <2 \varepsilon+d\left(p^{\prime}, q^{\prime}\right) \leq 2 \varepsilon+\operatorname{diam} E
\end{aligned}
$$

It follows that

$$
\operatorname{diam} \bar{E} \leq 2 \varepsilon+\operatorname{diam} E
$$

and since $\varepsilon$ was arbitrary, $(a)$ is proved.

(b) Put $K=\bigcap_{1}^{\infty} K_{n}$. By Theorem 2.36, $K$ is not empty. If $K$ contains more than one point, then $\operatorname{diam} K>0$. But for each $n, K_{n} \supset K$, so that $\operatorname{diam} K_{n} \geq \operatorname{diam} K$. This contradicts the assumption that diam $K_{n} \rightarrow 0$.

\subsection{Theorem}
(a) In any metric space $X$, every convergent sequence is a Cauchy sequence.

(b) If $X$ is a compact metric space and if $\left\{p_{n}\right\}$ is a Cauchy sequence in $X$, then $\left\{p_{n}\right\}$ converges to some point of $X$.

(c) In $R^{k}$, every Cauchy sequence converges.

Note: The difference between the definition of convergence and the definition of a Cauchy sequence is that the limit is explicitly involved in the former, but not in the latter. Thus Theorem 3.11(b) may enable us
to decide whether or not a given sequence converges without knowledge of the limit to which it may converge.

The fact (contained in Theorem 3.11) that a sequence converges in $R^{k}$ if and only if it is a Cauchy sequence is usually called the Cauchy criterion for convergence.

\section{Proof}
(a) If $p_{n} \rightarrow p$ and if $\varepsilon>0$, there is an integer $N$ such that $d\left(p, p_{n}\right)<\varepsilon$ for all $n \geq N$. Hence

$$
d\left(p_{n}, p_{m}\right) \leq d\left(p_{n}, p\right)+d\left(p, p_{m}\right)<2 \varepsilon
$$

as soon as $n \geq N$ and $m \geq N$. Thus $\left\{p_{n}\right\}$ is a Cauchy sequence.

(b) Let $\left\{p_{n}\right\}$ be a Cauchy sequence in the compact space $X$. For $N=1,2,3, \ldots$, let $E_{N}$ be the set consisting of $p_{N}, p_{N+1}, p_{N+2}, \ldots$. Then

$$
\lim _{N \rightarrow \infty} \operatorname{diam} \bar{E}_{N}=0
$$

by Definition 3.9 and Theorem 3.10(a). Being a closed subset of the compact space $X$, each $\bar{E}_{N}$ is compact (Theorem 2.35). Also $E_{N} \supset E_{N+1}$, so that $\bar{E}_{N} \supset \bar{E}_{N+1}$.

Theorem 3.10(b) shows now that there is a unique $p \in X$ which lies in every $\bar{E}_{N}$.

Let $\varepsilon>0$ be given. By (3) there is an integer $N_{0}$ such that $\operatorname{diam} \bar{E}_{N}<\varepsilon$ if $N \geq N_{0}$. Since $p \in \bar{E}_{N}$, it follows that $d(p, q)<\varepsilon$ for every $q \in \bar{E}_{N}$, hence for every $q \in E_{N}$. In other words, $d\left(p, p_{n}\right)<\varepsilon$ if $n \geq N_{0}$. This says precisely that $p_{n} \rightarrow p$.

(c) Let $\left\{\mathbf{x}_{n}\right\}$ be a Cauchy sequence in $R^{k}$. Define $E_{N}$ as in (b), with $\mathbf{x}_{i}$ in place of $p_{i}$. For some $N$, diam $E_{N}<1$. The range of $\left\{\mathbf{x}_{n}\right\}$ is the union of $E_{N}$ and the finite set $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N-1}\right\}$. Hence $\left\{\mathbf{x}_{n}\right\}$ is bounded. Since every bounded subset of $R^{k}$ has compact closure in $R^{k}$ (Theorem 2.41), $(c)$ follows from $(b)$.

3.12 Definition A metric space in which every Cauchy sequence converges is said to be complete.

Thus Theorem 3.11 says that all compact metric spaces and all Euclidean spaces are complete. Theorem 3.11 implies also that every closed subset $E$ of a complete metric space $X$ is complete. (Every Cauchy sequence in $E$ is a Cauchy sequence in $X$, hence it converges to some $p \in X$, and actually $p \in E$ since $E$ is closed.) An example of a metric space which is not complete is the space of all rational numbers, with $d(x, y)=|x-y|$.

Theorem 3.2(c) and example (d) of Definition 3.1 show that convergent sequences are bounded, but that bounded sequences in $R^{k}$ need not converge. However, there is one important case in which convergence is equivalent to boundedness; this happens for monotonic sequences in $R^{\mathbf{1}}$.

3.13 Definition A sequence $\left\{s_{n}\right\}$ of real numbers is said to be

(a) monotonically increasing if $s_{n} \leq s_{n+1}(n=1,2,3, \ldots)$;

(b) monotonically decreasing if $s_{n} \geq s_{n+1}(n=1,2,3, \ldots)$.

The class of monotonic sequences consists of the increasing and the decreasing sequences.

3.14 Theorem Suppose $\left\{s_{n}\right\}$ is monotonic. Then $\left\{s_{n}\right\}$ converges if and only if it is bounded.

Proof Suppose $s_{n} \leq s_{n+1}$ (the proof is analogous in the other case). Let $E$ be the range of $\left\{s_{n}\right\}$. If $\left\{s_{n}\right\}$ is bounded, let $s$ be the least upper bound of $E$. Then

$$
s_{n} \leq s \quad(n=1,2,3, \ldots)
$$

For every $\varepsilon>0$, there is an integer $N$ such that

$$
s-\varepsilon<s_{N} \leq s
$$

for otherwise $s-\varepsilon$ would be an upper bound of $E$. Since $\left\{s_{n}\right\}$ increases, $n \geq N$ therefore implies

$$
s-\varepsilon<s_{n} \leq s
$$

which shows that $\left\{s_{n}\right\}$ converges (to $s$ ).

The converse follows from Theorem 3.2(c).

\section{UPPER AND LOWER LIMITS}
3.15 Definition Let $\left\{s_{n}\right\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_{n} \geq M$. We then write

$$
s_{n} \rightarrow+\infty
$$

Similarly, if for every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_{n} \leq M$, we write

$$
s_{n} \rightarrow-\infty
$$

It should be noted that we now use the symbol $\rightarrow$ (introduced in Definition 3.1) for certain types of divergent sequences, as well as for convergent sequences, but that the definitions of convergence and of limit, given in Definition 3.1, are in no way changed.

3.16 Definition Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_{k}} \rightarrow x$ for some subsequence $\left\{s_{n_{k}}\right\}$. This set $E$ contains all subsequential limits as defined in Definition 3.5, plus possibly the numbers $+\infty,-\infty$.

We now recall Definitions 1.8 and 1.23 and put

$$
\begin{aligned}
s^{*} & =\sup E \\
s_{*} & =\inf E
\end{aligned}
$$

The numbers $s^{*}, s_{*}$ are called the upper and lower limits of $\left\{s_{n}\right\}$; we use the notation

$$
\limsup _{n \rightarrow \infty} s_{n}=s^{*}, \quad \liminf _{n \rightarrow \infty} s_{n}=s_{*} .
$$

3.17 Theorem Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ and $s^{*}$ have the same meaning as in Definition 3.16. Then $s^{*}$ has the following two properties:

(a) $s^{*} \in E$.

(b) If $x>s^{*}$, there is an integer $N$ such that $n \geq N$ implies $s_{n}<x$.

Moreover, $s^{*}$ is the only number with the properties $(a)$ and $(b)$.

Of course, an analogous result is true for $s_{*}$.

\section{Proof}
(a) If $s^{*}=+\infty$, then $E$ is not bounded above; hence $\left\{s_{n}\right\}$ is not bounded above, and there is a subsequence $\left\{s_{n_{k}}\right\}$ such that $s_{n_{k}} \rightarrow+\infty$.

If $s^{*}$ is real, then $E$ is bounded above, and at least one subsequential limit exists, so that $(a)$ follows from Theorems 3.7 and 2.28.

If $s^{*}=-\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Hence, for any real $M, s_{n}>M$ for at most a finite number of values of $n$, so that $s_{n} \rightarrow-\infty$.

This establishes $(a)$ in all cases.

(b) Suppose there is a number $x>s^{*}$ such that $s_{n} \geq x$ for infinitely many values of $n$. In that case, there is a number $y \in E$ such that $y \geq x>s^{*}$, contradicting the definition of $s^{*}$.

Thus $s^{*}$ satisfies $(a)$ and $(b)$.

To show the uniqueness, suppose there are two numbers, $p$ and $q$, which satisfy $(a)$ and $(b)$, and suppose $p<q$. Choose $x$ such that $p<x<q$. Since $p$ satisfies $(b)$, we have $s_{n}<x$ for $n \geq N$. But then $q$ cannot satisfy $(a)$.

\subsection{Examples}
(a) Let $\left\{s_{n}\right\}$ be a sequence containing all rationals. Then every real number is a subsequential limit, and

$$
\limsup _{n \rightarrow \infty} s_{n}=+\infty, \quad \liminf _{n \rightarrow \infty} s_{n}=-\infty
$$

(b) Let $s_{n}=\left(-1^{n}\right) /[1+(1 / n)]$. Then

$$
\limsup _{n \rightarrow \infty} s_{n}=1, \quad \liminf _{n \rightarrow \infty} s_{n}=-1 .
$$

(c) For a real-valued sequence $\left\{s_{n}\right\}, \lim _{n \rightarrow \infty} s_{n}=s$ if and only if

$$
\limsup _{n \rightarrow \infty} s_{n}=\liminf _{n \rightarrow \infty} s_{n}=s .
$$

We close this section with a theorem which is useful, and whose proof is quite trivial:

3.19 Theorem If $s_{n} \leq t_{n}$ for $n \geq N$, where $N$ is fixed, then

$$
\liminf _{n \rightarrow \infty} s_{n} \leq \liminf _{n \rightarrow \infty} t_{n}
$$

$\limsup _{n \rightarrow \infty} s_{n} \leq \limsup _{n \rightarrow \infty} t_{n}$.

\section{SOME SPECIAL SEQUENCES}
We shall now compute the limits of some sequences which occur frequently. The proofs will all be based on the following remark: If $0 \leq x_{n} \leq s_{n}$ for $n \geq N$, where $N$ is some fixed number, and if $s_{n} \rightarrow 0$, then $x_{n} \rightarrow 0$.

\subsection{Theorem}
(a) If $p>0$, then $\lim _{n \rightarrow \infty} \frac{1}{n^{p}}=0$.

(b) If $p>0$, then $\lim _{n \rightarrow \infty} \sqrt[n]{p}=1$.

(c) $\lim _{n \rightarrow \infty} \sqrt[n]{n}=1$.

(d) If $p>0$ and $\alpha$ is real, then $\lim _{n \rightarrow \infty} \frac{n^{\alpha}}{(1+p)^{n}}=0$.

(e) If $|x|<1$, then $\lim _{n \rightarrow \infty} x^{n}=0$.

\section{Proof}
(a) Take $n>(1 / \varepsilon)^{1 / p}$. (Note that the archimedean property of the real number system is used here.)

(b) If $p>1$, put $x_{n}=\sqrt[n]{p}-1$. Then $x_{n}>0$, and, by the binomial theorem,

$$
1+n x_{n} \leq\left(1+x_{n}\right)^{n}=p
$$

so that

$$
0<x_{n} \leq \frac{p-1}{n} \text {. }
$$

Hence $x_{n} \rightarrow 0$. If $p=1,(b)$ is trivial, and if $0<p<1$, the result is obtained by taking reciprocals.

(c) Put $x_{n}=\sqrt[n]{n}-1$. Then $x_{n} \geq 0$, and, by the binomial theorem,

$$
n=\left(1+x_{n}\right)^{n} \geq \frac{n(n-1)}{2} x_{n}^{2}
$$

Hence

$$
0 \leq x_{n} \leq \sqrt{\frac{2}{n-1}} \quad(n \geq 2)
$$

(d) Let $k$ be an integer such that $k>\alpha, k>0$. For $n>2 k$,

$$
(1+p)^{n}>\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}=\frac{n(n-1) \cdots(n-k+1)}{k !} p^{k}>\frac{n^{k} p^{k}}{2^{k} k !}
$$

Hence

$$
0<\frac{n^{\alpha}}{(1+p)^{n}}<\frac{2^{k} k !}{p^{k}} n^{\alpha-k} \quad(n>2 k)
$$

Since $\alpha-k<0, n^{\alpha-k} \rightarrow 0$, by $(a)$.

(e) Take $\alpha=0$ in $(d)$.

\section{SERIES}
In the remainder of this chapter, all sequences and series under consideration will be complex-valued, unless the contrary is explicitly stated. Extensions of some of the theorems which follow, to series with terms in $R^{k}$, are mentioned in Exercise 15.

3.21 Definition Given a sequence $\left\{a_{n}\right\}$, we use the notation

$$
\sum_{n=p}^{q} a_{n} \quad(p \leq q)
$$

to denote the sum $a_{p}+a_{p+1}+\cdots+a_{q}$. With $\left\{a_{n}\right\}$ we associate a sequence $\left\{s_{n}\right\}$, where

$$
s_{n}=\sum_{k=1}^{n} a_{k}
$$

For $\left\{s_{n}\right\}$ we also use the symbolic expression

or, more concisely,

$$
a_{1}+a_{2}+a_{3}+\cdots
$$

$$
\sum_{n=1}^{\infty} a_{n}
$$

The symbol (4) we call an infinite series, or just a series. The numbers $s_{n}$ are called the partial sums of the series. If $\left\{s_{n}\right\}$ converges to $s$, we say that the series converges, and write

$$
\sum_{n=1}^{\infty} a_{n}=s
$$

The number $s$ is called the sum of the series; but it should be clearly understood that $s$ is the limit of a sequence of sums, and is not obtained simply by addition.

If $\left\{s_{n}\right\}$ diverges, the series is said to diverge.

Sometimes, for convenience of notation, we shall consider series of the form

$$
\sum_{n=0}^{\infty} a_{n}
$$

And frequently, when there is no possible ambiguity, or when the distinction is immaterial, we shall simply write $\Sigma a_{n}$ in place of (4) or (5).

It is clear that every theorem about sequences can be stated in terms of series (putting $a_{1}=s_{1}$, and $a_{n}=s_{n}-s_{n-1}$ for $n>1$ ), and vice versa. But it is nevertheless useful to consider both concepts. form:

The Cauchy criterion (Theorem 3.11) can be restated in the following

3.22 Theorem $\Sigma a_{n}$ converges if and only if for every $\varepsilon>0$ there is an integer $N$ such that

if $m \geq n \geq N$.

$$
\left|\sum_{k=n}^{m} a_{k}\right| \leq \varepsilon
$$

In particular, by taking $m=n$, (6) becomes

$$
\left|a_{n}\right| \leq \varepsilon \quad(n \geq N)
$$

In other words:

3.23 Theorem If $\Sigma a_{n}$ converges, then $\lim _{n \rightarrow \infty} a_{n}=0$.

The condition $a_{n} \rightarrow 0$ is not, however, sufficient to ensure convergence of $\Sigma a_{n}$. For instance, the series

$$
\sum_{n=1}^{\infty} \frac{1}{n}
$$

diverges; for the proof we refer to Theorem 3.28.

Theorem 3.14, concerning monotonic sequences, also has an immediate counterpart for series.

3.24 Theorem $A$ series of nonnegative ${ }^{1}$ terms converges if and only if its partial sums form a bounded sequence.

We now turn to a convergence test of a different nature, the so-called "comparison test."

\subsection{Theorem}
(a) If $\left|a_{n}\right| \leq c_{n}$ for $n \geq N_{0}$, where $N_{0}$ is some fixed integer, and if $\Sigma c_{n}$ converges, then $\Sigma a_{n}$ converges.

(b) If $a_{n} \geq d_{n} \geq 0$ for $n \geq N_{0}$, and if $\Sigma d_{n}$ diverges, then $\Sigma a_{n}$ diverges.

Note that (b) applies only to series of nonnegative terms $a_{n}$.

Proof Given $\varepsilon>0$, there exists $N \geq N_{0}$ such that $m \geq n \geq N$ implies

$$
\sum_{k=n}^{m} c_{k} \leq \varepsilon
$$

by the Cauchy criterion. Hence

$$
\left|\sum_{k=n}^{m} a_{k}\right| \leq \sum_{k=n}^{m}\left|a_{k}\right| \leq \sum_{k=n}^{m} c_{k} \leq \varepsilon
$$

and (a) follows.

Next, (b) follows from (a), for if $\Sigma a_{n}$ converges, so must $\Sigma d_{n}$ [note that $(b)$ also follows from Theorem 3.24].

1 The expression " nonnegative" always refers to real numbers.

The comparison test is a very useful one; to use it efficiently, we have to become familiar with a number of series of nonnegative terms whose convergence or divergence is known.

\section{SERIES OF NONNEGATIVE TERMS}
The simplest of all is perhaps the geometric series.

3.26 Theorem If $0 \leq x<1$, then

$$
\sum_{n=0}^{\infty} x^{n}=\frac{1}{1-x}
$$

If $x \geq 1$, the series diverges.

Proof If $x \neq 1$,

$$
s_{n}=\sum_{k=0}^{n} x^{k}=\frac{1-x^{n+1}}{1-x}
$$

The result follows if we let $n \rightarrow \infty$. For $x=1$, we get

$$
1+1+1+\cdots,
$$

which evidently diverges.

In many cases which occur in applications, the terms of the series decrease monotonically. The following theorem of Cauchy is therefore of particular interest. The striking feature of the theorem is that a rather "thin" subsequence of $\left\{a_{n}\right\}$ determines the convergence or divergence of $\Sigma a_{n}$.

3.27 Theorem Suppose $a_{1} \geq a_{2} \geq a_{3} \geq \cdots \geq 0$. Then the series $\sum_{n=1}^{\infty} a_{n}$ converges if and only if the series

$$
\sum_{k=0}^{\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\cdots
$$

converges.

Proof By Theorem 3.24, it suffices to consider boundedness of the partial sums. Let

$$
\begin{aligned}
& s_{n}=a_{1}+a_{2}+\cdots+a_{n}, \\
& t_{k}=a_{1}+2 a_{2}+\cdots+2^{k} a_{2^{k}}
\end{aligned}
$$

For $n<2^{k}$,

$$
\begin{aligned}
s_{n} & \leq a_{1}+\left(a_{2}+a_{3}\right)+\cdots+\left(a_{2^{k}}+\cdots+a_{2^{k+1}-1}\right) \\
& \leq a_{1}+2 a_{2}+\cdots+2^{k} a_{2^{k}} \\
& =t_{k}
\end{aligned}
$$

so that

On the other hand, if $n>2^{k}$,

$$
\begin{aligned}
s_{n} & \geq a_{1}+a_{2}+\left(a_{3}+a_{4}\right)+\cdots+\left(a_{2^{k-1}+1}+\cdots+a_{2^{k}}\right) \\
& \geq \frac{1}{2} a_{1}+a_{2}+2 a_{4}+\cdots+2^{k-1} a_{2^{k}} \\
& =\frac{1}{2} t_{k},
\end{aligned}
$$

so that

$$
2 s_{n} \geq t_{k}
$$

By (8) and (9), the sequences $\left\{s_{n}\right\}$ and $\left\{t_{k}\right\}$ are either both bounded or both unbounded. This completes the proof.

3.28 Theorem $\sum \frac{1}{n^{p}}$ converges if $p>1$ and diverges if $p \leq 1$.

Proof If $p \leq 0$, divergence follows from Theorem 3.23. If $p>0$, Theorem 3.27 is applicable, and we are led to the series

$$
\sum_{k=0}^{\infty} 2^{k} \cdot \frac{1}{2^{k p}}=\sum_{k=0}^{\infty} 2^{(1-p) k}
$$

Now, $2^{1-p}<1$ if and only if $1-p<0$, and the result follows by comparison with the geometric series (take $x=2^{1-p}$ in Theorem 3.26).

As a further application of Theorem 3.27, we prove:

3.29 Theorem If $p>1$,

$$
\sum_{n=2}^{\infty} \frac{1}{n(\log n)^{p}}
$$

converges; if $p \leq 1$, the series diverges.

Remark " $\log n$ " denotes the logarithm of $n$ to the base $e$ (compare Exercise 7, Chap. 1); the number $e$ will be defined in a moment (see Definition 3.30). We let the series start with $n=2$, since $\log 1=0$.

Proof The monotonicity of the logarithmic function (which will be discussed in more detail in Chap. 8) implies that $\{\log n\}$ increases. Hence $\{1 / n \log n\}$ decreases, and we can apply Theorem 3.27 to (10); this leads us to the series

$$
\sum_{k=1}^{\infty} 2^{k} \cdot \frac{1}{2^{k}\left(\log 2^{k}\right)^{p}}=\sum_{k=1}^{\infty} \frac{1}{(k \log 2)^{p}}=\frac{1}{(\log 2)^{p}} \sum_{k=1}^{\infty} \frac{1}{k^{p}}
$$

and Theorem 3.29 follows from Theorem 3.28.

This procedure may evidently be continued. For instance,

$$
\sum_{n=3}^{\infty} \frac{1}{n \log n \log \log n}
$$

diverges, whereas

$$
\sum_{n=3}^{\infty} \frac{1}{n \log n(\log \log n)^{2}}
$$

converges.

We may now observe that the terms of the series (12) differ very little from those of (13). Still, one diverges, the other converges. If we continue the process which led us from Theorem 3.28 to Theorem 3.29, and then to (12) and (13), we get pairs of convergent and divergent series whose terms differ even less than those of (12) and (13). One might thus be led to the conjecture that there is a limiting situation of some sort, a "boundary" with all convergent series on one side, all divergent series on the other side-at least as far as series with monotonic coefficients are concerned. This notion of "boundary" is of course quite vague. The point we wish to make is this: No matter how we make this notion precise, the conjecture is false. Exercises $11(b)$ and $12(b)$ may serve as illustrations.

We do not wish to go any deeper into this aspect of convergence theory, and refer the reader to Knopp's "Theory and Application of Infinite Series," Chap. IX, particularly Sec. 41 .

\section{THE NUMBER $e$}
3.30 Definition $e=\sum_{n=0}^{\infty} \frac{1}{n !}$.

Here $n !=1 \cdot 2 \cdot 3 \cdots n$ if $n \geq 1$, and $0 !=1$.

Since

$$
\begin{aligned}
s_{n} & =1+1+\frac{1}{1 \cdot 2}+\frac{1}{1 \cdot 2 \cdot 3}+\cdots+\frac{1}{1 \cdot 2 \cdots n} \\
& <1+1+\frac{1}{2}+\frac{1}{2^{2}}+\cdots+\frac{1}{2^{n-1}}<3
\end{aligned}
$$

the series converges, and the definition makes sense. In fact, the series converges very rapidly and allows us to compute $e$ with great accuracy.

It is of interest to note that $e$ can also be defined by means of another limit process; the proof provides a good illustration of operations with limits:

3.31 Theorem $\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$.

Proof Let

$$
s_{n}=\sum_{k=0}^{n} \frac{1}{k !}, \quad t_{n}=\left(1+\frac{1}{n}\right)^{n}
$$

By the binomial theorem,

$t_{n}=1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\frac{1}{3 !}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)+\cdots$

$$
+\frac{1}{n !}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdots\left(1-\frac{n-1}{n}\right)
$$

Hence $t_{n} \leq s_{n}$, so that

$$
\limsup _{n \rightarrow \infty} t_{n} \leq e,
$$

by Theorem 3.19. Next, if $n \geq m$,

$$
t_{n} \geq 1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\cdots+\frac{1}{m !}\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{m-1}{n}\right) .
$$

Let $n \rightarrow \infty$, keeping $m$ fixed. We get

$$
\liminf _{n \rightarrow \infty} t_{n} \geq 1+1+\frac{1}{2 !}+\cdots+\frac{1}{m !},
$$

so that

$$
s_{m} \leq \liminf _{n \rightarrow \infty} t_{n}
$$

Letting $m \rightarrow \infty$, we finally get

$$
e \leq \liminf _{n \rightarrow \infty} t_{n}
$$

The theorem follows from (14) and (15).

The rapidity with which the series $\sum \frac{1}{n !}$ converges can be estimated as follows: If $s_{n}$ has the same meaning as above, we have

$$
\begin{aligned}
e-s_{n} & =\frac{1}{(n+1) !}+\frac{1}{(n+2) !}+\frac{1}{(n+3) !}+\cdots \\
& <\frac{1}{(n+1) !}\left\{1+\frac{1}{n+1}+\frac{1}{(n+1)^{2}}+\cdots\right\}=\frac{1}{n ! n}
\end{aligned}
$$

so that

$$
0<e-s_{n}<\frac{1}{n ! n}
$$

Thus $s_{10}$, for instance, approximates $e$ with an error less than $10^{-7}$. The inequality (16) is of theoretical interest as well, since it enables us to prove the irrationality of $e$ very easily.

3.32 Theorem $e$ is irrational.

Proof Suppose $e$ is rational. Then $e=p / q$, where $p$ and $q$ are positive integers. By (16),

$$
0<q !\left(e-s_{q}\right)<\frac{1}{q}
$$

By our assumption, $q ! e$ is an integer. Since

$$
q ! s_{q}=q !\left(1+1+\frac{1}{2 !}+\cdots+\frac{1}{q !}\right)
$$

is an integer, we see that $q !\left(e-s_{q}\right)$ is an integer.

Since $q \geq 1$, (17) implies the existence of an integer between 0 and 1 . We have thus reached a contradiction.

Actually, $e$ is not even an algebraic number. For a simple proof of this, see page 25 of Niven's book, or page 176 of Herstein's, cited in the Bibliography.

\section{THE ROOT AND RATIO TESTS}
3.33 Theorem (Root Test) Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$.

Then

(a) if $\alpha<1, \Sigma a_{n}$ converges;

(b) if $\alpha>1, \Sigma a_{n}$ diverges;

(c) if $\alpha=1$, the test gives no information.

Proof If $\alpha<1$, we can choose $\beta$ so that $\alpha<\beta<1$, and an integer $N$ such that

$$
\sqrt[n]{\left|a_{n}\right|}<\beta
$$

for $n \geq N$ [by Theorem 3.17(b)]. That is, $n \geq N$ implies

$$
\left|a_{n}\right|<\beta^{n} \text {. }
$$

Since $0<\beta<1, \Sigma \beta^{n}$ converges. Convergence of $\Sigma a_{n}$ follows now from the comparison test. that

If $\alpha>1$, then, again by Theorem 3.17, there is a sequence $\left\{n_{k}\right\}$ such

$$
\sqrt[n_{k}]{\left|a_{n_{k}}\right|} \rightarrow \alpha .
$$

Hence $\left|a_{n}\right|>1$ for infinitely many values of $n$, so that the condition $a_{n} \rightarrow 0$, necessary for convergence of $\Sigma a_{n}$, does not hold (Theorem 3.23).

To prove $(c)$, we consider the series

$$
\sum \frac{1}{n}, \sum \frac{1}{n^{2}} .
$$

For each of these series $\alpha=1$, but the first diverges, the second converges.

\subsection{Theorem (Ratio Test) The series $\Sigma a_{n}$}
(a) converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$,

(b) diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.

Proof If condition (a) holds, we can find $\beta<1$, and an integer $N$, such that

$$
\left|\frac{a_{n+1}}{a_{n}}\right|<\beta
$$

for $n \geq N$. In particular,

$$
\begin{aligned}
& \left|a_{N+1}\right|<\beta\left|a_{N}\right|, \\
& \left|a_{N+2}\right|<\beta\left|a_{N+1}\right|<\beta^{2}\left|a_{N}\right|, \\
& \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
& \left|a_{N+p}\right|<\beta^{p}\left|a_{N}\right|
\end{aligned}
$$

That is,

$$
\left|a_{n}\right|<\left|a_{N}\right| \beta^{-N} \cdot \beta^{n}
$$

for $n \geq N$, and (a) follows from the comparison test, since $\Sigma \beta^{n}$ converges. If $\left|a_{n+1}\right| \geq\left|a_{n}\right|$ for $n \geq n_{0}$, it is easily seen that the condition $a_{n} \rightarrow 0$ does not hold, and $(b)$ follows.

Note: The knowledge that $\lim a_{n+1} / a_{n}=1$ implies nothing about the convergence of $\Sigma a_{n}$. The series $\Sigma 1 / n$ and $\Sigma 1 / n^{2}$ demonstrate this.

\subsection{Examples}
(a) Consider the series

$$
\frac{1}{2}+\frac{1}{3}+\frac{1}{2^{2}}+\frac{1}{3^{2}}+\frac{1}{2^{3}}+\frac{1}{3^{3}}+\frac{1}{2^{4}}+\frac{1}{3^{4}}+\cdots
$$

for which

$$
\begin{aligned}
& \liminf _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=\lim _{n \rightarrow \infty}\left(\frac{2}{3}\right)^{n}=0 \\
& \liminf _{n \rightarrow \infty} \sqrt[n]{a_{n}}=\lim _{n \rightarrow \infty} \sqrt[2 n]{\frac{1}{3^{n}}}=\frac{1}{\sqrt{3}}, \\
& \limsup _{n \rightarrow \infty} \sqrt[n]{a_{n}}=\lim _{n \rightarrow \infty} \sqrt[2 n]{\frac{1}{2^{n}}}=\frac{1}{\sqrt{2}}, \\
& \limsup _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=\lim _{n \rightarrow \infty} \frac{1}{2}\left(\frac{3}{2}\right)^{n}=+\infty
\end{aligned}
$$

The root test indicates convergence; the ratio test does not apply.

(b) The same is true for the series

$$
\frac{1}{2}+1+\frac{1}{8}+\frac{1}{4}+\frac{1}{32}+\frac{1}{16}+\frac{1}{128}+\frac{1}{64}+\cdots
$$

where

$$
\begin{gathered}
\liminf _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=\frac{1}{8}, \\
\limsup _{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}}=2,
\end{gathered}
$$

but

$$
\lim \sqrt[n]{a_{n}}=\frac{1}{2}
$$

3.36 Remarks The ratio test is frequently easier to apply than the root test, since it is usually easier to compute ratios than $n$th roots. However, the root test has wider scope. More precisely: Whenever the ratio test shows convergence, the root test does too; whenever the root test is inconclusive, the ratio test is too. This is a consequence of Theorem 3.37, and is illustrated by the above examples.

Neither of the two tests is subtle with regard to divergence. Both deduce divergence from the fact that $a_{n}$ does not tend to zero as $n \rightarrow \infty$.

\subsection{Theorem For any sequence $\left\{c_{n}\right\}$ of positive numbers,}
$$
\begin{aligned}
& \liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
& \limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$

Proof We shall prove the second inequality; the proof of the first is quite similar. Put

$$
\alpha=\limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
$$

If $\alpha=+\infty$, there is nothing to prove. If $\alpha$ is finite, choose $\beta>\alpha$. There is an integer $N$ such that

$$
\frac{c_{n+1}}{c_{n}} \leq \beta
$$

for $n \geq N$. In particular, for any $p>0$,

$$
c_{N+k+1} \leq \beta c_{N+k} \quad(k=0,1, \ldots, p-1)
$$

Multiplying these inequalities, we obtain

or

$$
c_{N+p} \leq \beta^{p} c_{N}
$$

$$
c_{n} \leq c_{N} \beta^{-N} \cdot \beta^{n} \quad(n \geq N)
$$

Hence

$$
\sqrt[n]{c_{n}} \leq \sqrt[n]{c_{N} \beta^{-N}} \cdot \beta
$$

so that

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \beta
$$

by Theorem 3.20(b). Since (18) is true for every $\beta>\alpha$, we have

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \alpha
$$

\section{POWER SERIES}
3.38 Definition Given a sequence $\left\{c_{n}\right\}$ of complex numbers, the series

$$
\sum_{n=0}^{\infty} c_{n} z^{n}
$$

is called a power series. The numbers $c_{n}$ are called the coefficients of the series; $z$ is a complex number.

In general, the series will converge or diverge, depending on the choice of $z$. More specifically, with every power series there is associated a circle, the circle of convergence, such that (19) converges if $z$ is in the interior of the circle and diverges if $z$ is in the exterior (to cover all cases, we have to consider the plane as the interior of a circle of infinite radius, and a point as a circle of radius zero). The behavior on the circle of convergence is much more varied and cannot be described so simply.

3.39 Theorem Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|,} \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.

Proof Put $a_{n}=c_{n} z^{n}$, and apply the root test:

$$
\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}=|z| \limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}=\frac{|z|}{R}
$$

Note: $R$ is called the radius of convergence of $\Sigma c_{n} z^{n}$.

\subsection{Examples}
(a) The series $\Sigma n^{n} z^{n}$ has $R=0$.

(b) The series $\sum \frac{z^{n}}{n !}$ has $R=+\infty$. (In this case the ratio test is easier to apply than the root test.)
(c) The series $\Sigma z^{n}$ has $R=1$. If $|z|=1$, the series diverges, since $\left\{z^{n}\right\}$ does not tend to 0 as $n \rightarrow \infty$.

(d) The series $\sum \frac{z^{n}}{n}$ has $R=1$. It diverges if $z=1$. It converges for all other $z$ with $|z|=1$. (The last assertion will be proved in Theorem 3.44.)

(e) The series $\sum \frac{z^{n}}{n^{2}}$ has $R=1$. It converges for all $z$ with $|z|=1$, by the comparison test, since $\left|z^{n} / n^{2}\right|=1 / n^{2}$.

\section{SUMMATION BY PARTS}
3.41 Theorem Given two sequences $\left\{a_{n}\right\},\left\{b_{n}\right\}$, put

$$
A_{n}=\sum_{k=0}^{n} a_{k}
$$

if $n \geq 0$; put $A_{-1}=0$. Then, if $0 \leq p \leq q$, we have

$$
\sum_{n=p}^{q} a_{n} b_{n}=\sum_{n=p}^{q-1} A_{n}\left(b_{n}-b_{n+1}\right)+A_{q} b_{q}-A_{p-1} b_{p}
$$

Proof

$$
\sum_{n=p}^{q} a_{n} b_{n}=\sum_{n=p}^{q}\left(A_{n}-A_{n-1}\right) b_{n}=\sum_{n=p}^{q} A_{n} b_{n}-\sum_{n=p-1}^{q-1} A_{n} b_{n+1}
$$

and the last expression on the right is clearly equal to the right side of (20).

Formula (20), the so-called "partial summation formula," is useful in the investigation of series of the form $\Sigma a_{n} b_{n}$, particularly when $\left\{b_{n}\right\}$ is monotonic. We shall now give applications.

\subsection{Theorem Suppose}
(a) the partial sums $A_{n}$ of $\Sigma a_{n}$ form a bounded sequence;

(b) $b_{0} \geq b_{1} \geq b_{2} \geq \cdots$;

(c) $\lim _{n \rightarrow \infty} b_{n}=0$.

Then $\Sigma a_{n} b_{n}$ converges.

Proof Choose $M$ such that $\left|A_{n}\right| \leq M$ for all $n$. Given $\varepsilon>0$, there is an integer $N$ such that $b_{N} \leq(\varepsilon / 2 M)$. For $N \leq p \leq q$, we have

$$
\begin{aligned}
\left|\sum_{n=p}^{q} a_{n} b_{n}\right| & =\left|\sum_{n=p}^{q-1} A_{n}\left(b_{n}-b_{n+1}\right)+A_{q} b_{q}-A_{p-1} b_{p}\right| \\
& \leq M\left|\sum_{n=p}^{q-1}\left(b_{n}-b_{n+1}\right)+b_{q}+b_{p}\right| \\
& =2 M b_{p} \leq 2 M b_{N} \leq \varepsilon .
\end{aligned}
$$

Convergence now follows from the Cauchy criterion. We note that the first inequality in the above chain depends of course on the fact that $b_{n}-b_{n+1} \geq 0$.

\subsection{Theorem Suppose}
(a) $\left|c_{1}\right| \geq\left|c_{2}\right| \geq\left|c_{3}\right| \geq \cdots$;

(b) $c_{2 m-1} \geq 0, c_{2 m} \leq 0 \quad(m=1,2,3, \ldots)$;

(c) $\lim _{n \rightarrow \infty} c_{n}=0$.

Then $\Sigma c_{n}$ converges.

Series for which $(b)$ holds are called "alternating series"; the theorem was known to Leibnitz.

Proof Apply Theorem 3.42, with $a_{n}=(-1)^{n+1}, b_{n}=\left|c_{n}\right|$.

3.44 Theorem Suppose the radius of convergence of $\Sigma c_{n} z^{n}$ is 1, and suppose $c_{0} \geq c_{1} \geq c_{2} \geq \cdots, \lim _{n \rightarrow \infty} c_{n}=0$. Then $\Sigma c_{n} z^{n}$ converges at every point on the circle $|z|=1$, except possibly at $z=1$.

Proof Put $a_{n}=z^{n}, b_{n}=c_{n}$. The hypotheses of Theorem 3.42 are then satisfied, since

$$
\left|A_{n}\right|=\left|\sum_{m=0}^{n} z^{m}\right|=\left|\frac{1-z^{n+1}}{1-z}\right| \leq \frac{2}{|1-z|}
$$

if $|z|=1, z \neq 1$.

\section{ABSOLUTE CONVERGENCE}
The series $\Sigma a_{n}$ is said to converge absolutely if the series $\Sigma\left|a_{n}\right|$ converges.

3.45 Theorem If $\Sigma a_{n}$ converges absolutely, then $\Sigma a_{n}$ converges.

Proof The assertion follows from the inequality

$$
\left|\sum_{k=n}^{m} a_{k}\right| \leq \sum_{k=n}^{m}\left|a_{k}\right|
$$

plus the Cauchy criterion.

3.46 Remarks For series of positive terms, absolute convergence is the same as convergence.

If $\Sigma a_{n}$ converges, but $\Sigma\left|a_{n}\right|$ diverges, we say that $\Sigma a_{n}$ converges nonabsolutely. For instance, the series

$$
\sum \frac{(-1)^{n}}{n}
$$

converges nonabsolutely (Theorem 3.43).

The comparison test, as well as the root and ratio tests, is really a test for absolute convergence, and therefore cannot give any information about nonabsolutely convergent series. Summation by parts can sometimes be used to handle the latter. In particular, power series converge absolutely in the interior of the circle of convergence.

We shall see that we may operate with absolutely convergent series very much as with finite sums. We may multiply them term by term and we may change the order in which the additions are carried out, without affecting the sum of the series. But for nonabsolutely convergent series this is no longer true, and more care has to be taken when dealing with them.

\section{ADDITION AND MULTIPLICATION OF SERIES}
3.47 Theorem If $\Sigma a_{n}=A$, and $\Sigma b_{n}=B$, then $\Sigma\left(a_{n}+b_{n}\right)=A+B$, and $\Sigma c a_{n}=c A$, for any fixed $c$.

Proof Let

$$
A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k} .
$$

Then

$$
A_{n}+B_{n}=\sum_{k=0}^{n}\left(a_{k}+b_{k}\right) .
$$

Since $\lim _{n \rightarrow \infty} A_{n}=A$ and $\lim _{n \rightarrow \infty} B_{n}=B$, we see that

$$
\lim _{n \rightarrow \infty}\left(A_{n}+B_{n}\right)=A+B
$$

The proof of the second assertion is even simpler.

Thus two convergent series may be added term by term, and the resulting series converges to the sum of the two series. The situation becomes more complicated when we consider multiplication of two series. To begin with, we have to define the product. This can be done in several ways; we shall consider the so-called "Cauchy product."

3.48 Definition Given $\Sigma a_{n}$ and $\Sigma b_{n}$, we put

$$
c_{n}=\sum_{k=0}^{n} a_{k} b_{n-k} \quad(n=0,1,2, \ldots)
$$

and call $\Sigma c_{n}$ the product of the two given series.

This definition may be motivated as follows. If we take two power series $\Sigma a_{n} z^{n}$ and $\Sigma b_{n} z^{n}$, multiply them term by term, and collect terms containing the same power of $z$, we get

$$
\begin{aligned}
\sum_{n=0}^{\infty} a_{n} z^{n} \cdot \sum_{n=0}^{\infty} b_{n} z^{n} & =\left(a_{0}+a_{1} z+a_{2} z^{2}+\cdots\right)\left(b_{0}+b_{1} z+b_{2} z^{2}+\cdots\right) \\
& =a_{0} b_{0}+\left(a_{0} b_{1}+a_{1} b_{0}\right) z+\left(a_{0} b_{2}+a_{1} b_{1}+a_{2} b_{0}\right) z^{2}+\cdots \\
& =c_{0}+c_{1} z+c_{2} z^{2}+\cdots
\end{aligned}
$$

Setting $z=1$, we arrive at the above definition.

\subsection{Example If}
$$
A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k}, \quad C_{n}=\sum_{k=0}^{n} c_{k},
$$

and $A_{n} \rightarrow A, B_{n} \rightarrow B$, then it is not at all clear that $\left\{C_{n}\right\}$ will converge to $A B$, since we do not have $C_{n}=A_{n} B_{n}$. The dependence of $\left\{C_{n}\right\}$ on $\left\{A_{n}\right\}$ and $\left\{B_{n}\right\}$ is quite a complicated one (see the proof of Theorem 3.50). We shall now show that the product of two convergent series may actually diverge.

The series

$$
\sum_{n=0}^{\infty} \frac{(-1)^{n}}{\sqrt{n+1}}=1-\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}-\frac{1}{\sqrt{4}}+\cdots
$$

converges (Theorem 3.43). We form the product of this series with itself and obtain

$$
\begin{aligned}
\sum_{n=0}^{\infty} c_{n}=1-\left(\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}\right)+( & \left.\frac{1}{\sqrt{3}}+\frac{1}{\sqrt{2} \sqrt{2}}+\frac{1}{\sqrt{3}}\right) \\
& -\left(\frac{1}{\sqrt{4}}+\frac{1}{\sqrt{3} \sqrt{2}}+\frac{1}{\sqrt{2} \sqrt{3}}+\frac{1}{\sqrt{4}}\right)+\cdots,
\end{aligned}
$$

so that

Since

$$
c_{n}=(-1)^{n} \sum_{k=0}^{n} \frac{1}{\sqrt{(n-k+1)(k+1)}}
$$

$$
(n-k+1)(k+1)=\left(\frac{n}{2}+1\right)^{2}-\left(\frac{n}{2}-k\right)^{2} \leq\left(\frac{n}{2}+1\right)^{2} .
$$

we have

$$
\left|c_{n}\right| \geq \sum_{k=0}^{n} \frac{2}{n+2}=\frac{2(n+1)}{n+2}
$$

so that the condition $c_{n} \rightarrow 0$, which is necessary for the convergence of $\Sigma c_{n}$, is not satisfied.

In view of the next theorem, due to Mertens, we note that we have here considered the product of two nonabsolutely convergent series.

\subsection{Theorem Suppose}
(a) $\sum_{n=0}^{\infty} a_{n}$ converges absolutely,

(b) $\sum_{n=0}^{\infty} a_{n}=A$,

(c) $\sum_{n=0}^{\infty} b_{n}=B$,

(d) $\quad c_{n}=\sum_{k=0}^{n} a_{k} b_{n-k} \quad(n=0,1,2, \ldots)$.

Then

$$
\sum_{n=0}^{\infty} c_{n}=A B
$$

That is, the product of two convergent series converges, and to the right value, if at least one of the two series converges absolutely.

Proof Put

$$
A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k}, \quad C_{n}=\sum_{k=0}^{n} c_{k}, \quad \beta_{n}=B_{n}-B
$$

Then

$$
\begin{aligned}
C_{n} & =a_{0} b_{0}+\left(a_{0} b_{1}+a_{1} b_{0}\right)+\cdots+\left(a_{0} b_{n}+a_{1} b_{n-1}+\cdots+a_{n} b_{0}\right) \\
& =a_{0} B_{n}+a_{1} B_{n-1}+\cdots+a_{n} B_{0} \\
& =a_{0}\left(B+\beta_{n}\right)+a_{1}\left(B+\beta_{n-1}\right)+\cdots+a_{n}\left(B+\beta_{0}\right) \\
& =A_{n} B+a_{0} \beta_{n}+a_{1} \beta_{n-1}+\cdots+a_{n} \beta_{0}
\end{aligned}
$$

Put

$$
\gamma_{n}=a_{0} \beta_{n}+a_{1} \beta_{n-1}+\cdots+a_{n} \beta_{0}
$$

We wish to show that $C_{n} \rightarrow A B$. Since $A_{n} B \rightarrow A B$, it suffices to show that

$$
\lim _{n \rightarrow \infty} \gamma_{n}=0
$$

Put

$$
\alpha=\sum_{n=0}^{\infty}\left|a_{n}\right|
$$

[It is here that we use (a).] Let $\varepsilon>0$ be given. By $(c), \beta_{n} \rightarrow 0$. Hence we can choose $N$ such that $\left|\beta_{n}\right| \leq \varepsilon$ for $n \geq N$, in which case

$$
\begin{aligned}
\left|\gamma_{n}\right| & \leq\left|\beta_{0} a_{n}+\cdots+\beta_{N} a_{n-N}\right|+\left|\beta_{N+1} a_{n-N-1}+\cdots+\beta_{n} a_{0}\right| \\
& \leq\left|\beta_{0} a_{n}+\cdots+\beta_{N} a_{n-N}\right|+\varepsilon \alpha .
\end{aligned}
$$

Keeping $N$ fixed, and letting $n \rightarrow \infty$, we get

$$
\limsup _{n \rightarrow \infty}\left|\gamma_{n}\right| \leq \varepsilon \alpha
$$

since $a_{k} \rightarrow 0$ as $k \rightarrow \infty$. Since $\varepsilon$ is arbitrary, (21) follows.

Another question which may be asked is whether the series $\Sigma c_{n}$, if convergent, must have the sum $A B$. Abel showed that the answer is in the affirmative.

3.51 Theorem If the series $\Sigma a_{n}, \Sigma b_{n}, \Sigma c_{n}$ converge to $A, B, C$, and $c_{n}=a_{0} b_{n}+\cdots+a_{n} b_{0}$, then $C=A B$.

Here no assumption is made concerning absolute convergence. We shall give a simple proof (which depends on the continuity of power series) after Theorem 8.2.

\section{REARRANGEMENTS}
3.52 Definition Let $\left\{k_{n}\right\}, n=1,2,3, \ldots$, be a sequence in which every positive integer appears once and only once (that is, $\left\{k_{n}\right\}$ is a 1-1 function from $J$ onto $J$, in the notation of Definition 2.2). Putting

$$
a_{n}^{\prime}=a_{k_{n}} \quad(n=1,2,3, \ldots)
$$

we say that $\Sigma a_{n}^{\prime}$ is a rearrangement of $\Sigma a_{n}$.

If $\left\{s_{n}\right\},\left\{s_{n}^{\prime}\right\}$ are the sequences of partial sums of $\Sigma a_{n}, \Sigma a_{n}^{\prime}$, it is easily seen that, in general, these two sequences consist of entirely different numbers. We are thus led to the problem of determining under what conditions all rearrangements of a convergent series will converge and whether the sums are necessarily the same.

3.53 Example Consider the convergent series

$$
1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{3}-\frac{1}{6}+\cdots
$$

and one of its rearrangements

$$
1+\frac{1}{3}-\frac{1}{2}+\frac{1}{5}+\frac{1}{7}-\frac{1}{4}+\frac{1}{9}+\frac{1}{11}-\frac{1}{6}+\cdots
$$

in which two positive terms are always followed by one negative. If $s$ is the sum of (22), then

$$
s<1-\frac{1}{2}+\frac{1}{3}=\frac{5}{6} .
$$

Since

$$
\frac{1}{4 k-3}+\frac{1}{4 k-1}-\frac{1}{2 k}>0
$$

for $k \geq 1$, we see that $s_{3}^{\prime}<s_{6}^{\prime}<s_{9}^{\prime}<\cdots$, where $s_{n}^{\prime}$ is $n$th partial sum of (23). Hence

$$
\limsup _{n \rightarrow \infty} s_{n}^{\prime}>s_{3}^{\prime}=\frac{5}{6}
$$

so that (23) certainly does not converge to $s$ [we leave it to the reader to verify that (23) does, however, converge].

This example illustrates the following theorem, due to Riemann.

3.54 Theorem Let $\Sigma a_{n}$ be a series of real numbers which converges, but not absolutely. Suppose

$$
-\infty \leq \alpha \leq \beta \leq \infty
$$

Then there exists a rearrangement $\Sigma a_{n}^{\prime}$ with partial sums $s_{n}^{\prime}$ such that

$$
\liminf _{n \rightarrow \infty} s_{n}^{\prime}=\alpha, \quad \limsup _{n \rightarrow \infty} s_{n}^{\prime}=\beta .
$$

Proof Let

$$
p_{n}=\frac{\left|a_{n}\right|+a_{n}}{2}, \quad q_{n}=\frac{\left|a_{n}\right|-a_{n}}{2} \quad(n=1,2,3, \ldots)
$$

Then $p_{n}-q_{n}=a_{n}, p_{n}+q_{n}=\left|a_{n}\right|, p_{n} \geq 0, q_{n} \geq 0$. The series $\Sigma p_{n}, \Sigma q_{n}$ must both diverge.

For if both were convergent, then

$$
\Sigma\left(p_{n}+q_{n}\right)=\Sigma\left|a_{n}\right|
$$

would converge, contrary to hypothesis. Since

$$
\sum_{n=1}^{N} a_{n}=\sum_{n=1}^{N}\left(p_{n}-q_{n}\right)=\sum_{n=1}^{N} p_{n}-\sum_{n=1}^{N} q_{n}
$$

divergence of $\Sigma p_{n}$ and convergence of $\Sigma q_{n}$ (or vice versa) implies divergence of $\Sigma a_{n}$, again contrary to hypothesis.

Now let $P_{1}, P_{2}, P_{3}, \ldots$ denote the nonnegative terms of $\Sigma a_{n}$, in the order in which they occur, and let $Q_{1}, Q_{2}, Q_{3}, \ldots$ be the absolute values of the negative terms of $\Sigma a_{n}$, also in their original order.

The series $\Sigma P_{n}, \Sigma Q_{n}$ differ from $\Sigma p_{n}, \Sigma q_{n}$ only by zero terms, and are therefore divergent.

We shall construct sequences $\left\{m_{n}\right\},\left\{k_{n}\right\}$, such that the series

$$
\begin{aligned}
& P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots \\
& \quad+P_{m_{2}}-Q_{k_{1}+1}-\cdots-Q_{k_{2}}+\cdots,
\end{aligned}
$$

which clearly is a rearrangement of $\Sigma a_{n}$, satisfies (24).

Choose real-valued sequences $\left\{\alpha_{n}\right\},\left\{\beta_{n}\right\}$ such that $\alpha_{n} \rightarrow \alpha, \beta_{n} \rightarrow \beta$, $\alpha_{n}<\beta_{n}, \beta_{1}>0$.

Let $m_{1}, k_{1}$ be the smallest integers such that

$$
\begin{gathered}
P_{1}+\cdots+P_{m_{1}}>\beta_{1} \\
P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}<\alpha_{1}
\end{gathered}
$$

let $m_{2}, k_{2}$ be the smallest integers such that

$$
\begin{array}{r}
P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots+P_{m_{2}}>\beta_{2} \\
P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots+P_{m_{2}}-Q_{k_{1}+1} \\
-\cdots-Q_{k_{2}}<\alpha_{2}
\end{array}
$$

and continue in this way. This is possible since $\Sigma P_{n}$ and $\Sigma Q_{n}$ diverge.

If $x_{n}, y_{n}$ denote the partial sums of (25) whose last terms are $P_{m_{n}}$, $-Q_{k_{n}}$, then

$$
\left|x_{n}-\beta_{n}\right| \leq P_{m_{n}}, \quad\left|y_{n}-\alpha_{n}\right| \leq Q_{k_{n}} .
$$

Since $P_{n} \rightarrow 0$ and $Q_{n} \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_{n} \rightarrow \beta, y_{n} \rightarrow \alpha$.

Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25).

3.55 Theorem If $\Sigma a_{n}$ is a series of complex numbers which converges absolutely, then every rearrangement of $\Sigma a_{n}$ converges, and they all converge to the same sum.

Proof Let $\Sigma a_{n}^{\prime}$ be a rearrangement, with partial sums $s_{n}^{\prime}$. Given $\varepsilon>0$, there exists an integer $N$ such that $m \geq n \geq N$ implies

$$
\sum_{i=n}^{m}\left|a_{i}\right| \leq \varepsilon
$$

Now choose $p$ such that the integers $1,2, \ldots, N$ are all contained in the set $k_{1}, k_{2}, \ldots, k_{p}$ (we use the notation of Definition 3.52). Then if $n>p$, the numbers $a_{1}, \ldots, a_{N}$ will cancel in the difference $s_{n}-s_{n}^{\prime}$, so that $\left|s_{n}-s_{n}^{\prime}\right| \leq \varepsilon$, by (26). Hence $\left\{s_{n}^{\prime}\right\}$ converges to the same sum as $\left\{s_{n}\right\}$.

\section{EXERCISES}
\begin{enumerate}
  \item Prove that convergence of $\left\{s_{n}\right\}$ implies convergence of $\left\{\left|s_{n}\right|\right\}$. Is the converse true?

  \item Calculate $\lim _{n \rightarrow \infty}\left(\sqrt{n^{2}+n}-n\right)$.

  \item If $s_{1}=\sqrt{2}$, and

\end{enumerate}

$$
s_{n+1}=\sqrt{2+\sqrt{s_{n}}} \quad(n=1,2,3, \ldots)
$$

prove that $\left\{s_{n}\right\}$ converges, and that $s_{n}<2$ for $n=1,2,3, \ldots$

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Find the upper and lower limits of the sequence $\left\{s_{n}\right\}$ defined by
\end{enumerate}

$$
s_{1}=0 ; \quad s_{2 m}=\frac{s_{2 m-1}}{2} ; \quad s_{2 m+1}=\frac{1}{2}+s_{2 m}
$$

\begin{enumerate}
  \setcounter{enumi}{4}
  \item For any two real sequences $\left\{a_{n}\right\},\left\{b_{n}\right\}$, prove that
\end{enumerate}

$$
\limsup _{n \rightarrow \infty}\left(a_{n}+b_{n}\right) \leq \limsup _{n \rightarrow \infty} a_{n}+\limsup _{n \rightarrow \infty} b_{n},
$$

provided the sum on the right is not of the form $\infty-\infty$.

\begin{enumerate}
  \setcounter{enumi}{5}
  \item Investigate the behavior (convergence or divergence) of $\Sigma a_{n}$ if
\end{enumerate}

(a) $a_{n}=\sqrt{n+1}-\sqrt{n}$;

(b) $a_{n}=\frac{\sqrt{n+1}-\sqrt{n}}{n}$;

(c) $a_{n}=(\sqrt[n]{n}-1)^{n}$;

(d) $a_{n}=\frac{1}{1+z^{n}}, \quad$ for complex values of $z$.

\begin{enumerate}
  \setcounter{enumi}{6}
  \item Prove that the convergence of $\Sigma a_{n}$ implies the convergence of
\end{enumerate}

$$
\sum \frac{\sqrt{a_{n}}}{n}
$$

if $a_{n} \geq 0$.

\begin{enumerate}
  \setcounter{enumi}{7}
  \item If $\Sigma a_{n}$ converges, and if $\left\{b_{n}\right\}$ is monotonic and bounded, prove that $\Sigma a_{n} b_{n}$ converges.

  \item Find the radius of convergence of each of the following power series:
(a) $\sum n^{3} z^{n}$
(b) $\sum \frac{2^{n}}{n !} z^{n}$
(c) $\sum \frac{2^{n}}{n^{2}} z^{n}$
(d) $\sum \frac{n^{3}}{3^{n}} z^{n}$

  \item Suppose that the coefficients of the power series $\sum a_{n} z^{n}$ are integers, infinitely many of which are distinct from zero. Prove that the radius of convergence is at most 1 .

  \item Suppose $a_{n}>0, s_{n}=a_{1}+\cdots+a_{n}$, and $\Sigma a_{n}$ diverges.

\end{enumerate}

(a) Prove that $\sum \frac{a_{n}}{1+a_{n}}$ diverges.

(b) Prove that

$$
\frac{a_{N+1}}{s_{N+1}}+\cdots+\frac{a_{N+k}}{s_{N+k}} \geq 1-\frac{s_{N}}{s_{N+k}}
$$

and deduce that $\sum \frac{a_{n}}{s_{n}}$ diverges.

(c) Prove that

$$
\frac{a_{n}}{s_{n}^{2}} \leq \frac{1}{s_{n-1}}-\frac{1}{s_{n}}
$$

and deduce that $\sum \frac{a_{n}}{s_{n}^{2}}$ converges.

(d) What can be said about

$$
\sum \frac{a_{n}}{1+n a_{n}} \text { and } \sum \frac{a_{n}}{1+n^{2} a_{n}} ?
$$

\begin{enumerate}
  \setcounter{enumi}{11}
  \item Suppose $a_{n}>0$ and $\Sigma a_{n}$ converges. Put
\end{enumerate}

$$
r_{n}=\sum_{m=n}^{\infty} a_{m}
$$

(a) Prove that

$$
\frac{a_{m}}{r_{m}}+\cdots+\frac{a_{n}}{r_{n}}>1-\frac{r_{n}}{r_{m}}
$$

if $m<n$, and deduce that $\sum \frac{a_{n}}{r_{n}}$ diverges.
(b) Prove that

$$
\frac{a_{n}}{\sqrt{r_{n}}}<2\left(\sqrt{r_{n}}-\sqrt{r_{n+1}}\right)
$$

and deduce that $\sum \frac{a_{n}}{\sqrt{r_{n}}}$ converges.

\begin{enumerate}
  \setcounter{enumi}{12}
  \item Prove that the Cauchy product of two absolutely convergent series converges absolutely.

  \item If $\left\{s_{n}\right\}$ is a complex sequence, define its arithmetic means $\sigma_{n}$ by

\end{enumerate}

$$
\sigma_{n}=\frac{s_{0}+s_{1}+\cdots+s_{n}}{n+1} \quad(n=0,1,2, \ldots)
$$

(a) If $\lim s_{n}=s$, prove that $\lim \sigma_{n}=s$.

(b) Construct a sequence $\left\{s_{n}\right\}$ which does not converge, although $\lim \sigma_{n}=0$.

(c) Can it happen that $s_{n}>0$ for all $n$ and that $\lim \sup s_{n}=\infty$, although $\lim \sigma_{n}=0$ ?

(d) Put $a_{n}=s_{n}-s_{n-1}$, for $n \geq 1$. Show that

$$
s_{n}-\sigma_{n}=\frac{1}{n+1} \sum_{k=1}^{n} k a_{k}
$$

Assume that $\lim \left(n a_{n}\right)=0$ and that $\left\{\sigma_{n}\right\}$ converges. Prove that $\left\{s_{n}\right\}$ converges. [This gives a converse of $(a)$, but under the additional assumption that $n a_{n} \rightarrow 0$.] (e) Derive the last conclusion from a weaker hypothesis: Assume $M<\infty$, $\left|n a_{n}\right| \leq M$ for all $n$, and $\lim \sigma_{n}=\sigma$. Prove that $\lim s_{n}=\sigma$, by completing the following outline:

If $m<n$, then

$$
s_{n}-\sigma_{n}=\frac{m+1}{n-m}\left(\sigma_{n}-\sigma_{m}\right)+\frac{1}{n-m} \sum_{\imath=m+1}^{n}\left(s_{n}-s_{l}\right)
$$

For these $i$,

$$
\left|s_{n}-s_{l}\right| \leq \frac{(n-i) M}{i+1} \leq \frac{(n-m-1) M}{m+2}
$$

Fix $\varepsilon>0$ and associate with each $n$ the integer $m$ that satisfies

$$
m \leq \frac{n-\varepsilon}{1+\varepsilon}<m+1
$$

Then $(m+1) /(n-m) \leq 1 / \varepsilon$ and $\left|s_{n}-s_{i}\right|<M \varepsilon$. Hence

$$
\limsup _{n \rightarrow \infty}\left|s_{n}-\sigma\right| \leq M \varepsilon \text {. }
$$

Since $\varepsilon$ was arbitrary, $\lim s_{n}=\sigma$.

\begin{enumerate}
  \setcounter{enumi}{14}
  \item Definition 3.21 can be extended to the case in which the $a_{n}$ lie in some fixed $R^{k}$. Absolute convergence is defined as convergence of $\Sigma\left|\mathbf{a}_{n}\right|$. Show that Theorems $3.22,3.23,3.25(a), 3.33,3.34,3.42,3.45,3.47$, and 3.55 are true in this more general setting. (Only slight modifications are required in any of the proofs.)

  \item Fix a positive number $\alpha$. Choose $x_{1}>\sqrt{\alpha}$, and define $x_{2}, x_{3}, x_{4}, \ldots$, by the recursion formula

\end{enumerate}

$$
x_{n+1}=\frac{1}{2}\left(x_{n}+\frac{\alpha}{x_{n}}\right)
$$

(a) Prove that $\left\{x_{n}\right\}$ decreases monotonically and that $\lim x_{n}=\sqrt{ } \bar{\alpha}$.

(b) Put $\varepsilon_{n}=x_{n}-\sqrt{\alpha}$, and show that

$$
\varepsilon_{n+1}=\frac{\varepsilon_{n}^{2}}{2 x_{n}}<\frac{\varepsilon_{n}^{2}}{2 \sqrt{\alpha}}
$$

so that, setting $\beta=2 \sqrt{\alpha}$,

$$
\varepsilon_{n+1}<\beta\left(\frac{\varepsilon_{1}}{\beta}\right)^{2^{n}} \quad(n=1,2,3, \ldots)
$$

(c) This is a good algorithm for computing square roots, since the recursion formula is simple and the convergence is extremely rapid. For example, if $\alpha=3$ and $x_{1}=2$, show that $\varepsilon_{1} / \beta<\frac{1}{10}$ and that therefore

$$
\varepsilon_{5}<4 \cdot 10^{-16}, \quad \varepsilon_{6}<4 \cdot 10^{-32}
$$

\begin{enumerate}
  \setcounter{enumi}{16}
  \item Fix $\alpha>1$. Take $x_{1}>\sqrt{ } \bar{\alpha}$, and define
\end{enumerate}

$$
x_{n+1}=\frac{\alpha+x_{n}}{1+x_{n}}=x_{n}+\frac{\alpha-x_{n}^{2}}{1+x_{n}}
$$

(a) Prove that $x_{1}>x_{3}>x_{5}>\cdots$.

(b) Prove that $x_{2}<x_{4}<x_{6}<\cdots$.

(c) Prove that $\lim x_{n}=\sqrt{ } \alpha$.

(d) Compare the rapidity of convergence of this process with the one described in Exercise 16.

\begin{enumerate}
  \setcounter{enumi}{17}
  \item Replace the recursion formula of Exercise 16 by
\end{enumerate}

$$
x_{n+1}=\frac{p-1}{p} x_{n}+\frac{\alpha}{p} x_{n}^{-p+1}
$$

where $p$ is a fixed positive integer, and describe the behavior of the resulting sequences $\left\{x_{n}\right\}$.

\begin{enumerate}
  \setcounter{enumi}{18}
  \item Associate to each sequence $a=\left\{\alpha_{n}\right\}$, in which $\alpha_{n}$ is 0 or 2 , the real number
\end{enumerate}

$$
x(a)=\sum_{n=1}^{\infty} \frac{\alpha_{n}}{3^{n}}
$$

Prove that the set of all $x(a)$ is precisely the Cantor set described in Sec. 2.44.

\begin{enumerate}
  \setcounter{enumi}{19}
  \item Suppose $\left\{\boldsymbol{p}_{\boldsymbol{n}}\right\}$ is a Cauchy sequence in a metric space $X$, and some subsequence $\left\{p_{n}\right\}$ converges to a point $p \in X$. Prove that the full sequence $\left\{p_{n}\right\}$ converges to $p$.

  \item Prove the following analogue of Theorem $3.10(b)$ : If $\left\{E_{n}\right\}$ is a sequence of closed nonempty and bounded sets in a complete metric space $X$, if $E_{n} \supset E_{n+1}$, and if

\end{enumerate}

$$
\lim _{n \rightarrow \infty} \operatorname{diam} E_{n}=0 \text {, }
$$

then $\bigcap_{1}^{\infty} E_{n}$ consists of exactly one point.

\begin{enumerate}
  \setcounter{enumi}{21}
  \item Suppose $X$ is a nonempty complete metric space, and $\left\{G_{n}\right\}$ is a sequence of dense open subsets of $X$. Prove Baire's theorem, namely, that $\bigcap_{1}^{\infty} G_{n}$ is not empty. (In fact, it is dense in $X$.) Hint: Find a shrinking sequence of neighborhoods $E_{n}$ such that $E_{n} \subset G_{n}$, and apply Exercise 21 .

  \item Suppose $\left\{p_{n}\right\}$ and $\left\{q_{n}\right\}$ are Cauchy sequences in a metric space $X$. Show that the sequence $\left\{d\left(p_{n}, q_{n}\right)\right\}$ converges. Hint: For any $m, n$,

\end{enumerate}

$$
d\left(p_{n}, q_{n}\right) \leq d\left(p_{n}, p_{m}\right)+d\left(p_{m}, q_{m}\right)+d\left(q_{m}, q_{n}\right)
$$

it follows that

$$
\left|d\left(p_{n}, q_{n}\right)-d\left(p_{m}, q_{m}\right)\right|
$$

is small if $m$ and $n$ are large.

\begin{enumerate}
  \setcounter{enumi}{23}
  \item Let $X$ be a metric space.
\end{enumerate}

(a) Call two Cauchy sequences $\left\{p_{n}\right\},\left\{q_{n}\right\}$ in $X$ equivalent if

$$
\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)=0
$$

Prove that this is an equivalence relation.

(b) Let $X^{*}$ be the set of all equivalence classes so obtained. If $P \in X^{*}, Q \in X^{*}$, $\left\{p_{n}\right\} \in P,\left\{q_{n}\right\} \in Q$, define

$$
\Delta(P, Q)=\lim _{n \rightarrow \infty} d\left(p_{n}, q_{n}\right)
$$

by Exercise 23, this limit exists. Show that the number $\Delta(P, Q)$ is unchanged if $\left\{p_{n}\right\}$ and $\left\{q_{n}\right\}$ are replaced by equivalent sequences, and hence that $\Delta$ is a distance function in $X^{*}$.

(c) Prove that the resulting metric space $X^{*}$ is complete.

(d) For each $p \in X$, there is a Cauchy sequence all of whose terms are $p$; let $P_{p}$ be the element of $X^{*}$ which contains this sequence. Prove that

$$
\Delta\left(P_{p}, P_{q}\right)=d(p, q)
$$

for all $p, q \in X$. In other words, the mapping $\varphi$ defined by $\varphi(p)=P_{p}$ is an isometry (i.e., a distance-preserving mapping) of $X$ into $X^{*}$.

(e) Prove that $\varphi(X)$ is dense in $X^{*}$, and that $\varphi(X)=X^{*}$ if $X$ is complete. By $(d)$, we may identify $X$ and $\varphi(X)$ and thus regard $X$ as embedded in the complete metric space $X^{*}$. We call $X^{*}$ the completion of $X$.

\begin{enumerate}
  \setcounter{enumi}{24}
  \item Let $X$ be the metric space whose points are the rational numbers, with the metric $d(x, y)=|x-y|$. What is the completion of this space? (Compare Exercise 24.)
\end{enumerate}

The function concept and some of the related terminology were introduced in Definitions 2.1 and 2.2. Although we shall (in later chapters) be mainly interested in real and complex functions (i.e., in functions whose values are real or complex numbers) we shall also discuss vector-valued functions (i.e., functions with values in $R^{k}$ ) and functions with values in an arbitrary metric space. The theorems we shall discuss in this general setting would not become any easier if we restricted ourselves to real functions, for instance, and it actually simplifies and clarifies the picture to discard unnecessary hypotheses and to state and prove theorems in an appropriately general context.

The domains of definition of our functions will also be metric spaces, suitably specialized in various instances.

\section{LIMITS OF FUNCTIONS}
4.1 Definition Let $X$ and $Y$ be metric spaces; suppose $E \subset X, f$ maps $E$ into $Y$, and $p$ is a limit point of $E$. We write $f(x) \rightarrow q$ as $x \rightarrow p$, or

$$
\lim _{x \rightarrow p} f(x)=q
$$


\end{document}