O FUNCTIONS OF SEVERAL VARIABLES LINEAR TRANSFORMATIONS We begin this chapter with a discussion of sets of vectors in euclidean n-space $\textstyle{\mathcal{R}}^{n}$ The algebraic facts presented here extend without change to finite-dimensional vector spaces over any field of scalars. However, for our purposes it is quite sufficient to stay within the familiar framework provided by the euclidean spaces 9.1 Definitions (a）A nonempty set Xe R” is a vector space if x+ye X and cxe X for all xe X, y e X, and for all scalars c. (b）If x,…， $\mathbf{X}_{k}$ e $\textstyle R^{n}$ and c,……, Cx are scalars, the vector $$ c_{1}\mathbf{x}_{1}+\cdot\cdot\cdot+\ c_{k}\mathbf{x}_{k} $$ is called a linear combination of x,…，x.If SC R" and if ${\mathcal{H}}^{\nu}$ is the set of all linear combinations of elements of $\mathbf{3}_{s}$ we say that $|\operatorname{s}{\mathfrak{p}}$ spans E, or tha E is the span of S. Observe that every span is a vector space.rUNCTONs or sEVERAL VARIABLES 205 CAx + … + cx.= 0 implies that (c）A set consisting of vectors x,……， x（we shall use the notation . Otherwise {x,……， x: {x,…..,x} for such a set） is said to be independent if the relation $c_{1}=\cdots=c_{k}=0$ is said to be dependent. Observe that no independent set contains the null vector. (d）If a vector space X contains an independent set of r vectors but con- tains no independent set of $r\ +\ 1$ l vectors, we say that X has dimension r and write: dim X = r. The set consisting of O alone is a vector space;its dimension is O (e）An independent subset of a vector space X which spans $X$ is called a bas is of X. Observe that if $\textstyle B=\{\mathbf{x}_{1},\dots,\mathbf{x}_{r}\}$ is a basis of X, then every x∈ X has a unique representation of the form x = Ec,X;. Such a representation exists since B spans X，and it is unique since B is independent. The numbers c， ..…,c, are called the coordinates of x with respect to the basis B The most familiar example of a basis is the set {e,. …，e,}, where e,is the vector in $R^{n}$ whose jth coordinate is l and whose other coordinates are all O. If xe $\textstyle R^{n}\!.$ x = (x，,.…, X) then x = 2x;e,、We shall call {e,...，e,} the standard basis of $\textstyle R^{n}$ 9.2 Theorem Let r be a positive integer.If a vector space X is spanned by d set of r vectors, then dim $\scriptstyle x\leq r$ Proof If this is false,there is a vector space X which contains an inde pendent set Q ={y,….,y,+1} and which is spanned by a set S $\mathrm{S}_{0}$ , consisting of r vectors. Suppose 0 ≤i<r, and suppose a set $\mathbf{S}_{i}$ has been constructed which spans X and which consists of al $\mathbf{y}_{j}$ with l≤j≤i plus a certain collection is obtained of r-i members of So，say x1,. .，x,-:.(In other words, $\mathbf{S}_{i}$ from $\mathrm{S}_{0}$ by replacing i of its elements by members of Q, without altering the span.）Since $\mathbf{S}_{i}$ spans X, $\mathbf{y}_{i+1}$ is in the span of $S_{i\,};$ hence there are scalars a, ….， :+1 $,\,b_{1},\,\ldots,\,b_{r-i}\,,$ with $a_{i+1}=1,$ such that $$ \sum_{j=1}^{i+1}a_{j}\mathbf{y}_{j}+\sum_{k=1}^{r-i}b_{k}\mathbf{x}_{k}=0. $$ If all bk's were O,the independence of $\leq\bigcap_{\mathrm{{z}}}$ would force all a's to be O,a contradiction. It follows that some $\mathbf{x}_{k}\in S_{i}$ is a linear combination of the other members of $T_{i}=S_{i}$ u {y:+1}.Remove this $\mathbf{X}_{k}$ from $\textstyle T_{i}$ and call the remaining set $S_{i+1}$ Then $S_{i+1}$ 1 spans the same set as $T_{i},$ namely X, so that $S_{i+1}$ has the properties postulated for $\mathbf{S}_{i}$ with i+ 1 in place of i206 PRINCIPLES OF MATHEMATICAL ANALYSIs Starting with $\mathrm{S}_{0}$ ，we thus construct sets S,.…., S,.The last of .This contra- these consists of yi,… ..y,, and our construction shows that it spans X ${\mathfrak{S}}_{p}$ But Q is independent; hence y,+ is not in the span of diction establishes the theorem. Corollary dim $R^{n}=n$ Proof Since {e,， .… ${\bf e}_{n}\}$ spans R",the theorem shows that dim $R^{n}$ '≤n. Since {e,,.….，e} is independent, dim $\scriptstyle\kappa\;\Xi\;n$ 9.3 Theorem Suppose X is a vector space, and dim X =n. (a）A set ${\mathcal{R}}^{*}$ of n vectors in X spans X if and only if $\textstyle{\mathcal{F}}$ is independent. (b）X has a basis, and every basis consists of n vectors. (c）If 1 ≤r≤n and $\left\{\mathbf{V}_{1}\right\}\,,\,\,\mathbf{\epsilon}_{*}\mathbf{\epsilon}_{,\,\,}\mathbf{\psi}_{p}\mathbf{\hat{\mathrm{\tiny~J}}}$ is an independent set in X, then $X$ has a basis containing {y1,...， y,}. Proof Suppose ${\cal E}=\{{\bf x_{1}},\ .\ .\ ,{\bf x_{n}}.$ }. Since dim $X=n,$ the set {x,...，Xm，y} is dependent, for every y e X. If $\widehat{H}^{\nu}$ is independent, it follows that $\mathcal{Y}$ is in the span of E; hence ${\hat{H}}^{\prime}$ spans X. Conversely, if ${\mathcal{F}}^{\dagger}$ is dependent, one of its members can be removed without changing the span of E. Hence E cannot span X, by Theorem 9.2. This proves (a) Since dim X = n,X contains an independent set of n vectors, and (a) shows that every such set is a basis of X;(b) now follows from 9.1(d) and 9.2 To prove Gc), let {x,.….，x,) be a basis of X. The set $$ S=\{{\bf y_{1}},\cdot\cdot\cdot,{\bf y_{r}},{\bf x_{1}},\ldots,{\bf x_{n}}\} $$ spans X and is dependent, since it contains more than n vectors. The argument used in the proof of Theorem 9.2 shows that one of the x's is a linear combination of the other members of S. If we remove this x; from S, the remaining set still spans X. This process can be repeated r times and leads to a basis of $X$ which contains {y1,.….，y,},by (a) 9.4 Definitions A mapping A of a vector space Xinto a vector space Yis said to be a linear transformation if $$ A({\bf x}_{1}+{\bf x}_{2})=A{\bf x}_{1}+A{\bf x}_{2}\,,\qquad A(c{\bf x})=c A{\bf x} $$ for all x $\mathbf{x}_{1},$ $\mathbf{X}_{2}$ E $X$ Y and all scalars c. Note that one often writes Ax instead of A(x) if A is linear. Observe that A0 = 0 if A is linear. Observe also that a linear transforma tion A of X into Y is completely determined by its action on any basis:：1FUNCriONS OF SEVERAL VARIABLES 207 form $\{\mathbf{x}_{1},\dots,\mathbf{x}_{n}\}$ is a basis of X, then every xe $X$ has a unique representation of the $$ \mathbf{x}=\sum_{i=1}^{n}c_{i}\mathbf{x}_{i}\,, $$ and the linearity of A allows us to compute Ax from the vectors Ax,….…, Ax and the coordinates C, ...，C,by the formula $$ A\mathbf{x}=\sum_{i=1}^{n}c_{i}\,A\,\mathbf{x}_{i}. $$ Linear transformations of X into X are often called linear operators on X If A is a linear operator on X which (i) is one-to-one and (i) maps X onto X, we say that A is invertible. In this case we can define an operator $A^{-1}$ lon $X$ by requiring that $A^{-1}(M x)=x$ for all xe X. It is trivial to verify that we then also have $A(A^{-1}\mathbf{x})=\mathbf{x},$ for all x ∈ X, and that A $A^{-1}$ is linear. An important fact about linear operators on finite-dimensional vector spaces is that each of the above conditions Gi) and (i)） implies the other: 9.5 Theorem A linear operator A on a fnite-dimensional vector space X is one-to-one if and only if the range of A is all of X. its range Proof Let {x, …x,} be a basis of X. The linearity of $Q=\{A\mathbf{x}_{1},\ \cdot\cdot\cdot,\ A\mathbf{x}_{n}\}$ . We therefore ${\mathcal{A}}$ shows that ${\mathcal{R}}(A)$ is the span of the set infer from Theorem 9.3(a) that O(A)= X if and only if $\leq\sup_{\mathbf{x}}$ is independent. We have to prove that this happens if and only if A is one-to-one. Suppose A is one-to-one and $\Sigma c_{i}\,A{\bf x}_{i}=0\,$ 0.Then $A(\Sigma c_{i}{\bf x}_{i})=0,$ hence Ecx;= 0, hence $c_{1}=\cdot\cdot\cdot=c_{n}=0,$ , and we conclude that Q is independent. Conversely,suppose Q is independent and $A(\Sigma c_{i}\mathbf{x}_{i})=0$ Then $\Sigma c_{t}\,A X_{t}=0$ hence c， = C,= 0,and we conclude： Ax = 0 only if x = 0.If now Ax = Ay,then A(x - y)= Ax- Ay = 0,so that ${\mathfrak{X}}-{\mathfrak{Y}}={\mathfrak{g}},$ and this says that A is one-to-one. 9.6 Definitions (a）Let L(X, Y)be the set ofall linear transformations of the vector spac $\mathcal{N}$ into the vector space Y. Instead of L(X, X), we shall simply write L(X) If A, Az∈ L(X, Y) and if c， $c_{\mathrm{2}}$ are scalars, define ciA，+ C, A。 by (c A + C2 Az)x = C Aix + C2 Azx （x∈ X) It is then clear that cnA + C, A,e L(X, Y). (b）If X,Y, Z are vector spaces, and if A∈ L(X, Y) and B e L(Y, Z), we define their product BA to be the composition of A and B: $$ (B A){\bf x}=B(A{\bf x})\qquad({\bf x}\in X). $$ Then BA ∈ L(X,Z).208 PRINCIPLES OF MATHEMATICAL ANALYSis Note that BA need not be the same as AB,even if X = Y = Z (c）For $A\in L(R^{n},\,R^{m})$ ,define the norm MAll of A to be the sup of all numbers |Ax|, where x ranges over all vectors in $\textstyle R^{n}$ with」x≤1. Observe that the inequality $$ |A{\mathbf{x}}|\leq\|A\|\ |\mathbf{x}| $$ holds for all xe R.Also, if 入is such that 」Ax ≤入xl for all xe R" then $\|A\|\leq\lambda$ 9.7 Theorem (a）If A ∈ L(R",R"),then Al|< OO and A is a uniformly continuous mapping of R" into $R^{m},$ (b）1f A,Be L(R", R") and cis a scalar, then $$ \|A+B\|\leq\|A\|+\|B\|,\qquad\|c A\|=|c|\ \ \|A\|. $$ With the distance between A and B defined as |A - B|, L(R",R") is a metric space. (c）If A ∈ L(R",R") and Be L(R", R'), then l|BAl| ≤| B|| Al| Proof (a） Let {e,….，e,} be the standard basis in R" and suppose $\mathbf{x}=\Sigma c_{i}\mathbf{e}_{i},$ lx≤1,so that |c;l≤1 for i= 1,...,n. Then $$ |A{\bf x}|=|\sum c_{i}A{\bf e}_{i}|\leq\sum|c_{i}|~|A{\bf e}_{i}|\leq\sum|A{\bf e}_{i} $$ so that $$ \|A\|\le\sum_{i=1}^{n}\vert A{\bf e}_{i}\vert<\infty. $$ Since |Ax- Ay|≤|Al| 」x -y」if x,y ∈ R", we see that A is uniformly continuous (b）The inequality in (b)） follows from $$ |(A+B)\mathbf{x}|=|A\mathbf{x}+B\mathbf{x}|\leq|A\mathbf{x}|+|B\mathbf{x}|\leq(\|A\|+\|B\|)|) $$ x| The second part of（b) is proved in the same manner.If $$ A,\;B,\;C\in L(R^{n},\;R^{m}), $$ we have the triangle inequality $$ \|A-C\|=\|(A-B)+(B-C)\|\leq\|A-B\|+\|B-C\|, $$FUNCTIONs OF SEVERAL VARIABLES 209 and it is easily verified that lA - Bl| has the other properties of a metric (Definition 2.15) (c）Finally, C follows from $$ |(B A)\mathbf{x}|=|B(A\mathbf{x})|\leq\|B\|\mathbf{\psi}|A\mathbf{x}|\leq\|B\|\mathbf{\psi}\|A\|\mathbf{\psi}|\mathbf{x}|. $$ Since we now have metrics in the spaces L(R",R"),the concepts of open set,continuity,etc.,make sense for these spaces. Our next theorem utilizes these concepts. 9.8 Theorem Let Q be the set of all invertible linear operators on R" (a）f Ae SQ, Be L(R"), and $$ \|B-A\|\cdot\|A^{-1}\|<1, $$ then Be Q (b） Sn is an open subset of L(R"), and the mapping A→A- is continuow on S. (This mapping is also obviously a 1-l mapping of Q onto Q which is its own inverse.） Proof (a）Put | A 1|| = 1/α, put || B - A」 = β.Then β<α. For every x∈ R” α $$ \begin{array}{r}{{\left|\mathbf{x}\right|=\alpha|A\ \cdot A\mathbf{x}|\leq\alpha|A\ \cdot\left\|A\mathbf{x}\right\|}}\\ {{=|A\mathbf{x}|\leq|(A-B)\mathbf{x}|+|B\mathbf{x}|\leq\beta|\mathbf{x}|+|B\mathbf{x}|,}}\end{array} $$ so that (1） $$ (\alpha-\beta)|\mathbf{x}|\leq|B\mathbf{x}|\qquad(\mathbf{x}\in R^{n}). $$ Since α $-\,\beta>0.$ ,(I) shows that Bx ≠ O if x ≠ 0. Hence B is l-1 By Theorem 9.5，B ∈ S2. This holds for all B with IB - A|<α. Thus we have (a) and the fact that s is open. (b）Next, replace x by B-ly in(1). The resulting inequality (2） $$ (x-\beta)!B^{-1}y|\leq|B B^{-1}y|=|y|\qquad(y\in R^{n}) $$ shows that $\|B^{-1}\|$ ≤(α-β)-1 . The identity $$ B^{-1}-A^{-1}=B^{-1}(A-B)A^{-1}, $$ combined with Theorem 9.7(c), implies therefore that $$ \|B^{-1}-A^{-1}\|\leq\|B^{-1}\|\,\|A-B\|\,\|A^{-1}\|\leq{\frac{\beta}{\alpha(\alpha-\beta)}}. $$ This establishes the continuity assertion made in(b), since $\beta\to0$ as B→ A.210 PRINCIPLES OF MATHEMATICAL ANALYSis 9.9 Matrices Suppose {x, ...， x,} and {y,. $\textstyle{\mathrm{Y}}_{m}\rangle$ are bases of vector spaces $X$ and Y, respectively.Then every A ∈ L(X, Y) determines a set of numbers $Q_{i j}$ such that (3） $$ A\mathbf{x}_{j}=\sum_{i=1}^{m}a_{i j}\mathbf{y}_{i}\qquad(1\leq j\leq n). $$ It is convenient to visualize these numbers in a rectangular array of m rows and n columns, called an m by n matrix： $$ [A]={\left[\begin{array}{l l l l}{a_{11}}&{a_{12}}&{\cdot\cdot}&{a_{1n}}\\ {a_{21}}&{a_{22}}&{\cdot\cdot}&{a_{2n}}\\ {\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot}\\ {\cdot a_{m1}}&{a_{m2}}&{\cdot\cdot}&{a_{m n}}\end{array}\right]} $$ $\{\mathbf{y}_{1},\ \cdot\cdot\cdot,\ \mathbf{y}_{m}\}\}$ Observe that the coordinates $Q_{\iota J}$ of the vector Ax,（with respect to the basis appear in the jith column of [A]. The vectors Ax, are therefore sometimes called the column vectors of [A] With this terminology, the range of A is spanned by the column vectors of [A] If x = Zc,xy, the linearity of ${\mathcal{A}}_{!}$ combined with （3), shows that (4） $$ A{\bf x}=\sum_{i=1}^{m}\left(\sum_{j=1}^{n}a_{i j}c_{j}\right){\bf y}_{i}\,. $$ Thus the coordinates of Ax are $\Sigma_{j}\,a_{i j}\,c_{j}$ Note that in（3）the summation ranges over the first subscript of $a_{i j}.$ but that we sum over the second subscrip when computing coordinates. 1::.If A is Suppose next that an m by n matrix is given, with real entries $Q_{i j}$ then defined by (4), it is clear that $A\in L(X)$ Y) and that [A]is the given matrix. Thus there is a natural l-l correspondence between L(X, Y) and the set of al real m by n matrices. We emphasize, though, that [A] depends not only on A but also on the choice of bases in X and Y. The same A may give rise to many different matrices if we change bases, and vice versa. We shall not pursue this observation any further, since we shall usually work with fixed bases.(Some remarks on this may be found in Sec. 9.37.) If Z is a third vector space, with basis $\{\mathbf{z}_{1},\cdot\cdot\cdot,$ zp},if A is given by (3) and if $$ B{\bf y}_{i}=\sum_{k}b_{k i}{\bf z}_{k},~~~~~~(B A){\bf x}_{j}=\sum_{k}c_{k j}{\bf z}_{k}, $$ then A∈ L(X,Y),Be L(Y, Z), BA e L(X,Z), and since $$ \begin{array}{l}{{\displaystyle{\cal B}(A{\bf x}_{j})=B\sum_{i}a_{i j}{\bf y}_{i}=\sum a_{i j}B{\bf y}_{i}}}\\ {{}}\\ {{}}\\ {{}}\\ {{}}\\ {{=\sum a_{i j}\sum_{k}b_{k i}{\bf z}_{k}=\sum_{k}\left(\sum b_{k i}a_{i j}\right){\bf z}_{k}\,,}}\end{array} $$FUNCTIONs OF SEVERAL VARIABLES211 the independence of $\{\mathbb{Z}_{1},\ \cdot\cdot\cdot,\ \mathbb{Z}_{p}\}$ implies that （5） $$ c_{k j}=\sum_{i}b_{k i}a_{i j}\quad\quad(1\leq k\leq p,\;1\leq j\leq n). $$ This shows how to compute the $\mathbf{\mathcal{P}}$ by n matrix [BA] from [B] and [A] If we define the product [B][A} to be [BA], then (5) describes the usual rule of matrix multiplication. Finally, suppose {X1,...，X,} and $\left\{\bigvee_{1},\ \circ\ ,\ \vee_{m}\right\}$ are standard bases of $\textstyle{R^{n}}$ ”and R", and A is given by (4). The Schwarz inequality shows that $$ \vert A{\bf x}\vert^{2}=\sum_{i}\left(\sum_{j}a_{i j}c_{j}\right)^{2}\leq\sum_{i}\left(\sum_{j}a_{i j}^{2}\cdot\sum_{j}c_{j}^{2}\right)=\sum_{i,j}a_{i j}^{2}\vert{\bf x}\vert^{2}. $$ Thus (6） $$ \left\|A\right\|\leq\left\{\sum_{i,j}a_{i j}^{2}\right\}^{1/2}. $$ If we apply (6)to B- A in place of A,where A,B e L(R", R"),we se $Q_{i j}$ that if the matrix elements au are continuous functions of a parameter, then the same is true of A. More precisely: 1f $\mathbf{\mu}_{k}^{\infty}$ is a metric space, if ai, .…， amm are real continuous functions on $\mathrm{S}_{\!_{J}}$ and if, for each p ∈ $\operatorname{\mathrm{exp}}^{*}$ $\textstyle{A_{p}}$ ,is the linear transformation of $R^{n}$ into Rm whose matrix has entries a;(p)then the mapping p→ $\textstyle A_{p}$ is a continuous mapping of $|\stackrel{ arrow}{k}\rangle$ into L(R",R") DIFFERENTIATION 9.10 Preliminaries In order to arrive at a definition of the derivative of a function whose domain is A R" ” (or an open subset of R"),let us take another look $R^{n}$ at the familiar case n = 1,and let us see how to interpret the derivative in that case in a way which will naturally extend to n > 1. If fis a real function with domain (a, $\scriptstyle b\neq R^{n}$ and if x∈(a,b), then f'(x is usually defined to be the real number （7） $$ \operatorname*{lim}_{h\to0}{\frac{f(x+h)-f(x)}{h}}, $$ provided, of course, that this limit exists. Thus (8） $$ f(x+h)-f(x)=f^{\prime}(x)h+r(h) $$ where the“remainder”’r(h) is small, in the sense that (9） $$ \operatorname*{lim}_{h arrow0}{\frac{r(h)}{h}}=0. $$212 PRINCTPLES Or MATHEMATICAL ANALYsIs Note that （8) expresses the difference f(x + h-f(x) as the sum of the linear function that takes $\textstyle{\int}_{\theta}$ to f(x)h, plus a small remainder but as the linear operator on we can therefore regard the derivative of f at x, not as a real number $\textstyle{\mathcal{Q}}^{1}$ that takes h to f'(x)h. [Observe that every real number c gives rise to a linear operator on $\textstyle{\mathcal{N}}$ the operator in question is simply multiplication by α. Conversely, every linear natural 1-l correspondence between $\textstyle{\mathcal{R}}^{1}$ is multiplication by some real number. It is this function that carries Rl to $\textstyle{\mathcal{R}}^{\mathrm{L}}$ l and L(Rl) which motivates the pre ceding statements.1 Let us next consider a function f that maps $(a,b)\subset R^{\operatorname{I}}$ into $R^{m}.$ In that case,f'(x) was defined to be that vector y e R”(if there is one) for which (10) $$ \operatorname*{lim}_{h\to0}{\Big(}{\frac{f(x+h)-\mathbf{f}(x)}{h}}-\mathbf{y}{\Big)}=0. $$ We can again rewrite this in the form (11） $$ \mathbf{f}(x+h)-\mathbf{f}(x)=h\mathbf{y}+\mathbf{r}(h), $$ where rCh)/h→0O as h→0.The main term on the right side of（1) is again a linear function of h.Every ye $R^{m}$ induces a linear transformation of $\textstyle{\mathcal{R}}^{\mathrm{T}}$ R into R", by associating to each he $:R^{1}$ the vector hy e R”. This identification or $\textstyle K^{m}$ with L(R', R") allows us to regard f(Xx) as a member of L(R',R") into $R^{m}$ that satisfies and if xe (a, b), Thus,iffis differentiable mapping of (a,b)e $\textstyle{\mathcal{R}}^{1}$ Rl into $R^{m},$ $\textstyle{\mathcal{R}}^{1}$ then f(x) is the linear transformation of (12） $$ \operatorname*{lim}_{h\to0}{\frac{f(x+h)-\mathbf{f}(x)-\mathbf{f}^{\prime}(x)h}{h}}=0, $$ or, equivalently, (13) $$ \operatorname*{lim}_{h\to0}{\frac{|f(x+h)-\mathbf{f}(x)-\mathbf{f}^{\prime}(x)h|}{|h|}}=0 $$ We are now ready for the case n > 1. 9.11 Definition Suppose ${\widehat{\mathcal{H}}}^{\nu}$ is an open set in R",f maps E into R", and xe ${\mathcal{F}},$ If there exists a linear transformation A of $\textstyle R^{n}$ into R" such that (14) $$ \operatorname*{lim}_{\mathbf{h\to0}}{\frac{|f(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A\mathbf{h}|}{|\mathbf{h}|}}=0, $$ then we say that $\frac{\Theta}{k}$ is differentiable at x, and we write (15) $$ \hat{I}\left(x\right)=A. $$ If f is differentiable at every xe E, we say that f is diferentiable in EFUNcriONs OF SEVERAL VARIABLES 213 $\overline{{\sharp}}($ is of course understood in(14) that he R". If |h|is small enough, then x ＋ h e $\textstyle E,$ E, since ${\widehat{\mathcal{H}}}$ is open. Thus f(x + h) is defined, f(x + h) e R",and since A ∈ L(R"，R"),Ah e R". Thus $$ \mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A\mathbf{h}\in R^{m}. $$ The norm in the numerator of（14) is that of ${\boldsymbol{R}}^{m}$ 、In the denominator we have the R"-norm of h. There is an obvious uniqueness problem which has to be settled before we go any further. 9.12 Theorem Suppose E and f are as in Defnition 9.11, xe E,and (14) holds with A = A, and with A = Az. Then $A_{1}=A_{2}$ Proof If B8 = 4i- A,，the incquality $$ |B{\bf h}|\leq|{\bf f}({\bf x}+{\bf h})-{\bf f}({\bf x})-A_{1}{\bf h}|+|f({\bf x}+{\bf h})-{\bf f}({\bf x})-A_{2}{\bf h}| $$ shows that |Bh|/|h」→0 as h→ 0. For fixed h≠ 0, it follows that (16） $$ {\frac{|B(t)|}{|t\mathbf{h}|}}\to0\qquad{\mathrm{~as~}}\ t\to0. $$ The linearity of B shows that the left side of（16）is independent of t Thus Bh = 0 for every he R".Hence B = 0. 9.13 Remarks (a） The relation (14) can be rewritten in the form (17） $$ \mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})=\mathbf{f}^{\prime}(\mathbf{x})\mathbf{h}+\mathbf{r}(\mathbf{h}) $$ where the remainder r(h) satisfies (18） $$ \operatorname*{lim}_{h\to0}{\frac{|\mathbf{r}(\mathbf{b})|}{|\mathbf{h}|}}=0. $$ We may interpret (17), as in Sec. 9.10,by saying that for fixed x and small h, the left side of (17)is approximately equal to f'(x)h, that is, to the valu of a linear transformation applied to h (b）Suppose f and ${\widehat{\mathcal{H}}}$ are as in Definition 9.11, and fis differentiable in E For every xe E, f(x) is then a function, namely, a linear transformation of R" into R”.But $\underline{{\oplus}}$ is also a function:f’maps E into L(R", R"). (c）A glance at (17) shows that fis continuous at any point at which f is differentiable (d）The derivative defined by (14) or (17) is often called the differentia of f at x,or the total derivative offat x,to distinguish it from the partial derivatives that will occur later.214 PRINCTPLES OF MATHEMATICAL ANALYSIS 9.14 Example We have defined derivatives of functions carrying $\textstyle R^{n}$ to R" to be linear transformations of $\textstyle R^{n}$ into R”. What is the derivative of such a linea transformation？ The answer is very simple If A ∈ L(R", R") and if xe R",then (19) $$ A^{\prime}(\mathbf{x})=A. $$ Note that x appears on the left side of（19),but not on the right. Both sides of(19) are members of L(R",R"),whereas Ax e R” The proof of（19） is a triviality, since (20) $$ A(\mathbf{x}+\mathbf{h})-A\mathbf{x}=A| $$ h. by the linearity of A、With f(x) = Ax, the numerator in (14) is thus O for every h e R". In (17), r(h) = 0 We now extend the chain rule (Theorem 5.5) to the present situation 9.15 Theorem Suppose E is an open set in R",fmaps E into R”,f is differentiable at Xo∈ E, g maps an open set containing f(E) into R', and g is differentiable a f(xo).Then the mapping ${\widehat{\left|x\right|}}$ of E into R' defined b $$ \mathbf{F}(\mathbf{x})=\mathbf{g}([\mathbf{x})) $$ is differentiable at $\mathbf{X}_{0}$ ，and (21） $$ {\bf F}^{\prime}({\bf x}_{0})={\bf g}^{\prime}({\bf f}({\bf x}_{0})){\bf f}^{\prime}({\bf x}_{0}). $$ On the right side of (21), we have the product of two linear transforma tions, as defined in Sec. 9.6. Proof Put $$ \mathbf{y}_{0}=\mathbf{f}(\mathbf{x}_{0}),\,A={\mathbf{f}}^{\prime}(\mathbf{x}_{0}),\,B=\mathbf{g}^{\prime}(\mathbf{y}_{0}),\,\mathbf{and} $$ define $$ \mathrm{u}({\bf h})=f({\bf x}_{0}+{\bf h})-f({\bf x}_{0})-A{\bf h}, $$ $$ \mathbf{v}(\mathbf{k})=g(\mathbf{y}_{0}+\mathbf{k})-g(\mathbf{y}_{0})-B\mathbf{k} $$ for all h e R” and ${\bf k}\in R^{n}$ for which $\mathbf{f}(\mathbf{x}_{\mathrm{0}}+\mathbf{h})$ and g(y。+ k) are defined. Then (22） $$ \left|\mathbf{u}(\mathbf{h})\right|=s(\mathbf{h})\left|\mathbf{h}\right|,\qquad\left|\mathbf{v}(\mathbf{k})\right|=\eta(\mathbf{k})\left|\mathbf{k}\right|, $$ where $s(\mathbf{h})\to0$ as h ${\mathbf{h}} arrow{\mathfrak{g}}$ and n(k)→0 as k→0 Given h, put k =f(xo ＋ h) -f(xo)、Then (23） and $$ |\mathbf{k}|=|A\mathbf{h}+\mathbf{u}(\mathbf{h})|\leq[\|A\|+s(\mathbf{h})]\mathbf{~}|\mathbf{h} $$ , $$ \begin{array}{r}{\mathbf{F}(\mathbf{x}_{0}+\mathbf{h})-\mathbf{F}(\mathbf{x}_{0})-B A\mathbf{h}=\mathbf{g}(\mathbf{y}_{0}+\mathbf{k})-\mathbf{g}(\mathbf{y}_{0})-B\gamma}\\ {=B(\mathbf{k}-A\mathbf{h})+\mathbf{v}(\mathbf{k})}\\ {=B\mathbf{u}(\mathbf{h})+\mathbf{v}(\mathbf{k}).}\end{array} $$ AhFUNCrIONs OF SEVERAL VARIABLES 215 Hence （22) and （23) imply, for h ≠ 0, that $$ {\frac{|\mathbf{F}(\mathbf{x}_{0}+\mathbf{h})-\mathbf{F}(\mathbf{x}_{0})-B A\mathbf{h}|}{|\mathbf{h}|}}\leq\|B\|\,\theta(\mathbf{h})+[\|A\|+s(\mathbf{h})]\eta(\mathbf{k}) $$ Let h→0. Then e(h)→0.Also,k→ 0,by （23),so that n(k)→0 It follows that F’(xo) = BA, which is what (21） asserts. 9.16 Partial derivatives We again consider a function f that maps an open set Ec R" into R”.Let $\{\mathbf{e_{1}},\cdot\cdot\cdot,\mathbf{e}_{n}\}$ and {u1,….，um} be the standard bases of R” and $R^{m},$ The components of f are the real functions f, ...,.Jm defined by (24） $$ \mathbf{f}(\mathbf{x})={\frac{m}{2\omega}}\,f_{i}(\mathbf{x})\mathbf{u}_{i}\qquad(\mathbf{x}\in E), $$ or, equivalently, by f,(x) = f(x)·u,，1 ≤i≤ m For xe E, 1 ≤i≤ m, $\textstyle{\frac{\omega}{\beta_{\mathrm{i}}}}$ ≤i≤n, we define (25） $$ (D_{j}f_{i})({\bf x})=\operatorname*{lim}_{t arrow0}{\frac{f_{i}({\bf x}+t e_{j})-f_{i}({\bf x})}{t}}, $$ provided the limit exists.、 Writing f(Kx,.….，x,)in place of f,(x),we see that $D_{J}f_{i}$ is the derivative of f with respect to x,，,keeping the other variables fixed The notation (26) 0f ax, is therefore often used in place of $D_{i j}f_{i},$ and $D_{j}f_{i}$ f, is called a partial derivative In many cases where the existence of a derivatve s uffcient when dealing with functions of one variable, continuity or at least boundedness of the partial derivatives is needed for functions of several variables. For example,the functions f and g described in Exercise 7, Chap. 4, are not continuous, although their partial derivatives exist at every point of R. Even for continuous functions the existence of all partial derivatives does not imply differentiability in the sense of Definition 9.11； see Exercises G and 14, and Theorem 9.21. However, if f is known to be differentiable at a point x, then its partia derivatives exist at x, and they determine the linear transformation f'(x completely: 9.17 Theorem Suppose fmaps an open set E c R" into R",andf is differentiable at a point xe E. Then the partial derivatives（D,f)(x) exist, and (27) $$ \mathbf{f}^{\prime}(\mathbf{x})\mathbf{e}_{j}=\sum_{i=1}^{m}\,(D_{j}f_{i})(\mathbf{x})\mathbf{u}_{i}\qquad(1\leq j\leq n). $$216 PRINCrPLES OF MATHEMATICAL ANALYSIS Here, as in Sec. 9.16, {e,…,e,} and $\{\mathbf{u}_{1},\dots,\mathbf{u}_{m}\}$ are the standard bases of $\textstyle{\mathcal{R}}^{n}$ and R” Proof Fix j. Sincefis differentiable at x $$ \mathbf{f}(\mathbf{x}+t\mathbf{e}_{j})-\mathbf{f}(\mathbf{x})={\hat{\mathbf{f}}}^{\prime}(\mathbf{x})(t\mathbf{e}_{j})+\mathbf{r}(t\mathbf{e}_{j}) $$ where 」r(te,)|/1→0 as t→0. The linearity of f'(x) shows therefore that (28) $$ \operatorname*{lim}_{t arrow0}{\frac{\mathbf{f}(\mathbf{x}+t\mathbf{e}_{j})-\mathbf{f}(\mathbf{x})}{t}}=\mathbf{f}^{\prime}(\mathbf{x})\mathbf{e}_{j}. $$ If we now represent f in terms of its components,as in（24), then（28 becomes (29) $$ \operatorname*{lim}_{t arrow0}\;\sum_{i=1}^{m}\frac{f_{i}({\bf x}+t\Theta_{j})-f_{i}({\bf x})}{t}\,{\bf u}_{i}=f^{\prime}({\bf x})\mathrm{e}_{j}\,. $$ It follows that each quotient in this sum has a limit, as t→0 (see Theorem 4.10)so that each（D,/)(x) exists, and then (27) follows from (29) Here are some consequences of Theorem 9.17: Let [f'(x)] be the matrix that represents f’(x) with respect to our standard bases,as in Sec. 9.9. Then f(x)e;is the jth column vector of [f′(x)], and（27) shows therefore that the number（D,f)x) occupies the spot in the ith row and jth column of [f'(x)]. Thus $$ [{\bf f}^{\prime}({\bf x})]=\left[\begin{array}{l l}{{\left(D_{1}f_{1}\right)({\bf x})}}&{{\cdots}}&{{\left(D_{n}f_{1}\right)({\bf x})}}\\ {{\cdots}}&{{\cdots}}&{{\cdots\cdots\cdots\cdots\cdots\cdots\right].}}\\ {{\left[(D_{1}f_{m})\left({\bf x}\right)\right.\cdots}}&{{\left(D_{n}f_{m}({\bf x})\right)}}\end{array}. $$ If $\mathbf{\hat{n}}=\Sigma h_{j}\mathbf{e}_{j}$ is any vector in I $\textstyle R^{n}\!\!_{*}$ R",then (27) implies that (30) $$ \mathbf{f}^{\prime}(\mathbf{x})\mathbf{h}=\sum_{i=1}^{m}\left\{\sum_{j=1}^{n}\,(D_{j}f_{i})(\mathbf{x})h_{j}\right\}\mathbf{u}_{i}\,. $$ 9.18 Example Let ，be a differentiable mapping of the segment $(a,b)\subset R^{\prime}$ into an open set $E\subset R^{n},$ in other words, y is a differentiable curve in E. Let $\int_{0}^{x}H_{\mathbf{\delta}}$ be a real-valued differentiable function with domain ${\boldsymbol{E}}.$ Thus fis a differentiable mapping of ${\widehat{\mathcal{H}}}$ into $\textstyle{\mathcal{R}}^{1}$ . Define (31） $$ g(t)=f(\gamma(t))\quad(a<t<b). $$ The chain rule asserts then that (32) $$ g^{\prime}(t)=f^{\prime}(\gamma(t))\gamma^{\prime}(t)\qquad(a<t<b). $$FUNCrIONS OF SEVERAL VARIABLES 217 Since y'(t)e L(R',R"）and f'(y(t))e L(R", R'),（32） defines g(t）as a linear ${\mathfrak{g}}^{\prime}(t)$ operator on $\textstyle{\mathcal{R}}^{1}$ .This agrees with the fact that g maps (a, b) into R'. However can also be regarded as a real number.（This was discussed in Sec. 9.10. This number can be computed in terms of the partial derivatives of f and the derivatives of the components of y,as we shall now see. With respect to the standard basis {e,...,e,} of R"，[y′(t)〕 is the n by 1 matrix (a “column matrix”) which has y(t）in the ith row, where y1,.…. Yn are the components of y. For every xe E,[f"(x)] is the l by n matrix (a“row matrix”) which has $(D_{J}f)(\mathbf{x})$ in the jth column. Hence[g'(t)] is the l by l matrix whose only entry is the real number (33） $$ {\mathfrak{g}}^{\prime}(t)=\sum_{i=1}^{n}(D_{i}f)(\gamma(t))\gamma_{i}^{\prime}(t). $$ This is a frequently encountered special case of the chain rule. It can be rephrased in the following manner. Associate with each xe E a vector,the so-called“gradient”of f at x defined by (34） $$ (\nabla f)(\mathbf{x})=\textstyle{\frac{\pi}{2\pi}}(D_{i}f)(\mathbf{x})\mathbf{e}_{i}. $$ Since (35) $$ \gamma^{\prime}(t)=\sum_{i=1}^{n}\gamma_{i}^{\prime}(t)\mathrm{}\mathrm{}\mathrm{}\mathrm{}\gamma_{i} $$ (33) can be written in the form (36) $$ g^{\prime}(t)=(\nabla f)(\gamma(t))\cdot\gamma^{\prime}(t), $$ the scalar product of the vectors(VfX((t)） and y'(t) Let us now fix an $\mathbf{x}\in E,$ let $\mathbf{u}\in R^{u}$ be a unit vector (that is, |ul= 1), and specialize ）so that (37） $$ \gamma(t)={\bf x}+t{\bf u}\qquad(-\;\infty<t<\infty) $$ Then ′(t) = u for every t. Hence(36) shows that (38) $$ {\mathfrak{g}}^{\prime}(0)=(\nabla f)(\mathbf{x})\cdot\mathbf{u}. $$ On the other hand,(37) shows that $$ g(t)-g(0)=f(\mathbf{x}+t\mathbf{u})-f(\mathbf{x}). $$ Hence （3B) gives (39） $$ \operatorname*{lim}_{t arrow0}{\frac{f(\mathbf{x}+t\mathbf{u})-f(\mathbf{x})}{t}}=(\nabla f)\left(\mathbf{x}\right)\cdot\mathbf{u}. $$218 PRINCIPLES OF MATHEMATICAL ANALYSIs The limit in (39) is usually called the directional derivative of fat x,in the direction of the unit vector u, and may be denoted by（D,f)XKx) f f and x are fixed, but u varies, then (39) shows that（D,f）Xx) attains its maximum when u is a positive scalar multiple of（Vf)(x).[The case（Vf)(x) = 0 should be excluded here.1 If u = Zu,e, then (39) shows that（D.,J)Xx) can be exprssed in terms of the partial derivatives of f at x by the formula (40） $$ (D_{\mathrm{u}}f)(\mathbf{x})=\sum_{i=1}^{n}(D_{i}f)(\mathbf{x})u_{i}. $$ Some of these ideas will play a role in the following theorem. 9.19 Theorem Suppose f maps a convex open set $E<K^{n}$ into R",f is differen tiable in E, and there is a real number M such that $$ \|f^{\prime}(\mathbf{x})\|\leq M $$ for every xe E. Then $$ |f({\bf b})-{\bf f}({\bf a})|\leq M|{\bf b}-{\bf a}| $$ for all a e E,be $\textstyle E.$ Proof Fix a ∈ E, b e E. Define $$ \gamma(t)=(1-t)_{\mathrm{a}}+t{\mathrm{b}} $$ for all t∈ $\textstyle{\mathcal{R}}^{1}$ such that y(t)∈ E、 Since E is convex,y(t)e E if O ≤t≤ 1. Put $$ \mathbf{g}(t)={\bf f}(\gamma(t)). $$ Then $$ {\mathfrak{g}}^{\prime}(t)=f^{\prime}(\gamma(t))\gamma^{\prime}(t)=f^{\prime}(\gamma(t))({\mathfrak{h}}-{\mathfrak{a}}), $$ so that $$ \left|\mathbf{g}^{\prime}(t)\right|\leq\left|\mathbb{F}^{\prime}(\gamma(t))\right|\left|\left|\mathbf{b}-\mathbf{a}\right|\leq M |\mathbf{b}-\mathbf{a}\right| $$ for all t e [0,1]. By Theorem 5.19 $$ |\mathbf{g}(1)-\mathbf{g}(0)|\leq M|\mathbf{b}-\mathbf{a}|. $$ But g(0) = f(a) and g(1) =f(b). This completes the proof Corollary I1, in addition, f(x) = 0 for all x e E, then f is constant. Proof To prove this, note that the hypotheses of the theorem hold now with M = 0.rUNcrioNs or SEVERAL VAULABLEs 219 9.20 Definition A diffrentiable mapping f of an open set $E\prec R^{n}$ into $R^{m}$ is said to be continvously differentiable in E if f’is a continuous mapping of E into L(R"，R") and to every s> 0 More cxplicitly, it is required that to every xe ${\widetilde{\cal H}},$ corresponds a > 0 such that $$ \|{\dot{S}}^{\prime}({\bf y})-{\dot{\bf f}}^{\prime}({\bf x})\|<\varepsilon $$ if y e E and lx- y」<员 If this is so, we also say that fis a G'-mapping, or that fe G′(E） 9.21 Theorem Suppose f maps an open set Ec R" into R”.Then fe G'(E) V and only f th partial derivatives D,f, exist ond are contimuous on E for l≤i≤ m 1 ≤j≤n. Proof Assume first that fe G′(E).By (27) $$ (D_{J}f_{i})({\bf x})=(\bar{\bf r}^{\prime}({\bf x})\mathrm{e}_{j})\cdot\mathrm{u}_{i} $$ for all i, j, and for all xe E. Hence $$ (D_{J}f_{i})({\bf y})-(D_{j}f_{i})({\bf x})=\{[f^{\prime}({\bf y})-{\bf f^{\prime}}({\bf x})]\mathrm{e}_{j}\}\cdot\mathbf{u}_{i} $$ and since $|\mathbf{u}_{i}|=|\mathbf{e}_{j}|=1,$ it follows that $$ |(D_{j}f_{i})({\bf y})-(D_{j}f_{i})({\bf x})|\leq\left|\,[\xi^{\prime}({\bf y})-\xi^{\prime}({\bf x})]\mathrm{e}_{j}\right| $$ ≤If"Gy) -f′(x)川 Hence D, f is continuous. For the converse,it suffices to consider the case m =1.（Why ?) Fix xe ${\mathcal{H}}^{\nu}$ and :> 0.Since E is open, there is an open ball $\textstyle s\in E,$ with center at x and radius r, and the continuity of the functions Df shows that r can be chosen so that (41） $$ \left. |(D_{j}f)({\bf y})-(D_{j}f)({\bf x})\right|<\frac{\hbar}{n}\quad\quad({\bf y}\in S,\,1\leq j\leq n). $$ Suppose h = Zh,e,|h|<r, put $v_{0}=0,\,$ and v,= h,eq +·' + h,e for 1≤k≤n. Then (42) $$ f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})=\sum_{j=1}^{n}\,[f(\mathbf{x}+\mathbf{v}_{j})-f(\mathbf{x}+\mathbf{v}_{j-1})]. $$ Since |v|<r for l≤k≤n and since Sis convex, the segments with end points x + V,- and $\mathbf{x}+\mathbf{v}_{j}$ lie in S. Since $\mathbf{v}_{j}=\mathbf{v}_{j-1}+h_{j}\mathbf{e}_{j}$ ,the mean value theorem （5.10) shows that the jth summand in(42) is equal to $$ h_{j}(D_{j}f)(\mathbf{x}+\mathbf{v}_{j-1}+\theta_{j}h_{j}e_{j}) $$220 PRINCIPLES OF MATHEMATICAL ANALYSIs for some 0,∈(0,1),and this differs from h,D,f(x) by less than h,|8/n using (41)、By (42), it follows that $$ \left|f({\bf x}+{\bf h})-f({\bf x})-\sum_{j=1}^{n}h_{j}(D_{j}f)({\bf x})\right|\leq\frac{1}{n}\sum_{j=1}^{n}\,\vert\,h_{j}\vert\,s\leq\vert{\bf h}\vert\,d_{j}\vert\,d{\bf t}\,\leq\,\vert\,. $$ for all h such that $\mathbf{a}|<r,$ This says that f is differentiable at x and that f"(x)is the linear function which assigns the number Eh,(D,f)(x) to the vector $\mathbf{h}=\Sigma h_{j}\mathbf{e}_{j}$ The matrix [f'(x)] consists of the row（D,J)(x),...,（D,f)(x);and since D, .… $D_{n}f$ are continuous functions on E, the concluding remarks of Sec. 9.9 show that f∈ C′(E) THE CONTRACTION PRINCIPLE We now interrupt our discussion of differentiation to insert a fixed point theorem that is valid in arbitrary complete metric spaces. It will be used in the proof of the inverse function theorem. 9.22 Definition Let $\textstyle X$ be a metric space, with metric d. If p maps $X$ into X and if there is a number C<l such that (43) $$ d(\varphi(x),\;\varphi(y))\leq c\;d(x,\,y) $$ for all x,ye X, then op is said to be a contraction of X into X. 9.23 Theorem If X is a complete metric space,and if p is a contraction of $\lambda$ into X, then there exists one and only one xe X such that qo(x) = x. In other words,gp has a unique fixed point. The uniqueness is a triviality for if (x) =x and p(y) = y, then (43) gives d(x, y)≤c d(x, y), which can only happen when d(x, )= 0. The existence of a fixed point of p is the essential part of the theorem The proof actually furnishes a constructive method for locating the fixed point Proof Pick xo∈ X arbitrarily, and define {x,} recursively,by setting (44） $$ x_{n+1}=\varphi(x_{n})\qquad(n=0,\,1,\,2,\,\cdot\cdot) $$ Choose c<lso that (43) holds. For n ≥l we then have $$ d(x_{n+1},\,x_{n})=d(\varphi(x_{n}),\,\varphi(x_{n-1}))\leq c\,d(x_{n},\,x_{n-1}). $$ Hence induction gives (45) $$ d(x_{n+1},\,\,x_{n})\leq c^{n}\,d(x_{1},\,x_{0})\qquad(n=0,\,1,\,2,\,\cdot\cdot). $$FUNCrIONs OF SEVERAL VARIABLES 221 If n <m,it follows that $$ \begin{array}{l}{{d(x_{n},\,x_{m})\leq\sum_{i={n+1}}^{m}d(x_{i},\,x_{i-1})}}\\ {{\,}}\\ {{\leq\left(c^{n}+c^{n+1}+\cdots+c^{m-1}\right)d(x_{1},\,x_{0})}}\\ {{\leq\left[(1-c)^{-1}\,d(x_{1},\,x_{0})\right]c^{n}.}}\end{array} $$ Thus {x,}is a Cauchy sequence.Since X is complete, lim $x_{n}=x$ for some x ∈ X Since p is a contraction,p is continuous (in fact, uniformly con- tinuous) on X. Hence $$ \varphi(x)=\operatorname*{lim}_{n\to\infty}\varphi(x_{n})=\operatorname*{lim}_{n\to\infty}x_{n+1}=x. $$ THE INVERSE FUNCTION THEOREM The inverse function theorem states, roughly speaking,that a continuousl diferentiable mapping fis invertible in a neighborhood of any point x at which the linear transformation f'(x) is invertible： 9.24 Theorem Suppose fis a G'-mapping of an open set Ec R" into R",f′(a) is invertible for some a ∈ ${\mathcal{F}}^{\gamma}$ ,and I ${\mathfrak{b}}={\mathfrak{f}}({\mathfrak{a}})$ Then (a）there exist open sets $\textstyle\zeta J$ and Vin $\textstyle{R^{\prime}}$ such that ae U, be V,f is one-to one on U,and f(U) = V; (b）i gis the inverse of f [which exists, by(a)] defined in Vby $$ g(\mathbf{f}(\mathbf{x}))=\mathbf{x}\qquad(\mathbf{x}\in U), $$ then ge 6′(V). Writing the equation $\mathbf{y}=\mathbf{f}(\mathbf{x})$ ）in component form, we arrive at the follow ing interpretation of the conclusion of the theorem: The system of n equations $$ y_{i}=\mathcal{P}_{i}(\mathcal{X}_{1},\ \cdot\ ,\ \times_{n})\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, $$ can be solved for x,， ..x, in terms of y,.………， A，if we restrict x and y to sma enough neighborhoods of a and b; the solutions are unique and continuously differentiable. Proof (a）Put f'(a) = A, and choose ${\mathcal{N}}$ so that (46） $$ 2\lambda||A^{-1}||=1. $$222 PRINCrPLES OF MATHEMATICAL ANALYSIs Since f'is continuous at a, there is an open ball U c E, with center at a, such that (47） $$ \|\mathbb{F}^{\prime}(\mathbf{x})-A\|<\lambda $$ We associate to each y e R" a function g, defined by (48) $$ \varphi(\mathbf{x})=\mathbf{x}+A^{-1}(\mathbf{y}-\mathbf{f}(\mathbf{x}))\qquad(\mathbf{x}\in E). $$ Note that f(x) =y if and only ifx is a fixed point of p Since p'(x) =1- A-1f'(x) = 4-1(A -f'(x),（46) and（47）imply that (49) $$ \|\varphi^{\prime}(\mathbf{x})\|<{\frac{1}{2}}\qquad(\mathbf{x}\in U). $$ Hence (50) $$ |\varphi(\mathbf{x}_{1})-\varphi(\mathbf{x}_{2})|\leq{\frac{1}{2}}|\mathbf{x}_{1}-\mathbf{x}_{2}|\qquad(\mathbf{x}_{1},\mathbf{x}_{2}\in U), $$ by Theorem 9.19.It follows that p has at most one fixed point in U, so that f(x) =y for at most one xe U Thus f is 1 -1 in U. Next, put V =f(U),and pick yoe V. Then yo=f(xo） for some ${\bf x}_{0}\in U.$ Let B be an open ball with center at $\mathbf{X}_{\mathrm{0}}$ and radius r> 0, so small that its closure B is in U. Wewill show that y e V whenever ly- yol<入 This proves,of course,that V is open. Fix y,ly - yo|<Ar.With p as in（48) $$ |\varphi({\bf x}_{o})-{\bf x}_{o}|=|A^{-1}({\bf y}-{\bf y}_{o})|<\|A^{-1}\|\lambda r=\frac{r}{2}. $$ If x e ${\overline{{B}}},$ it therefore follows from(50) that $$ |\varphi(\mathbf{x})-\mathbf{x}_{0}|\leq|\varphi(\mathbf{x})-\varphi(\mathbf{x}_{0})|+|\varphi(\mathbf{x}_{0})-\mathbf{x}_{0}| $$ $$ <\frac{1}{2}\left|\mathbf{x}-\mathbf{x}_{0}\right|+\frac{r}{2}\leq r; $$ hence g(x)e B.Note that (50) holds if $\mathbf{X}_{1}$ e B,X,e B Thus p is a contraction of $\overline{{\beta}}$ into B. Being a closed subset of $\textstyle R^{n}\!.$ $\overline{{\beta}}$ is complete. Theorem 9.23 implies therefore that cp has a fixed point x e B. For this x,f(x) = y. Thus y e f(B) C f(U)= V. This proves part (a) of the theorem (b）Pick $\mathbf{y}\in V,\mathbf{y}+\mathbf{k}$ e V. Then there exist xe U,x + h∈ U,so that $\mathbf{y}=f(\mathbf{x}),\,\mathbf{y}+\mathbf{k}=f(\mathbf{x}+\mathbf{h}).$ With p as in(48), (51） By（ $$ \begin{array}{c}{{\varphi({\bf x}+{\bf h})-\varphi({\bf x})={\bf h}+A^{-1}[f({\bf x})-{\bf f}({\bf x}+{\bf h})]={\bf h}-A^{-1}{\bf k}.}}\\ {{50),\ |{\bf h}-A^{-1}{\bf k}|\leq\frac{1}{2}|{\bf h}|\cdot\mathrm{~Hence~}|A^{-1}{\bf k}|\geq\frac{1}{2}|{\bf k}|,\mathrm{~and~}}}\\ {{\mathrm{~[h}|\leq2||A^{-1}||\cdot\mathrm{~}|{\bf k}|=\lambda^{-1}|{\bf k}|.}}\end{array} $$FUNcrioNs or SEVERAL VARUIABLES 223 By (46),(47), and Theorem 9.8,f'(x) has an inverse, say T.Since 8 $$ f(\mathbf{y}+\mathbf{k})-\mathbf{g}(\mathbf{y})-T\mathbf{k}=\mathbf{h}-T\mathbf{k}=-T[\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-\mathbf{f}^{\prime}(\mathbf{x})\mathbf{h}], $$ (51) implies $$ {\frac{|\mathbf{g}(\mathbf{y}+\mathbf{k})-\mathbf{g}(\mathbf{y})-T\mathbf{k}|}{|\mathbf{k}|}}\leq{\frac{|T|}{\lambda}}\cdot{\frac{|f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})-\mathbf{f}^{\prime}(\mathbf{x})\mathbf{h}|}{|\mathbf{h}|}}\,. $$ As k→0,(51) shows that h→0. The right side of the last inequality thus tends to O. Hence the same is true of the left. We have thus proved that g'(y) = 7. But T was chosen to be the inverse off'(x) =f'(g(y). Thus (52） $$ {\mathfrak{G}}^{\prime}({\mathbf{y}})=\{{\mathfrak{F}}^{\prime}({\mathfrak{g}}({\mathbf{y}}))\}^{-1}\qquad({\mathfrak{y}}\in V). $$ Finally, note that g is a continuous mapping of ${\mathfrak{I}}^{\gamma}$ onto U (since g is differentiable), that f $\textstyle{\oplus_{\mathrm{i}}}^{\infty}r$ is a continuous mapping of $\bar{\boldsymbol{\theta}}$ into the set $\underline{{\left(\gamma\right)}}$ of all invertible elements of L(R"), and that inversion is a continuous mapping of Q onto Q, by Theorem 9.8.If we combine these facts with (52), we see that g e C′(V) This completes the proof. Remark. The full force of the assumption that fe G(E) was only used in the last paragraph of the preceding proof. Everything else, down to Eq.(52) was derived from the existence of f'(x) for xe E, the invertibility of f'(a), and the continuity of f’at just the point a. In this connection, we refer to the article by A. Nijenhuis in Amer. Math. Monthly, vol.81,1974, pp. 969-980. The following is an immediate consequence of part (a)of the inverse function theorem. 9.25 Theorem U/f isaC'-mapping of an open st ${\widetilde{\cal H}}$ e $R^{n}$ into R" and ifI(x八 is invertible for every xe E, then f（W) is an open subset of R" for every open set W c E. In other words,fis an open mapping of E into R” The hypotheses made in this theorem ensure that each point x∈ $\textstyle{\mathcal{F}}$ has a neighborhood in which f is 1-1. This may be expressed by saying that f is locally one-to-one in E. But f need not be 1-1 in E under these circumstances For an example, see Exercise 17. THE IMPLICIT FUNCTION THEOREM If f is a continuously differentiable real function in the plane, then the equation /(x, y) =0 can be solved for y in terms of xin a neighborhood of any point224 PRINCIPLES OF MATHEMATICAL ANALYSIs (a,b) at which f(a,b) = 0 and af/ay ≠ 0. Likewise, one can solve for x in terms of y near (a,b) if ef/8x ≠ 0 at (a,b). For a simple example which illustrates the need for assuming af/ay ≠ 0, consider f(x $y)=x^{2}+y^{2}-1$ The preceding very informal statement is the simplest case（the case $m=n=1$ of Theorem 9.28) of the so-called“implicit function theorem.”Its proof makesstrong use of the fact that continuously differentiable transformations behave locally very much like their derivatives. Accordingly,we first prove Theorem 9.27, the linear version of Theorem 9.28. 9.26 Notation If $$ {}^{\mathbf{\chi}}\mathbf{x}=\left(x_{1},\mathbf{\ddots}\circ\mathbf{\lambda}\cdot\mathbf{\lambda}\cdot x_{n}\right)\in R^{n}{\mathrm{~and~}}\mathbf{y}=\left(y_{1},\mathbf{\lambda}\cdot\mathbf{\lambda}\cdot\mathbf{\lambda}\cdot y_{m}\right)\in $$ R”, let us write (x, y) for the point (or vectorp $$ \left(x_{1},\ \cdot\cdot\cdot\cdot x_{n},\ y_{1},\ \cdot\cdot\cdot,\ y_{m}\right)\in R^{n+m}. $$ In what follows, the first entry in (x,y) or in a similar symbol will always be a vector in R", the second will be a vector in R". Every Ae L(R"*~ ${\boldsymbol{R}}^{n}{\boldsymbol{\beta}}\,$ ) can be split into two linear transformations ${\mathcal{A}}_{x}$ and 4,, defined by (53) $$ A_{x}\mathbf{h}=A(\mathbf{h},\mathbf{0}),\qquad A_{y}\mathbf{k}=A(\mathbf{0},\mathbf{k}) $$ for any he R",k e Rm. Then A,e L(R"),A,e L(R”，R"), and (54) $$ A(\mathbf{h},\mathbf{k})=A_{x}\mathbf{h}+A_{y}\mathbf{k}. $$ The linear version of the implicit function theorem is now almost obvious 9.27 Theorem If A e L(R"+”,R") and if A,is invertible, then there corresponds to every k∈ R" a unique he R”such that A(h, k) = 0. This h can be computed from k by the formula (55) $$ \mathbf{h}=-(A_{x})^{-1}A_{y}\mathbf{k}. $$ Proof By (54),A(h, k) = 0 if and only if $$ A_{x}\mathbf{h}+A_{y}\mathbf{k}=0, $$ which is the same as (55)） when ${\mathcal{A}}_{x}$ is invertible. The conclusion of Theorem 9.27is, in other words,that the equation ${\mathit{A}}(\mathrm{h,k})=0$ can be solved（uniquely) for h if k is given, and that the solution h is a linear function of k. Those who have some acquaintance with linear alebra will recognize this as a very familiar statement about systems of linear equations 9.28 Theorem Let f be a G'-mapping of an open set $E\subset R^{n+m}$ into ${\boldsymbol{R}}^{n},$ such that f(a,b) = 0 for some point (a,b)∈ E Put A =f′(a,b) and assume that ${\mathcal{A}}_{x}$ is invertible.FUNCriONs or SEVERAL VARIABLES 225 Then there exist open sets $U\subset R^{n+m}$ and W c R”，with（a，b) e $\bar{\zeta}\,\bar{\zeta}\,$ J and b∈ W, having the following property: To every ye W corresponds a unique x such that (56) (x,y)e U and f(x, y) = 0. lf this xis defined to be g(y), then g is a G'-mapping of W into R", g(b） = a (57） f(g(y), y) = 0 (y e W), and (58) $$ {\bf g}^{\prime}({\bf b})=-(A_{x})^{-1}A_{y}. $$ The function g is “implicitly”defined by(57).Hence the name of the theorem The equation f(x, y) = 0 can be written as a system of n equations in n + m variables: (59） $$ \begin{array}{l}{{f_{1}(x_{1},\ \cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot}}\\ {{\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot}}\\ {{f_{n}(x_{1},\ \cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot}}\\ {{f_{n}(x_{1},\ \cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot}}\end{array} $$ The assumption that ${\mathcal{A}}_{x}$ is invertible means that the n by n matrix $$ \begin{array}{c l l}{{\left[D_{1}f_{1}}}&{{\cdot\cdot\cdot}}&{{D_{n}f_{1}\right]}}\\ {{\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot}}&{{}}\\ {{\left[D_{1}f_{n}}}&{{\cdot\cdot}}&{{D_{n}f_{n}\right]}}\end{array} $$ evaluated at （a,b) defines an invertible linear operator in R"; in other words its column vectors should be independent,or, equivalently,its determinant should be +0.(See Theorem 9.36.）If, furthermore,(59) holds when x = a and y = b, then the conclusion of the theorem is that(59) can be solved for x1,.….,X。 in terms of y,.…，ym for every y near b, and that these solutions are continu ously differentiable functions of y Proof Define F by (60） $$ \mathbf{F}(\mathbf{x},\mathbf{y})=(f(\mathbf{x},\mathbf{y}),\mathbf{y})\qquad((\mathbf{x},\mathbf{y})\in E). $$ Then F is a C'-mapping of E into ${\boldsymbol{R}}^{n+m}$ 、We claim that $\mathbf{F}^{\prime}(\mathbf{a},\mathbf{b})$ is an invertible element of L(R"+“): Since f(a,b) = 0, we have $$ \mathbf{f}(\mathbf{a}+\mathbf{h},\,\mathbf{b}+\mathbf{k})=A(\mathbf{h},\,\mathbf{k})+\mathbf{r}(\mathbf{h},\,\mathbf{k}), $$ where r is the remainder that occurs in the definition of f'(a, b).Since $$ \begin{array}{c}{{\mathbf{F}(\mathbf{a}+\mathbf{h},\,\mathbf{b}+\mathbf{k})-\mathbf{F}(\mathbf{a},\,\mathbf{b})=(f(\mathbf{a}+\mathbf{h},\,\mathbf{b}+\mathbf{k}),\,\mathbf{k})}}\\ {{}}\\ {{}}&{{=(\mathbf{A}(\mathbf{h},\,\mathbf{k}),\,\mathbf{k})+(\mathbf{r}(\mathbf{h},\,\mathbf{k}),\,\mathbf{0})}}\end{array} $$226 PRINCIPLES OF MATHEMATICAL ANALYSIS it follows that F’(a，b) is the linear operator on $R^{n+m}$ that maps (h, k) to (A(h, k), k).If this image vector is O,then A(h, k) = 0 and k = 0, hence 4(h,0) = 0, and Theorem 9.27 implies that h = 0.It follows that F”(a,b) is 1-1； hence it is invertible（Theorem 9.5). The inverse function theorem can therefore be applied to F. It shows that there exist open sets U and V in R"+”, with (a,b) e U,(0,b) ∈ V, such that F is a 1-1 mapping of U onto V. We let W be the set of all $\mathbf{y}\in R^{m}$ such that (0,y)e V.Note that b e W It is clear that W is open since V is open. If y e W, then (0, y) = F(x, y) for some (x, y)e U. By (60), f(x, y) = 0 for this x Suppose, with the same y,that (x，y) ∈ $\bar{\zeta}\,\!\bar{\zeta}$ and f(x’，y) = 0. Then $$ \mathbf{F}(\mathbf{x}^{\prime},\mathbf{y})=(I(\mathbf{x}^{\prime},\mathbf{y}),\mathbf{y})=(I(\mathbf{x},\mathbf{y}),\mathbf{y})=\mathbf{F}(\mathbf{x},\mathbf{y}). $$ Since ${\sqrt{1}}^{1}$ is 1-1 in U, it follows that x’= x. This proves the first part of the theorem. For the second part, define g(y), for y e W,so that (g(y), y)∈ U and (5T) holds. Then (61） $$ \mathrm{{F}}(\mathrm{g}(\mathrm{y}),\mathrm{{y}})=\mathrm{(0,y)}\qquad(\mathrm{y}\in\mathcal{W}) $$ If G is the mapping of ${\mathfrak{J}}^{\gamma}$ onto U that inverts F, then G∈ C’，by the inverse function theorem, and (61) gives (62） $$ (\mathbf{g}(\mathbf{y}),\mathbf{y})=\mathbf{G}(\mathbf{0},\mathbf{y})\qquad(\mathbf{y}\in{\mathcal{W}}). $$ Since G e G'，(62) shows that ge C”. Finally, to compute g'(b), put (g(y),y) = 0(y). Then (63） $$ \Phi^{\prime}({\bf y}){\bf k}=({\bf g}^{\prime}({\bf y}){\bf k},\,{\bf k})\qquad({\bf y}\in W,\,{\bf k}\in R^{m}). $$ By (5T), f(0(y)）= 0 in W. The chain rule shows therefore that $$ f^{\prime}(\Phi(y))\Phi^{\prime}(y)=0. $$ When y = b,then Q(y) =(a,b), and f'(Q(y)） = A. Thus (64） $$ A\Phi^{\prime}({\mathfrak{h}})=0. $$ It now follows from (64),(63), and(54), that $$ A_{x}g^{\prime}({\bf b}){\bf k}\,+\,A_{y}{\bf k}\,=\,A({\bf g}^{\prime}({\bf b}){\bf k},\,{\bf k})\,=\,A\Phi^{\prime}({\bf b}){\bf k}\,=\,0 $$ for every ke R”. Thus (65） $$ A_{x}\wp^{\prime}({\bf b})+A_{y}=0. $$ruNcrroxs or SEVERAL VARIADLEs 227 This is equivalent to (58), and completes the proof Note. In terms of the components of f and g,(65) becomes $$ \sum_{j=1}^{n}(D_{j}f_{i})(\mathbf{a},\mathbf{b})(D_{k}g_{j})(\mathbf{b})=-(D_{n+k}f_{i})(\mathbf{a},\mathbf{b}) $$ or $$ \sum_{J=1}^{n}\,\left(\frac{\partial f_{i}}{\partial x_{J}}\right)\left(\frac{\partial g_{J}}{\partial y_{k}}\right)\,=\,-\,\left(\frac{\partial f_{i}}{\partial y_{k}}\right) $$ where 1≤i≤n,1≤k ≤m For each k,this is a system of n linear equations in which the derivatives og,/oyx(1 ≤i≤n) are the unknowns. 9.29 Example Take n = 2, m = 3, and consider the mapping f=（f,/2） of $R^{5}$ into $R^{2}$ given by $$ \begin{array}{c}{{f_{1}(x_{1},\,x_{2}\,,\,y_{1},\,y_{2}\,,\,y_{3})=2e^{x_{1}}+x_{2}\,y_{1}-4y_{2}+3}}\\ {{f_{2}(x_{1},\,x_{2}\,,\,y_{1},\,y_{2}\,,\,y_{3})=x_{2}\,\cos x_{1}-6x_{1}+2y_{1}-y_{3}\,.}}\end{array} $$ If a = (0,1） and b =(3,2,7), then f(a,b) = 0 With respect to the standard bases,the matrix of the transformation A =f′(a,b) is $$ [A]={\left[\begin{array}{l l l l l}{2}&{3}&{1}&{-4}&{0}\\ {-6}&{1}&{1}&{2}&{0}&{-1\right]}. $$ Hence $$ [A_{x}]={\left[\begin{array}{l l l}{2}&{3}\\ {-6}&{1}\end{array}\right]},\qquad[A_{y}]={\left[\frac{1}{2}}&{-4}&{0}\\ {0}&{-1\right]}. $$ We see that the column vectors of [A,] are independent. Hence A,is invertible and the implicit function theorem asserts the existence ofa G'-mapping g, defined in a neighborhood of(3,2,7), such that g(3,2,7) = (0,1） and f(g(y), y) = 0 We can use （58)to compute g'(3,2,T)：Since $$ [(A_{x})^{-1}]=[A_{x}]^{-1}={\frac{1}{20}}\left[{\frac{1}{6}}\right.\qquad2 ] $$ (58)gives [g' $$ (3,2,7)]=-\frac{1}{20}\left[\stackrel{1}{6}~-\frac{3}{2}\right]\left[2\sqrt{\mid2}\quad-\frac{4}{0}\quad-1\right]=\left[\frac{\frac{\lambda}{2}}{-\frac{\lambda}{2}}\quad\stackrel{\lambda}{\bar{\lambda}}\quad-\frac{3}{16}\right]. $$223 PRINCIPLES OF MATHEMATICAL ANALYSIs In terms of partial derivatives,the conclusion is that $$ \begin{array}{r l r l}{{D_{1}\,g_{1}={\frac{4}{2}}\,\,\,\,}}&{{D_{2}\,g_{1}={\frac{1}{3}}\,\,\,}}&{{D_{3}\,g_{1}=-{\frac{3}{16}}}}\\ {{D_{1}g_{2}=-{\frac{1}{2}}\,\,\,\,}}&{{D_{2}\,g_{2}={\frac{g}{3}}\,\,\,}}&{{D_{3}\,g_{2}={\frac{1}{16}}}\end{array} $$ at the point (3,2,7). THE RANK THEOREM Although this theorem is not as important as the inverse function theorem or the implicit function theorem, we include it as another interesting illstration of the general principle that the local behavior of a continuously differentiable mapping ${\sqrt{1}}^{1}$ near a point x is similar to that of the linear transformation F”(x) Before stating it, we need a few more facts about linear transformations 9.30 Definitions Suppose X and Y are vector spaces, and A∈ L(X, Y), as in Definition 9.6. The null space of A,.M(A), is the set of all xe X at which Ax $=0.$ It is clear that ${\mathcal{N}}(A)$ ）is a vector space in X. Likewise, the range of A,JX(A),is a vector space in Y The rank of A is defined to be the dimension of S(A) For example, the invertible elements of L(R"）are precisely those whose rank is n.This follows from Theorem 9.5. If A∈ L(X, Y)and A has rank O, then Ax = 0for all xe A,hence ${\mathcal{N}}(A)=X.$ In this connection, see Exercise 25. 9.31 Projections Let $X$ be a vector space. An operator Pe L(X) is said to be for every xe X. In a projection in X if P $\scriptstyle P^{2}=I$ P More explicitly, the requirement is that $P(P\mathbf{x})=P\mathbf{x}$ other words, $\stackrel{\mathbf{\mathcal{D}}}{\ =}$ fixes every vector in its range SX(P） Here are some elementary properties of projections: (a）If P is a projection in X, then every x∈ X has a unique representation of the form $$ \scriptstyle\mathbf{x}=\mathbf{x}_{1}+\mathbf{x}_{2} $$ where X,e SR(P),X2∈W(P) To obtain the representation, put X1= Px，X2 = X一 X1. Then follows that x, = Px Px, = Px - Px,= Px - P-x = 0. As regards the uniqueness, apply $\textstyle{\mathcal{D}}$ to the equation $\scriptstyle\mathbf{x}=\mathbf{x}_{1}+\mathbf{x}$ K2.Since x e A(P),Px,= x,; since $\scriptstyle\exp{-0}$ it (b）If X is a finite-dimensional vector space and if $X_{1}$ is a vector space in $X,$ then there is a projection ${\mathcal{L}}^{\mathcal{I}}$ in X with SR(P) = X1.rUNCTONs or SEVERAL VARIABLEs 229 If $X_{1}$ contains only O, this is trivial: put $\scriptstyle P_{X}=0$ for all x ∈ X Assume dim $X_{1}=k>0$ By Theorem 9.3,X has then a basis {u,，, such that $\left\{\Pi|_{1},\ \circ\ ,\ \mathbb{N}_{k}\right\}$ is a basis of X. Define $$ P(c_{1}\mathbf{u}_{1}+\cdots+c_{n}\mathbf{u}_{n})=c_{1}\mathbf{u}_{1}+\cdots+c_{k}\mathbf{u}_{k} $$ for arbitrary scalars $C_{1},$ ···Cn Then Px = x for every x ∈ $X_{1},$ ，and $X_{1}={\mathcal{B}}(P)$ Note that {ux+, .…，,is a basis of.M(P). Note also that there are infinitely many projections in X, with range $X_{1},$ if O < dim X,< dim X 9.32 Theorem Suppose m, n, r are nonnegative integers, m ≥r,n ≥r,上 ${\widehat{\operatorname{Q}^{1}}}$ is a C'-mapping of an open set $E\subset R^{n}$ into $R^{m}.$ R”, and F′(x) has rank r for every x∈ E Fix a ∈ $\textstyle E,$ put $A=\mathbb{F}^{\prime}(\mathbf{a}),$ let $\textstyle Y_{1}$ be the range of A,and let P be a projection in ${\boldsymbol{R}}^{m}$ whose range is $Y_{1}.$ . Let $\textstyle Y_{2}$ be the null space of P. Then there are open sets $\bar{\boldsymbol{\Pi}}$ and V in ${\boldsymbol{R}}^{n},$ ”, with a e U, $U\subset E,$ and there is a 1-1 6*'-mapping H of onto U（whose inverse is also of class G'） such that (66） $$ \operatorname{F}(\mathrm{H}({\bf x}))=A{\bf x}+\varphi(A{\bf x})\qquad({\bf x}\in V) $$ where op is a G'-mapping of the open set A4(V)c Y into Y, After the proof we shall give a more geometric description of the informa- tion that (66) contains. Proof If r = 0, Theorem 9.19 shows that F(x) is constant in a neighbor hood U of a, and (66) holds trivially, with $V=U,\,\mathrm{H}({\bf x})={\bf x},$ cp(0) = F(a) From now on we assume r> 0. Since dim Y =r， $Y_{1}$ has a basis $\{\mathbf{y}_{1},\dots,\mathbf{y}_{r}\}.$ Choose z,e R" so that Az;= y:(1 ≤i≤r), and define a linear mapping S of $\textstyle Y_{1}$ into $R^{n}$ by setting (67） $$ \begin{array}{r}{\mathbf{S}(c_{1}\mathbf{y}_{1}+\cdot\cdot\cdot\mathbf{\nabla}+\,c_{r}\mathbf{y}_{r})=c_{1}\mathbf{z}_{1}+\cdot\cdot\cdot\cdot\mathbf{\nabla}+\,c_{r}\,\mathbf{\bar{z}}_{i}}\end{array} $$ for all scalars $c_{1},\,\cdot\,\cdot\,\cdot\,,\,c_{r}$ Then $4S{\bf y}_{i}=A{\bf z}_{i}={\bf y}_{i}\;{\mathrm{for~}}1\leq i\leq r.\;\;\mathrm{Th}w$ (68） $$ A S\mathbf{y}=\mathbf{y}\qquad(\mathbf{y}\in Y_{1}). $$ Define a mapping $\overline{{\left({\frac{v}{3}}y\right)^{4}}}$ of ${\widetilde{F}}^{\nu}$ E into $R^{n}$ by setting (69） $$ \mathrm{G(x)=x+SP[F(x)-Ax]}\qquad(\mathrm{x}\in E). $$ Since F”(a） = 4,differentiation of（69) shows that C'(a) = I, the identity operator on R". By the inverse function theorem, there are open sets U and V in R",with a ∈ U, such that G is a 1-1 mapping of ${\mathcal{Y}}{\mathcal{Y}}$ onto V whose inverse His also ofclass G”：Moreover, by shrinking t and V, if ncessary we can arrange it so that V is convex and H(x) is invertible for every xe V230 PRINCrPLES OF MATHEMATICAL ANALYSIs Note that ASPA = A,since P4= A and(68) holds. Therefore (69 gives (70) $$ A{\mathrm{G}}({\bf{x}})=P{\mathrm{F}}({\bf{x}})\qquad({\bf{x}}\in E). $$ In particular,(70) holds for xe U. If we replace x by H(x), we obtain (71） Define $$ P{\mathrm{R}}({\mathrm{H}}({\bf x}))=A{\bf x}\qquad({\bf x}\in V). $$ (72） $$ \psi({\bf x})={\bf F}({\bf h i}({\bf x}))-A{\bf x}\qquad({\bf x}\in V). $$ Since PA = A,(71）implies that Pw(x) = 0 for all xe V. Thus y is a f'-mapping of ${\mathfrak{J}}^{\prime}$ is open, it is clear that A(V) is an open subset of its range Sinc ${\mathfrak{I}}^{\gamma}$ into Y.. X2(4) = Y To complete the proof,i.e.,to go from (72) to (66), we have to show that there is a G'-mapping p of A(V) into $\textstyle Y_{2}$ which satisfies (73） $$ \varphi(A\mathbf{x})=\psi(\mathbf{x})\qquad(\mathbf{x}\in V). $$ As a step toward （73), we will frst prove that (74) $$ \psi(\mathbf{x}_{1})=\psi(\mathbf{x}_{2}) $$ if x, e V, xz∈ V, Ax1= Ax2. Put O(x) = FGHIx))、 fr xe V Since H(Xx) has rank n for every xe V, and F"(x) has rank r for every xe U, it follows that (75） $$ \mathrm{\boldmath~\Delta~}\Phi^{\prime}({\bf x})=\mathrm{\boldmath~\rank~}\mathrm{F}^{\prime}({\bf H(x)}){\bf H}^{\prime}({\bf x})=r\qquad({\bf x}\in V). $$ Fix xe V. Let M be the range of b'(x). Then M c R”, dim M =r By (71), (76) $$ P\Phi^{\prime}({\bf x})=A. $$ Thus P maps M onto S2(A) = Y.Since M and Y, have the same di- mension, it follows that P (restricted to M） is 1-1. Suppose now that Ah = 0. Then PO'(x)h = 0,by（76). But Q'(x)h e M, and P is 1-1 on M. Hence O'(x)h = 0. A look at(72) shows now that we have proved the following: If x∈ V and Ah = 0, then y’(x)h = 0 We can now prove(74)、 Suppose $\mathbf{X}_{1}$ e V, xze V, Ax,= Axz、Put h = X2一 X, and define (77） $$ {\mathfrak{g}}(t)=\psi({\mathbf{x}}_{1}+t{\mathbf{h}})\qquad(0\leq t\leq1). $$ The convexity of ${\mathfrak{I}}^{\gamma}$ shows that $\mathbf{x}_{1}+t\mathbf{h}\,,$ e V for these t. Hence (78） 8 $$ {\mathfrak{g}}^{\prime}(t)=\psi^{\prime}({\mathbf{x}}_{1}+t{\mathbf{h}}){\mathbf{h}}=0\qquad(0\leq t\leq1), $$rUNCrioNs or sEVERAL VARIABLEs 231 so that g(1) = g(0). But g(1） = W(x,） and g(0) = W(x,)、 This proves(74) By (74),W(x) depends only on Ax,for xe V. Hence(73) defines p unambiguously in A(V).It only remains to be proved that gp ∈ G”. Fix yoe A(V), fx $\mathbf{X}_{0}$ e V so that $A\mathbf{x}_{0}=\mathbf{y}_{0}$ 、Since V is open, $\mathbf{y}_{0}$ has a neighborhood W in $\textstyle Y_{1}$ such that the vector (79) $$ {\bf x}={\bf x}_{0}\,+\,S({\bf y}-{\bf y}_{0}) $$ lies in V for all y ∈ W.By (68) $$ A x=A x_{0}+y-y_{0}=y. $$ Thus (73) and (79) give (80) $$ \varphi(\mathbf{y})=\psi(\mathbf{x}_{0}-S\mathbf{y}_{0}+S\mathbf{y})\qquad(\mathbf{y}\in W). $$ This formula shows that gp ∈ B’in W, hence in A(V), since yo was chosen arbitrarily in A(V) The proof is now complete Here is what the theorem tells us about the geometry of the mapping F If y e F(U) then y = F(H(x)） for some xe V, and (66) shows that Py = 4x. Therefore (81） $$ \mathbf{y}=P\mathbf{y}+\varphi(P\mathbf{y})\qquad(\mathbf{y}\in\mathbf{F}(U)). $$ This shows that $\mathcal{Y}$ is determined by its projection Py, and that P, restricted to F(U), is a l-l mapping of F(U） onto A(V). Thus F(U）is an “r-dimensional surface”with precisely one point “over”each point of A(V). We may also regard F(U)as the graph of cp. If O(x) = F(H(x), as in the proof, then （66) shows that the level sets of O (these are the sets on which b attains a given value) are precisely the level sets of A in V. These are “flat'”’since they are intersections with V of translates of the vector space W(A). Note that dim .M(A) = n -r(Exercise 25) The level sets of F in U are the images under H of the flat level sets of D in V. They are thus “(n - r)-dimensional surfaces”in $\textstyle\Gamma\$ DETERMINANTS Determinants are numbers associated to square matrices, and hence to the operators represented by such matrices. They are O if and only if the corre- sponding operator fails to be invertible. They can therefore be used to decide whether the hypotheses of some of the preceding theorems are satisfied. They will play an even more important role in Chap. 10232 PRINCIPLEs Oor MATHEMATICAL ANALYsis 9.33 Definition If （,……,),） s an ordered n-tuple of integers, defne (82) $$ s(j_{1},\cdot\cdot\cdot\cdot\cdot j_{n})=\prod_{p<q}{\mathrm{sgn}}\,(j_{q}-j_{p}) $$ 。 where sgn x=1 if x>0,sgn x = -1 ifx<0、sgn x = 0 if x=0. Then sCi,……….) = 1, -1, or O, and it changes sign if any two of the j's are inter- changed Let [A] be the matrix of a linear operator A on $\textstyle{R^{n}},$ ", relative to the standard basis $\left\{\Theta_{1},\ \cdot\cdot\cdot\right\}$ e,}, with entries ai,)） in the ith row and jth column. The deter- minant of [A]is defined to be the number (83) d $$ \mathrm{et}\;[A]=\sum s(j_{1},\dots,j_{n})a(1,j_{1})a(2,j_{2})\cdot\cdot\cdot a(n,j_{n}). $$ 1 ≤j, ≤ n. The sum in （83) extends overall ordered n-tuples of integers（,….)）with The column vectors $\mathbf{X}_{j}$ of [A] are (84) $$ {\bf x}_{j}=\sum_{i=1}^{n}a(i,j){\bf e}_{i}\qquad(1\leq j\leq n). $$ It will be convenient to think of det LA] as a function of the column vector of [A]. If we write $$ \operatorname*{det}\left(\mathbf{x}_{1},\dots,\mathbf{x}_{n}\right)=\operatorname*{det}\left[A\right]\!, $$ det is now a real function on the set of all ordered n-tuples of vectors in $\textstyle R^{n}$ 9.34 Theorem (a）If I is the identity operator on $\textstyle R^{n}$ , then $$ \operatorname*{det}\left[I\right]=\operatorname*{det}\left(\mathbf{e}_{1},\dots,\mathbf{e}_{n}\right)=1. $$ (b） det is a linear function of each of the column vectors $\mathbf{x}_{j}\,,$ if the others are held fixed. (c） 1f [A] is obtained from 【A]by interchanging two columns, then det [A] = -det [A]. (d）If[A] has two equal columns, then det L4 = 0 Proof If $A=I.$ then a(i, i) = 1 and a(i, j) = 0 for if ,. Hence $$ \operatorname*{det}\left[I\right]=s(1,2,\dots,n)=1, $$ which proves (a)、By (82), s(i,….,)）=0 if any two of the j's are equal. Each of the remaining n! products in （83） contains exactly one factor from each column. This proves (b)、 Part (c is an immediate consequence of the fact that $S(j_{1},\ \cdot\cdot\cdot,j_{n})$ changes sign if any two of the j's are inter changed, and（d) is a corollary of(c).FUNCriONs OF SEVERAL VARIABLES 233 9.35 Theorem If[A] and [B] are n by n matrices, then det ([BI[A) = det [B] det [A] Proof If $\mathbf{x}_{1},\ \cdot\cdot\cdot,\ \mathbf{x}_{n}$ are the columns of [A], define (85） $$ \Delta_{B}({\bf x_{1}},\cdot\cdot\cdot,{\bf x_{n}})=\Delta_{B}[A]=\operatorname*{det}{([B][A])}. $$ The columns of [B][A] are the vectors Bx1,...，Bx, Thus (86） $$ \Delta_{B}({\bf x}_{1},\cdot\cdot\cdot,{\bf x}_{n})=\operatorname*{det}{(B{\bf x}_{1},\cdot\cdot\cdot,\,B{\bf x}_{n})}. $$ By (86) and Theorem 9.34, $\Delta_{B}$ also has properties 9.34 （6) to (d)By (b and （84), $$ \Delta_{B}[A]=\Delta_{B}\left(\sum_{i}a(i,\,1)\mathbf{e}_{i},\,\mathbf{x}_{2}\,,\,\cdot\cdot\cdot,\,\mathbf{x}_{n}\right)=\sum_{i}a(i,\,1)\,\Delta_{B}(\mathbf{e}_{i},\,\mathbf{x}_{2},\,\cdot\cdot\cdot,\,\mathbf{x}_{n}) $$ Repeating this process with xz X,，we obtain (87） $$ \Delta_{B}[A]=\sum a(i_{1},\;1)a(i_{2},\,2)\cdot\cdot\cdot a(i_{n},\,n)\,\Delta_{B}(\mathbf{e}_{i_{1}},\,\cdot\cdot,\,\mathbf{e}_{i_{n}}). $$ the sum being extended over all ordered n-tuples $(i_{1},\cdot\cdot\cdot,\ i_{n})$ with 1 ≤i,≤ n.By (c) and (d), (88） $$ \Delta_{B}({\bf e}_{i_{1}},\cdot\cdot\cdot,{\bf e}_{i_{n}})=t(i_{1},\cdot\cdot\cdot,\,i_{n})\;\Delta_{B}({\bf e}_{1},\cdot\cdot\cdot,\,{\bf e}_{n}), $$ where $\scriptstyle t\ =1.0.$ or -1, and since [B][1] = [B],(85) shows that (89） $$ \Delta_{B}({\mathbf{e}}_{1},\dots,\,{\mathbf{e}}_{n})=\operatorname*{det}\left[B\right]. $$ Substituting (89)） and （88） into (87), we obtain $$ \operatorname*{det}\left([B][A]\right)=\left\{\sum a(i_{1},1)\cdot\cdot\cdot a(i_{n},n)t(i_{1},\ldots,i_{n})\right\}\operatorname*{det}\left[B\right] $$ 3], for all n by n matrices [A] and [B]. Taking B = 1, we see that the above sum in braces is det [A]. This proves the theorem. 9.36 Theorem A linear operator A on $R^{n}$ is invertible if and only if det [A] ≠ 0 Proof If A is invertible, Theorem 9.35 shows that $$ \operatorname*{det}\left[A\right]\operatorname*{det}\left[A^{-1}\right]=\operatorname*{det}\left[A A^{-1}\right]=\operatorname*{det}\left[I\right]=1, $$ so that det [A]≠ 0 If A is not invertible, the columns X, .… $\mathbf{X}_{n}$ of [A] are dependen (Theorem 9.5); hence there is one, say, $\mathbf{x}_{k}\,,$ such that (90) $$ {\bf x}_{k}+\sum_{j\neq k}c_{j}{\bf x}_{j}=0 $$ for certain scalars cy，By 9.34 （6) and (d), x can be replaced by $\mathbf{X}_{k}$ + cyx without altering the determinant, $\,\mathbb{F}\,j\neq k.$ Repeating, we see that $\mathbf{X}_{k}$ can234 PRINCIPLEs Or MATHEMATICAL ANALYsis be replaced by the left side of (9O0), .e., by O, without altering the deter- minant. But a matrix which has O for one column has determinant O Hence det [A] = 0. 9.37 Remark Suppose {e,..…,e,} and $\left\{\Pi_{1},\ \cdot\cdot\cdot,\ \mathbb{U}_{n}\right\}$ are bases in $\textstyle\theta^{n}.$ a; and $\alpha_{i j}\,,$ Every linear operator A on $\textstyle{\mathcal{Q}}^{n}$ determines matrices [AJ and 【Aly，with entries given by $$ A\mathbf{e}_{j}=\sum_{i}a_{i j}\mathbf{e}_{i},\qquad A\mathbf{u}_{j}=\sum_{i}\alpha_{i j}\mathbf{u}_{i}. $$ If $\mathbf{u}_{j}=B\mathbf{e}_{j}=\Sigma b_{i j}\mathbf{e}_{i},$ then $A\mathbf{u}_{j}$ is cqual to $$ \sum_{k}\alpha_{k j}B\mathbf{e}_{k}=\sum_{k}\alpha_{k j}\sum_{i}b_{i k}\mathbf{e}_{i}=\sum_{i}\left(\sum_{k}b_{i k}\alpha_{k j}\right)\mathbf{e}_{i}, $$ and also to $$ A B\mathbf{e}_{j}=A\sum_{k}b_{k j}\mathbf{e}_{k}=\sum_{i}\left(\sum_{k}a_{i k}b_{k j}\right)\mathbf{e}_{i} $$ Thus $\Sigma b_{i k}\,\alpha_{k j}=\Sigma a_{i k}\,b_{k j}\,,$ or (91） [B][A ]y = [A][B] Since Bis invertible, det [B] ≠ 0. Hence （91), combined with Theorem 9.35 shows that (92) $$ \operatorname*{det}\left[A\right]_{U}=\operatorname*{det}\left[A\right]. $$ The determinant of the matrix of a linear operator does therefore not depend on the basis which is used to construct the matrix、 1t is thus meaningful to speak of the determinan of alinear operator, without having any basisin mind 9.38 Jacobians Iff maps an open set E $E\in{\mathcal{R}}^{n}$ R” into R", and if fis differen- tiable at a point xe E, the determinant of the linear operator f'(x） is calle the Jacobian of f at x.In symbols, (93） $$ J_{t}(\mathbf{x})=\operatorname*{det}\mathbf{f}^{\prime}(\mathbf{x}). $$ We shall also use the notation (94） $$ {\frac{{\tilde{\mathcal{O}}}(y_{1},\ \cdot\cdot,\ y_{n})}{{\tilde{\mathcal{O}}}(x_{1},\ \cdot\cdot\cdot,x_{n})}} $$ for JAKx), if（O，,.…， )) =1(x,. ……) In terms of Jacobians, the crucial hypothesis in the inverse function theorem is that J-(a）+ 0 （compare Theorem 9.36). If the implicit function theorem is stated in terms of the functions (59), the assumption made there on A amounts to $$ {\frac{\partial(J_{1},\ \cdot\cdot,f_{n})}{\partial(x_{1},\ \cdot\cdot\cdot,\,x_{n})}}\neq0. $$FuNcrioNs or SEVERAL VAUIABLES 235 DERIVATIVES OF HIGHER ORDER 9.39 Definition Suppose f is a real function defined in an open set Ec R", with partial derivatives D,f,...，D,f.If the functions $D_{j}f$ are themselves differentiable, then the second-order partial derivatives of f are defined by $$ D_{i j}f=D_{i}\,D_{j}f\qquad(i,j=1,\ldots,n). $$ If all these functions D,/f are continuous in E, we say that fis of class ${\mathcal{C}}^{\mu}$ ”in $\textstyle E_{\mathrm{{J}}}$ or that f∈ B"(E). A mapping f of E into 人 ${\boldsymbol{R}}^{m}$ m is said to be of class G”if each component of f R” is of class G” It can happen that $D_{i j}f\neq D_{j i}f$ at some point, although both derivatives exist (see Exercise 27). However, we shall see below that $D_{i j}f=D_{j i}f$ whenever these derivatives are continuous For simplicity (and without loss of generality）we state our next two theorems for real functions of two variables. The first one is a mean value theorem. 9.40 Theorem Suppose f is defined in an open set $E\subset R^{2}$ ，,and D f and D,, exist at every point of E. Suppose Q c E is a closed rectangle with sides parallel to the coordinate axes, having（a, b）and（a +h, b + k）as opposite vertices (h ≠ 0,k ≠ 0).Put $$ \Delta(f,Q)=f(a+h,b+k)-f(a+h,b)-f(a,b+k)+f(a,b). $$ Then there is a point (x, y) in the interior of Q such that (95） $$ \Delta(f,Q)=h k(D_{21}f)(x,y). $$ Note the analogy between （95)）and Theorem 5.10;the area of Q is hk Proof Put ut) = f(t,b + k)-f(1,b). Two applications of Theorem 5.10 show that there is an x between a and a+ h, and that there is a y between $\stackrel{\mathcal{J}}{\mathcal{J}}$ and $b+k,$ such that $$ \begin{array}{r l}{\Delta(f,Q)=u(a+h)-u(a)}\\ {\mathbf{\Phi=}h u^{\prime}(x)}\\ {\mathbf{\Phi=}h h^{\prime}(x)}\\ {\mathbf{\Phi=}h[(D_{1}f)(x,b+k)-(D_{1}f)(x,b)}\end{array} $$ )】 9.41 Theorem Suppose f is defned in an open set $E=R^{\circ}$ suppose that D,f, D,f, and D,f exist at every point of E, and $D_{21}J$ is continuous at some point (a, b)e E.236 rRINCIPLEs or MATHEMATICAL ANALYsis Then D,/ exists at(a,b) and (96) $$ (D_{12}f)(a,b)=(D_{21}f)(a,b). $$ CorollaryD1f = D,zf if fe 8″(E） Theorem 9.40, and if ${\ {\J}\atop{\sqrt{\ J}}}$ and Proot Put A =（D,uJXa, b) Choose e > 0.If Q is a rectangle as in ${\hat{N}}\,$ are sufficiently small, we have $$ |\,A-(D_{21}f)(x,\,y)|\,<\varepsilon $$ for all （(x,y)e Q. Thus $$ \left\vert{\frac{\Delta(f,Q)}{h k}}-A\right\vert<s, $$ by (95). Fix $\textstyle{\iint_{\varnothing}}$ and let $k\to0.$ Since D,f exists in ${\bar{\cal K}},$ the last inequality implies that (97） $$ \Big\vert\frac{(D_{2}f)(a+h,b)-(D_{2}f)(a,b)}{h}-A\Big\vert\leq\varepsilon. $$ Since s was arbitrary, and since （97） holds for all sficiently smal h ≠ 0,it follows that $(D_{12}f)(a,b)=A$ This gives (96). DIFFERENTIATION OF INTEGRALS Suppose ${\mathcal{O}}$ is a function of two variables which can be integrated with respect to one and which can be differentiated with respect to the other. Under what conditions will the result be the same if these two limit processes are carried out in the opposite order ？ To state the question more precisely：Under what conditions on p can one prove that the equation (98） $$ {\frac{d}{d t}}\int_{a}^{b}\!\varphi(x,t)\,d x=\int_{a}^{b}{\frac{\partial\varphi}{\partial t}}\left(x,t\right)d x $$ is true？（A counter example is furnished by Exercise 28. It will be convenient to use the notation (99) $$ \varphi^{t}(x)=\varphi(x,t). $$ Thus p' is, for each t, a function of one variable 9.42 Theorem Suppose (a）p(x,t）is defined for a ≤x≤b,c≤t≤ d; (b）α is an increasing function on [a,b];FUNCTIONs or SEVERAL VARIABLEs 237 (c）pt ∈ .2(c) for every t∈ [c, d]j (d）c<s< d, and to every s>0 corresponds a $\scriptstyle\partial>0$ O such that $$ |(D_{2}\,\varphi)(x,t)-(D_{2}\,\varphi)(x,s)|<\d t $$ 8 for all xe [a, b] and for all t ∈(s - 6,s + 6) Define （100) $$ f(t)=\int_{a}^{b}\!\varphi(x,t)\,d x(x)\qquad(c\leq t\leq d). $$ Then（D,9）”∈ 9W(x), f'(s) exists, and (101） $$ f^{\prime}(s)=\int_{a}^{b}(D_{2}\,\varphi)(x,\,s)\,d x(x). $$ Note that （c） simply asserts the existence of the integrals（100） for al te [c, d].Note also that (d) certainly holds whenever $\ D_{2}$ gp is continuous on the rectangle on which op is defined. Proof Consider the difference quotients $$ \psi(x,t)={\frac{\varphi(x,t)-\varphi(x,s)}{t-s}} $$ for O<|t-s|<6. By Theorem 5.10 there corresponds to each(x, ${\big\langle}\mathbb{X}{\big\rangle}$ a number u between s and t such that $$ \psi(x,t)=(D_{2}\,\varphi)(x,u). $$ Hence (d)implies that (102） $$ |\psi(x,t)-(D_{2}\,\varphi)(x,s)|<\varepsilon\qquad(a\leq x\leq b,\quad0<|t-s|<\delta). $$ Note that (103） $$ {\frac{f(t)-f(s)}{t-s}}=\int_{a}^{b}\!\psi(x,\,t)\,d x(x). $$ By（102)、W'→（D2 o)", uniformly on 【α,b],as 1→s. Since each yt ∈ X*(α),the desired conclusion follows from（103) and Theorem 7.16. 9.43 Example One can of course prove analogues of Theorem 9.42 with (- OO，oo）in place of [a,b]. Instead of doing this,let us simply look at an example. Define （104） $$ f(t)=\int_{-\infty}^{\infty}e^{-x^{2}}\cos\left(x t\right)d x $$238 PRINCIPLES OF MATHEMATICAL ANALYSIS and (105) $$ g(t)=-\int_{-\infty}^{\infty}x e^{-x^{2}}\sin\left(x t\right)d x, $$ for -O <t<OO.Both integrals exist（they converge absolutely)since the absolute values of the integrands are at most exp（一x) and x| exp（-x） respectively. Note that g is obtained from f by differentiating the integrand with respect to t.We claim that fis differentiable and that (106) $$ f^{\prime\prime}(t)=g(t)\qquad(-\infty<t<\infty) $$ To prove this, let us first examine the difference quotients of the cosine: if $\beta>0,$ then (107） $$ {\frac{\cos\,(\alpha+\beta)-\cos\,\alpha}{\beta}}+\sin\,\alpha={\frac{1}{\beta}}\int_{x}^{x+\beta}\,(\sin\,\alpha-\sin\,t)\,d t. $$ Since |sin α- sin t|≤|t -α|, the right side of（107) is at most β/2 in absolut value; the case β<O is handled similarly. Thus (108) $$ \left\vert{\frac{\cos\left(\alpha+\beta\right)-\cos\alpha}{\beta}}+\sin\alpha\right\vert\leq1\beta\vert $$ for all β(if the left side is interpreted to be O when β = 0) Now fix t, and fix h≠ 0. Apply （108) with α = xt, β = xh;it follows from (104) and（105) that $$ \lfloor{\frac{ |{\mathcal{I}}(t+h)-f(t)}{h}}-g(t) \rceil\leq|h|\ \rfloor^{\infty}x^{2}e^{-x^{2}}\,d x. $$ When $h arrow0,$ ,we thus obtain(106) Let us go a step further： An integration by parts,applied to （104), shows that (109） $$ f(t)=2\int_{-\infty}^{\infty}x e^{-x^{2}}{\frac{\sin\left(x t\right)}{t}}\,d x. $$ Thus tJ(t) = - 29(t), and（106）implies now that $\mathbb{P}$ satisfies the diffrentia equation (110） $$ 2f^{\prime}(t)+i f(t)=0. $$ If we solve this differential equation and use the fact that f(0) = /m（see Sec 8.21), we find that (11) $$ f(t)=\sqrt{\pi}\,\exp\,\left(-\,\frac{t^{2}}{4}\right). $$ The integral (104）is thus explicitly determinedFUNCTONs Or SEVERAL VARIABLES 239 EXERCISES 1. 1 $\operatorname{sgn}$ is a nonempty subset of a vector space X, prove (as asserted in Sec. 9.1） that the span of S is a vector space. 2. Prove (as asserted in Sec. 9.6) that BA is linear if A and $\bar{\boldsymbol{D}}$ are linear transformations Prove also that A- is linear and invertible. 3.Assume A∈ L(X, Y) and Ax = 0 only when x= 0. Prove that A is then 1-1. 4. Prove as asserted in Scc. 9.30) that null spaces and ranges of linear transforma- tions are vector spaces 5. Prove that to every Ae L(R", R') corresponds a unique $\scriptstyle y\in K^{\prime}$ R" such that Ax =x·y Prove also that $||A||=|\mathbf{y}|$ Hint: Under certain conditions,equality holds in the Schwarz inequality. 6. If f(0,0) = 0 and $$ f(x,y)=\frac{x y}{x^{2}+y^{2}}\qquad\mathrm{if}\left(x,y\right)\neq(0,0), $$ prove that（D,f)(x, y) and（D,f)(x, J) exist at every point of $\textstyle R^{2},$ although fis not continuous at (0,0). 7. Suppose that $\mathbb{J}^{\prime}$ is a real-valued function defined in an open set $E\subset R^{n},$ and that the partial derivatives D,f,...，D,f are bounded in E. Prove that fis continuous in E. Hint： Proceed as in the proof of Theorem 9.21. 8. Suppose that fis a differentiable real function in an open set $E\subset R^{n},$ , and that f has a local maximum at a point xe E. Prove that f(x)= 0. 9.If f is a differentiable mapping of a connected open set L $E\prec R^{\prime}$ ”into R", and if $\mathrm{f}\left(\mathbf{x}\right)=0$ for every x∈ E, prove that fis constant in ${\hat{H}}^{\nu}.$ 10. If /is a real function defined in a convex open set $E\subset R^{n}$ , such that （D,/Xx) == 0 for every xe E, prove that /(x) depends only on xz， ·，X, Show that the convexity of $\underline{{\mathbf{}}}$ can be replaced by a weaker condition, but that some condition is required. For example,if $n=2$ p and $\underline{{\mathbf{}}}$ is shaped like a horseshoe, the statement may be false. 11. Iff and g are differentiable real functions in ${\boldsymbol{R}}^{n},$ prove that $$ \nabla(f g)=f\,\nabla g+g\,\nabla f $$ and that V(1//） = -/-Vf wherever f≠ 0. 12. Fix two real numbers a and $b_{\mathrm{,}}0<a<b.$ Define a mapping f= （f, f2,/a) of $R^{2}$ into $\textstyle R^{3}$ s by $$ \begin{array}{l}{{f_{1}(s,t)=(b+a\cos s)\cos t}}\\ {{f_{2}(s,t)=(b+a\cos s)\sin t}}\\ {{f_{3}(s,t)=a\sin s.}}\end{array} $$240 PRINCIPLES OF MATHEMATICAL ANALYSIs Describe the range K of f.(It is a certain compact subset of $R^{3}.$ (a) Show that there are exactly 4 points p ∈ K such that $$ (\nabla f_{1})(\mathbf{f}^{-1}(\mathbf{p}))=0. $$ Find these points (b） Determine the set of all qe K such tha $$ (\nabla f_{3})(\mathbf{f}^{-1}(\mathbf{q}))=0. $$ (c） Show that one of the points p found in part (a) corresponds to a local maxi mum of $f_{1},$ ,one corresponds to a local minimum,and that the other two are neither (they are so-called “saddle points"”)） Which of the points qfound in part (b) correspond to maxima or minima ？ (d) Let A be an irrational real number, and define g(t)=f(t, 入t)、 Prove that g is a 1-1 mapping of $\textstyle R^{1}$ l onto a dense subset of $\textstyle{\mathcal{N}}$ .Prove that $$ |\mathbf{g}^{\prime}(t)|^{2}=a^{2}+\lambda^{2}(b+a\cos t)^{2}. $$ 13. Suppose $\frac{\lambda\xi^{\mathrm{CP}}}{\lambda\mathbf{\beta}}$ is a differentiable mapping of $\textstyle R^{1}$ into ${\boldsymbol{R}}^{3}$ such that |f(t)|= 1 for every t Prove that f(t)·f(t)= 0. Interpret this result geometrically 14. Define /(0, $\scriptstyle0\;0\;=\;0$ and $$ f(x,y)=\frac{x^{3}}{x^{2}+y^{2}}\qquad\mathrm{if~}(x,y)\ne(0,0). $$ (a) Prove that ${\mathcal{D}}_{1}j$ f and D,f are bounded functions in $R^{2}$ .(Hence f is continuous.) (b) Let u be any unit vector in ${\boldsymbol{R}}^{2}$ .Show that the directional derivative（D,f)(0,0 exists, and that its absolute value is at most 1 (c) Let y be a differentiable mapping of ${\boldsymbol{R}}^{1}$ into ${\boldsymbol{R}}^{2}$ （in other words,y is a differ- entiable curve in R-),with y(0) = (0,0) and |y(O)|> 0. Put g(t)= /(y(t))） and prove that g is differentiable for every ${\mathsf{t}}\in R^{\prime}$ If ye G', prove that g e %′ (d) In spite of this, prove that fis not differentiable at (0,0) Hint: Formula（40) fails. 15. Define f(0, $\scriptstyle0\;n\;=\;0,$ , and put $$ f(x,y)=x^{2}+y^{2}-2x^{2}y-{\frac{4x^{6}y^{2}}{(x^{4}+y^{2})^{2}}} $$ if (x,y)≠ (0,0). (a) Prove, for all $(x,y)\in R^{2}$ , that $$ 4x^{4}y^{2}\leq(x^{4}+y^{2})^{2}. $$ Conclude that fis continuousFUNCrioNs or SEVERAL vARIABLEs 241 (b） For 0≤0≤2m, -0 <1<O, define $$ g_{\theta}(t)=f(t\cos\theta,\,t\sin\theta). $$ Show that go(0) = 0,g6(0) = 0, g″(0) = 2. Each go has therefore a strict local minimum at t= 0. In other words, the restriction offto each line through （0,O） has a strict local minimum at (0,0). (c） Show that (O,O) is nevertheless not a local minimum for f, since f(x, $x^{2})=-x^{4},$ 16. Show that the continuity of $\widehat{\bigcup{}^{*}}$ at the point a is needed in the inverse function theorem, even in the case $n=1{\mathrm{:}}$ If $$ f(t)=t+2t^{2}\sin\left({\frac{1}{t}}\right) $$ for $t\neq0,$ and f(0) = 0, then $f^{\prime}(0)=1,\,f^{\prime}$ is bounded in（一1,1),but ${\mathcal{T}}^{*}$ is not one-to-one in any neighborhood of O. 17. Let $\mathbf{f}=(f_{1},f_{2})$ be the mapping of $\textstyle R^{2}$ $R^{2}$ 2 into R- given by $$ f_{1}(x,y)=e^{x}\cos y,\qquad f_{2}(x,y)=e^{x}\sin y. $$ (a) What is the range of f？ (b）Show that the Jacobian of fis not zero at any point of $\textstyle R^{2},$ Thus every point of $R^{2}$ 2 has a neighborhood in which fis one-to-one. Nevertheless, f is not one-to- one on ${\boldsymbol{R}}^{2}$ (c) Put a =(0,/3), b =f(a),let g be the continuous inverse of f, defined in a neighborhood of b,such that g(b) = a.Find an explicit formula for g, compute f(a) and g(b), and verify the formula （52). (d） What are the images under f of lines parallel to the coordinate axes ？ 18. Answer analogous questions for the mapping defined by $$ u=x^{2}-y^{2},\qquad v=2x y. $$ 19.Show that the system of equations $$ \begin{array}{c}{{3x+y-z+u^{2}=0}}\\ {{x-y+2z+u=0}}\\ {{2x+2y-3z+2u=0}}\end{array} $$ can be solved for x, ${\mathcal{V}}_{\mathfrak{g}}$ u in terms of z; for x,Z,u in terms of y for y,z,u in terms of x; but not for x, y,z in terms of u 20. Take n = m = 1 in the implicit function theorem, and interpret the theorem (as well as its proof) graphically 21. Define f in $R^{2}$ by $$ f(x,y)=2x^{3}-3x^{2}+2y^{3}+3y^{2}. $$ (a） Find the four points in ${\boldsymbol{R}}^{2}$ at which the gradient of f is zero. Show that f has exactly one local maximum and one local minimum in $\textstyle{R^{2}}$242 PRINCIPLES OF MATHEMATICAL ANALYSsis (b） Let S be the set of all Xx, y)e R at which f(x, )= 0. Find those points of S that have no neighborhoods in which the equation f(x, y) = 0 can be solved for y in terms of x(or for x in terms of y). Describe S as precisely as you can. 22.Give a similar discussion for $$ f(x,y)=2x^{3}+6x y^{2}-3x^{2}+3y^{2}. $$ 23. Define f in $\textstyle R^{3}$ by $$ f(x,y_{1},y_{2})=x^{2}y_{1}+e^{x}+y_{2}\,. $$ Show that f(0,1,-1) =0,（D,f)(0,1,-1）≠ 0,and that there exists therefore a differentiable function g in some neighborhood of（1,-1）in R,such that g(1, $-1)=0$ and $$ f(g(y_{1},y_{2}),y_{1},y_{2})=0. $$ Find（Dig)(1,-1) and（D29X1,-1). 24.For (x, y) + (0,0), define f=（f,fz) by $$ f_{1}\left(x,y\right)=\frac{x^{2}-y^{2}}{x^{2}+y^{2}},\qquad f_{2}(x,y)=\frac{x y}{x^{2}+y^{2}}. $$ Compute the rank of f(x, ), and find the range of f (a Defne 25.Suppose Aé L(R",R"), let r be the rank of A. $R^{n}$ $*_{k}^{\bullet}$ as in the proof of Theorem 9.32. Show that SA is a projction in whose null space is ${\mathcal{N}}(A)$ and whose range is SX(S). Hint: By (68),SASA = S A. (b） Use (a) to show that $$ \mathrm{dim}\ {\mathcal W}(A)+\mathrm{dim}\ {\mathcal R}(A)=n. $$ 26. Show that the existence (and even the continuity）of D,f does not imply the existence of Df. For example, let f(x, y)= 9(X), where g is nowhere differentiable 27. Put f(0,0)= 0, and $$ f(x,y)={\frac{x y(x^{2}-y^{2})}{x^{2}+y^{2}}} $$ if (x, y)≠ (0,0)Prove that (a)f, Df, D,f are continuous in $R^{2}\,;$ (6）D,f and Daif exist at every point of $R^{2},$ and are continuous except at (0,0) (c)（D,/)(0,0) = 1, and $(D_{21}f)(0,0)=-1.$ 28. For t≥0, put $$ \varphi(x,t)={\binom{x}{-x+2{\sqrt{t}}}}\quad\underbrace{(0\leq x\leq{\sqrt{t}})}_{({\sqrt{t}}\leq x\leq2{\sqrt{t}})}_{({\mathrm{othiss}})} $$ and put g(x, 1)= -9(x,|t|) if t<0.FUNCTioNs Or SEVERAL VARIABLES 243 Show that gp is continuous on $R^{2},$ and $$ (D_{2}\varphi)(x,0)=0 $$ for all x.Define $$ f(t)=\int_{-1}^{1}\varphi(x,t)\,d x. $$ Show that f(t)= t it $\scriptstyle t_{i}<{\frac{1}{4}}$ 、Hence $$ f^{\prime}(0)\neq\int_{-1}^{1}(D_{2}\varphi)(x,0)\,d x. $$ 29、Let ${\widehat{H_{2}^{\prime}}}$ be an open set in ${\boldsymbol{R}}^{n}$ The classes G′(E) and C″(E) are defined in the text. By induction, G4*0(E) can be defined as follows, for all positive integers k:To say that fe G*(E)means that the partial derivatives D,f,.…，D,f belong to C*-1(ED) Assume f∈ 6*(E). and show（by repeated application of Theorem 9.41 that the kth-order derivative $$ D_{i_{1}i_{2}}\dots\iota_{k}f=D_{i_{1}}D_{i_{2}}\dots D_{i_{k}}f $$ is unchanged if the subscripts $\ddot{\vec{l}}_{\vec{l}}{\bf l.}\ \circ\ e\ ,\ \circ\ \cdot\ \vec{l}\vec{l}_{\vec{k}}$ are permuted For instance, if n≥3, then $$ D_{1213}f=D_{3112}f $$ for every fe G“” 30. Let f∈ C(”)(E), where $\underline{{T}}$ is an open subset of $\textstyle R^{n}$ ，Fix a∈ E, and suppose xe R" is so close to O that the points $$ \mathbf{p}(t)=\mathbf{a}+t\mathbf{x} $$ lie in E whenever 0≤t≤1. Define $$ h(t)=f(\wp(t)) $$ for all t e Rl for which p(t)∈ E (a) For 1≤k≤m, show(by repeated application of the chain rule) that $$ h^{(k)}(t)=\sum\left(D_{i_{1}}\ldots\iota_{k}f\right)\left({\bf p}(t)\right)\,x_{i_{1}}\ldots x_{i_{k}}\,. $$ The sum extends over all ordered k-tuples (i,….,ik） in which each ${\frac{s}{\ell}}j$ is one of the integers 1,.…，. (b）By Taylor's theorem (5.15), $$ h(1)=\sum_{k=0}^{m-1}\frac{h^{(k)}(0)}{k!}+\frac{h^{(m)}(t)}{m!} $$ for some te(0,1). Use this to prove Taylor's theorem in n variables by showing that the formula244 PRINCIPLES OF MATHEMATICAL ANALYSIs $$ f(\mathbf{a}+\mathbf{x})=\sum_{k=0}^{m-1}{\frac{1}{k!}}\sum(D_{i_{1}}\dots\iota_{k}f)(\mathbf{a})x_{i_{1}}\,\cdot\cdot\cdot x_{i_{k}}+r(\mathbf{x}) $$ represents $f(\mathbf{a}+\mathbf{x})$ as the sum of its so-called“Taylor polynomial of degree $m-1,$ plus a remainder that satisfes $$ \operatorname*{lim}_{\bf x arrow0}\frac{r({\bf x})}{|{\bf x}|^{m-1}}=0. $$ Each of the inner sums extends over all ordered k-tuples （i,.….,i), as in part (a); as usual, the zero-order derivative of fis simply f, so that the constant term of the Taylor polynomial of fat a is f(a)、 (c） Exercise 29 shows that repetition occurs in the Taylor polynomial as written in part（b)、 For instance, $D_{1,13}$ occurs three times, as Ds，D.s1，D3:: The sum of the corresponding three terms can be written in the form $$ 3(D_{1}^{2}\,D_{3}f)(\mathrm{a})x_{1}^{2}\,x_{3}\,. $$ Prove (by calculating how often each derivative occurs) that the Taylor polynomia in (b) can be written in the form $$ \sum{\frac{(D_{1}^{s_{1}}\cdot\cdot\cdot D_{n}^{s_{n}}f)(\mathbf{a})}{s_{1}!\cdot\cdot\cdot s_{n}!}}\,x_{1}^{s_{1}}\cdot\cdot\cdot x_{n}^{s_{n}}. $$ Here the summation extends over all ordered n-tuples （S1,...，Sn) such that each s is a nonnegative integer, and $s_{1}+\cdot\cdot\cdot+s_{n}\leq m-1$ is 0 31. Suppose fe G0in some neighborhood of a point a e RP, the gradient of $\mathbb{P}$ at a, but not all second-order derivatives of f are O at a.Show how one can then determine from the Taylor polynomial of fat a (of degree 2） whether f has a locan maximum, or a local minimum, or neither,at the point a. Extend this to R" in place of R $R^{2}$