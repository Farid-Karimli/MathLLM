\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{mathrsfs}

\begin{document}
\section{TAUBERIAN THEORY}
\section{Wiener's Theorem}
9.1 Introduction A tauberian theorem is one in which the asymptotic behavior of a sequence or of a function is deduced from the behavior of some of its averages. Tauberian theorems are often converses of fairly obvious results, but usually these converses depend on some additional assumption, called a tauberian condition. To see an example of this, consider the following three properties of a sequence of complex numbers $s_{n}=a_{0}+\cdots+a_{n}$.

(a) $\lim _{n \rightarrow \infty} s_{n}=s$.

(b) If $f(r)=\sum_{0}^{\infty} a_{n} r^{n}, 0<r<1$, then $\lim _{r \rightarrow 1} f(r)=s$.

(c) $\lim _{n \rightarrow \infty} n a_{n}=0$.

Since $f(r)=(1-r) \sum s_{n} r^{n}$ and $(1-r) \sum r^{n}=1, f(r)$ is, for each $r \in(0,1)$, an average of the sequence $\left\{s_{n}\right\}$. It is extremely easy to prove that $(a)$ implies $(b)$. The converse is not true, but $(b)$ and $(c)$ together imply $(a)$; this is also quite easy and was proved by Tauber. The tauberian condition (c) can be replaced by the weaker assumption that
$\left\{n a_{n}\right\}$ is bounded (Littlewood). It is remarkable how much more difficult this weaken-
ing of $(c)$ makes the proof.

Wiener's tauberian theorem deals with bounded measurable functions, originally on the real line. If $\phi \in L^{\infty}(R)$ and if $\phi(x) \rightarrow 0$ as $x \rightarrow+\infty$, then it is almost trivial that $(K * \phi)(x) \rightarrow 0$ as $x \rightarrow+\infty$ for every $K \in L^{1}(R)$. The convolutions $K * \phi$ may be regarded as averages of $\phi$, at least when $\int K=1$. Wiener's converse $[(a)$ of Theorem 9.7] states that if $(K * \phi)(x) \rightarrow 0$ for one $K \in L^{1}(R)$ and if the Fourier transform of this $K$ vanishes at no point of $R$, then $(f * \phi)(x) \rightarrow 0$ for every $f \in L^{1}(R)$; the stronger conclusion that $\phi(x) \rightarrow 0$ need not hold under these hypotheses, but it does hold if a slight additional condition (slow oscillation) is imposed on $\phi[(b)$ of Theorem 9.7].

The unexpected tauberian condition-the nonvanishing of $\hat{K}$-enters the proof in the following manner: If $(K * \phi)(x) \rightarrow 0$, the same is true if $K$ is replaced by any of its translates, hence also if $K$ is replaced by any finite linear combination $g$ of translates of $K$. When $\widehat{K}$ has no zero, it turns out that the set of these functions $g$ is dense in $L^{1}$ (Theorem 9.5). One is thus led to the study of translation-invariant subspaces
of $L^{1}$.

9.2 Lemma Suppose $f \in L^{1}\left(R^{n}\right), t \in R^{n}$, and $\varepsilon>0$. Then there exists $h \in L^{1}\left(R^{n}\right)$, with $\|h\|_{1}<\varepsilon$, such that

$$
\hat{h}(s)=\hat{f}(t)-\hat{f}(s)
$$

for all $s$ in some neighborhood of $t$.

The lemma states that $f$ is approximated, in the $L^{1}$-norm, by a function $f+h$ whose Fourier transform is constant in a neighborhood of the point $t$. PROOF Choose $g \in L^{1}\left(R^{n}\right)$ so that $\hat{g}=1$ in some neighborhood of the origin.
For $\lambda>0$, put

and define

$$
g_{\lambda}(x)=e^{i t \cdot x} \lambda^{-n} g(x / \lambda) \quad\left(x \in R^{n}\right)
$$

$$
h_{\lambda}(x)=\hat{f}(t) g_{\lambda}(x)-\left(f * g_{\lambda}\right)(x)
$$

Since $\hat{g}_{\lambda}(s)=1$ in some neighborhood $V_{\lambda}$ of $t$, (3) shows that (1) holds for $s \in V_{\lambda}$, with $h_{\lambda}$ in place of $h$. Next,

$$
h_{\lambda}(x)=\int_{R^{n}} f(y)\left[e^{-i t \cdot y} g_{\lambda}(x)-g_{\lambda}(\dot{x}-y)\right] d m_{n}(y)
$$

The absolute value of the expression in brackets is

$$
\left|\lambda^{-n} g\left(\lambda^{-1} x\right)-\lambda^{-n} g\left(\lambda^{-1}(x-y)\right)\right|
$$

It follows that

$$
\left\|h_{\lambda}\right\|_{1} \leq \int_{R^{n}}|f(y)| d m_{n}(y) \int_{R^{n}}\left|g(\xi)-g\left(\xi-\lambda^{-1} y\right)\right| d m_{n}(\xi)
$$

by the change of variables $x=\lambda \xi$. The inner integral in (6) is at most $2\|g\|_{1}$, and it tends to 0 for every $y \in R^{n}$, as $\lambda \rightarrow \infty$. Hence $\left\|h_{\lambda}\right\|_{1} \rightarrow 0$, as $\lambda \rightarrow \infty$, by the dominated convergence theorem.

9.3 Theorem If $\phi \in L^{\infty}\left(R^{n}\right), Y$ is a subspace of $L^{1}\left(R^{n}\right)$, and

$$
f * \phi=0 \text { for every } f \in Y \text {, }
$$

then the set

$$
Z(Y)=\bigcap_{f \in Y}\left\{s \in R^{n}: \hat{f}(s)=0\right\}
$$

contains the support of the tempered distribution $\hat{\phi}$.

PROOF Fix a point $t$ in the complement of $Z(Y)$. Then $\hat{f}(t)=1$ for a certain $f \in Y$. Lemma 9.2 furnishes $h \in L^{1}\left(R^{n}\right)$, with $\|h\|_{1}<1$, such that $\hat{h}(s)=1-\hat{f}(\hat{s})$ in some neighborhood $V$ of $t$.

To prove the theorem, it suffices to show that $\hat{\phi}=0$ in $V$, or, equivalently, that $\hat{\psi}(\hat{\psi})=0$ for every $\psi \in \mathscr{S}_{n}$ whose Fourier transform $\hat{\psi}$ has its support in $V$. Since

$$
\hat{\phi}(\hat{\psi})=\phi(\check{\psi})=(\phi * \psi)(0)
$$

it suffices to show that $\phi * \psi=0$.

Fix such a $\psi$. Put $g_{0}=\psi, g_{m}=h * g_{m-1}$ for $m \geq 1$. Then $\left\|g_{m}\right\|_{1} \leq\|h\|_{1}^{m}\|\psi\|_{1}$, and since $\|h\|_{1}<1$, the function $G=\sum g_{m}$ is in $L^{1}\left(R^{n}\right)$. Since $\hat{h}(s)=1-\hat{f}(s)$ on the support of $\hat{\psi}$, we have

$$
(\hat{1}-\hat{h}(s)) \hat{\psi}(s)=\hat{\psi}(s) \hat{f}(s) \quad\left(s \in R^{n}\right)
$$

or

$$
\hat{\psi}=\sum_{m=0}^{\infty} \hat{h}^{m} \hat{\psi} \hat{f}=\hat{G} \hat{f}
$$

Thus $\psi=G * f$, and (1) implies

$$
\psi * \phi=G * f * \phi=0
$$

9.4 Wiener's theorem If $Y$ is a closed translation-invariant subspace of $L^{1}\left(R^{n}\right)$ and if $Z(Y)$ is empty, then $Y=L^{1}\left(R^{n}\right)$.

PROOF To say that $Y$ is translation-invariant means that $\tau_{x} f \in Y$ if $f \in Y$ and $x \in R^{n}$. If $\phi \in L^{\infty}\left(R^{n}\right)$ is such that $\int f \check{\phi}=0$ for every $f \in Y$, the translationinvariance of $Y$ implies that $f * \phi=0$ for every $f \in Y$. By Theorem 9.3, the support of the distribution $\hat{\phi}$ is therefore empty, hence $\hat{\phi}=0$ (Theorem 6.24), and since the Fourier transform maps $\mathscr{S}_{n}^{\prime}$ to $\mathscr{S}_{n}^{\prime}$ in a one-to-one fashion (Theorem 7.15), it follows that $\phi=0$ as a distribution. Hence $\phi$ is the zero element of $L^{\infty}\left(R^{n}\right)$.

Thus $Y^{\perp},=\{0\}$. By the Hahn-Banach theorem, this implies that $Y=L^{1}\left(R^{n}\right)$.

9.5. Theorem Suppose $K \in L^{1}\left(R^{n}\right)$ and $Y$ is the smallest closed translation-invariant subspace of $L^{1}\left(R^{n}\right)$ that contains $K$. Then $Y=L^{1}\left(R^{n}\right)$ if and only if $\hat{K}(t) \neq 0$ for every $t \in R^{n}$.

PROOF Note that $Z(Y)=\left\{t \in R^{n}: \hat{K}(t)=0\right\}$. The theorem thus asserts that $Y=L^{1}\left(R^{n}\right)$ if and only if $Z(Y)$ is empty. One-half of this is Theorem 9.4; the other half is trivial.

9.6 Definition A function $\phi \in L^{\infty}\left(R^{n}\right)$ is said to be slowly oscillating if to every $\varepsilon>0$ correspond an $A<\infty$ and a $\delta>0$ such that

$$
|\phi(x)-\phi(y)|<\varepsilon \quad \text { if }|x|>A,|y|>A,|x-y|<\delta \text {. }
$$

If $n=1$, one can also derine what it means for $\phi$ to be slowly oscillating at $+\infty$ : the requirement (1) is replaced by

$$
|\phi(x)-\phi(y)|<\varepsilon \quad \text { if } x>A, y>A,|x-y|<\delta \text {. }
$$

The same definition can of course be made at $-\infty$.

Note that every uniformly continuous bounded function is slowly oscillating but that some slowly oscillating functions are not continuous.

We now come to Wiener's tauberian theorem; part $(b)$ was added by Pitt.

\subsection{Theorem}
(a) Suppose $\phi \in L^{\infty}\left(R^{n}\right), K \in L^{1}\left(R^{n}\right), \widehat{K}(t) \neq 0$ for every $t \in R^{n}$, and

$$
\lim _{|x| \rightarrow \infty}(K * \phi)(x)=a \hat{K}(0)
$$

Then

$$
\lim _{|x| \rightarrow \infty}(f * \phi)(x)=a \hat{f}(0)
$$

for every $f \in L^{1}\left(R^{n}\right)$.
(b) If, in addition, $\phi$ is slowly oscillating, then

$$
\lim _{|x| \rightarrow \infty} \phi(x)=a
$$

PROOF Put $\psi(x)=\phi(x)-a$. Let $Y$ be the set of all $f \in L^{1}\left(R^{n}\right)$ for which

$$
\lim _{|x| \rightarrow \infty}(f * \psi)(x)=0
$$

It is clear that $Y$ is a vector space. Also, $Y$ is closed. To see this, suppose $f_{i} \in Y,\left\|f-f_{i}\right\|_{1} \rightarrow 0$. Since

$$
\left\|f * \psi-f_{i} * \psi\right\|_{\infty} \leq\left\|f-f_{i}\right\|_{1}\|\psi\|_{\infty},
$$

$f_{i} * \psi \rightarrow f * \psi$ uniformly on $R^{n}$; hence (4) holds. Since

$$
\left(\left(\tau_{y} f\right) * \psi\right)(x)=\left(\tau_{y}(f * \psi)\right)(x)=(f * \psi)(x-y)
$$

$Y$ is translation-invariant. Finally, $K \in Y$, by (1), since $K * a=a \hat{K}(0)$.

Theorem 9.5 now applies and shows that $Y=L^{1}\left(R^{n}\right)$. Thus every $f \in L^{1}\left(R^{n}\right)$ satisfies (4), which is the same as (2). This proves part $(a)$.

If $\phi$ is slowly oscillating and if $\varepsilon>0$, choose $A$ and $\delta$ as in Definition 9.6, and choose $f \in L^{1}\left(R^{n}\right)$ so that $f \geq 0, \hat{f}(0)=1$, and $f(x)=0$ if $|x| \geq \delta$. By (2),

Also,

$$
\lim _{|x| \rightarrow \infty}(f * \phi)(x)=a
$$

$$
\phi(x)-(f * \phi)(x)=\int_{|y|<\delta}[\phi(x)-\phi(x-y)] f(y) d m_{n}(y)
$$

If $|x|>A+\delta$, our choice of $A, \delta$, and $f$ shows that

$$
|\phi(x)-(f * \phi)(x)|<\varepsilon
$$

Now (3) follows from (7) and (9).

This completes the proof.

9.8 Remark If $n=1$, Theorem 9.7 can be modified in an obvious fashion, by writing $x \rightarrow+\infty$ in place of $|x| \rightarrow \infty$ wherever the latter occurs and by assuming in (b) that $\phi$ is slowly oscillating at $+\infty$. The proof remains unchanged.

\section{The Prime Number Theorem}
9.9 Introduction For any positive number $x, \pi(x)$ denotes the number of primes $p$ that satisfy $p \leq x$. The prime number theorem is the statement that

$$
\lim _{x \rightarrow \infty} \frac{\pi(x) \log x}{x}=1
$$

We shall prove this by means of a tauberian theorem due to Ingham, based on that of Wiener. The idea is to replace the rather irregular function $\pi$ by a function $F$ whose asymptotic behavior is very easily established and to use the tauberian theorem to draw a conclusion about $\pi$ from knowledge of $F$.

9.10 Preparation. The letter $p$ will now always denote a prime; $m$ and $n$ will be positive integers; $x$ will be a positive number; $[x]$ is the integer that satisfies $x-1<$ $[x] \leq x$; the symbol $d \mid n$ means that $d$ and $n / d$ are positive integers. Define

$$
\begin{aligned}
& \Lambda(n)= \begin{cases}\log p & \text { if } n=p, p^{2}, p^{3}, \ldots, \\
0 & \text { otherwise, }\end{cases} \\
& \psi(x)=\sum_{n \leq x} \Lambda(n), \\
& F(x)=\sum_{m=1}^{\infty} \psi\left(\frac{x}{m}\right) .
\end{aligned}
$$

The following properties of $\psi$ and $F$ will be used:

$$
\frac{\psi(x)}{x} \leq \frac{\pi(x) \log x}{x}<\frac{1}{\log x}+\frac{\psi(x) \log x}{x \log \left(x / \log ^{2} x\right)}
$$

if $x>e$, and

$$
F(x)=x \log x-x+b(x) \log x
$$

where $b(x)$ remains bounded as $x \rightarrow \infty$.

By (4), the prime number theorem is a consequence of the relation

$$
\lim _{x \rightarrow \infty} \frac{\psi(x)}{x}=1
$$

which will be proved from (3) and (5) by a tauberian theorem.

PROOF OF (4) $\quad[\log x / \log p]$ is the number of powers of $p$ that do not exceed $x$. Hence

$$
\psi(x)=\sum_{p \leq x}\left[\frac{\log x}{\log p}\right] \log p \leq \sum_{p \leq x} \log x=\pi(x) \log x
$$

This gives the first inequality in (4). If $1<y<x$, then

$$
\pi(x)-\pi(y)=\sum_{y<p \leq x} 1 \leq \sum_{y<p \leq x} \frac{\log p}{\log y} \leq \frac{\psi(x)}{\log y} .
$$

Hence $\pi(x)<y+\psi(x) / \log y$. With $y=x / \log ^{2} x$, this gives the second half of (4).

PROOF OF (5) If $n>1$, then

$$
F(n)-F(n-1)=\sum_{m=1}^{\infty}\left\{\psi\left(\frac{n}{m}\right)-\psi\left(\frac{n-1}{m}\right)\right\}
$$

The $m$ th summand is 0 except when $n / m$ is an integer, in which case it is $\Lambda(n / m)$. Hence

$$
F(n)-F(n-1)=\sum_{m \mid n} \Lambda\left(\frac{n}{m}\right)=\sum_{d \mid n} \Lambda(d)=\log n
$$

The last equality depends on the factorization of $n$ into a product of powers of distinct primes. Since $F(1)=0$, we have computed that

$$
F(n)=\sum_{m=1}^{n} \log m=\log (n !) \quad(n=1,2,3, \ldots)
$$

which suggests comparison of $F(x)$ with the integral

$$
J(x)=\int_{1}^{x} \log t d t=x \log x-x+1
$$

If $n \leq x \leq n+1$ then

$$
J(n)<F(n) \leq F(x) \leq F(n+1)<J(n+2)
$$

so that

$$
|F(x)-J(x)|<2 \log (x+2) .
$$

Now (5) follows from (8) and (10).

9.11 The Riemann zeta function As is the custom in analytic number theory, complex variables will now be written in the form $s=\sigma+i$. In the half-plane $\sigma>1$, the zeta function is defined by the series

$$
\zeta(s)=\sum_{n=1}^{\infty} n^{-s}
$$

Since $\left|n^{-s}\right|=n^{-\sigma}$, the series converges uniformly on every compact subset of this half-plane, and $\zeta$ is holomorphic there.

A simple computation gives

$$
s \int_{1}^{N+1}[x] x^{-1-s} d x=s \sum_{n=1}^{N} n \int_{n}^{n+1} x^{-1-s} d x=\sum_{n=1}^{N} n^{-s}-N(N+1)^{-s}
$$

When $\sigma>1, N(N+1)^{-s} \rightarrow 0$ as $N \rightarrow \infty$. Hence

$$
\zeta(s)=s \int_{1}^{\infty}[x] x^{-1-s} d x \quad(\sigma>1)
$$

If $b(x)=[x]-x$, it follows from (2) that

$$
\zeta(s)=\frac{-s}{s-1}+s \int_{1}^{\infty} b(x) x^{-1-s} d x \quad(\sigma>1) .
$$

Since $b$ is bounded, the last integral defines a holomorphic function in the half-plane $\sigma>0$. Thus (3) furnishes an analytic continuation of $\zeta$ to $\sigma>0$, which is holomorphic except for a simple pole at $s=1$, with residue 1 . The most important property we shall need is that $\zeta$ has no zeros on the line $\sigma=1$ :

$$
\zeta(1+i t) \neq 0 \quad(-\infty<t<\infty)
$$

The proof of (4) depends on the identity

$$
\zeta(s)=\prod_{p}\left(1-p^{-s}\right)^{-1} \quad(\sigma>1)
$$

Since $\left(1-p^{-s}\right)^{-1}=1+p^{-s}+p^{-2 s}+\cdots$, the fact that the product (5) equals the series (1) is an immediate consequence of the fact that every positive integer has a unique factorization into a product of powers of primes. Since $\sum p^{-\sigma}<\infty$ if $\sigma>1$, (5) shows that $\zeta(s) \neq 0$ if $\sigma>1$ and that

(6) $\cdot$

$$
\log \zeta(s)=\sum_{p} \sum_{m=1}^{\infty} m^{-1} p^{-m s} \quad(\sigma>1)
$$

Fix a real $t \neq 0$. If $\sigma>1$, (6) implies that

$$
\log \left|\zeta^{3}(\sigma) \zeta^{4}(\sigma+i t) \zeta(\sigma+2 i t)\right|=\sum_{p, m} m^{-1} p^{-m \sigma} \operatorname{Re}\left\{3+4 p^{-i m t}+p^{-2 i m t}\right\} \geq 0
$$

because $\operatorname{Re}\left\{3+4 e^{i \theta}+e^{2 i \theta}\right\}=2(1+\cos \theta)^{2}$ for all real $\theta$. Hence

$$
|(\sigma-1) \zeta(\sigma)|^{3}\left|\frac{\zeta(\sigma+i t)}{\sigma-1}\right|^{4}|\zeta(\sigma+2 i t)| \geq \frac{1}{\sigma-1}
$$

If $\zeta(1+i t)$ were 0 , the left side of (8) would converge to a limit, namely, $\left|\zeta^{\prime}(1+i t)\right|^{4}|\zeta(1+2 i t)|$, as $\sigma$ decreases to 1 . Since the right side of (8) tends to infinity, this is impossible, and (4) is proved.

9.12 Ingham's tauberian theorem Suppose $g$ is a real nondecreasing function on $(0, \infty), g(x)=0$ if $x<1$,

and

$$
G(x)=\sum_{n=1}^{\infty} g\left(\frac{x}{n}\right) \quad(0<x<\infty)
$$

$$
G(x)=a x \log x+b x+x \varepsilon(x),
$$

where $a, b$ are constants and $\varepsilon(x) \rightarrow 0$ as $x \rightarrow \infty$. Then

$$
\lim _{x \rightarrow \infty} x^{-1} g(x)=a
$$

If $g$ is the function $\psi$ defined in Section 9.10, Ingham's theorem implies, in view of Equations (3) and (5) of Section 9.10, that (6) of Section 9.10 holds, and this, as we saw there, gives the prime number theorem.

PROOF We first show that $x^{-1} g(x)$ is bounded. Since $g$ is nondecreasing,

$$
\begin{aligned}
g(x)-g\left(\frac{x}{2}\right) & \leq \sum_{n=1}^{\infty}(-1)^{n+1} g\left(\frac{x}{n}\right)=G(x)-2 G\left(\frac{x}{2}\right) \\
& =x\left\{a \log 2+\varepsilon(x)-\varepsilon\left(\frac{x}{2}\right)\right\}<A x,
\end{aligned}
$$

where $A$ is some constant. Since

$$
g(x)=g(x)-g\left(\frac{x}{2}\right)+g\left(\frac{x}{2}\right)-g\left(\frac{x}{4}\right)+\cdots
$$

it follows that

$$
g(x)<A\left(x+\frac{x}{2}+\frac{x}{4}+\cdots\right)=2 A x
$$

We now make a change of variables that will enable us to use Fourier transforms in a familiar setting. For $-\infty<x<\infty$, define

$$
h(x)=g\left(e^{x}\right), \quad H(x)=\sum_{n=1}^{\infty} h(x-\log n)
$$

Then $h(x)=0$ if $x<0, H(x)=G\left(e^{x}\right)$; hence (2) becomes

$$
H(x)=e^{x}\left(a x+b+\varepsilon_{1}(x)\right)
$$

where $\varepsilon_{1}(x) \rightarrow 0$ as $x \rightarrow \infty$. If

$$
\phi(x)=e^{-x} h(x) \quad(-\infty<x<\infty)
$$

then $\phi$ is bounded, by (4). We have to prove that

$$
\lim _{x \rightarrow \infty} \phi(x)=a .
$$

Put $k(x)=\left[e^{x}\right] e^{-x}$, let $\lambda$ be a positive irrational number, and define

$$
K(x)=2 k(x)-k(x-1)-k(x-\lambda) \quad(-\infty<x<\infty) .
$$

Then $K \in L^{1}(-\infty, \infty)$; in fact, $e^{x} K(x)$ is bounded. (See Exercise 8). If $s=$ $\sigma+i t, \sigma>0$, then formula (2) of Section 9.11 shows that

$$
\int_{-\infty}^{\infty} k(x) e^{-x s} d x=\int_{0}^{\infty}\left[e^{x}\right] e^{-x(s+1)} d x=\int_{1}^{\infty}[y] y^{-2-s} d y=\frac{\zeta(1+s)}{1+s}
$$

Repeat this with $k(x-1)$ and $k(x-\lambda)$ in place of $k(x)$, use (9), and then let $\sigma \rightarrow 0$. The result is

$$
\int_{-\infty}^{\infty} K(x) e^{-i t x} d x=\left(2-e^{-i t}-e^{-i \lambda t}\right) \frac{\zeta(1+i t)}{1+i t} .
$$

Since $\zeta(1+i t) \neq 0$ and since $\lambda$ is irrational, $\hat{K}(t) \neq 0$ if $t \neq 0$. Since $\zeta$ has a pole with residue 1 at $s=1$, the right side of (10) tends to $1+\lambda$ as $t \rightarrow 0$. Thus $\hat{K}(0) \neq 0$.

To apply Wiener's theorem, we have to estimate $K * \phi$. To do this, put $u(x)=\left[e^{x}\right]$, let $v$ be the characteristic function of $[0, \infty)$, and let $\mu$ be the measure . that assigns mass 1 to each point of the set $\{\log n: n=1,2,3, \ldots\}$ and whose support is this set. By (5), $H=h * \mu$. Also, $u=v * \mu$. Hence

$$
(h * u)(x)=(h * v * \mu)(x)=(H * v)(x)=\int_{0}^{x} H(y) d y
$$

(Note that we now take convolutions with respect to Lebesgue measure, not with respect to the normalized measure $m_{1}$.) Since

$(\phi * k)(x)=\int_{-\infty}^{\infty} e^{y-x} h(x-y)\left[e^{y}\right] e^{-y} d y=e^{-x}(h * u)(x)$,

(6) and (11) imply that

$$
(\phi * k)(x)=e^{-x} \int_{0}^{x} H(y) d y=a x+b-a+\varepsilon_{2}(x),
$$

where $\varepsilon_{2}(x) \rightarrow 0$ as $x \rightarrow \infty$. By (12) and (9),

$$
\lim _{x \rightarrow \infty}(K * \phi)(x)=(1+\lambda) a=a \int_{-\infty}^{\infty} K(y) d y
$$

Therefore Wiener's theorem 9.7 (see also Remark 9.8) implies that

$$
\lim _{x \rightarrow \infty}(f * \phi)(x)=a \int_{-\infty}^{\infty} f(y) d y
$$

for every $f \in L^{1}(-\infty, \infty)$.

Let $f_{1}$ and $f_{2}$ be nonnegative functions whose integral is 1 and whose supports lie in $[0, \varepsilon]$ and $[-\varepsilon, 0]$, respectively. By $(7), e^{x} \phi(x)$ is nondecreasing. Thus $\phi(y) \leq e^{\varepsilon} \phi(x)$ if $x-\varepsilon \leq y \leq x$, and $\phi(y) \geq e^{-\varepsilon} \phi(x)$ if $x \leq y \leq x+\varepsilon$. Consequently,

$$
e^{-\varepsilon}\left(f_{1} * \phi\right)(x) \leq \phi(x) \leq e^{\varepsilon}\left(f_{2} * \phi\right)(x)
$$

It follows from (14) and (15) that the upper and lower limits of $\phi(x)$, as $x \rightarrow \infty$, lie between $a e^{-\varepsilon}$ and $a e^{\varepsilon}$. Since $\varepsilon>0$ was arbitrary, (8) holds, and the proof is complete.

\section{The Renewal Equation}
As another application of Wiener's tauberian theorem we shall now give a brief discussion of the behavior of bounded solutions $\phi$ of the integral equation

$$
\phi(x)-\int_{-\infty}^{\infty} \phi(x-t) d \mu(t)=f(x)
$$

which occurs in probability theory. Here $\mu$ is a given Borel probability measure, $f$ is a given function, and $\phi$ is assumed to be a bounded Borel function, so that the integral exists for every $x \in R$. The equation can be written in the form

$$
\phi-\phi * \mu=f
$$

for brevity.

We begin with a uniqueness theorem.

9.13 Theorem If $\mu$ is a Borel probability measure on $R$ whose support does not lie in any cyclic subgroup of $R$, and if $\phi$ is a bounded Borel function that satisfies the homogeneous equation

$$
\phi(x)-(\phi * \mu)(x)=0
$$

for every $x \in R$, then there is a constant $A$ such that $\phi(x)=A$ except possibly in a set of Lebesgue measure 0 .

PROOF Since $\mu$ is a probability measure, $\hat{\mu}(0)=1$. Suppose that $\hat{\mu}(t)=1$ for some $t \neq 0$. Since

$$
\hat{\mu}(t)=\int_{-\infty}^{\infty} e^{-i x t} d \mu(x)
$$

it follows that $\mu$ must be concentrated on the set of all $x$ at which $e^{-i x t}=1$, that is, on the set of all integral multiples of $2 \pi / t$. But this is ruled out by the hypothesis of the theorem.

If $\sigma=\delta-\mu$, where $\delta$ is the Dirac measure, then $\hat{\sigma}=1-\hat{\mu}$. Hence $\hat{\sigma}(t)=0$ if and only if $t=0$, and (1) can be written in the form

$$
\phi * \sigma=0
$$

Put $g(x)=\exp \left(-x^{2}\right)$; put $K=g * \sigma$. Then $K \in L^{1}, \hat{K}(t)=0$ only if $\boldsymbol{t}=0$, and (3) shows that $K * \phi=0$. By Theorem 9.3 (with the one-dimensional space generated by $K$ in place of $Y$ ) the distribution $\hat{\phi}$ has its support in $\{0\}$. Hence $\hat{\phi}$ is a finite linear combination of $\delta$ and its derivatives (Theorem 6.25), so that $\phi$ is a polynomial, in the distribution sense. Since nonconstant polynomials are not bounded on $R$, and since $\phi$ is assumed to be bounded, we have reached the desired conclusion.

9.14 Convolutions of measures then

If $\mu$ and $\lambda$ are complex Borel measures on $R^{n}$,

$$
f \rightarrow \int_{R^{n}} \int_{R^{n}} f(x+y) d \mu(x) d \lambda(y)
$$

is a bounded linear functional on $C_{0}\left(R^{n}\right)$, the space of all continuous functions on $R^{n}$ that vanish at infinity. By the Riesz representation theorem, there is a unique Borel
measure $\mu * \lambda$ on $R^{n}$ that satisfies

$$
\int_{R^{n}} f d(\mu * \lambda)=\int_{R^{n}} \int_{R^{n}} f(x+y) d \mu(x) d \lambda(y) \quad\left[f \in C_{0}\left(R^{n}\right)\right]
$$

A standard approximation argument shows that (2) then holds also for every bounded Borel function $f$. In particular, we see that

$$
(\mu * \lambda)^{\wedge}=\hat{\mu} \hat{\lambda}
$$

Two other consequences of (2) will be used in the next theorem. One is the almost obvious inequality

$$
\|\mu * \lambda\| \leq\|\mu\|\|\lambda\|,
$$

where the norm denotes total variation. The other is the fact that $\mu * \lambda$ is absolutely continuous (relative to Lebesgue measure $m_{n}$ ) if this is true of $\mu$; for in that case,

$$
\int_{R^{n}} f(x+y) d \mu(x)=0
$$

for every $y \in R^{n}$, if $f$ is the characteristic function of a Borel set $E$ with $m_{n}(E)=0$, and (2) shows that $(\mu * \lambda)(E)=0$. tion

Recall that every complex Borel measure $\mu$ has a unique Lebesgue decomposi-

$$
\mu=\mu_{a}+\mu_{s}
$$

where $\mu_{a}$ is absolutely continuous relative to $m_{n}$ and $\mu_{s}$ is singular.

The next theorem is due to Karlin.

\subsection{Theorem Suppose $\mu$ is a Borel probability measure on $R$, such that}
$$
\begin{gathered}
\mu_{a} \neq 0, \\
\int_{-\infty}^{\infty}|x| d \mu(x)<\infty \\
M=\int_{-\infty}^{\infty} x d \mu(x) \neq 0 .
\end{gathered}
$$

Suppose that $f \in L^{1}(R)$, that $f(x) \rightarrow 0$ as $x \rightarrow \pm \infty$, and that $\phi$ is a bounded function that satisfies

$$
\phi(x)-(\phi * \mu)(x)=f(x) \quad(-\infty<x<\infty)
$$

Then the limits

$$
\phi(\infty)=\lim _{x \rightarrow \infty} \phi(x), \quad \phi(-\infty)=\lim _{x \rightarrow-\infty} \phi(x)
$$

exist, and

$$
\phi(\infty)-\phi(-\infty)=\frac{1}{M} \int_{-\infty}^{\infty} f(y) d y
$$

Proof Put $\sigma=\delta-\mu$, as in the proof of Theorem 9.13. Define

$$
K(x)=\sigma((-\infty, x))= \begin{cases}-\mu((-\infty, x)) & \text { if } x \leq 0 \\ \mu([x, \infty)) & \text { if } x>0\end{cases}
$$

The assumption (2) guarantees that $\bar{K} \in L^{1}(\bar{R})$. A straightforward computation, whose details we omit, shows that

$$
\int_{-\infty}^{\infty} K(x) e^{-i x t} d x= \begin{cases}\hat{\sigma}(t) / i t & \text { if } t \neq 0 \\ M & \text { if } t=0\end{cases}
$$

and that

$$
\int_{r}^{s} f(x) d x=(K * \phi)(s)-(K * \phi)(r) \quad(-\infty<r<s<\infty),
$$

since $f=\phi * \sigma$.

By (1), $\mu$ is not singular. The argument used at the beginning of the proof of Theorem 9.13 shows therefore that $\dot{\hat{\sigma}}(t) \neq 0$ if $t \neq 0$. Hence (8) and (3) imply that $\hat{K}$ has no zero in $R$.

Since $f \in L^{1}(R),(9)$ implies that $K * \phi$ has limits at $\pm \infty$, whose difference is $\int_{-\infty}^{\infty} f$.

We shall show that $\phi$ is slowly oscillating. Once this is done, (5) and (6) follow from the properties of $K$ and $K * \phi$ that we just proved, by Pitt's theorem (b) of 9.7.

Repeated substitution of $\phi=f+\phi * \mu$ into its right-hand side gives

$$
\begin{aligned}
\phi= & f+f * \mu+\cdots+f * \mu^{n-1}+\phi * \mu^{n} \\
& =f_{n}+g_{n}+h_{n} \quad(n=2,3,4, \ldots)
\end{aligned}
$$

where $\mu^{1}=\mu, \mu^{n}=\mu * \mu^{n-1}, f_{n}=f+\cdots+f * \mu^{n-1}$, and

$$
g_{n}=\phi *\left(\mu^{n}\right)_{a}, \quad h_{n}=\phi *\left(\mu^{n}\right)_{s} .
$$

For.each $n, f_{n}(x) \rightarrow 0$ as $x \rightarrow \pm \infty$, and $g_{n}$ is uniformly continuous. Hence $f_{n}+g_{n}$ is slowly oscillating. Since the total variations satisfy

we have

$$
\left\|\left(\mu^{n}\right)_{s}\right\| \leq\left\|\left(\mu_{s}\right)^{n}\right\| \leq\left\|\mu_{s}\right\|^{n}
$$

$$
\left|h_{n}(x)\right| \leq\|\phi\| \cdot\left\|\mu_{s}\right\|^{n} \quad(-\infty<x<\infty)
$$

where $\|\phi\|$ is the supremum of $|\phi|$ on $R$. By (1), $\left\|\mu_{s}\right\|<1$. Hence $h_{n} \rightarrow 0$, uniformly on $R$. Consequently, $\phi$ is the uniform limit of the slowly oscillating functions $f_{n}+g_{n}$. This implies that $\phi$ is slowly oscillating, and completes the
proof.


\end{document}