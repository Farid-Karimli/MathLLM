Theorem,Statement,Proof,References
1.19,"There exists an ordered field $R$ which has the least-upper-bound property. Moreover, $R$ contains $Q$ as a subfield.",The proof of Theorem 1.19 is rather long and a bit tedious and is therefore presented in an Appendix to Chap. 1. The proof actually constructs $R$ from $Q$.,"There exists an ordered field $R$ which has the least-upper-bound property. Moreover, $R$ contains $Q$ as a subfield.;  ;  ; "
1.20 (a),"If $x \in R, y \in R$, and $x>0$, then there is a positive integer $n$ such that

$$
n x>y .
$$","Let $A$ be the set of all $n x$, where $n$ runs through the positive integers. If (a) were false, then $y$ would be an upper bound of $A$. But then $A$ has a least upper bound in $R$. Put $\alpha=\sup A$. Since $x>0, \alpha-x<\alpha$, and $\alpha-x$ is not an upper bound of $A$. Hence $\alpha-x<m x$ for some positive integer $m$. But then $\alpha<(m+1) x \in A$, which is impossible, since $\alpha$ is an upper bound of $A$.",
1.20 (b),"If $x \in R, y \in R$, and $x<y$, then there exists a $p \in Q$ such that $x<p<y$.","Since $x<y$, we have $y-x>0$, and (a) furnishes a positive integer $n$ such that  $$ n(y-x)>1 \text {. } $$  Apply (a) again, to obtain positive integers $m_{1}$ and $m_{2}$ such that $m_{1}>n x$, $m_{2}>-n x$. Then  $$ -m_{2}<n x<m_{1} \text {. } $$  Hence there is an integer $m$ (with $-m_{2} \leq m \leq m_{1}$ ) such that  $$ m-1 \leq n x<m . $$  If we combine these inequalities, we obtain  $$ n x<m \leq 1+n x<n y . $$  Since $n>0$, it follows that  $$ x<\frac{m}{n}<y \text {. } $$  This proves $(b)$, with $p=m / n$.",
1.21,"For every real $x>0$ and every integer $n>0$ there is one and only one positive real $y$ such that $y^{n}=x$.

This number $y$ is written $\sqrt[n]{x}$ or $x^{1 / n}$.","That there is at most one such $y$ is clear, since $0<y_{1}<y_{2}$ implies $y_{1}^{n}<y_{2}^{n}$.  Let $E$ be the set consisting of all positive real numbers $t$ such that $t^{n}<x$.  If $t=x /(1+x)$ then $0 \leq t<1$. Hence $t^{n} \leq t<x$. Thus $t \in E$, and $E$ is not empty.  If $t>1+x$ then $t^{n} \geq t>x$, so that $t \notin E$. Thus $1+x$ is an upper bound of $E.  Hence Theorem 1.19 implies the existence of  $$ y=\sup E . $$  To prove that $y^{n}=x$ we will show that each of the inequalities $y^{n}<x$ and $y^{n}>x$ leads to a contradiction.  The identity $b^{n}-a^{n}=(b-a)\left(b^{n-1}+b^{n-2} a+\cdots+a^{n-1}\right)$ yields the inequality  $$ b^{n}-a^{n}<(b-a) n b^{n-1} $$  when $0<a<b$.  Assume $y^{n}<x$. Choose $h$ so that $0<h<1$ and  $$ h<\frac{x-y^{n}}{n(y+1)^{n-1}} $$  Put $a=y, b=y+h$. Then  $$ (y+h)^{n}-y^{n}<h n(y+h)^{n-1}<h n(y+1)^{n-1}<x-y^{n} . $$  Thus $(y+h)^{n}<x$, and $y+h \in E$. Since $y+h>y$, this contradicts the fact that $y$ is an upper bound of $E$.  Assume $y^{n}>x$. Put  $$ k=\frac{y^{n}-x}{n y^{n-1}} $$  Then $0<k<y$. If $t \geq y-k$, we conclude that  $$ y^{n}-t^{n} \leq y^{n}-(y-k)^{n}<k n y^{n-1}=y^{n}-x $$  Thus $t^{n}>x$, and $t \notin E$. It follows that $y-k$ is an upper bound of $E.  But $y-k<y$, which contradicts the fact that $y$ is the least upper bound of $E.  Hence $y^{n}=x$, and the proof is complete.","There exists an ordered field $R$ which has the least-upper-bound property. Moreover, $R$ contains $Q$ as a subfield.;  ;  ; "
1.25,"These definitions of addition and multiplication turn the set of all complex numbers into a field, with $(0,0)$ and $(1,0)$ in the role of 0 and 1 .","We simply verify the field axioms, as listed in Definition 1.12. (Of course, we use the field structure of $R$.)  Let $x=(a, b), y=(c, d), z=(e, f)$. (A1) is clear. (A2) $x+y=(a+c, b+d)=(c+a, d+b)=y+x$. (A3)  $$ \begin{aligned} (x+y)+z & =(a+c, b+d)+(e, f) \\ & =(a+c+e, b+d+f) \\ & =(a, b)+(c+e, d+f)=x+(y+z) \end{aligned} $$  (A4) $x+0=(a, b)+(0,0)=(a, b)=x$.  (A5) Put $-x=(-a,-b)$. Then $x+(-x)=(0,0)=0$.  (M1) is clear.  (M2) $x y=(a c-b d, a d+b c)=(c a-d b, d a+c b)=y x$.  (M3) $(x y) z=(a c-b d, a d+b c)(e, f)$  $=(a c e-b d e-a d f-b c f, a c f-b d f+a d e+b c e)$  $=(a, b)(c e-d f, c f+d e)=x(y z)$.  (M4) $1 x=(1,0)(a, b)=(a, b)=x$.  (M5) If $x \neq 0$ then $(a, b) \neq(0,0)$, which means that at least one of the real numbers $a, b$ is different from 0 . Hence $a^{2}+b^{2}>0$, by Proposition $1.18(d)$, and we can define  $$ \frac{1}{x}=\left(\frac{a}{a^{2}+b^{2}}, \frac{-b}{a^{2}+b^{2}}\right) $$  Then  $$ x \cdot \frac{1}{x}=(a, b)\left(\frac{a}{a^{2}+b^{2}}, \frac{-b}{a^{2}+b^{2}}\right)=(1,0)=1 . $$  $$ \begin{aligned} x(y+z) & =(a, b)(c+e, d+f) \\ & =(a c+a e-b d-b f, a d+a f+b c+b e) \\ & =(a c-b d, a d+b c)+(a e-b f, a f+b e) \\ & =x y+x z . \end{aligned} $$",
1.26,"For any real numbers $a$ and $b$ we have

$$
(a, 0)+(b, 0)=(a+b, 0), \quad(a, 0)(b, 0)=(a b, 0)
$$

The proof is trivial.","Theorem 1.26 shows that the complex numbers of the form $(a, 0)$ have the same arithmetic properties as the corresponding real numbers $a$. We can therefore identify $(a, 0)$ with $a$. This identification gives us the real field as a subfield of the complex field.","For any real numbers $a$ and $b$ we have

$$
(a, 0)+(b, 0)=(a+b, 0), \quad(a, 0)(b, 0)=(a b, 0)
$$

The proof is trivial.;  ;  ; "
1.28,$i^{2}=-1$,"$i^{2}=(0,1)(0,1)=(-1,0)=-1$",
1.29,"If $a$ and $b$ are real, then $(a, b)=a+b i$","$$ \begin{aligned} a+b i & =(a, 0)+(b, 0)(0,1) \\ & =(a, 0)+(0, b)=(a, b) . \end{aligned} $$",
1.31 (d),"If $z$ and $w$ are complex, then $z \bar{z}$ is real and positive (except when $z=0$ ).","write $z=a+b i$, and note that $z \bar{z}=a^{2}+b^{2}$.",
1.33 (c),Let $z$ and $w$ be complex numbers. Then $|z w|=|z||w|$,"Put $z=a+b i, w=c+d i$, with $a, b, c, d$ real. Then  $$ |z w|^{2}=(a c-b d)^{2}+(a d+b c)^{2}=\left(a^{2}+b^{2}\right)\left(c^{2}+d^{2}\right)=|z|^{2}|w|^{2} $$  or $|z w|^{2}=(|z||w|)^{2}$. Now $(c)$ follows from the uniqueness assertion of Theorem 1.21.","For every real $x>0$ and every integer $n>0$ there is one and only one positive real $y$ such that $y^{n}=x$.

This number $y$ is written $\sqrt[n]{x}$ or $x^{1 / n}$.;  ;  ; "
1.33 (d),Let $z$ and $w$ be complex numbers. Then $|\operatorname{Re} z| \leq|z|$,"note that $a^{2} \leq a^{2}+b^{2}$, hence  $$ |a|=\sqrt{a^{2}} \leq \sqrt{a^{2}+b^{2}} \text {. } $$",
1.33 (e),Let $z$ and $w$ be complex numbers. Then $|z+w| \leq|z|+|w|$,"note that $\bar{z} w$ is the conjugate of $z \bar{w}$, so that $z \bar{w}+\bar{z} w=$ $2 \operatorname{Re}(z \bar{w})$. Hence  $$ \begin{aligned} |z+w|^{2} & =(z+w)(\bar{z}+\bar{w})=z \bar{z}+z \bar{w}+\bar{z} w+w \bar{w} \\ & =|z|^{2}+2 \operatorname{Re}(z \bar{w})+|w|^{2} \\ & \leq|z|^{2}+2|z \bar{w}|+|w|^{2} \\ & =|z|^{2}+2|z||w|+|w|^{2}=(|z|+|w|)^{2} . \end{aligned} $$  Now $(e)$ follows by taking square roots.",
1.35,"If $a_{1}, \ldots, a_{n}$ and $b_{1}, \ldots, b_{n}$ are complex numbers, then

$$
\left|\sum_{j=1}^{n} a_{j} b_{j}\right|^{2} \leq \sum_{j=1}^{n}\left|a_{j}\right|^{2} \sum_{j=1}^{n}\left|b_{j}\right|^{2}
$$","Put $A=\Sigma\left|a_{j}\right|^{2}, B=\Sigma\left|b_{j}\right|^{2}, C=\Sigma a_{j} b_{j}$ (in all sums in this proof, $j$ runs over the values $1, \ldots, n)$. If $B=0$, then $b_{1}=\cdots=b_{n}=0$, and the conclusion is trivial. Assume therefore that $B>0$. By Theorem 1.31 we have  $$ \begin{aligned} \sum\left|B a_{j}-C b_{j}\right|^{2} & =\sum\left(B a_{j}-C b_{j}\right)\left(B \bar{a}_{j}-\bar{C} b_{j}\right) \\ & =B^{2} \sum\left|a_{j}\right|^{2}-B \bar{C} \sum a_{j} \bar{b}_{j}-B C \sum \bar{a}_{j} b_{j}+|C|^{2} \sum\left|b_{j}\right|^{2} \\ & =B^{2} A-B|C|^{2} \\ & =B\left(A B-|C|^{2}\right) . \end{aligned} $$  Since each term in the first sum is nonnegative, we see that  $$ B\left(A B-|C|^{2}\right) \geq 0 $$  Since $B>0$, it follows that $A B-|C|^{2} \geq 0$. This is the desired inequality.","If $z$ and $w$ are complex, then $z \bar{z}$ is real and positive (except when $z=0$ ).;Suppose $E \in \mathfrak{M}$. If $\left\{f_{n}\right\}$ is a sequence of nonnegative measurable functions and

$$
f(x)=\liminf _{n \rightarrow \infty} f_{n}(x) \quad(x \in E),
$$

then

$$
\int_{E} f d \mu \leq \liminf _{n \rightarrow \infty} \int_{E} f_{n} d \mu .
$$

;  ;  ; "
1.37 (a),"Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then $|\mathbf{x}| \geq 0$","$(a),(b)$, and $(c)$ are obvious, and $(d)$ is an immediate consequence of the Schwarz inequality.",
1.37 (b),"Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then $|\mathbf{x}|=0$ if and only if $\mathbf{x}=\mathbf{0}$;","$(a),(b)$, and $(c)$ are obvious, and $(d)$ is an immediate consequence of the Schwarz inequality.",
1.37 (c),"Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then $|\alpha \mathbf{x}|=|\alpha||\mathbf{x}|$;","$(a),(b)$, and $(c)$ are obvious, and $(d)$ is an immediate consequence of the Schwarz inequality.",
1.37 (d),"Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then $|\mathbf{x} \cdot \mathbf{y}| \leq|\mathbf{x}||\mathbf{y}|$","$(a),(b)$, and $(c)$ are obvious, and $(d)$ is an immediate consequence of the Schwarz inequality.",
1.37 (e),"Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then $|\mathbf{x}+\mathbf{y}| \leq|\mathbf{x}|+|\mathbf{y}|$;",By $(d)$ we have  $$ \begin{aligned} |x+y|^{2} & =(x+y) \cdot(x+y) \\ & =x \cdot x+2 x \cdot y+y \cdot y \\ & \leq|x|^{2}+2|x||y|+|y|^{2} \\ & =(|\mathbf{x}|+|\mathbf{y}|)^{2} \end{aligned} $$  so that $(e)$ is proved.,
1.37 (f),"Suppose $\mathbf{x}, \mathbf{y}, \mathbf{z} \in R^{k}$, and $\alpha$ is real. Then $|\mathbf{x}-\mathbf{z}| \leq|\mathbf{x}-\mathbf{y}|+|\mathbf{y}-\mathbf{z}|$","Finally, $(f)$ follows from $(e)$ if we replace $\mathbf{x}$ by $\mathbf{x}-\mathbf{y}$ and $\mathbf{y}$ by $\mathbf{y}-\mathbf{z}$.",
10.2,"For every $f \in \mathscr{C}\left(I^{k}\right), L(f)=L^{\prime}(f)$","If $h(\mathbf{x})=h_{1}\left(x_{1}\right) \cdots h_{k}\left(x_{k}\right)$, where $h_{j} \in \mathscr{C}\left(\left[a_{j}, b_{j}\right]\right)$, then  $$ L(h)=\prod_{i=1}^{k} \int_{a_{i}}^{b_{i}} h_{i}\left(x_{i}\right) d x_{i}=L^{\prime}(h) $$  If $\mathscr{A}$ is the set of all finite sums of such functions $h$, it follows that $L(g)=$ $L^{\prime}(g)$ for all $g \in \mathscr{A}$. Also, $\mathscr{A}$ is an algebra of functions on $I^{k}$ to which the Stone-Weierstrass theorem applies.  Put $V=\prod_{1}^{k}\left(b_{i}-a_{i}\right)$. If $f \in \mathscr{C}\left(I^{k}\right)$ and $\varepsilon>0$, there exists $g \in \mathscr{A}$ such that $\|f-g\|<\varepsilon / V$, where $\|f\|$ is defined as $\max |f(\mathbf{x})|\left(\mathbf{x} \in I^{k}\right)$. Then $|L(f-g)|<\varepsilon,\left|L^{\prime}(f-g)\right|<\varepsilon$, and since  $$ L(f)-L^{\prime}(f)=L(f-g)+L^{\prime}(g-f) $$  we conclude that $\left|L(f)-L^{\prime}(f)\right|<2 \varepsilon$.  In this connection, Exercise 2 is relevant.",
10.22 (a),"With $E$ and $T$ as in Sec. 10.21, let $\omega$ and $\lambda$ be $k$ - and $m$-forms in $V$, respectively. Then $(\omega+\lambda)_{T}=\omega_{T}+\lambda_{T}$ if $k=m$",Part $(a)$ follows immediately from the definitions.,
10.22 (b),"With $E$ and $T$ as in Sec. 10.21, let $\omega$ and $\lambda$ be $k$ - and $m$-forms in $V$, respectively. Then $(\omega \wedge \lambda)_{T}=\omega_{T} \wedge \lambda_{T}$","Part $(b)$ is almost as obvious, once we realize that  $$ \left(d y_{i_{1}} \wedge \cdots \wedge d y_{i_{r}}\right)_{T}=d t_{i_{1}} \wedge \cdots \wedge d t_{i_{r}} $$  regardless of whether $\left\{i_{1}, \ldots, i_{r}\right\}$ is increasing or not; (68) holds because the same number of minus signs are needed on each side of (68) to produce increasing rearrangements.",
10.22 (c),"With $E$ and $T$ as in Sec. 10.21, let $\omega$ be a $k$-form of class $\mathscr{C}^{\prime}$ in $V$, and $T$ is of class $\mathscr{C}^{\prime \prime}$. Then $d\left(\omega_{T}\right)=(d \omega)_{T}$","If $f$ is a 0 -form of class $\mathscr{C}^{\prime}$ in $V$, then  $$ f_{T}(\mathbf{x})=f(T(\mathbf{x})), \quad d f=\sum_{i}\left(D_{i} f\right)(\mathbf{y}) d y_{i} $$  By the chain rule, it follows that  $$ \begin{aligned} d\left(f_{T}\right) & =\sum_{j}\left(D_{j} f_{T}\right)(\mathbf{x}) d x_{j} \\ & =\sum_{j} \sum_{i}\left(D_{i} f\right)(T(\mathbf{x}))\left(D_{j} t_{i}\right)(\mathbf{x}) d x_{j} \\ & =\sum_{i}\left(D_{i} f\right)(T(\mathbf{x})) d t_{i} \\ & =(d f)_{T} . \end{aligned} $$  If $d y_{I}=d y_{i_{1}} \wedge \cdots \wedge d y_{i_{k}}$, then $\left(d y_{I}\right)_{T}=d t_{i_{1}} \wedge \cdots \wedge d t_{i_{k}}$, and Theorem 10.20 shows that  $$ d\left(\left(d y_{1}\right)_{T}\right)=0 $$  (This is where the assumption $T \in \mathscr{C}^{\prime \prime}$ is used.)  Assume now that $\omega=f d y_{I}$. Then  $$ \omega_{T}=f_{T}(\mathbf{x})\left(d y_{I}\right)_{T} $$  and the preceding calculations lead to  $$ \begin{aligned} d\left(\omega_{T}\right) & =d\left(f_{T}\right) \wedge\left(d y_{I}\right)_{T}=(d f)_{T} \wedge\left(d y_{I}\right)_{T} \\ & =\left((d f) \wedge d y_{T}\right)_{T}=(d \omega)_{T} \end{aligned} $$  The first equality holds by (63) and (70), the second by (69), the third by part $(b)$, and the last by the definition of $d \omega$.  The general case of $(c)$ follows from the special case just proved, if we apply $(a)$. This completes the proof.",;  ;  ; 
10.23,"Suppose $T$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m}, S$ is a $\mathscr{C}^{\prime}$-mapping of $V$ into an open set $W \subset R^{p}$, and $\omega$ is a $k$-form in $W$, so that $\omega_{S}$ is a $k$-form in $V$ and both $\left(\omega_{S}\right)_{T}$ and $\omega_{S T}$ are $k$-forms in $E$, where $S T$ is defined by $(S T)(\mathbf{x})=S(T(\mathbf{x}))$. Then

$$
\left(\omega_{S}\right)_{T}=\omega_{S T}
$$

","If $\omega$ and $\lambda$ are forms in $W$, Theorem 10.22 shows that  $$ \left((\omega \wedge \lambda)_{S}\right)_{T}=\left(\omega_{S} \wedge \lambda_{S}\right)_{T}=\left(\omega_{S}\right)_{T} \wedge\left(\lambda_{S}\right)_{T} $$  and  $$ (\omega \wedge \lambda)_{S T}=\omega_{S T} \wedge \lambda_{S T} $$  Thus if (71) holds for $\omega$ and for $\lambda$, it follows that (71) also holds for $\omega \wedge \lambda$. Since every form can be built up from 0 -forms and 1 -forms by addition and multiplication, and since (71) is trivial for 0 -forms, it is enough to prove (71) in the case $\omega=d z_{q}, q=1, \ldots, p$. (We denote the points of $E, V, W$ by $\mathbf{x}, \mathbf{y}, \mathbf{z}$, respectively.)  Let $t_{1}, \ldots, t_{m}$ be the components of $T$, let $s_{1}, \ldots, s_{p}$ be the components of $S$, and let $r_{1}, \ldots, r_{p}$ be the components of $S T$. If $\omega=d z_{q}$, then  $$ \omega_{s}=d s_{q}=\sum_{j}\left(D_{j} s_{q}\right)(\mathbf{y}) d y_{j} $$  so that the chain rule implies  $$ \begin{aligned} \left(\omega_{S}\right)_{T} & =\sum_{j}\left(D_{j} s_{q}\right)(T(\mathbf{x})) d t_{j} \\ & =\sum_{j}\left(D_{j} s_{q}\right)(T(\mathbf{x})) \sum_{i}\left(D_{i} t_{j}\right)(\mathbf{x}) d x_{i} \\ & =\sum_{i}\left(D_{i} r_{q}\right)(\mathbf{x}) d x_{i}=d r_{q}=\omega_{S T} . \end{aligned} $$","With $E$ and $T$ as in Sec. 10.21, let $\omega$ and $\lambda$ be $k$ - and $m$-forms in $V$, respectively. Then $(\omega+\lambda)_{T}=\omega_{T}+\lambda_{T}$ if $k=m$;With $E$ and $T$ as in Sec. 10.21, let $\omega$ and $\lambda$ be $k$ - and $m$-forms in $V$, respectively. Then $(\omega \wedge \lambda)_{T}=\omega_{T} \wedge \lambda_{T}$;With $E$ and $T$ as in Sec. 10.21, let $\omega$ be a $k$-form of class $\mathscr{C}^{\prime}$ in $V$, and $T$ is of class $\mathscr{C}^{\prime \prime}$. Then $d\left(\omega_{T}\right)=(d \omega)_{T}$;  ;  ; "
10.24,"Suppose $\omega$ is a $k$-form in an open set $E \subset R^{n}, \Phi$ is a $k$-surface in $E$, with parameter domain $D \subset R^{k}$, and $\Delta$ is the $k$-surface in $R^{k}$, with parameter domain $D$, defined by $\Delta(\mathbf{u})=\mathbf{u}(\mathbf{u} \in D)$. Then

$$
\int_{\Phi} \omega=\int_{\Delta} \omega_{\Phi} .
$$
","We need only consider the case  $$ \omega=a(\mathbf{x}) d x_{i_{1}} \wedge \cdots \wedge d x_{i_{k}} . $$  If $\phi_{1}, \ldots, \phi_{n}$ are the components of $\Phi$, then  $$ \omega_{\Phi}=a(\Phi(\mathbf{u})) d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}} . $$  The theorem will follow if we can show that  $$ d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}}=J(\mathbf{u}) d u_{1} \wedge \cdots \wedge d u_{k} $$  where  $$ J(\mathbf{u})=\frac{\partial\left(x_{i_{1}}, \ldots, x_{i_{k}}\right)}{\partial\left(u_{1}, \ldots, u_{k}\right)} $$  since (72) implies  $$ \begin{aligned} \int_{\Phi} \omega & =\int_{D} a(\Phi(\mathbf{u})) J(\mathbf{u}) d \mathbf{u} \\ & =\int_{\Delta} a(\Phi(\mathbf{u})) J(\mathbf{u}) d u_{1} \wedge \cdots \wedge d u_{k}=\int_{\Delta} \omega_{\Phi} . \end{aligned} $$  Let $[A]$ be the $k$ by $k$ matrix with entries  $$ \alpha(p, q)=\left(D_{q} \phi_{i_{p}}\right)(\mathbf{u}) \quad(p, q=1, \ldots, k) $$  Then  $$ d \phi_{i_{p}}=\sum_{q} \alpha(p, q) d u_{q} $$  so that  $$ d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}}=\sum \alpha\left(1, q_{1}\right) \cdots \alpha\left(k, q_{k}\right) d u_{q_{1}} \wedge \cdots \wedge d u_{q_{k}} $$  In this last sum, $q_{1}, \ldots, q_{k}$ range independently over $1, \ldots, k$. The anticommutative relation (42) implies that  $$ d u_{q_{1}} \wedge \cdots \wedge d u_{q_{k}}=s\left(q_{1}, \ldots, q_{k}\right) d u_{1} \wedge \cdots \wedge d u_{k} $$  where $s$ is as in Definition 9.33; applying this definition, we see that  $$ d \phi_{i_{1}} \wedge \cdots \wedge d \phi_{i_{k}}=\operatorname{det}[A] d u_{1} \wedge \cdots \wedge d u_{k} $$  and since $J(\mathbf{u})=\operatorname{det}[A],(72)$ is proved.",
10.25,"Suppose $T$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m}, \Phi$ is a $k$-surface in $E$, and $\omega$ is a $k$-form in $V$: $$\int_{T \Phi} \omega=\int_{\Phi} \omega_{T}$$","Let $D$ be the parameter domain of $\Phi$ (hence also of $T \Phi$ ) and define $\Delta$ as in Theorem 10.24.  Then  $$\int_{T \Phi} \omega=\int_{\Delta} \omega_{T \Phi}=\int_{\Delta}\left(\omega_{T}\right)_{\Phi}=\int_{\Phi} \omega_{T}$$  The first of these equalities is Theorem 10.24, applied to $T \Phi$ in place of $\Phi$. The second follows from Theorem 10.23. The third is Theorem 10.24, with $\omega_{T}$ in place of $\omega$.","Suppose $\omega$ is a $k$-form in an open set $E \subset R^{n}, \Phi$ is a $k$-surface in $E$, with parameter domain $D \subset R^{k}$, and $\Delta$ is the $k$-surface in $R^{k}$, with parameter domain $D$, defined by $\Delta(\mathbf{u})=\mathbf{u}(\mathbf{u} \in D)$. Then

$$
\int_{\Phi} \omega=\int_{\Delta} \omega_{\Phi} .
$$
; Suppose $\omega$ is a $k$-form in an open set $E \subset R^{n}, \Phi$ is a $k$-surface in $E$, with parameter domain $D \subset R^{k}$, and $\Delta$ is the $k$-surface in $R^{k}$, with parameter domain $D$, defined by $\Delta(\mathbf{u})=\mathbf{u}(\mathbf{u} \in D)$. Then

$$
\int_{\Phi} \omega=\int_{\Delta} \omega_{\Phi} .
$$
; Suppose $T$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m}, S$ is a $\mathscr{C}^{\prime}$-mapping of $V$ into an open set $W \subset R^{p}$, and $\omega$ is a $k$-form in $W$, so that $\omega_{S}$ is a $k$-form in $V$ and both $\left(\omega_{S}\right)_{T}$ and $\omega_{S T}$ are $k$-forms in $E$, where $S T$ is defined by $(S T)(\mathbf{x})=S(T(\mathbf{x}))$. Then

$$
\left(\omega_{S}\right)_{T}=\omega_{S T}
$$

; Suppose $\omega$ is a $k$-form in an open set $E \subset R^{n}, \Phi$ is a $k$-surface in $E$, with parameter domain $D \subset R^{k}$, and $\Delta$ is the $k$-surface in $R^{k}$, with parameter domain $D$, defined by $\Delta(\mathbf{u})=\mathbf{u}(\mathbf{u} \in D)$. Then

$$
\int_{\Phi} \omega=\int_{\Delta} \omega_{\Phi} .
$$
;  ;  ; "
10.27,"If $\sigma$ is an oriented rectilinear $k$-simplex in an open set $E \subset R^{n}$ and if $\bar{\sigma}=\varepsilon \sigma$ then

$$
\int_{\bar{\sigma}} \omega=\varepsilon \int_{\sigma} \omega
$$

for every $k$-form $\omega$ in $E$.","For $k=0$, (81) follows from the preceding definition. So we assume $k \geq 1$ and assume that $\sigma$ is given by (75).  Suppose $1 \leq j \leq k$, and suppose $\bar{\sigma}$ is obtained from $\sigma$ by interchanging $\mathbf{p}_{0}$ and $\mathbf{p}_{j}$. Then $\varepsilon=-1$, and  $$ \bar{\sigma}(\mathbf{u})=\mathbf{p}_{j}+B \mathbf{u} \quad\left(\mathbf{u} \in Q^{k}\right) $$  where $B$ is the linear mapping of $R^{k}$ into $R^{n}$ defined by $B \mathbf{e}_{j}=\mathbf{p}_{0}-\mathbf{p}_{j}$, $B \mathbf{e}_{i}=\mathbf{p}_{i}-\mathbf{p}_{j}$ if $i \neq j$. If we write $A \mathbf{e}_{i}=\mathbf{x}_{i}(1 \leq i \leq k)$, where $A$ is given by (78), the column vectors of $B$ (that is, the vectors $B \mathbf{e}_{i}$ ) are  $$ \mathbf{x}_{1}-\mathbf{x}_{j}, \ldots, \mathbf{x}_{j-1}-\mathbf{x}_{j},-\mathbf{x}_{j}, \mathbf{x}_{j+1}-\mathbf{x}_{j}, \ldots, \mathbf{x}_{k}-\mathbf{x}_{j} $$  If we subtract the $j$ th column from each of the others, none of the determinants in (35) are affected, and we obtain columns $\mathbf{x}_{1}, \ldots, \mathbf{x}_{j-1},-\mathbf{x}_{j}$, $\mathbf{x}_{j+1}, \ldots, \mathbf{x}_{k}$. These differ from those of $A$ only in the sign of the $j$ th column. Hence (81) holds for this case.  Suppose next that $0<i<j \leq k$ and that $\bar{\sigma}$ is obtained from $\sigma$ by interchanging $\mathbf{p}_{i}$ and $\mathbf{p}_{j}$. Then $\bar{\sigma}(\mathbf{u})=\mathbf{p}_{0}+C \mathbf{u}$, where $C$ has the same columns as $A$, except that the $i$ th and $j$ th columns have been interchanged. This again implies that (81) holds, since $\varepsilon=-1$.  The general case follows, since every permutation of $\{0,1, \ldots, k\}$ is a composition of the special cases we have just dealt with.",
10.33,"If $\Psi$ is a $k$-chain of class $\mathscr{C}^{\prime \prime}$ in an open set $V \subset R^{m}$ and if $\omega$ is a $(k-1)$-form of class $\mathscr{C}^{\prime}$ in $V$, then

$$
\int_{\Psi} d \omega=\int_{\partial \Psi} \omega .
$$","It is enough to prove that  $$ \int_{\Phi} d \omega=\int_{\partial \Phi} \omega $$  for every oriented $k$-simplex $\Phi$ of class $\mathscr{C}^{\prime \prime}$ in $V$. For if (92) is proved and if $\Psi=\Sigma \Phi_{i}$, then (87) and (89) imply (91).  Fix such a $\Phi$ and put  $$ \sigma=\left[\mathbf{0}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right] $$  Thus $\sigma$ is the oriented affine $k$-simplex with parameter domain $Q^{k}$ which is defined by the identity mapping. Since $\Phi$ is also defined on $Q^{k}$ (see Definition 10.30) and $\Phi \in \mathscr{C}^{\prime \prime}$, there is an open set $E \subset R^{k}$ which contains $Q^{k}$, and there is a $\mathscr{C}^{\prime \prime}$-mapping $T$ of $E$ into $V$ such that $\Phi=T \circ \sigma$. By Theorems 10.25 and $10.22(c)$, the left side of (92) is equal to  $$ \int_{T \sigma} d \omega=\int_{\sigma}(d \omega)_{T}=\int_{\sigma} d\left(\omega_{T}\right) $$  Another application of Theorem 10.25 shows, by (89), that the right side of $(92)$ is  $$ \int_{\partial(T \sigma)} \omega=\int_{T(\partial \sigma)} \omega=\int_{\partial \sigma} \omega_{T} $$  Since $\omega_{T}$ is a $(k-1)$-form in $E$, we see that in order to prove (92) we merely have to show that  $$ \int_{\sigma}^{0} d \lambda=\int_{\partial \sigma} \lambda $$  for the special simplex (93) and for every $(k-1)$-form $\lambda$ of class $\mathscr{C}^{\prime}$ in $E.  If $k=1$, the definition of an oriented 0 -simplex shows that (94) merely asserts that  $$ \int_{0}^{1} f^{\prime}(u) d u=f(1)-f(0) $$  for every continuously differentiable function $f$ on $[0,1]$, which is true by the fundamental theorem of calculus.  From now on we assume that $k>1$, fix an integer $r(1 \leq r \leq k)$, and choose $f \in \mathscr{C}^{\prime}(E)$. It is then enough to prove (94) for the case  $$ \lambda=f(\mathbf{x}) d x_{1} \wedge \cdots \wedge d x_{r-1} \wedge d x_{r+1} \wedge \cdots \wedge d x_{k} $$  since every $(k-1)$-form is a sum of these special ones, for $r=1, \ldots, k$.  By (85), the boundary of the simplex (93) is  $$ \partial \sigma=\left[\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right]+\sum_{i=1}^{k}(-1)^{i} \tau_{i} $$  where  $$ \tau_{i}=\left[0, \mathbf{e}_{1}, \ldots, \mathbf{e}_{i-1}, \mathbf{e}_{i+1}, \ldots, \mathbf{e}_{k}\right] $$  for $i=1, \ldots, k$. Put  $$ \tau_{0}=\left[\mathbf{e}_{r}, \mathbf{e}_{1}, \ldots, \mathbf{e}_{r-1}, \mathbf{e}_{r+1}, \ldots, \mathbf{e}_{k}\right] $$  Note that $\tau_{0}$ is obtained from $\left[\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right]$ by $r-1$ successive interchanges of $e_{r}$ and its left neighbors. Thus  $$ \partial \sigma=(-1)^{r-1} \tau_{0}+\sum_{i=1}^{k}(-1)^{i} \tau_{i} $$  Each $\tau_{i}$ has $Q^{k-1}$ as parameter domain.  If $\mathbf{x}=\tau_{0}(\mathbf{u})$ and $\mathbf{u} \in Q^{k-1}$, then  $$ x_{j}= \begin{cases}u_{j} & (1 \leq j<r) \\ 1-\left(u_{1}+\cdots+u_{k-1}\right) & (j=r), \\ u_{j-1} & (r<j \leq k)\end{cases} $$  If $1 \leq i \leq k, \mathbf{u} \in Q^{k-1}$, and $\mathbf{x}=\tau_{i}(\mathbf{u})$, then  $$ x_{j}= \begin{cases}u_{j} & (1 \leq j<i) \\ 0 & (j=i) \\ u_{j-1} & (i<j \leq k)\end{cases} $$  For $0 \leq i \leq k$, let $J_{i}$ be the Jacobian of the mapping  $$ \left(u_{1}, \ldots, u_{k-1}\right) \rightarrow\left(x_{1}, \ldots, x_{r-1}, x_{r+1}, \ldots, x_{k}\right) $$  induced by $\tau_{i}$. When $i=0$ and when $i=r$, (100) and (101) show that (102) is the identity mapping. Thus $J_{0}=1, J_{r}=1$. For other $i$, the fact that $x_{i}=0$ in (101) shows that $J_{i}$ has a row of zeros, hence $J_{i}=0$. Thus  $$ \int_{\tau_{i}} \lambda=0 \quad(i \neq 0, i \neq r) $$  by (35) and (96). Consequently, (97) gives  $$ \begin{aligned}\int_{\partial \sigma} \lambda & =(-1)^{r-1} \int_{\tau_{0}} \lambda+(-1)^{r} \int_{\tau_{r}} \lambda \\ & =(-1)^{r-1} \int_{0}\left[f\left(\tau_{0}(\mathbf{u})\right)-f\left(\tau_{r}(\mathbf{u})\right)\right] d \mathbf{u} .\end{aligned} $$  On the other hand,  $$ \begin{aligned}d \lambda & =\left(D_{r} f\right)(\mathbf{x}) d x_{r} \wedge d x_{1} \wedge \cdots \wedge d x_{r-1} \wedge d x_{r+1} \wedge \cdots \wedge d x_{k} \\ & =(-1)^{r-1}\left(D_{r} f\right)(\mathbf{x}) d x_{1} \wedge \cdots \wedge d x_{k} \end{aligned} $$  so that  $$ \int_{0}^{0} d \lambda=(-1)^{r-1} \int_{Q^{k}}\left(D_{r} f\right)(\mathbf{x}) d \mathbf{x} $$  We evaluate (103) by first integrating with respect to $x_{r}$, over the interval  $$ \left[0,1-\left(x_{1}+\cdots+x_{r-1}+x_{r+1}+\cdots+x_{k}\right)\right] $$  put $\left(x_{1}, \ldots, x_{r-1}, x_{r+1}, \ldots, x_{k}\right)=\left(u_{1}, \ldots, u_{k-1}\right)$, and see with the aid of (98) that the integral over $Q^{k}$ in (103) is equal to the integral over $Q^{k-1}$ in (102). Thus (94) holds, and the proof is complete.","Suppose $T$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into an open set $V \subset R^{m}, \Phi$ is a $k$-surface in $E$, and $\omega$ is a $k$-form in $V$: $$\int_{T \Phi} \omega=\int_{\Phi} \omega_{T}$$;  ; ;  ; "
10.39,"If $E \subset R^{n}$ is convex and open, if $k \geq 1$, if $\omega$ is a $k$-form of class $\mathscr{C}^{\prime}$ in $E$, and if $d \omega=0$, then there is $a(k-1)$-form $\lambda$ in $E$ such that $\omega=d \lambda$","For $p=1, \ldots, n$, let $Y_{p}$ denote the set of all $k$-forms $\omega$, of class $\mathscr{C}^{\prime}$ in $E$, whose standard presentation  $$ \omega=\sum_{I} f_{I}(\mathbf{x}) d x_{I} $$  does not involve $d x_{p+1}, \ldots, d x_{n}$. In other words, $I \subset\{1, \ldots, p\}$ if $f_{I}(\mathbf{x}) \neq 0$ for some $\mathbf{x} \in E$.  We shall proceed by induction on $p$.  Assume first that $\omega \in Y_{1}$. Then $\omega=f(\mathbf{x}) d x_{1}$. Since $d \omega=0$, $\left(D_{j} f\right)(\mathbf{x})=0$ for $1<j \leq n, \mathbf{x} \in E$. By Theorem 10.38 there is an $F \in \mathscr{C}^{\prime}(E)$ such that $D_{1} F=f$ and $D_{j} F=0$ for $1<j \leq n$. Thus  $$ d F=\left(D_{1} F\right)(\mathbf{x}) d x_{1}=f(\mathbf{x}) d x_{1}=\omega . $$  Now we take $p>1$ and make the following induction hypothesis: Every closed $k$-form that belongs to $Y_{p-1}$ is exact in $E$.  Choose $\omega \in Y_{p}$ so that $d \omega=0$. By (118),  $$ \sum_{I} \sum_{j=1}^{n}\left(D_{j} f_{I}\right)(\mathbf{x}) d x_{j} \wedge d x_{I}=d \omega=0 $$ Consider a fixed $j$, with $p<j \leq n$. Each $I$ that occurs in (118) lies in $\{1, \ldots, p\}$. If $I_{1}, I_{2}$ are two of these $k$-indices, and if $I_{1} \neq I_{2}$, then the $(k+1)$-indices $\left(I_{1}, j\right),\left(I_{2}, j\right)$ are distinct. Thus there is no cancellation, and we conclude from (119) that every coefficient in (i18) satisfies  $$ \left(D_{j} f_{I}\right)(\mathbf{x})=0 \quad(\mathbf{x} \in E, p<j \leq n) . $$ We now gather those terms in (118) that contain $d x_{p}$ and rewrite $\omega$ in the form  $$ \omega=\alpha+\sum_{I_{0}} f_{I}(\mathbf{x}) d x_{I_{0}} \wedge d x_{p} $$ where $\alpha \in Y_{p-1}$, each $I_{0}$ is an increasing $(k-1)$-index in $\{1, \ldots, p-1\}$, and $I=\left(I_{0}, p\right)$. By $(120)$, Theorem 10.38 furnishes functions $F_{I} \in \mathscr{C}^{\prime}(E)$ such that  $$ D_{p} F_{I}=f_{I}, \quad D_{j} F_{I}=0 \quad(p<j \leq n) . $$ Put  $$ \beta=\sum_{I_{0}} F_{I}(\mathbf{x}) d x_{I_{0}} $$ and define $\gamma=\omega-(-1)^{k-1} d \beta$. Since $\beta$ is a $(k-1)$-form, it follows that  $$ \begin{aligned} \gamma & =\omega-\sum_{I_{0}} \sum_{j=1}^{p}\left(D_{j} F_{I}\right)(\mathbf{x}) d x_{I_{0}} \wedge d x_{j} \\ & =\alpha-\sum_{I_{0}} \sum_{j=1}^{p-1}\left(D_{j} F_{I}\right)(\mathbf{x}) d x_{I_{0}} \wedge d x_{j}, \end{aligned} $$ which is clearly in $Y_{p-1}$. Since $d \omega=0$ and $d^{2} \beta=0$, we have $d \gamma=0$. Our induction hypothesis shows therefore that $\gamma=d \mu$ for some $(k-1)$-form $\mu$ in $E$. If $\lambda=\mu+(-1)^{k-1} \beta$, we conclude that $\omega=d \lambda$.  By induction, this completes the proof.",; ;  ;  ; 
10.7,"Suppose $\mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}, 0 \in E$, $\mathbf{F}(\mathbf{0})=\mathbf{0}$, and $\mathbf{F}^{\prime}(\mathbf{0})$ is invertible.

Then there is a neighborhood of 0 in $R^{n}$ in which a representation

$$
\mathbf{F}(\mathbf{x})=B_{1} \cdots B_{n-1} \mathbf{G}_{n} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x})
$$

is valid.

In (16), each $\mathbf{G}_{i}$ is a primitive $\mathscr{C}^{\prime}$-mapping in some neighborhood of $\mathbf{0}$; $\mathbf{G}_{i}(\mathbf{0})=\mathbf{0}, \mathbf{G}_{i}^{\prime}(\mathbf{0})$ is invertible, and each $B_{i}$ is either a flip or the identity operator. and flips.

Briefly, (16) represents $\mathbf{F}$ locally as a composition of primitive mappings","Put $\mathbf{F}=\mathbf{F}_{1}$. Assume $1 \leq m \leq n-1$, and make the following induction hypothesis (which evidently holds for $m=1$ ):  $V_{m}$ is a neighborhood of $\mathbf{0}, \mathbf{F}_{m} \in \mathscr{C}^{\prime}\left(V_{m}\right), \mathbf{F}_{m}(\mathbf{0})=\mathbf{0}, \mathbf{F}_{m}^{\prime}(\mathbf{0})$ is invertible, and  $$ P_{m-1} \mathbf{F}_{m}(\mathbf{x})=P_{m-1} \mathbf{x} \quad\left(\mathbf{x} \in V_{m}\right) $$  By (17), we have  $$ \mathbf{F}_{m}(\mathbf{x})=P_{m-1} \mathbf{x}+\sum_{i=m}^{n} \alpha_{i}(\mathbf{x}) \mathbf{e}_{i} $$  where $\alpha_{m}, \ldots, \alpha_{n}$ are real $\mathscr{C}^{\prime}$-functions in $V_{m}$. Hence  $$ \mathbf{F}_{m}^{\prime}(\mathbf{0}) \mathbf{e}_{m}=\sum_{i=m}^{n}\left(D_{m} \alpha_{l}\right)(\mathbf{0}) \mathbf{e}_{i} $$  Since $\mathbf{F}_{m}^{\prime}(\mathbf{0})$ is invertible, the left side of (19) is not $\mathbf{0}$, and therefore there is a $k$ such that $m \leq k \leq n$ and $\left(D_{m} \alpha_{k}\right)(0) \neq 0$.  Let $B_{m}$ be the flip that interchanges $m$ and this $k$ (if $k=m, B_{m}$ is the identity) and define  $$ \mathbf{G}_{m}(\mathbf{x})=\mathbf{x}+\left[\alpha_{k}(\mathbf{x})-x_{m}\right] \mathbf{e}_{m} \quad\left(\mathbf{x} \in V_{m}\right) $$  Then $\mathbf{G}_{m} \in \mathscr{C}^{\prime}\left(V_{m}\right), \mathbf{G}_{m}$ is primitive, and $\mathbf{G}_{m}^{\prime}(\mathbf{0})$ is invertible, since $\left(D_{m} \alpha_{k}\right)(\mathbf{0}) \neq 0$.  The inverse function theorem shows therefore that there is an open set $U_{m}$, with $\mathbf{0} \in U_{m} \subset V_{m}$, such that $\mathbf{G}_{m}$ is a 1-1 mapping of $U_{m}$ onto a neighborhood $V_{m+1}$ of $\mathbf{0}$, in which $\mathbf{G}_{m}^{-1}$ is continuously differentiable. Define $\mathbf{F}_{m+1}$ by  $$ \mathbf{F}_{m+1}(\mathbf{y})=B_{m} \mathbf{F}_{m} \circ \mathbf{G}_{m}^{-1}(\mathbf{y}) \quad\left(\mathbf{y} \in V_{m+1}\right) $$  Then $\mathbf{F}_{m+1} \in \mathscr{C}^{\prime}\left(V_{m+1}\right), \mathbf{F}_{m+1}(\mathbf{0})=\mathbf{0}$, and $\mathbf{F}_{m+1}^{\prime}(\mathbf{0})$ is invertible (by the chain rule). Also, for $\mathbf{x} \in U_{m}$,  $$ \begin{aligned} P_{m} \mathbf{F}_{m+1}\left(\mathbf{G}_{m}(\mathbf{x})\right) & =P_{m} B_{m} \mathbf{F}_{m}(\mathbf{x}) \\ & =P_{m}\left[P_{m-1} \mathbf{x}+\alpha_{k}(\mathbf{x}) \mathbf{e}_{m}+\cdots\right] \\ & =P_{m-1} \mathbf{x}+\alpha_{k}(\mathbf{x}) \mathbf{e}_{m} \\ & =P_{m} \mathbf{G}_{m}(\mathbf{x}) \end{aligned} $$  so that  $$ P_{m} \mathbf{F}_{m+1}(\mathbf{y})=P_{m} \mathbf{y} \quad\left(\mathbf{y} \in V_{m+1}\right) $$  Our induction hypothesis holds therefore with $m+1$ in place of $m$.  [In (22), we first used (21), then (18) and the definition of $B_{m}$, then the definition of $P_{m}$, and finally (20)].  Since $B_{m} B_{m}=I$, (21), with $\mathbf{y}=\mathbf{G}_{m}(\mathbf{x})$, is equivalent to  $$ \mathbf{F}_{m}(\mathbf{x})=B_{m} \mathbf{F}_{m+1}\left(\mathbf{G}_{m}(\mathbf{x})\right) \quad\left(\mathbf{x} \in U_{m}\right) $$  If we apply this with $m=1, \ldots, n-1$, we successively obtain  $$ \begin{aligned} \mathbf{F}=\mathbf{F}_{1} & =B_{1} \mathbf{F}_{2} \circ \mathbf{G}_{1} \\ & =B_{1} B_{2} \mathbf{F}_{3} \circ \mathbf{G}_{2} \circ \mathbf{G}_{1}=\cdots \\ & =B_{1} \cdots B_{n-1} \mathbf{F}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_{1} \end{aligned} $$  in some neighborhood of $\mathbf{0}$. By (17), $\mathbf{F}_{n}$ is primitive. This completes the proof.",
10.8,"Suppose $K$ is a compact subset of $R^{n}$, and $\left\{\mathrm{V}_{\alpha}\right\}$ is an open cover of $K$. Then there exist functions $\psi_{1}, \ldots, \psi_{s} \in \mathscr{C}\left(R^{n}\right)$ such that

(a) $0 \leq \psi_{i} \leq 1$ for $1 \leq i \leq s$;

(b) each $\psi_{i}$ has its support in some $V_{\alpha}$, and

(c) $\psi_{1}(\mathbf{x})+\cdots+\psi_{s}(\mathbf{x})=1$ for every $\mathbf{x} \in K$.","Associate with each $\mathbf{x} \in K$ an index $\alpha(\mathbf{x})$ so that $\mathbf{x} \in V_{\alpha(\mathbf{x})}$. Then there are open balls $B(\mathbf{x})$ and $W(\mathbf{x})$, centered at $\mathbf{x}$, with  $$ \overline{B(\mathbf{x})} \subset W(\mathbf{x}) \subset \overline{W(\mathbf{x})} \subset V_{\alpha(\mathbf{x})} $$  Since $K$ is compact, there are points $\mathbf{x}_{1}, \ldots, \mathbf{x}_{s}$ in $K$ such that  $$ K \subset B\left(\mathbf{x}_{1}\right) \cup \cdots \cup B\left(\mathbf{x}_{s}\right) . $$  By (26), there are functions $\varphi_{1}, \ldots, \varphi_{s} \in \mathscr{C}\left(R^{n}\right)$, such that $\varphi_{i}(\mathbf{x})=1$ on $B\left(\mathbf{x}_{i}\right), $\varphi_{i}(\mathbf{x})=0$ outside $W\left(\mathbf{x}_{i}\right)$, and $0 \leq \varphi_{i}(\mathbf{x}) \leq 1$ on $R^{n}$. Define $\psi_{1}=\varphi_{1}$ and  for $i=1, \ldots, s-1$.  Properties $(a)$ and $(b)$ are clear. The relation  $$ \psi_{1}+\cdots+\psi_{i}=1-\left(1-\varphi_{1}\right) \cdots\left(1-\varphi_{i}\right) $$  is trivial for $i=1$. If (29) holds for some $i<s$, addition of (28) and (29) yields (29) with $i+1$ in place of $i$. It follows that  $$ \sum_{i=1}^{s} \psi_{i}(\mathbf{x})=1-\prod_{i=1}^{s}\left[1-\varphi_{i}(\mathbf{x})\right] \quad\left(\mathbf{x} \in R^{n}\right) $$  If $\mathbf{x} \in K$, then $\mathbf{x} \in B\left(\mathbf{x}_{i}\right)$ for some $i$, hence $\varphi_{i}(\mathbf{x})=1$, and the product in (30) is 0 . This proves (c).",
10.9,"Suppose $T$ is a 1-1 $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{k}$ into $R^{k}$ such that $J_{T}(\mathbf{x}) \neq 0$ for all $\mathbf{x} \in E$. If $f$ is a continuous function on $R^{k}$ whose support is compact and lies in $T(E)$, then

$$
\int_{R^{k}} f(\mathbf{y}) d \mathbf{y}=\int_{R^{k}} f(T(\mathbf{x}))\left|J_{T}(\mathbf{x})\right| d \mathbf{x} .
$$","It follows from the remarks just made that (31) is true if $T$ is a primitive $\mathscr{C}$ '-mapping (see Definition 10.5), and Theorem 10.2 shows that (31) is true if $T$ is a linear mapping which merely interchanges two coordinates. then  If the theorem is true for transformations $P, Q$, and if $S(\mathbf{x})=P(Q(\mathbf{x}))$,  $$ \begin{aligned} \int f(\mathbf{z}) d \mathbf{z} & =\int f(P(\mathbf{y}))\left|J_{P}(\mathbf{y})\right| d \mathbf{y} \\ & =\int f(P(Q(\mathbf{x})))\left|J_{P}(Q(\mathbf{x}))\right|\left|J_{Q}(\mathbf{x})\right| d \mathbf{x} \\ & =\int f(S(\mathbf{x}))\left|J_{S}(\mathbf{x})\right| d \mathbf{x}, \end{aligned} $$ since  $$ \begin{aligned} J_{P}(Q(\mathbf{x})) J_{Q}(\mathbf{x}) & =\operatorname{det} P^{\prime}(Q(\mathbf{x})) \operatorname{det} Q^{\prime}(\mathbf{x}) \\ & =\operatorname{det} P^{\prime}(Q(\mathbf{x})) Q^{\prime}(\mathbf{x})=\operatorname{det} S^{\prime}(\mathbf{x})=J_{S}(\mathbf{x}) \end{aligned} $$ by the multiplication theorem for determinants and the chain.rule. Thus the theorem is also true for $S$.  Each point $\mathbf{a} \in E$ has a neighborhood $U \subset E$ in which  $$ T(\mathbf{x})=T(\mathbf{a})+B_{1} \cdots B_{k-1} \mathbf{G}_{k} \circ \mathbf{G}_{k-1} \circ \cdots \circ \mathbf{G}_{\mathbf{1}}(\mathbf{x}-\mathbf{a}) $$ where $\mathbf{G}_{i}$ and $B_{i}$ are as in Theorem 10.7. Setting $V=T(U)$, it follows that (31) holds if the support of $f$ lies in $V$. Thus:  Each point $\mathbf{y} \in T(E)$ lies in an open set $V_{\mathbf{y}} \subset T(E)$ such that (31) holds for all continuous functions whose support lies in $V_{\mathbf{y}}$.  Now let $f$ be a continuous function with compact support $K \subset T(E)$. Since $\left\{V_{\mathbf{y}}\right\}$ covers $K$, the Corollary to Theorem 10.8 she'vs that $f=\Sigma \psi_{i} f$, where each $\psi_{i}$ is continuous, and each $\psi_{i}$ has its support in some $\mathbf{V}_{\mathbf{y}}$. Thus (31) holds for each $\psi_{i} f$, and hence also for their sum $f.","For every $f \in \mathscr{C}\left(I^{k}\right), L(f)=L^{\prime}(f)$; Suppose $\mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}, 0 \in E$, $\mathbf{F}(\mathbf{0})=\mathbf{0}$, and $\mathbf{F}^{\prime}(\mathbf{0})$ is invertible.

Then there is a neighborhood of 0 in $R^{n}$ in which a representation

$$
\mathbf{F}(\mathbf{x})=B_{1} \cdots B_{n-1} \mathbf{G}_{n} \circ \cdots \circ \mathbf{G}_{1}(\mathbf{x})
$$

is valid.

In (16), each $\mathbf{G}_{i}$ is a primitive $\mathscr{C}^{\prime}$-mapping in some neighborhood of $\mathbf{0}$; $\mathbf{G}_{i}(\mathbf{0})=\mathbf{0}, \mathbf{G}_{i}^{\prime}(\mathbf{0})$ is invertible, and each $B_{i}$ is either a flip or the identity operator. and flips.

Briefly, (16) represents $\mathbf{F}$ locally as a composition of primitive mappings; Suppose $K$ is a compact subset of $R^{n}$, and $\left\{\mathrm{V}_{\alpha}\right\}$ is an open cover of $K$. Then there exist functions $\psi_{1}, \ldots, \psi_{s} \in \mathscr{C}\left(R^{n}\right)$ such that

(a) $0 \leq \psi_{i} \leq 1$ for $1 \leq i \leq s$;

(b) each $\psi_{i}$ has its support in some $V_{\alpha}$, and

(c) $\psi_{1}(\mathbf{x})+\cdots+\psi_{s}(\mathbf{x})=1$ for every $\mathbf{x} \in K$.;  ; If $\mathbf{G}$ maps an open set $E \subset R^{n}$ into $R^{n}$, and if there is an integer $m$ and a real function $g$ with domain $E$ such that

$$
\mathbf{G}(\mathbf{x})=\sum_{i \neq m} x_{i} \mathbf{e}_{i}+g(\mathbf{x}) \mathbf{e}_{m} \quad(\mathbf{x} \in E)
$$

then we call $\mathbf{G}$ primitive. A primitive mapping is thus one that changes at most one coordinate. Note that (9) can also be written in the form

$$
\mathbf{G}(\mathbf{x})=\mathbf{x}+\left[g(\mathbf{x})-x_{m}\right] \mathbf{e}_{m} .
$$

If $g$ is differentiable at some point $\mathbf{a} \in E$, so is $\mathbf{G}$. The matrix $\left[\alpha_{i j}\right]$ of the operator $\mathbf{G}^{\prime}(\mathbf{a})$ has

$$
\left(D_{1} g\right)(\mathbf{a}), \ldots,\left(D_{m} g\right)(\mathbf{a}), \ldots,\left(D_{n} g\right)(\mathbf{a})
$$

as its $m$ th row. For $j \neq m$, we have $\alpha_{j j}=1$ and $\alpha_{i j}=0$ if $i \neq j$. The Jacobian of $\mathbf{G}$ at $\mathbf{a}$ is thus given by

$$
J_{\mathbf{G}}(\mathbf{a})=\operatorname{det}\left[\mathbf{G}^{\prime}(\mathbf{a})\right]=\left(D_{m} g\right)(\mathbf{a}),
$$

and we see (by Theorem 9.36) that $\mathbf{G}^{\prime}(\mathbf{a})$ is invertible if and only if $\left(D_{m} g\right)(\mathbf{a}) \neq 0$.;  ; "
11.10,"$\mathfrak{M}(\mu)$ is a $\sigma$-ring, and $\mu^{*}$ is countably additive on $\mathfrak{M}(\mu)$.","Suppose $A \in \mathfrak{M}_{F}(\mu), B \in \mathfrak{M}_{F}(\mu)$. Choose $\left\{A_{n}\right\}$, $\left\{B_{n}\right\}$ such that $A_{n} \in \mathscr{E} . B_{n} \in \mathscr{E}, A_{n} \rightarrow A, B_{n} \rightarrow B$. Then (29) and (30) show that  $$ \begin{gathered} A_{n} \cup B_{n} \rightarrow A \cup B, \\ A_{n} \cap B_{n} \rightarrow A \cap B, \\ A_{n}-B_{n} \rightarrow A-B, \\ \mu^{*}\left(A_{n}\right) \rightarrow \mu^{*}(A), \end{gathered} $$  and $\mu^{*}(A)<+\infty$ since $d\left(A_{n}, A\right) \rightarrow 0$. By (31) and (33), $\mathfrak{M}_{F}(\mu)$ is a ring. By (7),  $$ \mu\left(A_{n}\right)+\mu\left(B_{n}\right)=\mu\left(A_{n} \cup B_{n}\right)+\mu\left(A_{n} \cap B_{n}\right) . $$  Letting $n \rightarrow \infty$, we obtain, by (34) and Theorem 11.8(a),  $$ \mu^{*}(A)+\mu^{*}(B)=\mu^{*}(A \cup B)+\mu^{*}(A \cap B) $$  If $A \cap B=0$, then $\mu^{*}(A \cap B)=0$.  It follows that $\mu^{*}$ is additive on $\mathfrak{M}_{F}(\mu)$.  Now let $A \in \mathfrak{M}(\mu)$. Then $A$ can be represented as the union of a countable collection of disjoint sets of $\mathfrak{M}_{F}(\mu)$. For if $A=\bigcup A_{n}^{\prime}$ with $A_{n}^{\prime} \in \mathfrak{M}_{\mathrm{F}}(\mu)$, write $A_{1}=A_{1}^{\prime}$, and  $$ A_{n}=\left(A_{1}^{\prime} \cup \cdots \cup A_{n}^{\prime}\right)-\left(A_{n}^{\prime} \cup \cdots \cup A_{n-1}^{\prime}\right) \quad(n=2,3,4, \ldots) $$  Then  $$ A=\bigcup_{n=1}^{\infty} A_{n} $$  is the required representation. By (19)  $$ \mu^{*}(A) \leq \sum_{n=1}^{\infty} \mu^{*}\left(A_{n}\right) $$  On the other hand, $A \supset A_{1} \cup \cdots \cup A_{n}$; and by the additivity of $\mu^{*}$ on $\mathfrak{M}_{F}(\mu)$ we obtain  $$ \mu^{*}(A) \geq \mu^{*}\left(A_{1} \cup \cdots \cup A_{n}\right)=\mu^{*}\left(A_{1}\right)+\cdots+\mu^{*}\left(A_{n}\right) $$  Equations (36) and (37) imply  $$ \mu^{*}(A)=\sum_{n=1}^{\infty} \mu^{*}\left(A_{n}\right) $$  Suppose $\mu^{*}(A)$ is finite. Put $B_{n}=A_{1} \cup \cdots \cup A_{n}$. Then (38) shows that  $$ d\left(A, B_{n}\right)=\mu^{*}\left(\bigcup_{i=n+1}^{\infty} A_{i}\right)=\sum_{i=n+1}^{\infty} \mu^{*}\left(A_{i}\right) \rightarrow 0 $$  as $n \rightarrow \infty$. Hence $B_{n} \rightarrow A$; and since $B_{n} \in \mathfrak{M}_{F}(\mu)$, it is easily seen that $A \in \mathfrak{M}_{F}(\mu)$.  We have thus shown that $A \in \mathfrak{M}_{F}(\mu)$ if $A \in \mathfrak{M}(\mu)$ and $\mu^{*}(A)<+\infty$. It is now clear that $\mu^{*}$ is countably additive on $\mathfrak{M}(\mu)$. For if  $$ A=\bigcup A_{n}, $$  where $\left\{A_{n}\right\}$ is a sequence of disjoint sets of $\mathfrak{M}(\mu)$, we have shown that (38) holds if $\mu^{*}\left(A_{n}\right)<+\infty$ for every $n$, and in the other case (38) is trivial.  Finally, we have to show that $\mathfrak{M}(\mu)$ is a $\sigma$-ring. If $A_{n} \in \mathfrak{M}(\mu), n=1$, $2,3, \ldots$, it is clear that $\bigcup A_{n} \in \mathfrak{M}(\mu)$ (Theorem 2.12). Suppose $A \in \mathfrak{M}(\mu)$, $B \in \mathfrak{M}(\mu)$, and  $$ A=\bigcup_{n=1}^{\infty} A_{n}, \quad B=\bigcup_{n=1}^{\infty} B_{n} $$  where $A_{n}, B_{n} \in \mathfrak{M}_{F}(\mu)$. Then the identity  $$ A_{n} \cap B=\bigcup_{i=1}^{\infty}\left(A_{n} \cap B_{i}\right) $$  shows that $A_{n} \cap B \in \mathfrak{M}(\mu)$; and since  $$ \mu^{*}\left(A_{n} \cap B\right) \leq \mu^{*}\left(A_{n}\right)<+\infty, $$  $A_{n} \cap B \in \mathfrak{M}_{F}(\mu)$. Hence $\quad A_{n}-B \in \mathfrak{M}_{F}(\mu)$, and $A-B \in \mathfrak{M}(\mu) \quad$ since $A-B=\bigcup_{n=1}^{\infty}\left(A_{n}-B\right)$.  We now replace $\mu^{*}(A)$ by $\mu(A)$ if $A \in \mathfrak{M}(\mu)$. Thus $\mu$, originally only defined on $\mathscr{E}$, is extended to a countably additive set function on the $\sigma$-ring $\mathfrak{M}(\mu)$. This extended set function is called a measure. The special case $\mu=m$ is called the Lebesgue measure on $R^{p}$.","For every $A \in \mathscr{E}, \mu^{*}(A)=\mu(A)$;If $E=\bigcup_{1}^{\infty} E_{n}$, then

$$
\mu^{*}(E) \leq \sum_{n=1}^{\infty} \mu^{*}\left(E_{n}\right) .
$$; Let $\left\{E_{n}\right\}, n=1,2,3, \ldots$, be a sequence of countable sets, and put

Then $S$ is countable.

$$
S=\bigcup_{n=1}^{\infty} E_{n}
$$;  ;  ; "
11.15,Each of the following four conditions implies the other three: $\{x \mid f(x)>a\}$ is measurable for every real a. $\{x \mid f(x) \geq a\}$ is measurable for every real a. $\{x \mid f(x)<a\}$ is measurable for every real a. $\{x \mid f(x) \leq a\}$ is measurable for every real a.,"The relations  $$ \begin{aligned} & \{x \mid f(x) \geq a\}=\bigcap_{n=1}^{\infty}\left\{x \mid f(x)>a-\frac{1}{n}\right\}, \\ & \{x \mid f(x)<a\}=X-\{x \mid f(x) \geq a\}, \\ & \{x \mid f(x) \leq a\}=\bigcap_{n=1}^{\infty}\left\{x \mid f(x)<a+\frac{1}{n}\right\}, \\ & \{x \mid f(x)>a\}=X-\{x \mid f(x) \leq a\} \end{aligned} $$  show successively that (43) implies (44), (44) implies (45), (45) implies (46), and (46) implies (43).  Hence any of these conditions may be used instead of (42) to define measurability.",
11.16,"If $f$ is measurable, then $|f|$ is measurable.",$$ \{x|| f(x) \mid<a\}=\{x \mid f(x)<a\} \cap\{x \mid f(x)>-a\} . $$,
11.17,"Let $\left\{f_{n}\right\}$ be a sequence of measurable functions. For $x \in X$, put

$$
\begin{aligned}
& g(x)=\sup f_{n}(x) \quad(n=1,2,3, \ldots) \\
& h(x)=\limsup _{n \rightarrow \infty} f_{n}(x)
\end{aligned}
$$

Then $g$ and $h$ are measurable.",$$ \begin{aligned} \{x \mid g(x)>a\} & =\bigcup_{n=1}^{\infty}\left\{x \mid f_{n}(x)>a\right\} \\ h(x) & =\inf g_{m}(x) \end{aligned} $$  where $g_{m}(x)=\sup f_{n}(x)(n \geq m)$.,
11.18,"Let $f$ and $g$ be measurable real-valued functions defined on $X$, let $F$ be real and continuous on $R^{2}$, and put

$$
h(x)=F(f(x), g(x)) \quad(x \in X)
$$

Then $h$ is measurable.

In particular, $f+g$ and $f g$ are measurable.","Let  $$ G_{a} = \{(u, v) \mid F(u, v) > a\} $$  Then $G_{a}$ is an open subset of $R^{2}$, and we can write  $$ G_{a} = \bigcup_{n=1}^{\infty} I_{n} $$  where $\left\{I_{n}\right\}$ is a sequence of open intervals:  $$ I_{n} = \left\{(u, v) \mid a_{n} < u < b_{n}, c_{n} < v < d_{n}\right\} . $$  Since  $$ \left\{x \mid a_{n} < f(x) < b_{n}\right\} = \left\{x \mid f(x) > a_{n}\right\} \cap\left\{x \mid f(x) < b_{n}\right\} $$  is measurable, it follows that the set  $$ \left\{x \mid (f(x), g(x)) \in I_{n}\right\} = \left\{x \mid a_{n} < f(x) < b_{n}\right\} \cap\left\{x \mid c_{n} < g(x) < d_{n}\right\} $$  is measurable. Hence the same is true of  $$ \begin{aligned} \{x \mid h(x) > a\} & = \left\{x \mid (f(x), g(x)) \in G_{a}\right\} \\ & = \bigcup_{n=1}^{\infty}\left\{x \mid (f(x), g(x)) \in I_{n}\right\} \end{aligned} $$  Summing up, we may say that all ordinary operations of analysis, including limit operations, when applied to measurable functions, lead to measurable functions; in other words, all functions that are ordinarily met with are measurable.  That this is, however, only a rough statement is shown by the following example (based on Lebesgue measure, on the real line): If $h(x)=f(g(x))$, where $f$ is measurable and $g$ is continuous, then $h$ is not necessarily measurable. (For the details, we refer to McShane, page 241.)  The reader may have noticed that measure has not been mentioned in our discussion of measurable functions. In fact, the class of measurable functions on $X$ depends only on the $\sigma$-ring $\mathfrak{M}$ (using the notation of Definition 11.12). For instance, we may speak of Borel-measurable functions on $R^{p}$, that is, of function $f$ for which  $$ \{x \mid f(x) > a\} $$  is always a Borel set, without reference to any particular measure.",
11.20,"Let $f$ be a real function on $X$. There exists a sequence $\left\{s_{n}\right\}$ of simple functions such that $s_{n}(x) \rightarrow f(x)$ as $n \rightarrow \infty$, for every $x \in X$. If $f$ is measurable, $\left\{s_{n}\right\}$ may be chosen to be a sequence of measurable functions. If $f \geq 0,\left\{s_{n}\right\}$ may be chosen to be a monotonically increasing sequence.","If $f \geq 0$, define  $$ E_{n i}=\left\{x \mid \frac{i-1}{2^{n}} \leq f(x)<\frac{i}{2^{n}}\right\}, \quad F_{n}=\{x \mid f(x) \geq n\} $$  for $n=1,2,3, \ldots, i=1,2, \ldots, n 2^{n}$. Put  $$ s_{n}=\sum_{i=1}^{n 2^{n}} \frac{i-1}{2^{n}} K_{E_{n t}}+n K_{F_{n}} $$  In the general case, let $f=f^{+}-f^{-}$, and apply the preceding construction to $f^{+}$and to $f^{-}.  It may be noted that the sequence $\left\{s_{n}\right\}$ given by $(50)$ converges uniformly to $f$ if $f$ is bounded.",
11.26,"If $f \in \mathscr{L}(\mu)$ on $E$, then $|f| \in \mathscr{L}(\mu)$ on $E$, and

$$
\left|\int_{E} f d \mu\right| \leq \int_{E}|f| d \mu
$$","Write $E=A \cup B$, where $f(x) \geq 0$ on $A$ and $f(x)<0$ on $B$. By Theorem 11.24,  $$ \int_{E}|f| d \mu=\int_{A}|f| d \mu+\int_{B}|f| d \mu=\int_{A} f^{+} d \mu+\int_{B} f^{-} d \mu<+\infty, $$  so that $|f| \in \mathscr{L}(\mu)$. Since $f \leq|f|$ and $-f \leq|f|$, we see that  $$ \int_{\boldsymbol{E}} f d \mu \leq \int_{E}|f| d \mu, \quad-\int_{\boldsymbol{E}} f d \mu \leq \int_{E}|f| d \mu $$  and (63) follows.",;  ;  ; 
11.27,"Suppose $f$ is measurable on $E,|f| \leq g$, and $g \in \mathscr{L}(\mu)$ on $E$. Then $f \in \mathscr{L}(\mu)$ on $E",We have $f^{+} \leq g$ and $f^{-} \leq g$.,
11.28,"Lebesgue's monotone convergence theorem Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
0 \leq f_{1}(x) \leq f_{2}(x) \leq \cdots \quad(x \in E)
$$

Let $f$ be defined by

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. Then

$$
\int_{E} f_{n} d \mu \rightarrow \int_{E} f d \mu \quad(n \rightarrow \infty)
$$","By (64) it is clear that, as $n \rightarrow \infty$,  $$ \int_{E} f_{n} d \mu \rightarrow \alpha $$  for some $\alpha$; and since $\int f_{n} \leq \int f$, we have  $$ \alpha \leq \int_{E} f d \mu $$  Choose $c$ such that $0<c<1$, and let $s$ be a simple measurable function such that $0 \leq s \leq f$. Put  $$ E_{n}=\left\{x \mid f_{n}(x) \geq c s(x)\right\} \quad(n=1,2,3, \ldots) $$  By (64), $E_{1} \subset E_{2} \subset E_{3} \subset \cdots$; and by (65),  $$ E=\bigcup_{n=1}^{\infty} E_{n} $$  For every $n$,  $$ \int_{E} f_{n} d \mu \geq \int_{E_{n}} f_{n} d \mu \geq c \int_{E_{n}} s d \mu . $$  We let $n \rightarrow \infty$ in (70). Since the integral is a countably additive set function (Theorem 11.24), (69) shows that we may apply Theorem 11.3 to the last integral in (70), and we obtain  $$ \alpha \geq c \int_{E} s d \mu $$  Letting $c \rightarrow 1$, we see that  $$ \alpha \geq \int_{E} s d \mu $$  and (53) implies  $$ \alpha \geq \int_{E} f d \mu $$  The theorem follows from (67), (68), and (72).","; Suppose $\phi$ is countably additive on a ring $\mathscr{R}$. Suppose $A_{n} \in \mathscr{R}$ $(n=1,2,3, \ldots), A_{1} \subset A_{2} \subset A_{3} \subset \cdots, A \in \mathscr{R}$, and

$$
A=\bigcup_{n=1}^{\infty} A_{n}
$$

Then, as $n \rightarrow \infty$,

$$
\phi\left(A_{n}\right) \rightarrow \phi(A)
$$;  ;  ; "
11.29,"Suppose $f=f_{1}+f_{2}$, where $f_{i} \in \mathscr{L}(\mu)$ on $E(i=1,2)$. Then $f \in \mathscr{L}(\mu)$ on $E$, and

$$
\int_{E} f d \mu=\int_{E} f_{1} d \mu+\int_{E} f_{2} d \mu .
$$","First, suppose $f_{1} \geq 0, f_{2} \geq 0$. If $f_{1}$ and $f_{2}$ are simple, (73) follows trivially from (52) and (54). Otherwise, choose monotonically increasing sequences $\left\{s_{n}^{\prime}\right\},\left\{s_{n}^{\prime \prime}\right\}$ of nonnegative measurable simple functions which converge to $f_{1}, f_{2}$. Theorem 11.20 shows that this is possible. Put $s_{n}=s_{n}^{\prime}+s_{n}^{\prime \prime}$. Then  $$ \int_{E} s_{n} d \mu=\int_{E} s_{n}^{\prime} d \mu+\int_{E} s_{n}^{\prime \prime} d \mu, $$  and (73) follows if we let $n \rightarrow \infty$ and appeal to Theorem 11.28.  Next, suppose $f_{1} \geq 0, f_{2} \leq 0$. Put  $$ A=\{x \mid f(x) \geq 0\}, \quad B=\{x \mid f(x)<0\} . $$  Then $f, f_{1}$, and $-f_{2}$ are nonnegative on $A$. Hence  $$ \int_{A} f_{1} d \mu=\int_{A} f d \mu+\int_{A}\left(-f_{2}\right) d \mu=\int_{A} f d \mu-\int_{A} f_{2} d \mu . $$  Similarly, $-f, f_{1}$, and $-f_{2}$ are nonnegative on $B$, so that  $$ \int_{B}\left(-f_{2}\right) d \mu=\int_{B} f_{1} d \mu+\int_{B}(-f) d \mu $$  or  $$ \int_{B} f_{1} d \mu=\int_{B} f d \mu-\int_{B} f_{2} d \mu $$  and (73) follows if we add (74) and (75).  In the general case, $E$ can be decomposed into four sets $E_{i}$ on each of which $f_{1}(x)$ and $f_{2}(x)$ are of constant sign. The two cases we have proved so far imply  $$ \int_{E_{i}} f d \mu=\int_{E_{t}} f_{1} d \mu+\int_{E_{i}} f_{2} d \mu \quad(i=1,2,3,4), $$  and (73) follows by adding these four equations.  We are now in a position to reformulate Theorem 11.28 for series.","Let $f$ be a real function on $X$. There exists a sequence $\left\{s_{n}\right\}$ of simple functions such that $s_{n}(x) \rightarrow f(x)$ as $n \rightarrow \infty$, for every $x \in X$. If $f$ is measurable, $\left\{s_{n}\right\}$ may be chosen to be a sequence of measurable functions. If $f \geq 0,\left\{s_{n}\right\}$ may be chosen to be a monotonically increasing sequence.; Lebesgue's monotone convergence theorem Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
0 \leq f_{1}(x) \leq f_{2}(x) \leq \cdots \quad(x \in E)
$$

Let $f$ be defined by

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. Then

$$
\int_{E} f_{n} d \mu \rightarrow \int_{E} f d \mu \quad(n \rightarrow \infty)
$$; Lebesgue's monotone convergence theorem Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
0 \leq f_{1}(x) \leq f_{2}(x) \leq \cdots \quad(x \in E)
$$

Let $f$ be defined by

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. Then

$$
\int_{E} f_{n} d \mu \rightarrow \int_{E} f d \mu \quad(n \rightarrow \infty)
$$;  ;  ; "
11.3,"Suppose $\phi$ is countably additive on a ring $\mathscr{R}$. Suppose $A_{n} \in \mathscr{R}$ $(n=1,2,3, \ldots), A_{1} \subset A_{2} \subset A_{3} \subset \cdots, A \in \mathscr{R}$, and

$$
A=\bigcup_{n=1}^{\infty} A_{n}
$$

Then, as $n \rightarrow \infty$,

$$
\phi\left(A_{n}\right) \rightarrow \phi(A)
$$","Put $B_{1}=A_{1}$, and  $$ B_{n}=A_{n}-A_{n-1} \quad(n=2,3, \ldots) $$  Then $B_{i} \cap B_{j}=0$ for $i \neq j, A_{n}=B_{1} \cup \cdots \cup B_{n}$, and $A=\cup B_{n}$. Hence  $$ \phi\left(A_{n}\right)=\sum_{i=1}^{n} \phi\left(B_{i}\right) $$  and  $$ \phi(A)=\sum_{i=1}^{\infty} \phi\left(B_{i}\right) $$",
11.30,"Suppose $E \in \mathfrak{M}$. If $\left\{f_{n}\right\}$ is a sequence of nonnegative measurable functions and

$$
f(x)=\sum_{n=1}^{\infty} f_{n}(x) \quad(x \in E)
$$

then

$$
\int_{E} f d \mu=\sum_{n=1}^{\infty} \int_{E} f_{n} d \mu
$$",The partial sums of (76) form a monotonically increasing sequence.,
11.31,"Suppose $E \in \mathfrak{M}$. If $\left\{f_{n}\right\}$ is a sequence of nonnegative measurable functions and

$$
f(x)=\liminf _{n \rightarrow \infty} f_{n}(x) \quad(x \in E),
$$

then

$$
\int_{E} f d \mu \leq \liminf _{n \rightarrow \infty} \int_{E} f_{n} d \mu .
$$

","For $n=1,2,3, \ldots$ and $x \in E$, put  $$ g_{n}(x)=\inf f_{i}(x) \quad(i \geq n) $$  Then $g_{n}$ is measurable on $E$, and  $$ \begin{aligned} 0 & \leq g_{1}(x) \leq g_{2}(x) \leq \cdots, \\ g_{n}(x) & \leq f_{n}(x), \\ g_{n}(x) & \rightarrow f(x) \quad(n \rightarrow \infty) . \end{aligned} $$  By (78), (80), and Theorem 11.28,  $$ \int_{E} g_{n} d \mu \rightarrow \int_{E} f d \mu $$  so that (77) follows from (79) and (81).","Lebesgue's monotone convergence theorem Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
0 \leq f_{1}(x) \leq f_{2}(x) \leq \cdots \quad(x \in E)
$$

Let $f$ be defined by

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. Then

$$
\int_{E} f_{n} d \mu \rightarrow \int_{E} f d \mu \quad(n \rightarrow \infty)
$$;  ;  ; "
11.32,"Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. If there exists a function $g \in \mathscr{L}(\mu)$ on $E$, such that

$$
\left|f_{n}(x)\right| \leq g(x) \quad(n=1,2,3, \ldots, x \in E)
$$

then

$$
\lim _{n \rightarrow \infty} \int_{E} f_{n} d \mu=\int_{E} f d \mu \text {. }
$$","First, (83) and Theorem 11.27 imply that $f_{n} \in \mathscr{L}(\mu)$ and $f \in \mathscr{L}(\mu)$ on $E$.  Since $f_{n}+g \geq 0$, Fatou's theorem shows that  $$ \int_{E}(f+g) d \mu \leq \liminf _{n \rightarrow \infty} \int_{E}\left(f_{n}+g\right) d \mu, $$  or  $$ \int_{E} f d \mu \leq \liminf _{n \rightarrow \infty} \int_{E} f_{n} d \mu . $$  Since $g-f_{n} \geq 0$, we see similarly that  so that  $$ \int_{E}(g-f) d \mu \leq \liminf _{n \rightarrow \infty} \int_{E}\left(g-f_{n}\right) d \mu $$  $$ -\int_{E} f d \mu \leq \liminf _{n \rightarrow \infty}\left[-\int_{E} f_{n} d \mu\right] $$  which is the same as  $$ \int_{E} f d \mu \geq \limsup _{n \rightarrow \infty} \int_{E} f d \mu $$  The existence of the limit in (84) and the equality asserted by ( 84$)$ now follow from (85) and (86).","Suppose $f$ is measurable on $E,|f| \leq g$, and $g \in \mathscr{L}(\mu)$ on $E$. Then $f \in \mathscr{L}(\mu)$ on $E;  ;  ; "
11.33 (a),"If $f \in \mathscr{R}$ on $[a, b]$, then $f \in \mathscr{L}$ on $[a, b]$, and

$$
\int_{a}^{b} f d x=\mathscr{R} \int_{a}^{b} f d x
$$

","Suppose $f$ is bounded. By Definition 6.1 and Theorem 6.4 there is a sequence $\left\{P_{k}\right\}$ of partitions of $[a, b]$, such that $P_{k+1}$ is a refinement of $P_{k}$, such that the distance between adjacent points of $P_{k}$ is less than $1 / k$, and such that  $$ \lim _{k \rightarrow \infty} L\left(P_{k}, f\right)=\mathscr{R} \int \underline{\underline{\int}} f d x, \quad \lim _{k \rightarrow \infty} U\left(P_{k}, f\right)=\mathscr{R} \bar{\int} f d x $$  (In this proof, all integrals are taken over $[a, b]$.).  If $P_{k}=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$, with $x_{0}=a, x_{n}=b$, define  $$ U_{k}(a)=L_{k}(a)=f(a) $$  put $U_{k}(x)=M_{i}$ and $L_{k}(x)=m_{i}$ for $x_{i-1}<x \leq x_{i}, 1 \leq i \leq n$, using the notation introduced in Definition 6.1. Then  $$ L\left(P_{k}, f\right)=\int L_{k} d x, \quad U\left(P_{k}, f\right)=\int U_{k} d x $$  and  $$ L_{1}(x) \leq L_{2}(x) \leq \cdots \leq f(x) \leq \cdots \leq U_{2}(x) \leq U_{1}(x) $$  for all $x \in[a, b]$, since $P_{k+1}$ refines $P_{k}$. By (90), there exist  $$ L(x)=\lim _{k \rightarrow \infty} L_{k}(x), \quad U(x)=\lim _{k \rightarrow \infty} U_{k}(x) $$  Observe that $L$ and $U$ are bounded measurable functions on $[a, b]$, that  $$ L(x) \leq f(x) \leq U(x) \quad(a \leq x \leq b) $$  and that  $$ \int L d x=\mathscr{R} \int_{\underline{I}} f d x, \quad \int U d x=\mathscr{R} \bar{\int} f d x $$  by (88), (90), and the monotone convergence theorem.  So far, nothing has been assumed about $f$ except that $f$ is a bounded real function on $[a, b]$.  To complete the proof, note that $f \in \mathscr{R}$ if and only if its upper and lower Riemann integrals are equal, hence if and only if  $$ \int L d x=\int U d x $$  since $L \leq U$, (94) happens if and only if $L(x)=U(x)$ for almost all $x \in[a, b]$ (Exercise 1).  In that case, (92) implies that  $$ L(x)=f(x)=U(x) $$  almost everywhere on $[a, b]$, so that $f$ is measurable, and (87) follows from (93) and (95).  Furthermore, if $x$ belongs to no $P_{k}$, it is quite easy to see that $U(x)=$ $L(x)$ if and only if $f$ is continuous at $x$. Since the union of the sets $P_{k}$ is countable, its measure is 0 , and we conclude that $f$ is continuous almost everywhere on $[a, b]$ if and only if $L(x)=U(x)$ almost everywhere, hence (as we saw above) if and only if $f \in \mathscr{R}$.","If $P^{*}$ is a refinement of $P$, then

and

$$
L(P, f, \alpha) \leq L\left(P^{*}, f, \alpha\right)
$$

$$
U\left(P^{*}, f, \alpha\right) \leq U(P, f, \alpha) .
$$
;  ; For a given interval $[a, b]$ and a bounded real function $f$ defined on $[a, b]$: By a partition $P$ of $[a, b]$ we mean a finite set of points $x_{0}, x_{1}, \ldots, x_{n}$, where

$$
a=x_{0} \leq x_{1} \leq \cdots \leq x_{n-1} \leq x_{n}=b .
$$

We write

$$
\Delta x_{i}=x_{i}-x_{i-1} \quad(i=1, \ldots, n)
$$

Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put

$$
\begin{array}{rlrl}
M_{i} & =\sup f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
m_{i} & =\inf f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
U(P, f) & =\sum_{i=1}^{n} M_{i} \Delta x_{i}, & \\
L(P, f) & =\sum_{i=1}^{n} m_{i} \Delta x_{i}, &
\end{array}
$$

and finally

$$
\begin{aligned}
& \int_{a}^{b} f d x=\inf U(P, f), \\
& \int_{a}^{b} f d x=\sup L(P, f),
\end{aligned}
$$

where the inf and the sup are taken over all partitions $P$ of $[a, b] . The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively.

If the upper and lower integrals are equal, we say that $f$ is Riemannintegrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemannintegrable functions), and we denote the common value of (1) and (2) by

$$
\int_{a}^{b} f d x
$$

or by

$$
\int_{a}^{b} f(x) d x .
$$

This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that

$$
m \leq f(x) \leq M \quad(a \leq x \leq b)
$$

Hence, for every $P$,

$$
m(b-a) \leq L(P, f) \leq U(P, f) \leq M(b-a)
$$

so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. The question of their equality, and hence the question of the integrability of $f$, is a more delicate one. Instead of investigating it separately for the Riemann integral, we shall immediately consider a more general situation.; For a given interval $[a, b]$ and a bounded real function $f$ defined on $[a, b]$: By a partition $P$ of $[a, b]$ we mean a finite set of points $x_{0}, x_{1}, \ldots, x_{n}$, where

$$
a=x_{0} \leq x_{1} \leq \cdots \leq x_{n-1} \leq x_{n}=b .
$$

We write

$$
\Delta x_{i}=x_{i}-x_{i-1} \quad(i=1, \ldots, n)
$$

Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put

$$
\begin{array}{rlrl}
M_{i} & =\sup f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
m_{i} & =\inf f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
U(P, f) & =\sum_{i=1}^{n} M_{i} \Delta x_{i}, & \\
L(P, f) & =\sum_{i=1}^{n} m_{i} \Delta x_{i}, &
\end{array}
$$

and finally

$$
\begin{aligned}
& \int_{a}^{b} f d x=\inf U(P, f), \\
& \int_{a}^{b} f d x=\sup L(P, f),
\end{aligned}
$$

where the inf and the sup are taken over all partitions $P$ of $[a, b] . The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively.

If the upper and lower integrals are equal, we say that $f$ is Riemannintegrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemannintegrable functions), and we denote the common value of (1) and (2) by

$$
\int_{a}^{b} f d x
$$

or by

$$
\int_{a}^{b} f(x) d x .
$$

This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that

$$
m \leq f(x) \leq M \quad(a \leq x \leq b)
$$

Hence, for every $P$,

$$
m(b-a) \leq L(P, f) \leq U(P, f) \leq M(b-a)
$$

so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. The question of their equality, and hence the question of the integrability of $f$, is a more delicate one. Instead of investigating it separately for the Riemann integral, we shall immediately consider a more general situation.;  ; "
11.33 (b),"Suppose $f$ is bounded on $[a, b]$. Then $f \in \mathscr{R}$ on $[a, b]$ if and only if $f$ is continuous almost everywhere on $[a, b]$",This is a direct consequence of the proof of 11.33 (a),
11.36,"If $f \in \mathscr{L}^{2}(\mu)$ and $g \in \mathscr{L}^{2}(\mu)$, then $f+g \in \mathscr{L}^{2}(\mu)$, and

$$
\|f+g\| \leq\|f\|+\|g\|
$$",The Schwarz inequality shows that  $$ \begin{aligned} \|f+g\|^{2} & =\int|f|^{2}+\int f \bar{g}+\int f g+\int|g|^{2} \\ & \leq\|f\|^{2}+2\|f\|\|g\|+\|g\|^{2} \\ & =(\|f\|+\|g\|)^{2} \end{aligned} $$,
11.38,"The continuous functions form a dense subset of $\mathscr{L}^{2}$ on $[a, b]$.

More explicitly, this means that for any $f \in \mathscr{L}^{2}$ on $[a, b]$, and any $\varepsilon>0$, there is a function $g$, continuous on $[a, b]$, such that

$$
\|f-g\|=\left\{\int_{a}^{b}|f-g|^{2} d x\right\}^{1 / 2}<\varepsilon
$$","We shall say that $f$ is approximated in $\mathscr{L}^{2}$ by a sequence $\left\{g_{n}\right\}$ if $\left\|f-g_{n}\right\| \rightarrow 0$ as $n \rightarrow \infty$.  Let $A$ be a closed subset of $[a, b]$, and $K_{A}$ its characteristic function. Put  $$ t(x)=\inf |x-y| \quad(y \in A) $$  and  $$ g_{n}(x)=\frac{1}{1+n t(x)} \quad(n=1,2,3, \ldots) $$  Then $g_{n}$ is continuous on $[a, b], g_{n}(x)=1$ on $A$, and $g_{n}(x) \rightarrow 0$ on $B$, where $B=[a, b]-A$. Hence  $$ \left\|g_{n}-K_{A}\right\|=\left\{\int_{B} g_{n}^{2} d x\right\}^{1 / 2} \rightarrow 0 $$  by Theorem 11.32. Thus characteristic functions of closed sets can be approximated in $\mathscr{L}^{2}$ by continuous functions.  By (39) the same is true for the characteristic function of any measurable set, and hence also for simple measurable functions.  If $f \geq 0$ and $f \in \mathscr{L}^{2}$, let $\left\{s_{n}\right\}$ be a monotonically increasing sequence of simple nonnegative measurable functions such that $s_{n}(x) \rightarrow f(x)$. Since $\left|f-s_{n}\right|^{2} \leq f^{2}$, Theorem 11.32 shows that $\left\|f-s_{n}\right\| \rightarrow 0$.  The general case follows.","Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. If there exists a function $g \in \mathscr{L}(\mu)$ on $E$, such that

$$
\left|f_{n}(x)\right| \leq g(x) \quad(n=1,2,3, \ldots, x \in E)
$$

then

$$
\lim _{n \rightarrow \infty} \int_{E} f_{n} d \mu=\int_{E} f d \mu \text {. }
$$; Suppose $E \in \mathfrak{M}$. Let $\left\{f_{n}\right\}$ be a sequence of measurable functions such that

$$
f_{n}(x) \rightarrow f(x) \quad(x \in E)
$$

as $n \rightarrow \infty$. If there exists a function $g \in \mathscr{L}(\mu)$ on $E$, such that

$$
\left|f_{n}(x)\right| \leq g(x) \quad(n=1,2,3, \ldots, x \in E)
$$

then

$$
\lim _{n \rightarrow \infty} \int_{E} f_{n} d \mu=\int_{E} f d \mu \text {. }
$$;  ;  ; "
11.42,"If $\left\{f_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$, then there exists a function $f \in \mathscr{L}^{2}(\mu)$ such that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$.","Since $\left\{f_{n}\right\}$ is a Cauchy sequence, we can find a sequence $\left\{n_{k}\right\}$, $k=1,2,3, \ldots$, such that  $$ \left\|f_{n_{k}}-f_{n_{k+1}}\right\|<\frac{1}{2^{k}} \quad(k=1,2,3, \ldots) $$ Choose a function $g \in \mathscr{L}^{2}(\mu)$. By the Schwarz inequality,  $$ \int_{X}^{0}\left|g\left(f_{n_{k}}-f_{n_{k+1}}\right)\right| d \mu \leq \frac{\|g\|}{2^{k}} $$ Hence  $$ \sum_{k=1}^{\infty} \int_{X}\left|g\left(f_{n_{k}}-f_{n_{k+1}}\right)\right| d \mu \leq\|g\| $$ By Theorem 11.30, we may interchange the summation and integration in (102). It follows that  $$ |g(x)| \sum_{k=1}^{\infty}\left|f_{n_{k}}(x)-f_{n_{k+1}}(x)\right|<+\infty $$ almost everywhere on $X$. Therefore  $$ \sum_{k=1}^{\infty}\left|f_{n_{k+1}}(x)-f_{n_{k}}(x)\right|<+\infty $$ almost everywhere on $X. For if the series in (104) were divergent on a set $E$ of positive measure, we could take $g(x)$ to be nonzero on a subset of $E$ of positive measure, thus obtaining a contradiction to (103).  Since the $k$ th partial sum of the series  $$ \sum_{k=1}^{\infty}\left(f_{n_{k+1}}(x)-f_{n_{k}}(x)\right) $$ which converges almost everywhere on $X$, is  $$ f_{n_{k}+1}(x)-f_{n_{1}}(x) $$ we see that the equation  $$ f(x)=\lim _{k \rightarrow \infty} f_{n_{k}}(x) $$ defines $f(x)$ for almost all $x \in X$, and it does not matter how we define $f(x)$ at the remaining points of $X.  We shall now show that this function $f$ has the desired properties. Let $\varepsilon>0$ be given, and choose $N$ as indicated in Definition 11.41. If $n_{k}>N$, Fatou's theorem shows that  $$ \left\|f-f_{n_{k}}\right\| \leq \liminf _{i \rightarrow \infty}\left\|f_{n_{i}}-f_{n_{k}}\right\| \leq \varepsilon $$ Thus $f-f_{n_{k}} \in \mathscr{L}^{2}(\mu)$, and since $f=\left(f-f_{n_{k}}\right)+f_{n_{k}}$, we see that $f \in \mathscr{L}^{2}(\mu)$. Also, since $\varepsilon$ is arbitrary,  $$ \lim _{k \rightarrow \infty}\left\|f-f_{n_{k}}\right\|=0 $$Finally, the inequality  $$ \left\|f-f_{n}\right\| \leq\left\|f-f_{n_{k}}\right\|+\left\|f_{n_{k}}-f_{n}\right\| $$ shows that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$; for if we take $n$ and $n_{k}$ large enough, each of the two terms on the right of (105) can be made arbitrarily small.","Suppose $E \in \mathfrak{M}$. If $\left\{f_{n}\right\}$ is a sequence of nonnegative measurable functions and

$$
f(x)=\sum_{n=1}^{\infty} f_{n}(x) \quad(x \in E)
$$

then

$$
\int_{E} f d \mu=\sum_{n=1}^{\infty} \int_{E} f_{n} d \mu
$$;  ; For $f$ and $f_{n} \in \mathscr{L}^{2}(\mu)(n=1,2,3, \ldots)$: We say that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$ if $\left\|f_{n}-f\right\| \rightarrow 0$. We say that $\left\{f_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$ if for every $\varepsilon>0$ there is an integer $N$ such that $n \geq N, m \geq N$ implies $\left\|f_{n}-f_{m}\right\| \leq \varepsilon$.;  ; "
11.43,"Let $\left\{\phi_{n}\right\}$ be orthonormal on $X$. Suppose $\Sigma\left|c_{n}\right|^{2}$ converges, and put $s_{n}=c_{1} \phi_{1}+\cdots+c_{n} \phi_{n}$. Then there exists a function $f \in \mathscr{L}^{2}(\mu)$ such that $\left\{s_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$, and such that

$$
f \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$","For $n>m$,  $$ \left\|s_{n}-s_{m}\right\|^{2}=\left|c_{m+1}\right|^{2}+\cdots+\left|c_{n}\right|^{2} $$ so that $\left\{s_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$. By Theorem 11.42, there is a function $f \in \mathscr{L}^{2}(\mu)$ such that  $$ \lim _{n \rightarrow \infty}\left\|f-s_{n}\right\|=0 $$Now, for $n>k$,  $$ \int_{X} f \Phi_{k} d \mu-c_{k}=\int_{X} f \Phi_{k} d \mu-\int_{X} s_{n} \bar{\phi}_{k} d \mu $$so that  $$ \left|\int_{X} f \Phi_{k} d \mu-c_{k}\right| \leq\left\|f-s_{n}\right\| \cdot\left\|\phi_{k}\right\|+\left\|f-s_{n}\right\| $$Letting $n \rightarrow \infty$, we see that  $$ c_{k}=\int_{X} f \Phi_{k} d \mu \quad(k=1,2,3, \ldots) $$and the proof is complete.","If $\left\{f_{n}\right\}$ is a Cauchy sequence in $\mathscr{L}^{2}(\mu)$, then there exists a function $f \in \mathscr{L}^{2}(\mu)$ such that $\left\{f_{n}\right\}$ converges to $f$ in $\mathscr{L}^{2}(\mu)$.;  ;  ; "
11.45,"Let $\left\{\phi_{n}\right\}$ be a complete orthonormal set. If $f \in \mathscr{L}^{2}(\mu)$ and if

$$
f \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}
$$

then

$$
\int_{X}|f|^{2} d \mu=\sum_{n=1}^{\infty}\left|c_{n}\right|^{2}
$$","By the Bessel inequality, $\Sigma\left|c_{n}\right|^{2}$ converges. Putting  $$ s_{n}=c_{1} \phi_{1}+\cdots+c_{n} \phi_{n} $$  the Riesz-Fischer theorem shows that there is a function $g \in \mathscr{L}^{2}(\mu)$ such that  $$ g \sim \sum_{n=1}^{\infty} c_{n} \phi_{n} $$  and such that $\left\|g-s_{n}\right\| \rightarrow 0$. Hence $\left\|s_{n}\right\| \rightarrow\|g\|$. Since  $$ \left\|s_{n}\right\|^{2}=\left|c_{1}\right|^{2}+\cdots+\left|c_{n}\right|^{2} $$  we have  $$ \int_{X}|g|^{2} d \mu=\sum_{n=1}^{\infty}\left|c_{n}\right|^{2} $$  Now (106), (108), and the completeness of $\left\{\phi_{n}\right\}$ show that $\|f-g\|=0$, so that (109) implies (107).",
11.8 (a),"For every $A \in \mathscr{E}, \mu^{*}(A)=\mu(A)$","Choose $A \in \mathscr{E}$ and $\varepsilon>0$.  The regularity of $\mu$ shows that $A$ is contained in an open elementary set $G$ such that $\mu(G) \leq \mu(A)+\varepsilon$. Since $\mu^{*}(A) \leq \mu(G)$ and since $\varepsilon$ was arbitrary, we have  $$ \mu^{*}(A) \leq \mu(A) $$  The definition of $\mu^{*}$ shows that there is a sequence $\left\{A_{n}\right\}$ of open elementary sets whose union contains $A$, such that  $$ \sum_{n=1}^{\infty} \mu\left(A_{n}\right) \leq \mu^{*}(A)+\varepsilon . $$  The regularity of $\mu$ shows that $A$ contains a closed elementary set $F$ such that $\mu(F) \geq \mu(A)-\varepsilon$; and since $F$ is compact, we have  $$ F \subset A_{1} \cup \cdots \cup A_{N} $$  for some $N$. Hence  $$ \mu(A) \leq \mu(F)+\varepsilon \leq \mu\left(A_{1} \cup \cdots \cup A_{N}\right)+\varepsilon \leq \sum_{1}^{N} \mu\left(A_{n}\right)+\varepsilon \leq \mu^{*}(A)+2 \varepsilon . $$  In conjunction with (20), this proves $(a)$.",
11.8 (b),"If $E=\bigcup_{1}^{\infty} E_{n}$, then

$$
\mu^{*}(E) \leq \sum_{n=1}^{\infty} \mu^{*}\left(E_{n}\right) .
$$","Next, suppose $E=\bigcup E_{n}$, and assume that $\mu^{*}\left(E_{n}\right)<+\infty$ for all $n$. Given $\varepsilon>0$, there are coverings $\left\{A_{n k}\right\}, k=1,2,3, \ldots$, of $E_{n}$ by open elementary sets such that  $$ \sum_{k=1}^{\infty} \mu\left(A_{n k}\right) \leq \mu^{*}\left(E_{n}\right)+2^{-n} \varepsilon $$  Then  $$ \mu^{*}(E) \leq \sum_{n=1}^{\infty} \sum_{k=1}^{\infty} \mu\left(A_{n k}\right) \leq \sum_{n=1}^{\infty} \mu^{*}\left(E_{n}\right)+\varepsilon $$  and (19) follows. In the excluded case, i.e., if $\mu^{*}\left(E_{n}\right)=+\infty$ for some $n$, (19) is of course trivial.",
2.12,"Let $\left\{E_{n}\right\}, n=1,2,3, \ldots$, be a sequence of countable sets, and put

Then $S$ is countable.

$$
S=\bigcup_{n=1}^{\infty} E_{n}
$$","Let every set $E_{n}$ be arranged in a sequence $\left\{x_{n k}\right\}, k=1,2,3, \ldots$, and consider the infinite array  in which the elements of $E_{n}$ form the $n$th row. The array contains all elements of $S$. As indicated by the arrows, these elements can be arranged in a sequence  $$ x_{11} ; x_{21}, x_{12} ; x_{31}, x_{22}, x_{13} ; x_{41}, x_{32}, x_{23}, x_{14} ; \ldots $$  If any two of the sets $E_{n}$ have elements in common, these will appear more than once in (17). Hence there is a subset $T$ of the set of all positive integers such that $S \sim T$, which shows that $S$ is at most countable (Theorem 2.8). Since $E_{1} \subset S$, and $E_{1}$ is infinite, $S$ is infinite, and thus countable.",Every infinite subset of a countable set $A$ is countable.;  ;  ; 
2.13,"Let $A$ be a countable set, and let $B_{n}$ be the set of all $n$-tuples $\left(a_{1}, \ldots, a_{n}\right)$, where $a_{k} \in A(k=1, \ldots, n)$, and the elements $a_{1}, \ldots, a_{n}$ need not be distinct. Then $B_{n}$ is countable.","That $B_{1}$ is countable is evident, since $B_{1}=A$. Suppose $B_{n-1}$ is countable $(n=2,3,4, \ldots)$. The elements of $B_{n}$ are of the form  $$ (b, a) \quad\left(b \in B_{n-1}, a \in A\right) . $$  For every fixed $b$, the set of pairs $(b, a)$ is equivalent to $A$, and hence countable. Thus $B_{n}$ is the union of a countable set of countable sets. By Theorem 2.12, $B_{n}$ is countable.  The theorem follows by induction.","Let $\left\{E_{n}\right\}, n=1,2,3, \ldots$, be a sequence of countable sets, and put

Then $S$ is countable.

$$
S=\bigcup_{n=1}^{\infty} E_{n}
$$;  ;  ; "
2.14,Let $A$ be the set of all sequences whose elements are the digits 0 and 1 . This set $A$ is uncountable.,"Let $E$ be a countable subset of $A$, and let $E$ consist of the sequences $s_{1}, s_{2}, s_{3}, \ldots$ We construct a sequence $s$ as follows. If the $n$th digit in $s_{n}$ is 1 , we let the $n$th digit of $s$ be 0 , and vice versa. Then the sequence $s$ differs from every member of $E$ in at least one place; hence $s \notin E$. But clearly $s \in A$, so that $E$ is a proper subset of $A$.  We have shown that every countable subset of $A$ is a proper subset of $A$. It follows that $A$ is uncountable (for otherwise $A$ would be a proper subset of $A$, which is absurd).",
2.19,Every neighborhood is an open set.,"Consider a neighborhood $E=N_{r}(p)$, and let $q$ be any point of $E$. Then there is a positive real number $h$ such that  $$ d(p, q)=r-h \text {. } $$  For all points $s$ such that $d(q, s)<h$, we have then  $$ d(p, s) \leq d(p, q)+d(q, s)<r-h+h=r $$  so that $s \in E$. Thus $q$ is an interior point of $E$.",
2.20,"If $p$ is a limit point of a set $E$, then every neighborhood of $p$ contains infinitely many points of $E$.","Suppose there is a neighborhood $N$ of $p$ which contains only a finite number of points of $E$. Let $q_{1}, \ldots, q_{n}$ be those points of $N \cap E$, which are distinct from $p$, and put  $$ r=\min _{1 \leq m \leq n} d\left(p, q_{m}\right) $$  [we use this notation to denote the smallest of the numbers $d\left(p, q_{1}\right), \ldots$, $d\left(p, q_{n}\right)$ ]. The minimum of a finite set of positive numbers is clearly positive, so that $r>0$.  The neighborhood $N_{r}(p)$ contains no point $q$ of $E$ such that $q \neq p$, so that $p$ is not a limit point of $E$. This contradiction establishes the theorem.",
2.22,"Let $\left\{E_{\alpha}\right\}$ be a (finite or infinite) collection of sets $E_{a}$. Then

$$
\left(\bigcup_{a} E_{a}\right)^{c}=\bigcap_{a}\left(E_{a}^{c}\right) .
$$
","Let $A$ and $B$ be the left and right members of (20). If $x \in A$, then $x \notin \bigcup_{a} E_{a}$, hence $x \notin E_{a}$ for any $\alpha$, hence $x \in E_{a}^{c}$ for every $\alpha$, so that $x \in \cap E_{a}^{c}$. Thus $A \subset B$.  Conversely, if $x \in B$, then $x \in E_{\alpha}^{c}$ for every $\alpha$, hence $x \notin E_{\alpha}$ for any $\alpha$, hence $x \notin \bigcup_{a} E_{a}$, so that $x \in\left(\bigcup_{\alpha} E_{\alpha}\right)^{c}$. Thus $B \subset A$.  It follows that $A=B$.",
2.23,A set $E$ is open if and only if its complement is closed.,"First, suppose $E^{c}$ is closed. Choose $x \in E$. Then $x \notin E^{c}$, and $x$ is not a limit point of $E^{c}$. Hence there exists a neighborhood $N$ of $x$ such that $E^{c} \cap N$ is empty, that is, $N \subset E$. Thus $x$ is an interior point of $E$, and $E$ is open.  Next, suppose $E$ is open. Let $x$ be a limit point of $E^{c}$. Then every neighborhood of $x$ contains a point of $E^{c}$, so that $x$ is not an interior point of $E$. Since $E$ is open, this means that $x \in E^{c}$. It follows that $E^{c}$ is closed.",
2.24 (a),"For any collection $\left\{G_{a}\right\}$ of open sets, $\cup_{a} G_{a}$ is open.","Put $G=\bigcup_{a} G_{a}$. If $x \in G$, then $x \in G_{a}$ for some $\alpha$. Since $x$ is an interior point of $G_{a}, x$ is also an interior point of $G$, and $G$ is open. This proves $(a)$.",
2.24 (b),"For any collection $\left\{F_{a}\right\}$ of closed sets, $\bigcap_{a} F_{a}$ is closed.","By Theorem 2.22,  $$ \left(\bigcap_{a} F_{a}\right)^{c}=\bigcup_{a}\left(F_{a}^{c}\right) $$  and $F_{\alpha}^{c}$ is open, by Theorem 2.23. Hence (a) implies that (21) is open so that $\bigcap_{a} F_{a}$ is closed.","Let $\left\{E_{\alpha}\right\}$ be a (finite or infinite) collection of sets $E_{a}$. Then

$$
\left(\bigcup_{a} E_{a}\right)^{c}=\bigcap_{a}\left(E_{a}^{c}\right) .
$$
; A set $E$ is open if and only if its complement is closed.;  ;  ; "
2.24 (c),"For any finite collection $G_{1}, \ldots, G_{n}$ of open sets, $\bigcap_{i=1}^{n} G_{i}$ is open.","Next, put $H=\bigcap_{i=1}^{n} G_{l}$. For any $x \in H$, there exist neighborhoods $N_{l}$ of $x$, with radii $r_{l}$, such that $N_{l} \subset G_{l}(i=1, \ldots, n)$. Put  $$ r=\min \left(r_{1}, \ldots, r_{n}\right) $$  and let $N$ be the neighborhood of $x$ of radius $r$. Then $N \subset G_{i}$ for $i=1$, $\ldots, n$, so that $N \subset H$, and $H$ is open.",
2.24 (d),"For any finite collection $F_{1}, \ldots, F_{n}$ of closed sets, $\bigcup_{i=1}^{n} F_{i}$ is closed.","By taking complements, $(d)$ follows from $(c)$ :  $$ \left(\bigcup_{l=1}^{n} F_{l}\right)^{c}=\bigcap_{i=1}^{n}\left(F_{i}^{c}\right) . $$",
2.28,Let $E$ be a nonempty set of real numbers which is bounded above. Let $y=\sup E$. Then $y \in E$. Hence $y \in E$ if $E$ is closed.,"If $y \in E$ then $y \in E. Assume $y \notin E$. For every $h>0$ there exists then a point $x \in E$ such that $y-h<x<y$, for otherwise $y-h$ would be an upper bound of $E$. Thus $y$ is a limit point of $E$. Hence $y \in E.",
2.30,Suppose $Y \subset X . A$ subset $E$ of $Y$ is open relative to $Y$ if and only if $E=Y \cap G$ for some open subset $G$ of $X$.,"Suppose $E$ is open relative to $Y$. To each $p \in E$ there is a positive number $r_{p}$ such that the conditions $d(p, q)<r_{p}, q \in Y$ imply that $q \in E$. Let $V_{p}$ be the set of all $q \in X$ such that $d(p, q)<r_{p}$, and define  $$ G=\bigcup_{p \in E} V_{p} . $$  Then $G$ is an open subset of $X$, by Theorems 2.19 and 2.24.  Since $p \in V_{p}$ for all $p \in E$, it is clear that $E \subset G \cap Y$.  By our choice of $V_{p}$, we have $V_{p} \cap Y \subset E$ for every $p \in E$, so that  $G \cap Y \subset E$. Thus $E=G \cap Y$, and one half of the theorem is proved.  Conversely, if $G$ is open in $X$ and $E=G \cap Y$, every $p \in E$ has a  neighborhood $V_{p} \subset G$. Then $V_{p} \cap Y \subset E$, so that $E$ is open relative to $Y$.",
2.33,Suppose $K \subset Y \subset X$. Then $K$ is compact relative to $X$ if and only if $K$ is compact relative to $Y$.,"Suppose $K$ is compact relative to $X$, and let $\left\{V_{\alpha}\right\}$ be a collection of sets, open relative to $Y$, such that $K \subset \bigcup_{\alpha} V_{\alpha}$. By theorem 2.30, there are sets $G_{\alpha}$, open relative to $X$, such that $V_{\alpha}=\mathrm{Y} \cap G_{\alpha}$, for all $\alpha$; and since $K$ is compact relative to $X$, we have  $$ K \subset G_{\alpha_{1}} \cup \cdots \cup G_{\alpha_{n}} $$  for some choice of finitely many indices $\alpha_{1}, \ldots, \alpha_{n}$. Since $K \subset Y$, (22) implies  $$ K \subset V_{\alpha_{1}} \cup \cdots \cup V_{\alpha_{n}} . $$  This proves that $K$ is compact relative to $Y.  Conversely, suppose $K$ is compact relative to $Y$, let $\left\{G_{\alpha}\right\}$ be a collection of open subsets of $X$ which covers $K$, and put $V_{\alpha}=Y \cap G_{\alpha}$. Then (23) will hold for some choice of $\alpha_{1}, \ldots, \alpha_{n}$; and since $V_{\alpha} \subset G_{\alpha}$, (23) implies (22).  This completes the proof.",
2.34,Compact subsets of metric spaces are closed.,"Let $K$ be a compact subset of a metric space $X$. We shall prove that the complement of $K$ is an open subset of $X$.  Suppose $p \in X, p \notin K$. If $q \in K$, let $V_{q}$ and $W_{q}$ be neighborhoods of $p$ and $q$, respectively, of radius less than $\frac{1}{2} d(p, q)$ [see Definition 2.18(a)]. Since $K$ is compact, there are finitely many points $q_{1}, \ldots, q_{n}$ in $K$ such that  $$ K \subset W_{q_{1}} \cup \cdots \cup W_{q_{n}}=W . $$  If $V=V_{q_{1}} \cap \cdots \cap V_{q_{n}}$, then $V$ is a neighborhood of $p$ which does not intersect $W$. Hence $V \subset K^{c}$, so that $p$ is an interior point of $K^{c}$. The theorem follows.",
2.35,Closed subsets of compact sets are compact.,"Suppose $F \subset K \subset X, F$ is closed (relative to $X$ ), and $K$ is compact. Let $\left\{V_{\alpha}\right\}$ be an open cover of $F$. If $F^{c}$ is adjoined to $\left\{V_{\alpha}\right\}$, we obtain an open cover $\Omega$ of $K$. Since $K$ is compact, there is a finite subcollection $\Phi$ of $\Omega$ which covers $K$, and hence $F$. If $F^{c}$ is a member of $\Phi$, we may remove it from $\Phi$ and still retain an open cover of $F$. We have thus shown that a finite subcollection of $\left\{V_{\alpha}\right\}$ covers $F$.",
2.36,"If $\left\{K_{\alpha}\right\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\left\{K_{\alpha}\right\}$ is nonempty, then $\cap K_{\alpha}$ is nonempty.","Fix a member $K_{1}$ of $\left\{K_{a}\right\}$ and put $G_{a}=K_{\alpha}^{c}$. Assume that no point of $K_{1}$ belongs to every $K_{a}$. Then the sets $G_{a}$ form an open cover of $K_{1}$; and since $K_{1}$ is compact, there are finitely many indices $\alpha_{1}, \ldots, \alpha_{n}$ such that $K_{1} \subset G_{a_{1}} \cup \cdots \cup G_{a_{n}}$. But this means that  $$ K_{1} \cap K_{a_{1}} \cap \cdots \cap K_{\alpha_{n}} $$  is empty, in contradiction to our hypothesis.",
2.37,"If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K","If no point of $K$ were a limit point of $E$, then each $q \in K$ would have a neighborhood $V_{q}$ which contains at most one point of $E$ (namely, $q$, if $q \in E$ ). It is clear that no finite subcollection of $\left\{V_{q}\right\}$ can cover $E$; and the same is true of $K$, since $E \subset K$. This contradicts the compactness of $K.",
2.38,"If $\left\{I_{n}\right\}$ is a sequence of intervals in $R^{1}$, such that $I_{n} \supset I_{n+1}$ $(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} I_{n}$ is not empty.","If $I_{n}=\left[a_{n}, b_{n}\right]$, let $E$ be the set of all $a_{n}$. Then $E$ is nonempty and bounded above (by $b_{1}$ ). Let $x$ be the sup of $E$. If $m$ and $n$ are positive integers, then  $$ a_{n} \leq a_{m+n} \leq b_{m+n} \leq b_{m}, $$  so that $x \leq b_{m}$ for each $m$. Since it is obvious that $a_{m} \leq x$, we see that $x \in I_{m}$ for $m=1,2,3, \ldots$",
2.39,"Let $k$ be a positive integer. If $\left\{I_{n}\right\}$ is a sequence of $k$-cells such that $I_{n} \supset I_{n+1}(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} I_{n}$ is not empty.","Let $I_{n}$ consist of all points $\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right)$ such that  $$ a_{n, j} \leq x_{j} \leq b_{n, j} \quad(1 \leq j \leq k ; n=1,2,3, \ldots) $$  and put $I_{n, j}=\left[a_{n, j}, b_{n, j}\right]$. For each $j$, the sequence $\left\{I_{n, j}\right\}$ satisfies the hypotheses of Theorem 2.38. Hence there are real numbers $x_{j}^{*}(1 \leq j \leq k)$ such that  $$ a_{n, j} \leq x_{j}^{*} \leq b_{n, j} \quad(1 \leq j \leq k ; n=1,2,3, \ldots) $$  Setting $\mathbf{x}^{*}=\left(x_{1}^{*}, \ldots, x_{k}^{*}\right)$, we see that $\mathbf{x}^{*} \in I_{n}$ for $n=1,2,3, \ldots$ The theorem follows.","If $\left\{I_{n}\right\}$ is a sequence of intervals in $R^{1}$, such that $I_{n} \supset I_{n+1}$ $(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} I_{n}$ is not empty.;  ;  ; "
2.40,Every $k$-cell is compact.,"Let $I$ be a $k$-cell, consisting of all points $\mathbf{x}=\left(x_{1}, \ldots, x_{k}\right)$ such that $a_{j} \leq x_{j} \leq b_{j}(1 \leq j \leq k)$. Put  $$ \delta=\left\{\sum_{i}^{k}\left(b_{j}-a_{j}\right)^{2}\right\}^{1 / 2} $$  Then $|\mathbf{x}-\mathbf{y}| \leq \delta$, if $\mathbf{x} \in I, \mathbf{y} \in I$.  Suppose, to get a contradiction, that there exists an open cover $\left\{G_{\alpha}\right\}$ of $I$ which contains no finite subcover of $I$. Put $c_{j}=\left(a_{j}+b_{j}\right) / 2$. The intervals $\left[a_{j}, c_{j}\right]$ and $\left[c_{j}, b_{j}\right]$ then determine $2^{k} k$-cells $Q_{i}$ whose union is $I$. At least one of these sets $Q_{i}$, call it $I_{1}$, cannot be covered by any finite subcollection of $\left\{G_{\alpha}\right\}$ (otherwise $I$ could be so covered). We next subdivide $I_{1}$ and continue the process. We obtain a sequence $\left\{I_{n}\right\}$ with the following properties:  (a) $I \supset I_{1} \supset I_{2} \supset I_{3} \supset \cdots$;  (b) $I_{n}$ is not covered by any finite subcollection of $\left\{G_{\alpha}\right\}$;  (c) if $\mathbf{x} \in I_{n}$ and $\mathbf{y} \in I_{n}$, then $|\mathbf{x}-\mathbf{y}| \leq 2^{-n} \delta$.  By $(a)$ and Theorem 2.39, there is a point $\mathbf{x}^{*}$ which lies in every $I_{n}$. For some $\alpha, \mathbf{x}^{*} \in G_{\alpha}$. Since $G_{\alpha}$ is open, there exists $r>0$ such that $\left|\mathbf{y}-\mathbf{x}^{*}\right|<r$ implies that $\mathbf{y} \in G_{\alpha}$. If $n$ is so large that $2^{-n} \delta<r$ (there is such an $n$, for otherwise $2^{n} \leq \delta / r$ for all positive integers $n$, which is absurd since $R$ is archimedean), then (c) implies that $I_{n} \subset G_{\alpha}$, which contradicts $(b)$.","Let $k$ be a positive integer. If $\left\{I_{n}\right\}$ is a sequence of $k$-cells such that $I_{n} \supset I_{n+1}(n=1,2,3, \ldots)$, then $\bigcap_{1}^{\infty} I_{n}$ is not empty.;  ;  ; "
2.41,"If a set $E$ in $R^{k}$ has one of the following three properties, then it has the other two:

(a) $E$ is closed and bounded.

(b) $E$ is compact.

(c) Every infinite subset of $E$ has a limit point in $E$.","If (a) holds, then $E \subset I$ for some $k$-cell $I$, and $(b)$ follows from Theorems 2.40 and 2.35. Theorem 2.37 shows that $(b)$ implies $(c)$. It remains to be shown that $(c)$ implies $(a)$.  If $E$ is not bounded, then $E$ contains points $\mathbf{x}_{n}$ with  $$ \left|\mathbf{x}_{n}\right|>n \quad(n=1,2,3, \ldots) $$  The set $S$ consisting of these points $\mathbf{x}_{n}$ is infinite and clearly has no limit point in $R^{k}$, hence has none in $E$. Thus $(c)$ implies that $E$ is bounded.  If $E$ is not closed, then there is a point $\mathbf{x}_{0} \in R^{k}$ which is a limit point of $E$ but not a point of $E$. For $n=1,2,3, \ldots$, there are points $\mathbf{x}_{n} \in E$ such that $\left|\mathbf{x}_{n}-\mathbf{x}_{0}\right|<1 / n$. Let $S$ be the set of these points $\mathbf{x}_{n}$. Then $S$ is infinite (otherwise $\left|\mathbf{x}_{n}-\mathbf{x}_{0}\right|$ would have a constant positive value, for infinitely many $n$ ), $S$ has $\mathbf{x}_{0}$ as a limit point, and $S$ has no other limit point in $R^{k}$. For if $\mathbf{y} \in R^{k}, \mathbf{y} \neq \mathbf{x}_{0}$, then  $$ \begin{aligned} \left|\mathbf{x}_{n}-\mathbf{y}\right| & \geq\left|\mathbf{x}_{0}-\mathbf{y}\right|-\left|\mathbf{x}_{n}-\mathbf{x}_{0}\right| \\ & \geq\left|\mathbf{x}_{0}-\mathbf{y}\right|-\frac{1}{n} \geq \frac{1}{2}\left|\mathbf{x}_{0}-\mathbf{y}\right| \end{aligned} $$  for all but finitely many $n$; this shows that $\mathbf{y}$ is not a limit point of $S$ (Theorem 2.20).  Thus $S$ has no limit point in $E$; hence $E$ must be closed if $(c)$ holds.  We should remark, at this point, that $(b)$ and $(c)$ are equivalent in any metric space (Exercise 26) but that $(a)$ does not, in general, imply $(b)$ and $(c)$. Examples are furnished by Exercise 16 and by the space $\mathscr{L}^{2}$, which is discussed in Chap. 11.","If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K; If $p$ is a limit point of a set $E$, then every neighborhood of $p$ contains infinitely many points of $E$.;  ;  ; "
2.42,(Weierstrass) Every bounded infinite subset of $R^{k}$ has a limit point in $R^{k$.},"Being bounded, the set $E$ in question is a subset of a $k$-cell $I \subset R^{k}$. By Theorem 2.40, $I$ is compact, and so $E$ has a limit point in $I$, by Theorem 2.37.","Every $k$-cell is compact.; If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K;  ;  ; "
2.43,Let $P$ be a nonempty perfect set in $R^{k}$. Then $P$ is uncountable.,"Since $P$ has limit points, $P$ must be infinite. Suppose $P$ is countable, and denote the points of $P$ by $\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3}, \ldots$ We shall construct a sequence $\left\{V_{n}\right\}$ of neighborhoods, as follows.  Let $V_{1}$ be any neighborhood of $\mathbf{x}_{1}$. If $V_{1}$ consists of all $\mathbf{y} \in R^{k}$ such that $\left|\mathbf{y}-\mathbf{x}_{1}\right|<r$, the closure $\bar{V}_{1}$ of $V_{1}$ is the set of all $\mathbf{y} \in R^{k}$ such that $\left|\mathbf{y}-\mathbf{x}_{1}\right| \leq \boldsymbol{r}$.  Suppose $V_{n}$ has been constructed, so that $V_{n} \cap P$ is not empty. Since every point of $P$ is a limit point of $P$, there is a neighborhood $V_{n+1}$ such that (i) $\bar{V}_{n+1} \subset V_{n}$, (ii) $\mathbf{x}_{n} \notin \bar{V}_{n+1}$, (iii) $V_{n+1} \cap P$ is not empty. By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed.  Put $K_{n}=\bar{V}_{n} \cap P$. Since $\bar{V}_{n}$ is closed and bounded, $\bar{V}_{n}$ is compact. Since $\mathbf{x}_{n} \notin K_{n+1}$, no point of $P$ lies in $\bigcap_{1}^{\infty} K_{n}$. Since $K_{n} \subset P$, this implies that $\bigcap_{1}^{\infty} K_{n}$ is empty. But each $K_{n}$ is nonempty, by (iii), and $K_{n} \supset K_{n+1}$, by (i); this contradicts the Corollary to Theorem 2.36.","If $\left\{K_{\alpha}\right\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\left\{K_{\alpha}\right\}$ is nonempty, then $\cap K_{\alpha}$ is nonempty.;  ;  ; "
2.47,"A subset $E$ of the real line $R^{1}$ is connected if and only if it has the following property: If $x \in E, y \in E$, and $x<z<y$, then $z \in E$.","If there exist $x \in E, y \in E$, and some $z \in(x, y)$ such that $z \notin E$, then $E=A_{z} \cup B_{z}$ where  $$ A_{z}=E \cap(-\infty, z), \quad B_{z}=E \cap(z, \infty) . $$  Since $x \in A_{z}$ and $y \in B_{z}, A$ and $B$ are nonempty. Since $A_{z} \subset(-\infty, z)$ and $B_{z} \subset(z, \infty)$, they are separated. Hence $E$ is not connected.  To prove the converse, suppose $E$ is not connected. Then there are nonempty separated sets $A$ and $B$ such that $A \cup B=E$. Pick $x \in A, y \in B$, and assume (without loss of generality) that $x<y$. Define  $$ z=\sup (A \cap[x, y]) . $$  By Theorem 2.28, $z \in \bar{A}$; hence $z \notin B$. In particular, $x \leq z<y$.  If $z \notin A$, it follows that $x<z<y$ and $z \notin E$.  If $z \in A$, then $z \notin \bar{B}$, hence there exists $z_{1}$ such that $z<z_{1}<y$ and $z_{1} \notin B$. Then $x<z_{1}<y$ and $z_{1} \notin E$.",Let $E$ be a nonempty set of real numbers which is bounded above. Let $y=\sup E$. Then $y \in E$. Hence $y \in E$ if $E$ is closed.;  ;  ; 
2.8,Every infinite subset of a countable set $A$ is countable.,"Suppose $E \subset A$, and $E$ is infinite. Arrange the elements $x$ of $A$ in a sequence $\left\{x_{n}\right\}$ of distinct elements. Construct a sequence $\left\{n_{k}\right\}$ as follows:  Let $n_{1}$ be the smallest positive integer such that $x_{n_{1}} \in E$. Having chosen $n_{1}, \ldots, n_{k-1}(k=2,3,4, \ldots)$, let $n_{k}$ be the smallest integer greater than $n_{k-1}$ such that $x_{n_{k}} \in E$.  Putting $f(k)=x_{n_{k}}(k=1,2,3, \ldots)$, we obtain a 1-1 correspondence between $E$ and $J$.",
3.10 (a),"If $\bar{E}$ is the closure of a set $E$ in a metric space $X$, then $\operatorname{diam} \bar{E}=\operatorname{diam} E$.","Since $E \subset \bar{E}$, it is clear that  $$ \operatorname{diam} E \leq \operatorname{diam} \bar{E} . $$  Fix $\varepsilon>0$, and choose $p \in \bar{E}, q \in \bar{E}$. By the definition of $\bar{E}$, there are points $p^{\prime}, q^{\prime}$, in $E$ such that $d\left(p, p^{\prime}\right)<\varepsilon, d\left(q, q^{\prime}\right)<\varepsilon$. Hence  $$ \begin{aligned} d(p, q) & \leq d\left(p, p^{\prime}\right)+d\left(p^{\prime} q^{\prime}\right)+d\left(q^{\prime}, q\right) \\ & <2 \varepsilon+d\left(p^{\prime}, q^{\prime}\right) \leq 2 \varepsilon+\operatorname{diam} E \end{aligned} $$  It follows that  $$ \operatorname{diam} \bar{E} \leq 2 \varepsilon+\operatorname{diam} E $$  and since $\varepsilon$ was arbitrary, $(a)$ is proved.",
3.10 (b),"If $K_{n}$ is a sequence of compact sets in $X$ such that $K_{n} \supset K_{n+1}$ $(n=1,2,3, \ldots)$ and if

$$
\lim _{n \rightarrow \infty} \operatorname{diam} K_{n}=0 \text {, }
$$

then $\bigcap_{1}^{\infty} K_{n}$ consists of exactly one point.","Put $K=\bigcap_{1}^{\infty} K_{n}$. By Theorem 2.36, $K$ is not empty. If $K$ contains more than one point, then diam $K>0$. But for each $n, K_{n} \supset K$, so that $\operatorname{diam} K_{n} \geq \operatorname{diam} K$. This contradicts the assumption that $\operatorname{diam} K_{n} \rightarrow 0$.","If $\left\{K_{\alpha}\right\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\left\{K_{\alpha}\right\}$ is nonempty, then $\cap K_{\alpha}$ is nonempty.;  ;  ; "
3.11 (a),"In any metric space $X$, every convergent sequence is a Cauchy sequence.","If $p_{n} \rightarrow p$ and if $\varepsilon>0$, there is an integer $N$ such that $d\left(p, p_{n}\right)<\varepsilon$ for all $n \geq N$. Hence  $$ d\left(p_{n}, p_{m}\right) \leq d\left(p_{n}, p\right)+d\left(p, p_{m}\right)<2 \varepsilon $$  as soon as $n \geq N$ and $m \geq N$. Thus $\left\{p_{n}\right\}$ is a Cauchy sequence.",
3.11 (b),"If $X$ is a compact metric space and if $\left\{p_{n}\right\}$ is a Cauchy sequence in $X$, then $\left\{p_{n}\right\}$ converges to some point of $X$.","Let $\left\{p_{n}\right\}$ be a Cauchy sequence in the compact space $X$. For $N=1,2,3, \ldots$, let $E_{N}$ be the set consisting of $p_{N}, p_{N+1}, p_{N+2}, \ldots$ Then  $$ \lim _{N \rightarrow \infty} \operatorname{diam} \bar{E}_{N}=0 $$  by Definition 3.9 and Theorem 3.10(a). Being a closed subset of the compact space $X$, each $\bar{E}_{N}$ is compact (Theorem 2.35). Also $E_{N} \supset E_{N+1}$, so that $\bar{E}_{N} \supset \bar{E}_{N+1}$.  Theorem $3.10(b)$ shows now that there is a unique $p \in X$ which lies in every $\bar{E}_{N}$.  Let $\varepsilon>0$ be given. By (3) there is an integer $N_{0}$ such that $\operatorname{diam} \bar{E}_{N}<\varepsilon$ if $N \geq N_{0}$. Since $p \in \bar{E}_{N}$, it follows that $d(p, q)<\varepsilon$ for every $q \in \bar{E}_{N}$, hence for every $q \in E_{N}$. In other words, $d\left(p, p_{n}\right)<\varepsilon$ if $n \geq N_{0}$. This says precisely that $p_{n} \rightarrow p$.","If $\bar{E}$ is the closure of a set $E$ in a metric space $X$, then $\operatorname{diam} \bar{E}=\operatorname{diam} E$.;If $K_{n}$ is a sequence of compact sets in $X$ such that $K_{n} \supset K_{n+1}$ $(n=1,2,3, \ldots)$ and if

$$
\lim _{n \rightarrow \infty} \operatorname{diam} K_{n}=0 \text {, }
$$

then $\bigcap_{1}^{\infty} K_{n}$ consists of exactly one point.; Closed subsets of compact sets are compact.;  ; Let $E$ be a nonempty subset of a metric space $X$, and let $S$ be the set of all real numbers of the form $d(p, q)$, with $p \in E$ and $q \in E$. The sup of $S$ is called the diameter of $E$.

If $\left\{p_{n}\right\}$ is a sequence in $X$ and if $E_{N}$ consists of the points $p_{N}, p_{N+1}, p_{N+2}, \ldots$, it is clear from the two preceding definitions that $\left\{p_{n}\right\}$ is a Cauchy sequence if and only if

$$
\lim _{N \rightarrow \infty} \operatorname{diam} E_{N}=0 \text {. }
$$;  ; "
3.11 (c),"In $R^{k}$, every Cauchy sequence converges.","Let $\left\{\mathbf{x}_{n}\right\}$ be a Cauchy sequence in $R^{k}$. Define $E_{N}$ as in $(b)$, with $\mathbf{x}_{i}$ in place of $p_{i}$. For some $N$, diam $E_{N}<1$. The range of $\left\{\mathbf{x}_{n}\right\}$ is the union of $E_{N}$ and the finite set $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N-1}\right\}$. Hence $\left\{\mathbf{x}_{n}\right\}$ is bounded. Since every bounded subset of $R^{k}$ has compact closure in $R^{k}$ (Theorem 2.41), $(c)$ follows from $(b)$.","If a set $E$ in $R^{k}$ has one of the following three properties, then it has the other two:

(a) $E$ is closed and bounded.

(b) $E$ is compact.

(c) Every infinite subset of $E$ has a limit point in $E$.;  ;  ; "
3.14,Suppose $\left\{s_{n}\right\}$ is monotonic. Then $\left\{s_{n}\right\}$ converges if and only if it is bounded.,"Suppose $s_{n} \leq s_{n+1}$ (the proof is analogous in the other case). Let $E$ be the range of $\left\{s_{n}\right\}$. If $\left\{s_{n}\right\}$ is bounded, let $s$ be the least upper bound of $E$. Then  $$ s_{n} \leq s \quad(n=1,2,3, \ldots) $$  For every $\varepsilon>0$, there is an integer $N$ such that  $$ s-\varepsilon<s_{N} \leq s $$  for otherwise $s-\varepsilon$ would be an upper bound of $E$. Since $\left\{s_{n}\right\}$ increases, $n \geq N$ therefore implies  $$ s-\varepsilon<s_{n} \leq s, $$  which shows that $\left\{s_{n}\right\}$ converges (to $s$ ).  The converse follows from Theorem 3.2(c).","For a sequence $\left\{p_{n}\right\}$ in a metric space $X$: $\left\{p_{n}\right\}$ converges to $p \in X$ if and only if every neighborhood of $p$ contains $p_{n}$ for all but finitely many $n$.;For a sequence $\left\{p_{n}\right\}$ in a metric space $X$, and points $p, p^{\prime} \in X$: If $\left\{p_{n}\right\}$ converges to $p$ and to $p^{\prime}$, then $p^{\prime}=p$.;For a sequence $\left\{p_{n}\right\}$ in a metric space $X$: If $\left\{p_{n}\right\}$ converges, then $\left\{p_{n}\right\}$ is bounded.;For $E \subset X$ and $p$ a limit point of $E$ in a metric space $X$: There is a sequence $\left\{p_{n}\right\}$ in $E$ such that $p=\lim _{n \rightarrow \infty} p_{n}$.;If $p>0$, then $\lim _{n \rightarrow \infty} \frac{1}{n^{p}}=0$.;If $p>0$, then $\lim _{n \rightarrow \infty} \sqrt[n]{p}=1$.;$\lim _{n \rightarrow \infty} \sqrt[n]{n}=1$.;If $p>0$ and $\alpha$ is real, then $\lim _{n \rightarrow \infty} \frac{n^{\alpha}}{(1+p)^{n}}=0$.;If $|x|<1$, then $\lim _{n \rightarrow \infty} x^{n}=0$.;If $|a_{n}| \leq c_{n}$ for $n \geq N_{0}$, where $N_{0}$ is some fixed integer, and if $\Sigma c_{n}$ converges, then $\Sigma a_{n}$ converges.;If $a_{n} \geq d_{n} \geq 0$ for $n \geq N_{0}$, and if $\Sigma d_{n}$ diverges, then $\Sigma a_{n}$ diverges.;If $0 \leq x<1$, then

$$
\sum_{n=0}^{\infty} x^{n}=\frac{1}{1-x}
$$

If $x \geq 1$, the series diverges.;Suppose $a_{1} \geq a_{2} \geq a_{3} \geq \cdots \geq 0$. Then the series $\sum_{n=1}^{\infty} a_{n}$ converges if and only if the series

$$
\sum_{k=0}^{\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\cdots
$$

converges.;$\sum \frac{1}{n^{p}}$ converges if $p>1$ and diverges if $p \leq 1$.;If $p>1$,

$$
\sum_{n=2}^{\infty} \frac{1}{n(\log n)^{p}}
$$

converges; if $p \leq 1$, the series diverges.;  ;  ; "
3.17,"Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ and $s^{*}$ have the same meaning as in Definition 3.16. Then $s^{*}$ has the following two properties:

(a) $s^{*} \in E$.

(b) If $x>s^{*}$, there is an integer $N$ such that $n \geq N$ implies $s_{n}<x$.

Moreover, $s^{*}$ is the only number with the properties $(a)$ and $(b)$.

Of course, an analogous result is true for $s_{*}$.","If $s^{*}=+\infty$, then $E$ is not bounded above; hence $\left\{s_{n}\right\}$ is not bounded above, and there is a subsequence $\left\{s_{n_{k}}\right\}$ such that $s_{n_{k}} \rightarrow+\infty$.  If $s^{*}$ is real, then $E$ is bounded above, and at least one subsequential limit exists, so that (a) follows from Theorems 3.7 and 2.28.  If $s^{*}=-\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Hence, for any real $M, s_{n}>M$ for at most a finite number of values of $n$, so that $s_{n} \rightarrow-\infty$.  This establishes $(a)$ in all cases.  (b) Suppose there is a number $x>s^{*}$ such that $s_{n} \geq x$ for infinitely many values of $n$. In that case, there is a number $y \in E$ such that $y \geq x>s^{*}$, contradicting the definition of $s^{*}$.  Thus $s^{*}$ satisfies $(a)$ and $(b)$.  To show the uniqueness, suppose there are two numbers, $p$ and $q$, which satisfy $(a)$ and $(b)$, and suppose $p<q$. Choose $x$ such that $p<x<q$. Since $p$ satisfies $(b)$, we have $s_{n}<x$ for $n \geq N$. But then $q$ cannot satisfy $(a)$.",
3.2 (a),For a sequence $\left\{p_{n}\right\}$ in a metric space $X$: $\left\{p_{n}\right\}$ converges to $p \in X$ if and only if every neighborhood of $p$ contains $p_{n}$ for all but finitely many $n$.,"Suppose $p_{n} \rightarrow p$ and let $V$ be a neighborhood of $p$. For some $\varepsilon>0$, the conditions $d(q, p)<\varepsilon, q \in X$ imply $q \in V$. Corresponding to this $\varepsilon$, there exists $N$ such that $n \geq N$ implies $d\left(p_{n}, p\right)<\varepsilon$. Thus $n \geq N$ implies $p_{n} \in V$.  Conversely, suppose every neighborhood of $p$ contains all but finitely many of the $p_{n}$. Fix $\varepsilon>0$, and let $V$ be the set of all $q \in X$ such that $d(p, q)<\varepsilon$. By assumption, there exists $N$ (corresponding to this $V$ ) such that $p_{n} \in V$ if $n \geq N$. Thus $d\left(p_{n}, p\right)<\varepsilon$ if $n \geq N$; hence $p_{n} \rightarrow p$.",
3.2 (b),"For a sequence $\left\{p_{n}\right\}$ in a metric space $X$, and points $p, p^{\prime} \in X$: If $\left\{p_{n}\right\}$ converges to $p$ and to $p^{\prime}$, then $p^{\prime}=p$.","Let $\varepsilon>0$ be given. There exist integers $N, N^{\prime}$ such that  $$ \begin{aligned} & n \geq N \quad \text { implies } \quad d\left(p_{n}, p\right)<\frac{\varepsilon}{2}, \\ & n \geq N^{\prime} \text { implies } d\left(p_{n}, p^{\prime}\right)<\frac{\varepsilon}{2} . \end{aligned} $$  Hence if $n \geq \max \left(N, N^{\prime}\right)$, we have  $$ d\left(p, p^{\prime}\right) \leq d\left(p, p_{n}\right)+d\left(p_{n}, p^{\prime}\right)<\varepsilon . $$  Since $\varepsilon$ was arbitrary, we conclude that $d\left(p, p^{\prime}\right)=0$.",
3.2 (c),"For a sequence $\left\{p_{n}\right\}$ in a metric space $X$: If $\left\{p_{n}\right\}$ converges, then $\left\{p_{n}\right\}$ is bounded.","Suppose $p_{n} \rightarrow p$. There is an integer $N$ such that $n>N$ implies $d\left(p_{n}, p\right)<1$. Put  $$ r=\max \left\{1, d\left(p_{1}, p\right), \ldots, d\left(p_{N}, p\right)\right\} $$  Then $d\left(p_{n}, p\right) \leq r$ for $n=1,2,3, \ldots$.",
3.2 (d),For $E \subset X$ and $p$ a limit point of $E$ in a metric space $X$: There is a sequence $\left\{p_{n}\right\}$ in $E$ such that $p=\lim _{n \rightarrow \infty} p_{n}$.,"For each positive integer $n$, there is a point $p_{n} \in E$ such that $d\left(p_{n}, p\right)<1 / n$. Given $\varepsilon>0$, choose $N$ so that $N \varepsilon>1$. If $n>N$, it follows that $d\left(p_{n}, p\right)<\varepsilon$. Hence $p_{n} \rightarrow p$.",
3.20 (a),"If $p>0$, then $\lim _{n \rightarrow \infty} \frac{1}{n^{p}}=0$.",Take $n>(1 / \varepsilon)^{1 / p}$. (Note that the archimedean property of the real number system is used here.),
3.20 (b),"If $p>0$, then $\lim _{n \rightarrow \infty} \sqrt[n]{p}=1$.","If $p>1$, put $x_{n}=\sqrt[n]{p}-1$. Then $x_{n}>0$, and, by the binomial theorem,  $$ 1+n x_{n} \leq\left(1+x_{n}\right)^{n}=p $$  so that  $$ 0<x_{n} \leq \frac{p-1}{n} \text {. } $$  Hence $x_{n} \rightarrow 0$. If $p=1,(b)$ is trivial, and if $0<p<1$, the result is obtained by taking reciprocals.",
3.20 (c),$\lim _{n \rightarrow \infty} \sqrt[n]{n}=1$.,"Put $x_{n}=\sqrt[n]{n}-1$. Then $x_{n} \geq 0$, and, by the binomial theorem,  $$ n=\left(1+x_{n}\right)^{n} \geq \frac{n(n-1)}{2} x_{n}^{2} \text {. } $$  Hence  $$ 0 \leq x_{n} \leq \sqrt{\frac{2}{n-1}} \quad(n \geq 2) $$",
3.20 (d),"If $p>0$ and $\alpha$ is real, then $\lim _{n \rightarrow \infty} \frac{n^{\alpha}}{(1+p)^{n}}=0$.","Let $k$ be an integer such that $k>\alpha, k>0$. For $n>2 k$,  $$ (1+p)^{n}>\left(\begin{array}{l} n \\ k \end{array}\right) p^{k}=\frac{n(n-1) \cdots(n-k+1)}{k !} p^{k}>\frac{n^{k} p^{k}}{2^{k} k !} $$  Hence  $$ 0<\frac{n^{\alpha}}{(1+p)^{n}}<\frac{2^{k} k !}{p^{k}} n^{\alpha-k} \quad(n>2 k) . $$  Since $\alpha-k<0, n^{\alpha-k} \rightarrow 0$, by $(a)$.",
3.20 (e),"If $|x|<1$, then $\lim _{n \rightarrow \infty} x^{n}=0$.",Take $\alpha=0$ in $(d)$.,
3.25 (a),"If $|a_{n}| \leq c_{n}$ for $n \geq N_{0}$, where $N_{0}$ is some fixed integer, and if $\Sigma c_{n}$ converges, then $\Sigma a_{n}$ converges.","Given $\varepsilon>0$, there exists $N \geq N_{0}$ such that $m \geq n \geq N$ implies  $$ \sum_{k=n}^{m} c_{k} \leq \varepsilon $$  by the Cauchy criterion. Hence  $$ \left|\sum_{k=n}^{m} a_{k}\right| \leq \sum_{k=n}^{m}\left|a_{k}\right| \leq \sum_{k=n}^{m} c_{k} \leq \varepsilon $$  and $(a)$ follows.",
3.25 (b),"If $a_{n} \geq d_{n} \geq 0$ for $n \geq N_{0}$, and if $\Sigma d_{n}$ diverges, then $\Sigma a_{n}$ diverges.","$(b)$ follows from $(a)$, for if $\Sigma a_{n}$ converges, so must $\Sigma d_{n}$ [note that $(b)$ also follows from Theorem 3.24].",;  ;  ; 
3.26,"If $0 \leq x<1$, then

$$
\sum_{n=0}^{\infty} x^{n}=\frac{1}{1-x}
$$

If $x \geq 1$, the series diverges.","If $x \neq 1$,  $$ s_{n}=\sum_{k=0}^{n} x^{k}=\frac{1-x^{n+1}}{1-x} $$ The result follows if we let $n \rightarrow \infty$. For $x=1$, we get  $$ 1+1+1+\cdots, $$ which evidently diverges.",
3.27,"Suppose $a_{1} \geq a_{2} \geq a_{3} \geq \cdots \geq 0$. Then the series $\sum_{n=1}^{\infty} a_{n}$ converges if and only if the series

$$
\sum_{k=0}^{\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\cdots
$$

converges.","By Theorem 3.24, it suffices to consider boundedness of the partial sums. Let  $$ \begin{aligned} s_{n} & =a_{1}+a_{2}+\cdots+a_{n}, \\ t_{k} & =a_{1}+2 a_{2}+\cdots+2^{k} a_{2^{k}} . \end{aligned} $$  For $n<2^{k}$,  $$ \begin{aligned} s_{n} & \leq a_{1}+\left(a_{2}+a_{3}\right)+\cdots+\left(a_{2^{k}}+\cdots+a_{2^{k+1}-1}\right) \\ & \leq a_{1}+2 a_{2}+\cdots+2^{k} a_{2^{k}} \\ & =t_{k} \end{aligned} $$  so that  On the other hand, if $n>2^{k}$,  $$ s_{n} \leq t_{k} . $$  $$ \begin{aligned} s_{n} & \geq a_{1}+a_{2}+\left(a_{3}+a_{4}\right)+\cdots+\left(a_{2^{k-1}+1}+\cdots+a_{2^{k}}\right) \\ & \geq \frac{1}{2} a_{1}+a_{2}+2 a_{4}+\cdots+2^{k-1} a_{2^{k}} \\ & =\frac{1}{2} t_{k}, \end{aligned} $$  so that  $$ 2 s_{n} \geq t_{k} \text {. } $$  By (8) and (9), the sequences $\left\{s_{n}\right\}$ and $\left\{t_{k}\right\}$ are either both bounded or both unbounded. This completes the proof.",;  ;  ; 
3.28,$\sum \frac{1}{n^{p}}$ converges if $p>1$ and diverges if $p \leq 1$.,"If $p \leq 0$, divergence follows from Theorem 3.23. If $p>0$, Theorem 3.27 is applicable, and we are led to the series  $$ \sum_{k=0}^{\infty} 2^{k} \cdot \frac{1}{2^{k p}}=\sum_{k=0}^{\infty} 2^{(1-p) k} $$  Now, $2^{1-p}<1$ if and only if $1-p<0$, and the result follows by comparison with the geometric series (take $x=2^{1-p}$ in Theorem 3.26).","; Suppose $a_{1} \geq a_{2} \geq a_{3} \geq \cdots \geq 0$. Then the series $\sum_{n=1}^{\infty} a_{n}$ converges if and only if the series

$$
\sum_{k=0}^{\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\cdots
$$

converges.; If $0 \leq x<1$, then

$$
\sum_{n=0}^{\infty} x^{n}=\frac{1}{1-x}
$$

If $x \geq 1$, the series diverges.;  ;  ; "
3.29,"If $p>1$,

$$
\sum_{n=2}^{\infty} \frac{1}{n(\log n)^{p}}
$$

converges; if $p \leq 1$, the series diverges.","The monotonicity of the logarithmic function (which will be discussed in more detail in Chap. 8) implies that $\{\log n\}$ increases. Hence $\{1 / n \log n\}$ decreases, and we can apply Theorem 3.27 to (10); this leads us to the series  $$ \sum_{k=1}^{\infty} 2^{k} \cdot \frac{1}{2^{k}(\log 2^{k})^{p}}=\sum_{k=1}^{\infty} \frac{1}{(k \log 2)^{p}}=\frac{1}{(\log 2)^{p}} \sum_{k=1}^{\infty} \frac{1}{k^{p}} $$  and Theorem 3.29 follows from Theorem 3.28.  This procedure may evidently be continued. For instance,  $$ \sum_{n=3}^{\infty} \frac{1}{n \log n \log \log n} $$  diverges, whereas  $$ \sum_{n=3}^{\infty} \frac{1}{n \log n(\log \log n)^{2}} $$  converges.","Suppose $a_{1} \geq a_{2} \geq a_{3} \geq \cdots \geq 0$. Then the series $\sum_{n=1}^{\infty} a_{n}$ converges if and only if the series

$$
\sum_{k=0}^{\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\cdots
$$

converges.; If $p>1$,

$$
\sum_{n=2}^{\infty} \frac{1}{n(\log n)^{p}}
$$

converges; if $p \leq 1$, the series diverges.; $\sum \frac{1}{n^{p}}$ converges if $p>1$ and diverges if $p \leq 1$.;  ;  ; "
3.3 (a),"Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$","Given $\varepsilon>0$, there exist integers $N_{1}, N_{2}$ such that  $$ \begin{array}{lll} n \geq N_{1} \text { implies } & \left|s_{n}-s\right|<\frac{\varepsilon}{2}, \\ n \geq N_{2} \text { implies } & \left|t_{n}-t\right|<\frac{\varepsilon}{2} . \end{array} $$  If $N=\max \left(N_{1}, N_{2}\right)$, then $n \geq N$ implies  $$ \left|\left(s_{n}+t_{n}\right)-(s+t)\right| \leq\left|s_{n}-s\right|+\left|t_{n}-t\right|<\varepsilon . $$",
3.3 (b),"Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;",The proof of $(b)$ is trivial.,
3.3 (c),"Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$","We use the identity  $$ s_{n} t_{n}-s t=\left(s_{n}-s\right)\left(t_{n}-t\right)+s\left(t_{n}-t\right)+t\left(s_{n}-s\right) $$  Given $\varepsilon>0$, there are integers $N_{1}, N_{2}$ such that  $$ \begin{array}{lll} n \geq N_{1} \quad \text { implies } & \left|s_{n}-s\right|<\sqrt{\varepsilon} \\ n \geq N_{2} & \text { implies } & \left|t_{n}-t\right|<\sqrt{\varepsilon} \end{array} $$  If we take $N=\max \left(N_{1}, N_{2}\right), n \geq N$ implies  $$ \left|\left(s_{n}-s\right)\left(t_{n}-t\right)\right|<\varepsilon, $$  so that  $$ \lim _{n \rightarrow \infty}\left(s_{n}-s\right)\left(t_{n}-t\right)=0 $$  We now apply $(a)$ and $(b)$ to (1), and conclude that  $$ \lim _{n \rightarrow \infty}\left(s_{n} t_{n}-s t\right)=0 $$",
3.3 (d),"Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.","Choosing $m$ such that $\left|s_{n}-s\right|<\frac{1}{2}|s|$ if $n \geq m$, we see that  $$ \left|s_{n}\right|>\frac{1}{2}|s| \quad(n \geq m) \text {. } $$  Given $\varepsilon>0$, there is an integer $N>m$ such that $n \geq N$ implies  $$ \left|s_{n}-s\right|<\frac{1}{2}|s|^{2} \varepsilon \text {. } $$  Hence, for $n \geq N$,  $$ \left|\frac{1}{s_{n}}-\frac{1}{s}\right|=\left|\frac{s_{n}-s}{s_{n} s}\right|<\frac{2}{|s|^{2}}\left|s_{n}-s\right|<\varepsilon $$",
3.31,$\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$,"Let  $$ s_{n}=\sum_{k=0}^{n} \frac{1}{k !}, \quad t_{n}=\left(1+\frac{1}{n}\right)^{n} $$  By the binomial theorem,  $$ \begin{aligned} t_{n}=1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\frac{1}{3 !}\left(1-\frac{1}{n}\right) & \left(1-\frac{2}{n}\right)+\cdots \\ & +\frac{1}{n !}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdots\left(1-\frac{n-1}{n}\right) . \end{aligned} $$  Hence $t_{n} \leq s_{n}$, so that  $$ \limsup _{n \rightarrow \infty} t_{n} \leq e, $$  by Theorem 3.19. Next, if $n \geq m$,  $$ t_{n} \geq 1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\cdots+\frac{1}{m !}\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{m-1}{n}\right) $$  Let $n \rightarrow \infty$, keeping $m$ fixed. We get  $$ \liminf _{n \rightarrow \infty} t_{n} \geq 1+1+\frac{1}{2 !}+\cdots+\frac{1}{m !}, $$  so that  $$ s_{m} \leq \liminf _{n \rightarrow \infty} t_{n} . $$  Letting $m \rightarrow \infty$, we finally get  $$ e \leq \liminf _{n \rightarrow \infty} t_{n} . $$  The theorem follows from (14) and (15).",;  ;  ; 
3.32,$e$ is irrational.,"Suppose $e$ is rational. Then $e=p / q$, where $p$ and $q$ are positive integers. By (16),  $$ 0<q !\left(e-s_{q}\right)<\frac{1}{q} $$  By our assumption, $q ! e$ is an integer. Since  $$ q ! s_{q}=q !\left(1+1+\frac{1}{2 !}+\cdots+\frac{1}{q !}\right) $$  is an integer, we see that $q !\left(e-s_{q}\right)$ is an integer.  Since $q \geq 1$, (17) implies the existence of an integer between 0 and 1 . We have thus reached a contradiction.  Actually, $e$ is not even an algebraic number. For a simple proof of this, see page 25 of Niven's book, or page 176 of Herstein's, cited in the Bibliography.",
3.33 (a),"Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha<1, \Sigma a_{n}$ converges.","If $\alpha<1$, we can choose $\beta$ so that $\alpha<\beta<1$, and an integer $N$ such that  $$ \sqrt[n]{\left|a_{n}\right|}<\beta $$  for $n \geq N$ [by Theorem 3.17(b)]. That is, $n \geq N$ implies  $$ \left|a_{n}\right|<\beta^{n} . $$  Since $0<\beta<1, \Sigma \beta^{n}$ converges. Convergence of $\Sigma a_{n}$ follows now from the comparison test.","Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ and $s^{*}$ have the same meaning as in Definition 3.16. Then $s^{*}$ has the following two properties:

(a) $s^{*} \in E$.

(b) If $x>s^{*}$, there is an integer $N$ such that $n \geq N$ implies $s_{n}<x$.

Moreover, $s^{*}$ is the only number with the properties $(a)$ and $(b)$.

Of course, an analogous result is true for $s_{*}$.;  ;  ; "
3.33 (b),"Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha>1, \Sigma a_{n}$ diverges.","If $\alpha>1$, then, again by Theorem 3.17, there is a sequence $\left\{n_{k}\right\}$ such that  $$ \sqrt[n_{k}]{\left|a_{n_{k}}\right|} \rightarrow \alpha . $$  Hence $\left|a_{n}\right|>1$ for infinitely many values of $n$, so that the condition $a_{n} \rightarrow 0$, necessary for convergence of $\Sigma a_{n}$, does not hold (Theorem 3.23).","Let $\left\{s_{n}\right\}$ be a sequence of real numbers. Let $E$ and $s^{*}$ have the same meaning as in Definition 3.16. Then $s^{*}$ has the following two properties:

(a) $s^{*} \in E$.

(b) If $x>s^{*}$, there is an integer $N$ such that $n \geq N$ implies $s_{n}<x$.

Moreover, $s^{*}$ is the only number with the properties $(a)$ and $(b)$.

Of course, an analogous result is true for $s_{*}$.; ;  ;  ; "
3.33 (c),"Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha=1$, the test gives no information.","To prove $(c)$, we consider the series  $$ \sum \frac{1}{n}, \sum \frac{1}{n^{2}} $$  For each of these series $\alpha=1$, but the first diverges, the second converges.",
3.34 (a),The series $\Sigma a_{n}$ converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$.,"If condition (a) holds, we can find $\beta<1$, and an integer $N$, such that  $$ \left|\frac{a_{n+1}}{a_{n}}\right|<\beta $$  for $n \geq N$. In particular,  $$ \begin{aligned} & \left|a_{N+1}\right|<\beta\left|a_{N}\right|, \\ & \left|a_{N+2}\right|<\beta\left|a_{N+1}\right|<\beta^{2}\left|a_{N}\right|, \\ & \ldots \ldots \ldots \ldots \ldots \ldots \\ & \left|a_{N+p}\right|<\beta^{p}\left|a_{N}\right| \end{aligned} $$  That is,  $$ \left|a_{n}\right|<\left|a_{N}\right| \beta^{-N} \cdot \beta^{n} $$  for $n \geq N$, and (a) follows from the comparison test, since $\Sigma \beta^{n}$ converges.",
3.34 (b),"The series $\Sigma a_{n}$ diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.","If $\left|a_{n+1}\right| \geq\left|a_{n}\right|$ for $n \geq n_{0}$, it is easily seen that the condition $a_{n} \rightarrow 0$ does not hold, and $(b)$ follows.",
3.37,"For any sequence $\left\{c_{n}\right\}$ of positive numbers,

$$
\begin{aligned}
\liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} & \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} & \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$","We shall prove the second inequality; the proof of the first is quite similar. Put  $$ \alpha=\limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} $$  If $\alpha=+\infty$, there is nothing to prove. If $\alpha$ is finite, choose $\beta>\alpha$. There is an integer $N$ such that  $$ \frac{c_{n+1}}{c_{n}} \leq \beta $$  for $n \geq N$. In particular, for any $p>0$,  $$ c_{N+k+1} \leq \beta c_{N+k} \quad(k=0,1, \ldots, p-1) $$  Multiplying these inequalities, we obtain  or  $$ c_{N+p} \leq \beta^{p} c_{N} $$  $$ c_{n} \leq c_{N} \beta^{-N} \cdot \beta^{n} \quad(n \geq N) $$  Hence  $$ \sqrt[n]{c_{n}} \leq \sqrt[n]{c_{N} \beta^{-N}} \cdot \beta $$  so that  $$ \limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \beta $$  by Theorem 3.20(b). Since (18) is true for every $\beta>\alpha$, we have  $$ \limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} \leq \alpha $$","If $p>0$, then $\lim _{n \rightarrow \infty} \frac{1}{n^{p}}=0$.;If $p>0$, then $\lim _{n \rightarrow \infty} \sqrt[n]{p}=1$.;$\lim _{n \rightarrow \infty} \sqrt[n]{n}=1$.;If $p>0$ and $\alpha$ is real, then $\lim _{n \rightarrow \infty} \frac{n^{\alpha}}{(1+p)^{n}}=0$.;If $|x|<1$, then $\lim _{n \rightarrow \infty} x^{n}=0$.;  ;  ; "
3.39,"Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}, \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.","Put $a_{n}=c_{n} z^{n}$, and apply the root test:  $$ \limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}=|z| \limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}=\frac{|z|}{R} $$",
3.4 (a),"Suppose $\mathbf{x}_{n} \in R^{k}(n=1,2,3, \ldots)$ and

$$
\mathbf{x}_{n}=\left(\alpha_{1, n}, \ldots, \alpha_{k, n}\right)
$$

Then $\left\{\mathbf{x}_{n}\right\}$ converges to $\mathbf{x}=\left(\alpha_{1}, \ldots, \alpha_{k}\right)$ if and only if

$$
\lim _{n \rightarrow \infty} \alpha_{f, n}=\alpha_{j} \quad(1 \leq j \leq k) .
$$","If $\mathbf{x}_{n} \rightarrow \mathbf{x}$, the inequalities  $$ \left|\alpha_{j, n}-\alpha_{j}\right| \leq\left|\mathbf{x}_{n}-\mathbf{x}\right|, $$  which follow immediately from the definition of the norm in $R^{k}$, show that (2) holds.  Conversely, if (2) holds, then to each $\varepsilon>0$ there corresponds an integer $N$ such that $n \geq N$ implies  $$ \left|\alpha_{j, n}-\alpha_{j}\right|<\frac{\varepsilon}{\sqrt{k}} \quad(1 \leq j \leq k) $$  Hence $n \geq N$ implies  $$ \left|\mathbf{x}_{n}-\mathbf{x}\right|=\left\{\sum_{j=1}^{k}\left|\alpha_{j, n}-\alpha_{j}\right|^{2}\right\}^{1 / 2}<\varepsilon, $$  so that $\mathbf{x}_{n} \rightarrow \mathbf{x}$. This proves $(a)$.",
3.4 (b),"Suppose $\left\{\mathbf{x}_{n}\right\},\left\{\mathbf{y}_{n}\right\}$ are sequences in $R^{k},\left\{\beta_{n}\right\}$ is a sequence of real numbers, and $\mathbf{x}_{n} \rightarrow \mathbf{x}, \mathbf{y}_{n} \rightarrow \mathbf{y}, \beta_{n} \rightarrow \beta$. Then

$$
\lim _{n \rightarrow \infty}\left(\mathbf{x}_{n}+\mathbf{y}_{n}\right)=\mathbf{x}+\mathbf{y}, \quad \lim _{n \rightarrow \infty} \mathbf{x}_{n} \cdot \mathbf{y}_{n}=\mathbf{x} \cdot \mathbf{y}, \quad \lim _{n \rightarrow \infty} \beta_{n} \mathbf{x}_{n}=\beta \mathbf{x} .
$$",Part $(b)$ follows from $(a)$ and Theorem 3.3.,"Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.;$\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$;$e$ is irrational.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha<1, \Sigma a_{n}$ converges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha>1, \Sigma a_{n}$ diverges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha=1$, the test gives no information.;The series $\Sigma a_{n}$ converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$.;The series $\Sigma a_{n}$ diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.;For any sequence $\left\{c_{n}\right\}$ of positive numbers,

$$
\begin{aligned}
\liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} & \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} & \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$;Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}, \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.;  ;  ; "
3.41,"Given two sequences $\left\{a_{n}\right\},\left\{b_{n}\right\}$, put

$$
A_{n}=\sum_{k=0}^{n} a_{k}
$$

if $n \geq 0$; put $A_{-1}=0$. Then, if $0 \leq p \leq q$, we have

$$
\sum_{n=p}^{q} a_{n} b_{n}=\sum_{n=p}^{q-1} A_{n}\left(b_{n}-b_{n+1}\right)+A_{q} b_{q}-A_{p-1} b_{p}
$$",$$ \sum_{n=p}^{q} a_{n} b_{n}=\sum_{n=p}^{q}\left(A_{n}-A_{n-1}\right) b_{n}=\sum_{n=p}^{q} A_{n} b_{n}-\sum_{n=p-1}^{q-1} A_{n} b_{n+1} $$  and the last expression on the right is clearly equal to the right side of (20).,
3.42,"Suppose

(a) the partial sums $A_{n}$ of $\Sigma a_{n}$ form a bounded sequence;

(b) $b_{0} \geq b_{1} \geq b_{2} \geq \cdots$;

(c) $\lim _{n \rightarrow \infty} b_{n}=0$.

Then $\Sigma a_{n} b_{n}$ converges.","Choose $M$ such that $\left|A_{n}\right| \leq M$ for all $n$. Given $\varepsilon>0$, there is an integer $N$ such that $b_{N} \leq(\varepsilon / 2 M)$. For $N \leq p \leq q$, we have  $$ \begin{aligned} \left|\sum_{n=p}^{q} a_{n} b_{n}\right| & =\left|\sum_{n=p}^{q-1} A_{n}\left(b_{n}-b_{n+1}\right)+A_{q} b_{q}-A_{p-1} b_{p}\right| \\ & \leq M\left|\sum_{n=p}^{q-1}\left(b_{n}-b_{n+1}\right)+b_{q}+b_{p}\right| \\ & =2 M b_{p} \leq 2 M b_{N} \leq \varepsilon . \end{aligned} $$  Convergence now follows from the Cauchy criterion. We note that the first inequality in the above chain depends of course on the fact that $b_{n}-b_{n+1} \geq 0$.",
3.43,"Suppose

(a) $\left|c_{1}\right| \geq\left|c_{2}\right| \geq\left|c_{3}\right| \geq \cdots$;

(b) $c_{2 m-1} \geq 0, c_{2 m} \leq 0 \quad(m=1,2,3, \ldots)$;

(c) $\lim _{n \rightarrow \infty} c_{n}=0$.

Then $\Sigma c_{n}$ converges.","Apply Theorem 3.42, with $a_{n}=(-1)^{n+1}, b_{n}=\left|c_{n}\right|$.","Suppose

(a) the partial sums $A_{n}$ of $\Sigma a_{n}$ form a bounded sequence;

(b) $b_{0} \geq b_{1} \geq b_{2} \geq \cdots$;

(c) $\lim _{n \rightarrow \infty} b_{n}=0$.

Then $\Sigma a_{n} b_{n}$ converges.;  ;  ; "
3.45,"If $\Sigma a_{n}$ converges absolutely, then $\Sigma a_{n}$ converges.",The assertion follows from the inequality  $$ \left|\sum_{k=n}^{m} a_{k}\right| \leq \sum_{k=n}^{m}\left|a_{k}\right| $$  plus the Cauchy criterion.,
3.50,"Suppose $\sum_{n=0}^{\infty} a_{n}$ and $\sum_{n=0}^{\infty} b_{n}$ are series with $\sum_{n=0}^{\infty} a_{n}=A$ and $\sum_{n=0}^{\infty} b_{n}=B$, and let $c_{n}=\sum_{k=0}^{n} a_{k} b_{n-k}$ for $n=0,1,2, \ldots$. Then

$$
\sum_{n=0}^{\infty} c_{n}=A B
$$

That is, the product of two convergent series converges, and to the right value, if at least one of the two series converges absolutely.","Put  $$ A_{n}=\sum_{k=0}^{n} a_{k}, \quad B_{n}=\sum_{k=0}^{n} b_{k}, \quad C_{n}=\sum_{k=0}^{n} c_{k}, \quad \beta_{n}=B_{n}-B $$  Then  $$ \begin{aligned} C_{n} & =a_{0} b_{0}+\left(a_{0} b_{1}+a_{1} b_{0}\right)+\cdots+\left(a_{0} b_{n}+a_{1} b_{n-1}+\cdots+a_{n} b_{0}\right) \\ & =a_{0} B_{n}+a_{1} B_{n-1}+\cdots+a_{n} B_{0} \\ & =a_{0}\left(B+\beta_{n}\right)+a_{1}\left(B+\beta_{n-1}\right)+\cdots+a_{n}\left(B+\beta_{0}\right) \\ & =A_{n} B+a_{0} \beta_{n}+a_{1} \beta_{n-1}+\cdots+a_{n} \beta_{0} \end{aligned} $$  Put  $$ \gamma_{n}=a_{0} \beta_{n}+a_{1} \beta_{n-1}+\cdots+a_{n} \beta_{0} . $$  We wish to show that $C_{n} \rightarrow A B$. Since $A_{n} B \rightarrow A B$, it suffices to show that  $$ \lim _{n \rightarrow \infty} \gamma_{n}=0 $$  Put  $$ \alpha=\sum_{n=0}^{\infty}\left|a_{n}\right| $$  [It is here that we use $(a)$.] Let $\varepsilon>0$ be given. By $(c), \beta_{n} \rightarrow 0$. Hence we can choose $N$ such that $\left|\beta_{n}\right| \leq \varepsilon$ for $n \geq N$, in which case  $$ \begin{aligned} \left|\gamma_{n}\right| & \leq\left|\beta_{0} a_{n}+\cdots+\beta_{N} a_{n-N}\right|+\left|\beta_{N+1} a_{n-N-1}+\cdots+\beta_{n} a_{0}\right| \\ & \leq\left|\beta_{0} a_{n}+\cdots+\beta_{N} a_{n-N}\right|+\varepsilon \alpha . \end{aligned} $$  Keeping $N$ fixed, and letting $n \rightarrow \infty$, we get  $$ \limsup _{n \rightarrow \infty}\left|\gamma_{n}\right| \leq \varepsilon \alpha $$  since $a_{k} \rightarrow 0$ as $k \rightarrow \infty$. Since $\varepsilon$ is arbitrary, (21) follows.",
3.54,"Let $\Sigma a_{n}$ be a series of real numbers which converges, but not absolutely. Suppose

$$
-\infty \leq \alpha \leq \beta \leq \infty .
$$

Then there exists a rearrangement $\Sigma a_{n}^{\prime}$ with partial sums $s_{n}^{\prime}$ such that

$$
\liminf _{n \rightarrow \infty} s_{n}^{\prime}=\alpha, \quad \limsup _{n \rightarrow \infty} s_{n}^{\prime}=\beta .
$$
","Let  $$ p_{n}=\frac{\left|a_{n}\right|+a_{n}}{2}, \quad q_{n}=\frac{\left|a_{n}\right|-a_{n}}{2} \quad(n=1,2,3, \ldots) $$  Then $p_{n}-q_{n}=a_{n}, p_{n}+q_{n}=\left|a_{n}\right|, p_{n} \geq 0, q_{n} \geq 0$. The series $\Sigma p_{n}, \Sigma q_{n}$ must both diverge.  For if both were convergent, then  $$ \Sigma\left(p_{n}+q_{n}\right)=\Sigma\left|a_{n}\right| $$  would converge, contrary to hypothesis. Since  $$ \sum_{n=1}^{N} a_{n}=\sum_{n=1}^{N}\left(p_{n}-q_{n}\right)=\sum_{n=1}^{N} p_{n}-\sum_{n=1}^{N} q_{n} $$  divergence of $\Sigma p_{n}$ and convergence of $\Sigma q_{n}$ (or vice versa) implies divergence of $\Sigma a_{n}$, again contrary to hypothesis.  Now let $P_{1}, P_{2}, P_{3}, \ldots$ denote the nonnegative terms of $\Sigma a_{n}$, in the order in which they occur, and let $Q_{1}, Q_{2}, Q_{3}, \ldots$ be the absolute values of the negative terms of $\Sigma a_{n}$, also in their original order.  The series $\Sigma P_{n}, \Sigma Q_{n}$ differ from $\Sigma p_{n}, \Sigma q_{n}$ only by zero terms, and are therefore divergent.  We shall construct sequences $\left\{m_{n}\right\},\left\{k_{n}\right\}$, such that the series  $$ \begin{aligned} P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots & \\ & +P_{m_{2}}-Q_{k_{1}+1}-\cdots-Q_{k_{2}}+\cdots \end{aligned} $$  which clearly is a rearrangement of $\Sigma a_{n}$, satisfies (24).  Choose real-valued sequences $\left\{\alpha_{n}\right\},\left\{\beta_{n}\right\}$ such that $\alpha_{n} \rightarrow \alpha, \beta_{n} \rightarrow \beta$, $\alpha_{n}<\beta_{n}, \beta_{1}>0$.  Let $m_{1}, k_{1}$ be the smallest integers such that  $$ \begin{gathered} P_{1}+\cdots+P_{m_{1}}>\beta_{1} \\ P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}<\alpha_{1} \end{gathered} $$  let $m_{2}, k_{2}$ be the smallest integers such that  $$ \begin{aligned} & P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots+P_{m_{2}}>\beta_{2} \\ & P_{1}+\cdots+P_{m_{1}}-Q_{1}-\cdots-Q_{k_{1}}+P_{m_{1}+1}+\cdots+P_{m_{2}}-Q_{k_{1}+1} \\ & \end{aligned} $$ and continue in this way. This is possible since $\Sigma P_{n}$ and $\Sigma Q_{n}$ diverge. If $x_{n}, y_{n}$ denote the partial sums of (25) whose last terms are $P_{m_{n}}$, $-Q_{k_{n}}$, then  $$ \left|x_{n}-\beta_{n}\right| \leq P_{m_{n}}, \quad\left|y_{n}-\alpha_{n}\right| \leq Q_{k_{n}} . $$  Since $P_{n} \rightarrow 0$ and $Q_{n} \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_{n} \rightarrow \beta, y_{n} \rightarrow \alpha$.  Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25).",
3.55,"If $\Sigma a_{n}$ is a series of complex numbers which converges absolutely, then every rearrangement of $\Sigma a_{n}$ converges, and they all converge to the same sum.","Let $\Sigma a_{n}^{\prime}$ be a rearrangement, with partial sums $s_{n}^{\prime}$. Given $\varepsilon>0$, there exists an integer $N$ such that $m \geq n \geq N$ implies  $$ \sum_{i=n}^{m}\left|a_{i}\right| \leq \varepsilon $$  Now choose $p$ such that the integers $1,2, \ldots, N$ are all contained in the set $k_{1}, k_{2}, \ldots, k_{p}$ (we use the notation of Definition 3.52). Then if $n>p$, the numbers $a_{1}, \ldots, a_{N}$ will cancel in the difference $s_{n}-s_{n}^{\prime}$, so that $\left|s_{n}-s_{n}^{\prime}\right| \leq \varepsilon$, by (26). Hence $\left\{s_{n}^{\prime}\right\}$ converges to the same sum as $\left\{s_{n}\right\}$.",
3.6 (a),"If $\left\{p_{n}\right\}$ is a sequence in a compact metric space $X$, then some subsequence of $\left\{p_{n}\right\}$ converges to a point of $X$.","Let $E$ be the range of $\left\{p_{n}\right\}$. If $E$ is finite then there is a $p \in E$ and a sequence $\left\{n_{i}\right\}$ with $n_{1}<n_{2}<n_{3}<\cdots$, such that  $$ p_{n_{1}}=p_{n_{2}}=\cdots=p . $$  The subsequence $\left\{p_{n t}\right\}$ so obtained converges evidently to $p$.  If $E$ is infinite, Theorem 2.37 shows that $E$ has a limit point $p \in X$. Choose $n_{1}$ so that $d\left(p, p_{n_{1}}\right)<1$. Having chosen $n_{1}, \ldots, n_{i-1}$, we see from Theorem 2.20 that there is an integer $n_{i}>n_{i-1}$ such that $d\left(p, p_{n_{i}}\right)<1 / i$. Then $\left\{p_{n}\right\}$ converges to $p$.","If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K; If $p$ is a limit point of a set $E$, then every neighborhood of $p$ contains infinitely many points of $E$.;  ;  ; "
3.6 (b),Every bounded sequence in $R^{k}$ contains a convergent subsequence.,"This follows from 3.6 (a), since Theorem 2.41 implies that every bounded subset of $R^{k}$ lies in a compact subset of $R^{k}$.","If a set $E$ in $R^{k}$ has one of the following three properties, then it has the other two:

(a) $E$ is closed and bounded.

(b) $E$ is compact.

(c) Every infinite subset of $E$ has a limit point in $E$.;  ;  ; "
3.7,The subsequential limits of a sequence $\left\{p_{n}\right\}$ in a metric space $X$ form a closed subset of $X,"Let $E^{*}$ be the set of all subsequential limits of $\left\{p_{n}\right\}$ and let $q$ be a limit point of $E^{*}$. We have to show that $q \in E^{*}$.  Choose $n_{1}$ so that $p_{n_{1}} \neq q$. (If no such $n_{1}$ exists, then $E^{*}$ has only one point, and there is nothing to prove.) Put $\delta=d\left(q, p_{n_{1}}\right)$. Suppose $n_{1}, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E^{*}$, there is an $x \in E^{*}$ with $d(x, q)<2^{-1} \delta$. Since $x \in E^{*}$, there is an $n_{i}>n_{i-1}$ such that $d\left(x, p_{n_{1}}\right)<2^{-1} \delta$. Thus  $$ d\left(q, p_{n_{1}}\right) \leq 2^{1-i} \delta $$  for $i=1,2,3, \ldots$. This says that $\left\{p_{n}\right\}$ converges to $q$. Hence $q \in E^{*}$.",
4.10 (a),"Let $f_{1}, \ldots, f_{k}$ be real functions on a metric space $X$, and let $\mathbf{f}$ be the mapping of $X$ into $R^{k}$ defined by

$$
\mathbf{f}(x)=\left(f_{1}(x), \ldots, f_{k}(x)\right) \quad(x \in X)
$$

then $\mathbf{f}$ is continuous if and only if each of the functions $f_{1}, \ldots, f_{k}$ is continuous.","Part $(a)$ follows from the inequalities  $$ \left|f_{j}(x)-f_{j}(y)\right| \leq|\mathbf{f}(x)-\mathbf{f}(y)|=\left\{\sum_{i=1}^{k}\left|f_{i}(x)-f_{i}(y)\right|^{2}\right\}^{\frac{t}{2}}, $$  for $j=1, \ldots, k$.",
4.10 (b),"If $\mathbf{f}$ and $\mathbf{g}$ are continuous mappings of $X$ into $R^{k}$, then $\mathbf{f}+\mathbf{g}$ and $\mathbf{f} \cdot \mathbf{g}$ are continuous on $X$.",Part $(b)$ follows from $(a)$ and Theorem 4.9.,"Let $f$ and $g$ be complex continuous functions on a metric space $X$. Then $f+g, f g$, and $f / g$ are continuous on $X$. In the last case, we must of course assume that $g(x) \neq 0$, for all $x \in X$.;  ;  ; "
4.14,Suppose $f$ is a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f(X)$ is compact.,"Let $\left\{V_{\alpha}\right\}$ be an open cover of $f(X)$. Since $f$ is continuous, Theorem 4.8 shows that each of the sets $f^{-1}\left(V_{a}\right)$ is open. Since $X$ is compact, there are finitely many indices, say $\alpha_{1}, \ldots, \alpha_{n}$, such that  $$ X \subset f^{-1}\left(V_{\alpha_{1}}\right) \cup \cdots \cup f^{-1}\left(V_{\alpha_{n}}\right) . $$  Since $f\left(f^{-1}(E)\right) \subset E$ for every $E \subset Y$, (12) implies that  $$ f(X) \subset V_{a_{1}} \cup \cdots \cup V_{a_{n}} . $$  This completes the proof.",A mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous on $X$ if and only if $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$;  ;  ; 
4.15,"If $\mathbf{f}$ is a continuous mapping of a compact metric space $X$ into $R^{k}$, then $\mathbf{f}(X)$ is closed and bounded. Thus, $\mathbf{f}$ is bounded.",This follows from Theorem 2.41. The result is particularly important when $f$ is real:,"If a set $E$ in $R^{k}$ has one of the following three properties, then it has the other two:

(a) $E$ is closed and bounded.

(b) $E$ is compact.

(c) Every infinite subset of $E$ has a limit point in $E$.;  ;  ; "
4.17,"Suppose $f$ is a continuous 1-1 mapping of a compact metric space $X$ onto a metric space $Y$. Then the inverse mapping $f^{-1}$ defined on $Y$ by

$$
f^{-1}(f(x))=x \quad(x \in X)
$$
is a continuous mapping of $Y$ onto $X.","Applying Theorem 4.8 to $f^{-1}$ in place of $f$, we see that it suffices to prove that $f(V)$ is an open set in $Y$ for every open set $V$ in $X. Fix such a set $V$.  The complement $V^{c}$ of $V$ is closed in $X$, hence compact (Theorem 2.35); hence $f\left(V^{c}\right)$ is a compact subset of $Y$ (Theorem 4.14) and so is closed in $Y$ (Theorem 2.34). Since $f$ is one-to-one and onto, $f(V)$ is the complement of $f\left(V^{c}\right)$. Hence $f(V)$ is open.",A mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous on $X$ if and only if $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$; Closed subsets of compact sets are compact.; Suppose $f$ is a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then $f(X)$ is compact.; Compact subsets of metric spaces are closed.;  ;  ; 
4.19,Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$: Then $f$ is uniformly continuous on $X$.,"Since $f$ is continuous, we can associate to each point $p \in X$ a positive number $\phi(p)$ such that  $$ q \in X, d_{X}(p, q)<\phi(p) \quad \text { implies } \quad d_{Y}(f(p), f(q))<\frac{\varepsilon}{2} $$  Let $J(p)$ be the set of all $q \in X$ for which  We put  $$ d_{\mathbf{X}}(p, q)<\frac{1}{2} \phi(p) $$  Since $p \in J(p)$, the collection of all sets $J(p)$ is an open cover of $X$; and since $X$ is compact, there is a finite set of points $p_{1}, \ldots, p_{n}$ in $X$, such that  $$ X \subset J\left(p_{1}\right) \cup \cdots \cup J\left(p_{n}\right) $$  $$ \delta=\frac{1}{2} \min \left[\phi\left(p_{1}\right), \ldots, \phi\left(p_{n}\right)\right] $$  Then $\delta>0$. (This is one point where the finiteness of the covering, inherent in the definition of compactness, is essential. The minimum of a finite set of positive numbers is positive, whereas the inf of an infinite set of positive numbers may very well be 0 .)  Now let $q$ and $p$ be points of $X$, such that $d_{x}(p, q)<\delta$. By (18), there is an integer $m, 1 \leq m \leq n$, such that $p \in J\left(p_{m}\right)$; hence  $$ d_{X}\left(p, p_{m}\right)<\frac{1}{2} \phi\left(p_{m}\right) $$  and we also have  $$ d_{x}\left(q, p_{m}\right) \leq d_{x}(p, q)+d_{x}\left(p, p_{m}\right)<\delta+\frac{1}{2} \phi\left(p_{m}\right) \leq \phi\left(p_{m}\right) $$  Finally, (16) shows that therefore  $$ d_{Y}(f(p), f(q)) \leq d_{Y}\left(f(p), f\left(p_{m}\right)\right)+d_{Y}\left(f(q), f\left(p_{m}\right)\right)<\varepsilon $$  This completes the proof.",
4.20 (a),Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not bounded.,"Suppose first that $E$ is bounded, so that there exists a limit point $x_{0}$ of $E$ which is not a point of $E$. Consider  $$ f(x)=\frac{1}{x-x_{0}} \quad(x \in E) $$  This is continuous on $E$ (Theorem 4.9), but evidently unbounded.","Let $f$ and $g$ be complex continuous functions on a metric space $X$. Then $f+g, f g$, and $f / g$ are continuous on $X$. In the last case, we must of course assume that $g(x) \neq 0$, for all $x \in X$.;  ;  ; "
4.20 (b),Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous and bounded function on $E$ which has no maximum.,"The function $g$ given by  $$ g(x)=\frac{1}{1+\left(x-x_{0}\right)^{2}} \quad(x \in E) $$  is continuous on $E$, and is bounded, since $0<g(x)<1$. It is clear that  $$ \sup _{x \in E} g(x)=1 $$  whereas $g(x)<1$ for all $x \in E$. Thus $g$ has no maximum on $E$.",
4.20 (c),Let $E$ be a bounded noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not uniformly continuous.,"To see that (21) is not uniformly continuous, let $\varepsilon>0$ and $\delta>0$ be arbitrary, and choose a point $x \in E$ such that $\left|x-x_{0}\right|<\delta$. Taking $t$ close enough to $x_{0}$, we can then make the difference $|f(t)-f(x)|$ greater than $\varepsilon$, although $|t-x|<\delta$. Since this is true for every $\delta>0, f$ is not uniformly continuous on $E$.",
4.22,"If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and if $E$ is a connected subset of $X$, then $f(E)$ is connected.","Assume, on the contrary, that $f(E)=A \cup B$, where $A$ and $B$ are nonempty separated subsets of $Y$. Put $G=E \cap f^{-1}(A), H=E \cap f^{-1}(B)$.  Then $E=G \cup H$, and neither $G$ nor $H$ is empty.  Since $A \subset \bar{A}$ (the closure of $A$ ), we have $G \subset f^{-1}(\bar{A})$; the latter set is closed, since $f$ is continuous; hence $\bar{G} \subset f^{-1}(\bar{A})$. It follows that $f(\bar{G}) \subset \bar{A}$. Since $f(H)=B$ and $\bar{A} \cap B$ is empty, we conclude that $\bar{G} \cap H$ is empty.  The same argument shows that $G \cap \bar{H}$ is empty. Thus $G$ and $H$ are separated. This is impossible if $E$ is connected.",
4.23,"Let $f$ be a continuous real function on the interval $[a, b]$. If $f(a)<f(b)$ and if $c$ is a number such that $f(a)<c<f(b)$, then there exists a point $x \in(a, b)$ such that $f(x)=c$.","By Theorem 2.47, $[a, b]$ is connected; hence Theorem 4.22 shows that $f([a, b])$ is a connected subset of $R^{1}$, and the assertion follows if we appeal once more to Theorem 2.47.","A subset $E$ of the real line $R^{1}$ is connected if and only if it has the following property: If $x \in E, y \in E$, and $x<z<y$, then $z \in E$.; If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and if $E$ is a connected subset of $X$, then $f(E)$ is connected.; A subset $E$ of the real line $R^{1}$ is connected if and only if it has the following property: If $x \in E, y \in E$, and $x<z<y$, then $z \in E$.;  ;  ; "
4.29,"Let $f$ be monotonically increasing on $(a, b)$. Then $f(x+)$ and $f(x-)$ exist at every point of $x$ of $(a, b)$. More precisely,

$$
sup _{a<t<x} f(t)=f(x-) \leq f(x) \leq f(x+)=\inf _{x<t<b} f(t) .
$$

Furthermore, if $a<x<y<b$, then

$$
f(x+) \leq f(y-) .
$$

Analogous results evidently hold for monotonically decreasing functions.","By hypothesis, the set of numbers $f(t)$, where $a<t<x$, is bounded above by the number $f(x)$, and therefore has a least upper bound which we shall denote by $A$. Evidently $A \leq f(x)$. We have to show that $A=f(x-)$. Let $\varepsilon>0$ be given. It follows from the definition of $A$ as a least upper bound that there exists $\delta>0$ such that $a<x-\delta<x$ and  Since $f$ is monotonic, we have  $$ f(x-\delta) \leq f(t) \leq A \quad(x-\delta<t<x) $$  Combining (27) and (28), we see that  $$ |f(t)-A|<\varepsilon \quad(x-\delta<t<x) . $$  Hence $f(x-)=A$.  The second half of (25) is proved in precisely the same way.  Next, if $a<x<y<b$, we see from (25) that  $$ f(x+)=\inf _{x<t<b} f(t)=\inf _{x<t<y} f(t) . $$  The last equality is obtained by applying (25) to $(a, y)$ in place of $(a, b)$. Similarly,  $$ f(y-)=\sup _{a<t<y} f(t)=\sup _{x<t<y} f(t) . $$  Comparison of (29) and (30) gives (26).",
4.30,"Let $f$ be monotonic on $(a, b)$. Then the set of points of $(a, b)$ at which $f$ is discontinuous is at most countable.","Suppose, for the sake of definiteness, that $f$ is increasing, and let $E$ be the set of points at which $f$ is discontinuous. that  With every point $x$ of $E$ we associate a rational number $r(x)$ such  $$ f(x-)<r(x)<f(x+) . $$  Since $x_{1}<x_{2}$ implies $f\left(x_{1}+\right) \leq f\left(x_{2}-\right)$, we see that $r\left(x_{1}\right) \neq r\left(x_{2}\right)$ if $x_{1} \neq x_{2}$.  We have thus established a 1-1 correspondence between the set $E$ and a subset of the set of rational numbers. The latter, as we know, is countable.",
4.34,"Let $f$ and $g$ be defined on $E \subset R$. Suppose

$$
f(t) \rightarrow A, \quad g(t) \rightarrow B \quad \text { as } t \rightarrow x
$$

Then

(a) $f(t) \rightarrow A^{\prime}$ implies $A^{\prime}=A$.

(b) $(f+g)(t) \rightarrow A+B$

(c) $(f g)(t) \rightarrow A B$,

(d) $(f / g)(t) \rightarrow A / B$

provided the right members of $(b),(c)$, and $(d)$ are defined.

Note that $\infty-\infty, 0 \cdot \infty, \infty / \infty, A / 0$ are not defined (see Definition 1.23).","The analogue of Theorem 4.4 is still true, and the proof offers nothing new. We state it, for the sake of completeness.","Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f+g)(x)=A+B$;Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f g)(x)=A B$;Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}\left(\frac{f}{g}\right)(x)=\frac{A}{B}$, if $B \neq 0$;  ;  ; "
4.4 (a),"Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f+g)(x)=A+B$","In view of Theorem 4.2, these assertions follow immediately from the analogous properties of sequences (Theorem 3.3).","Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not bounded.;Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous and bounded function on $E$ which has no maximum.;Let $E$ be a bounded noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not uniformly continuous.;If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and if $E$ is a connected subset of $X$, then $f(E)$ is connected.;Let $f$ be a continuous real function on the interval $[a, b]$. If $f(a)<f(b)$ and if $c$ is a number such that $f(a)<c<f(b)$, then there exists a point $x \in(a, b)$ such that $f(x)=c$.;Let $f$ be monotonically increasing on $(a, b)$. Then $f(x+)$ and $f(x-)$ exist at every point of $x$ of $(a, b)$. More precisely,

$$
sup _{a<t<x} f(t)=f(x-) \leq f(x) \leq f(x+)=\inf _{x<t<b} f(t) .
$$

Furthermore, if $a<x<y<b$, then

$$
f(x+) \leq f(y-) .
$$

Analogous results evidently hold for monotonically decreasing functions.; Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.;$\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$;$e$ is irrational.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha<1, \Sigma a_{n}$ converges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha>1, \Sigma a_{n}$ diverges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha=1$, the test gives no information.;The series $\Sigma a_{n}$ converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$.;The series $\Sigma a_{n}$ diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.;For any sequence $\left\{c_{n}\right\}$ of positive numbers,

$$
\begin{aligned}
\liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} & \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} & \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$;Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}, \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.;  ;  ; "
4.4 (b),"Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f g)(x)=A B$","In view of Theorem 4.2, these assertions follow immediately from the analogous properties of sequences (Theorem 3.3).","Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not bounded.;Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous and bounded function on $E$ which has no maximum.;Let $E$ be a bounded noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not uniformly continuous.;If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and if $E$ is a connected subset of $X$, then $f(E)$ is connected.;Let $f$ be a continuous real function on the interval $[a, b]$. If $f(a)<f(b)$ and if $c$ is a number such that $f(a)<c<f(b)$, then there exists a point $x \in(a, b)$ such that $f(x)=c$.;Let $f$ be monotonically increasing on $(a, b)$. Then $f(x+)$ and $f(x-)$ exist at every point of $x$ of $(a, b)$. More precisely,

$$
sup _{a<t<x} f(t)=f(x-) \leq f(x) \leq f(x+)=\inf _{x<t<b} f(t) .
$$

Furthermore, if $a<x<y<b$, then

$$
f(x+) \leq f(y-) .
$$

Analogous results evidently hold for monotonically decreasing functions.; Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.;$\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$;$e$ is irrational.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha<1, \Sigma a_{n}$ converges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha>1, \Sigma a_{n}$ diverges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha=1$, the test gives no information.;The series $\Sigma a_{n}$ converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$.;The series $\Sigma a_{n}$ diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.;For any sequence $\left\{c_{n}\right\}$ of positive numbers,

$$
\begin{aligned}
\liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} & \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} & \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$;Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}, \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.;  ;  ; "
4.4 (c),"Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}\left(\frac{f}{g}\right)(x)=\frac{A}{B}$, if $B \neq 0$","In view of Theorem 4.2, these assertions follow immediately from the analogous properties of sequences (Theorem 3.3).","Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not bounded.;Let $E$ be a noncompact set in $R^{1}$. Then there exists a continuous and bounded function on $E$ which has no maximum.;Let $E$ be a bounded noncompact set in $R^{1}$. Then there exists a continuous function on $E$ which is not uniformly continuous.;If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and if $E$ is a connected subset of $X$, then $f(E)$ is connected.;Let $f$ be a continuous real function on the interval $[a, b]$. If $f(a)<f(b)$ and if $c$ is a number such that $f(a)<c<f(b)$, then there exists a point $x \in(a, b)$ such that $f(x)=c$.;Let $f$ be monotonically increasing on $(a, b)$. Then $f(x+)$ and $f(x-)$ exist at every point of $x$ of $(a, b)$. More precisely,

$$
sup _{a<t<x} f(t)=f(x-) \leq f(x) \leq f(x+)=\inf _{x<t<b} f(t) .
$$

Furthermore, if $a<x<y<b$, then

$$
f(x+) \leq f(y-) .
$$

Analogous results evidently hold for monotonically decreasing functions.; Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty}\left(s_{n}+t_{n}\right)=s+t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} c s_{n}=c s, \lim _{n \rightarrow \infty}\left(c+s_{n}\right)=c+s$, for any number $c$;;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} s_{n} t_{n}=s t$;Suppose $\left\{s_{n}\right\},\left\{t_{n}\right\}$ are complex sequences, and $\lim _{n \rightarrow \infty} s_{n}=s$, $\lim _{n \rightarrow \infty} t_{n}=t$. Then $\lim _{n \rightarrow \infty} \frac{1}{s_{n}}=\frac{1}{s}$, provided $s_{n} \neq 0(n=1,2,3, \ldots)$, and $s \neq 0$.;$\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$;$e$ is irrational.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha<1, \Sigma a_{n}$ converges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha>1, \Sigma a_{n}$ diverges.;Given $\Sigma a_{n}$, put $\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|a_{n}\right|}$. If $\alpha=1$, the test gives no information.;The series $\Sigma a_{n}$ converges if $\limsup _{n \rightarrow \infty}\left|\frac{a_{n+1}}{a_{n}}\right|<1$.;The series $\Sigma a_{n}$ diverges if $\left|\frac{a_{n+1}}{a_{n}}\right| \geq 1$ for all $n \geq n_{0}$, where $n_{0}$ is some fixed integer.;For any sequence $\left\{c_{n}\right\}$ of positive numbers,

$$
\begin{aligned}
\liminf _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}} & \leq \liminf _{n \rightarrow \infty} \sqrt[n]{c_{n}}, \\
\limsup _{n \rightarrow \infty} \sqrt[n]{c_{n}} & \leq \limsup _{n \rightarrow \infty} \frac{c_{n+1}}{c_{n}}
\end{aligned}
$$;Given the power series $\Sigma c_{n} z^{n}$, put

$$
\alpha=\limsup _{n \rightarrow \infty} \sqrt[n]{\left|c_{n}\right|}, \quad R=\frac{1}{\alpha}
$$

(If $\alpha=0, R=+\infty$; if $\alpha=+\infty, R=0$.) Then $\Sigma c_{n} z^{n}$ converges if $|z|<R$, and diverges if $|z|>R$.;  ;  ; "
4.6,"In the situation given in Definition 4.5, assume also that $p$ is a limit point of $E$. Then $f$ is continuous at $p$ if and only if $\lim _{x \rightarrow p} f(x)=f(p)$.",This is clear if we compare Definitions 4.1 and 4.5.,
4.8,A mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous on $X$ if and only if $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$,"Suppose $f$ is continuous on $X$ and $V$ is an open set in $Y. We have to show that every point of $f^{-1}(V)$ is an interior point of $f^{-1}(V). Conversely, suppose $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y. Fix $p \in X$ and $\varepsilon>0$, let $V$ be the set of all $y \in Y$ such that $d_{Y}(y, f(p))<\varepsilon$. Then $V$ is open; hence $f^{-1}(V)$ is open; hence there exists $\delta>0$ such that $x \in f^{-1}(V)$ as soon as $d_{x}(p, x)<\delta$. But if $x \in f^{-1}(V)$, then $f(x) \in V$, so that $d_{Y}(f(x), f(p))<\varepsilon$.",
4.9,"Let $f$ and $g$ be complex continuous functions on a metric space $X$. Then $f+g, f g$, and $f / g$ are continuous on $X$. In the last case, we must of course assume that $g(x) \neq 0$, for all $x \in X$.","At isolated points of $X$ there is nothing to prove. At limit points, the statement follows from Theorems 4.4 and 4.6.",
5.10,"If $f$ is a real continuous function on $[a, b]$ which is differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
f(b)-f(a)=(b-a) f^{\prime}(x) .
$$

",Take $g(x)=x$ in Theorem 5.9.,"If $f$ and $g$ are continuous real functions on $[a, b]$ which are differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
[f(b)-f(a)] g^{\prime}(x)=[g(b)-g(a)] f^{\prime}(x)
$$

;  ;  ; "
5.11 (a),"Suppose $f$ is differentiable in $(a, b)$: If $f^{\prime}(x) \geq 0$ for all $x \in(a, b)$, then $f$ is monotonically increasing.","All conclusions can be read off from the equation  $$ f\left(x_{2}\right)-f\left(x_{1}\right)=\left(x_{2}-x_{1}\right) f^{\prime}(x) $$  which is valid, for each pair of numbers $x_{1}, x_{2}$ in $(a, b)$, for some $x$ between $x_{1}$ and $x_{2}$.",
5.11 (b),"Suppose $f$ is differentiable in $(a, b)$: If $f^{\prime}(x)=0$ for all $x \in(a, b)$, then $f$ is constant.","All conclusions can be read off from the equation  $$ f\left(x_{2}\right)-f\left(x_{1}\right)=\left(x_{2}-x_{1}\right) f^{\prime}(x) $$  which is valid, for each pair of numbers $x_{1}, x_{2}$ in $(a, b)$, for some $x$ between $x_{1}$ and $x_{2}$.",
5.11 (c),"Suppose $f$ is differentiable in $(a, b)$: If $f^{\prime}(x) \leq 0$ for all $x \in(a, b)$, then $f$ is monotonically decreasing.","All conclusions can be read off from the equation  $$ f\left(x_{2}\right)-f\left(x_{1}\right)=\left(x_{2}-x_{1}\right) f^{\prime}(x) $$  which is valid, for each pair of numbers $x_{1}, x_{2}$ in $(a, b)$, for some $x$ between $x_{1}$ and $x_{2}$.",
5.12,"Suppose $f$ is a real differentiable function on $[a, b]$ and suppose $f^{\prime}(a)<\lambda<f^{\prime}(b)$. Then there is a point $x \in(a, b)$ such that $f^{\prime}(x)=\lambda$.","Put $g(t)=f(t)-\lambda t$. Then $g^{\prime}(a)<0$, so that $g\left(t_{1}\right)<g(a)$ for some $t_{1} \in(a, b)$, and $g^{\prime}(b)>0$, so that $g\left(t_{2}\right)<g(b)$ for some $t_{2} \in(a, b)$. Hence $g$ attains its minimum on $[a, b]$ (Theorem 4.16) at some point $x$ such that $a<x<b$. By Theorem 5.8, $g^{\prime}(x)=0$. Hence $f^{\prime}(x)=\lambda$.",; ;  ;  ; 
5.13,"Suppose $f$ and $g$ are real and differentiable in $(a, b)$, and $g^{
\prime}(x) \neq 0$ for all $x \in(a, b)$, where $-
\infty \leq a<b \leq+\infty$. Suppose

$$
\frac{f^{
\prime}(x)}{g^{
\prime}(x)} \rightarrow A \text { as } x \rightarrow a
$$

If

$$
f(x) \rightarrow 0 \text { and } g(x) \rightarrow 0 \text { as } x \rightarrow a \text {, }
$$

or if

$$
g(x) \rightarrow+\infty \text { as } x \rightarrow a \text {, }
$$

then

$$
\frac{f(x)}{g(x)} \rightarrow A \text { as } x \rightarrow a .
$$

The analogous statement is of course also true if $x \rightarrow b$, or if $g(x) \rightarrow-\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33.","We first consider the case in which $- \infty \leq A<+\infty$. Choose a real number $q$ such that $A<q$, and then choose $r$ such that $A<r<q$. By (13) there is a point $c \in(a, b)$ such that $a<x<c$ implies  $$ \frac{f^{ \prime}(x)}{g^{ \prime}(x)}<r $$  If $a<x<y<c$, then Theorem 5.9 shows that there is a point $t \in(x, y)$ such that  $$ \frac{f(x)-f(y)}{g(x)-g(y)}=\frac{f^{ \prime}(t)}{g^{ \prime}(t)}<r $$  Suppose (14) holds. Letting $x \rightarrow a$ in (18), we see that  $$ \frac{f(y)}{g(y)} \leq r<q \quad(a<y<c) $$ Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choose a point $c_{1} \in(a, y)$ such that $g(x)>g(y)$ and $g(x)>0$ if $a<x<c_{1}$. Multiplying (18) by $[g(x)-g(y)] / g(x)$, we obtain  $$ \frac{f(x)}{g(x)}<r-r \frac{g(y)}{g(x)}+\frac{f(y)}{g(x)} \quad\left(a<x<c_{1}\right) $$  If we let $x \rightarrow a$ in (20), (15) shows that there is a point $c_{2} \in\left(a, c_{1}\right)$ such that  $$ \frac{f(x)}{g(x)}<q \quad\left(a<x<c_{2}\right) $$  Summing up, (19) and (21) show that for any $q$, subject only to the condition $A<q$, there is a point $c_{2}$ such that $f(x) / g(x)<q$ if $a<x<c_{2}$.  In the same manner, if $- \infty<A \leq+\infty$, and $p$ is chosen so that $p<A$, we can find a point $c_{3}$ such that  $$ p<\frac{f(x)}{g(x)} \quad\left(a<x<c_{3}\right) $$  and (16) follows from these two statements.","If $f$ and $g$ are continuous real functions on $[a, b]$ which are differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
[f(b)-f(a)] g^{\prime}(x)=[g(b)-g(a)] f^{\prime}(x)
$$

;  ;  ; "
5.15,"Suppose $f$ is a real function on $[a, b], n$ is a positive integer, $f^{(n-1)}$ is continuous on $[a, b], f^{(n)}(t)$ exists for every $t \in(a, b)$. Let $\alpha, \beta$ be distinct points of $[a, b]$, and define

$$
P(t)=\sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k !}(t-\alpha)^{k}
$$

Then there exists a point $x$ between $\alpha$ and $\beta$ such that

$$
f(\beta)=P(\beta)+\frac{f^{(n)}(x)}{n !}(\beta-\alpha)^{n}
$$

For $n=1$, this is just the mean value theorem. In general, the theorem shows that $f$ can be approximated by a polynomial of degree $n-1$, and that (24) allows us to estimate the error, if we know bounds on $\left|f^{(n)}(x)\right|$.","Let $M$ be the number defined by  $$ f(\beta)=P(\beta)+M(\beta-\alpha)^{n} $$  and put  $$ g(t)=f(t)-P(t)-M(t-\alpha)^{n} \quad(a \leq t \leq b) . $$  We have to show that $n ! M=f^{(n)}(x)$ for some $x$ between $\alpha$ and $\beta$. By (23) and (26),  $$ g^{(n)}(t)=f^{(n)}(t)-n ! M \quad(a<t<b) $$  Hence the proof will be complete if we can show that $g^{(n)}(x)=0$ for some $x$ between $\alpha$ and $\beta$.  Since $P^{(k)}(\alpha)=f^{(k)}(\alpha)$ for $k=0, \ldots, n-1$, we have  $$ g(\alpha)=g^{\prime}(\alpha)=\cdots=g^{(n-1)}(\alpha)=0 . $$  Our choice of $M$ shows that $g(\beta)=0$, so that $g^{\prime}\left(x_{1}\right)=0$ for some $x_{1}$ between $\alpha$ and $\beta$, by the mean value theorem. Since $g^{\prime}(\alpha)=0$, we conclude similarly that $g^{\prime \prime}\left(x_{2}\right)=0$ for some $x_{2}$ between $\alpha$ and $x_{1}$. After $n$ steps we arrive at the conclusion that $g^{(n)}\left(x_{n}\right)=0$ for some $x_{n}$ between $\alpha$ and $x_{n-1}$, that is, between $\alpha$ and $\beta$.",
5.19,"Suppose $\mathbf{f}$ is a continuous mapping of $[a, b]$ into $R^{k}$ and $\mathbf{f}$ is differentiable in $(a, b)$. Then there exists $x \in(a, b)$ such that

$$
|\mathbf{f}(b)-\mathbf{f}(a)| \leq(b-a)|\mathbf{f}^{\prime}(x)|
$$

","Put $\mathbf{z}=\mathbf{f}(b)-\mathbf{f}(a)$, and define  $$ \varphi(t)=\mathbf{z} \cdot \mathbf{f}(t) \quad(a \leq t \leq b) $$  Then $\varphi$ is a real-valued continuous function on $[a, b]$ which is differentiable in $(a, b)$. The mean value theorem shows therefore that  $$ \varphi(b)-\varphi(a)=(b-a) \varphi^{\prime}(x)=(b-a) \mathbf{z} \cdot \mathbf{f}^{\prime}(x) $$  for some $x \in(a, b)$. On the other hand,  $$ \varphi(b)-\varphi(a)=\mathbf{z} \cdot \mathbf{f}(b)-\mathbf{z} \cdot \mathbf{f}(a)=\mathbf{z} \cdot \mathbf{z}=|\mathbf{z}|^{2} $$  The Schwarz inequality now gives  $$ |\mathbf{z}|^{2}=(b-a)|\mathbf{z} \cdot \mathbf{f}^{\prime}(x)| \leq(b-a)|\mathbf{z}||\mathbf{f}^{\prime}(x)| $$  Hence $|\mathbf{z}| \leq(b-a)|\mathbf{f}^{\prime}(x)|$, which is the desired conclusion.",
5.2,"Let $f$ be defined on $[a, b]$. If $f$ is differentiable at a point $x \in[a, b]$, then $f$ is continuous at $x$.","As $t \rightarrow x$, we have, by Theorem 4.4,  $$ f(t)-f(x)=\frac{f(t)-f(x)}{t-x} \cdot(t-x) \rightarrow f^{\prime}(x) \cdot 0=0 $$","Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f+g)(x)=A+B$;Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f g)(x)=A B$;Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}\left(\frac{f}{g}\right)(x)=\frac{A}{B}$, if $B \neq 0$;  ;  ; "
5.3 (a),"Suppose $f$ and $g$ are defined on $[a, b]$ and are differentiable at a point $x \in[a, b]$. Then $f+g$ is differentiable at $x$, and

$$(f+g)^{\prime}(x)=f^{\prime}(x)+g^{\prime}(x)$$;","$(a)$ is clear, by Theorem 4.4.","Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f+g)(x)=A+B$;Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}(f g)(x)=A B$;Suppose $E \subset X$, a metric space, $p$ is a limit point of $E, f$ and $g$ are complex functions on $E$, and

$$
\lim _{x \rightarrow p} f(x)=A, \quad \lim _{x \rightarrow p} g(x)=B
$$

Then $\lim _{x \rightarrow p}\left(\frac{f}{g}\right)(x)=\frac{A}{B}$, if $B \neq 0$;  ;  ; "
5.3 (b),"Suppose $f$ and $g$ are defined on $[a, b]$ and are differentiable at a point $x \in[a, b]$. Then $f g$ is differentiable at $x$, and

$$(f g)^{\prime}(x)=f^{\prime}(x) g(x)+f(x) g^{\prime}(x)$$;","Let $h=f g$. Then  $$ h(t)-h(x)=f(t)[g(t)-g(x)]+g(x)[f(t)-f(x)] $$  If we divide this by $t-x$ and note that $f(t) \rightarrow f(x)$ as $t \rightarrow x$ (Theorem 5.2), (b) follows.","Let $f$ be defined on $[a, b]$. If $f$ is differentiable at a point $x \in[a, b]$, then $f$ is continuous at $x$.;  ;  ; "
5.3 (c),"Suppose $f$ and $g$ are defined on $[a, b]$ and are differentiable at a point $x \in[a, b]$, and assume that $g(x) \neq 0$. Then $f / g$ is differentiable at $x$, and

$$\left(\frac{f}{g}\right)^{\prime}(x)=\frac{g(x) f^{\prime}(x)-g^{\prime}(x) f(x)}{g^{2}(x)}$$;","Next, let $h=f / g$. Then  $$ \frac{h(t)-h(x)}{t-x}=\frac{1}{g(t) g(x)}\left[g(x) \frac{f(t)-f(x)}{t-x}-f(x) \frac{g(t)-g(x)}{t-x}\right] \text {. } $$  Letting $t \rightarrow x$, and applying Theorems 4.4 and 5.2, we obtain $(c)$.",
5.5,"Suppose $f$ is continuous on $[a, b], f^{\prime}(x)$ exists at some point $x \in[a, b], g$ is defined on an interval $I$ which contains the range of $f$, and $g$ is differentiable at the point $f(x)$. If

$$
h(t)=g(f(t)) \quad(a \leq t \leq b)
$$

then $h$ is differentiable at $x$, and

$$
h^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x)
$$

","Let $y=f(x)$. By the definition of the derivative, we have  $$ \begin{aligned} f(t)-f(x) & =(t-x)\left[f^{\prime}(x)+u(t)\right], \\ g(s)-g(y) & =(s-y)\left[g^{\prime}(y)+v(s)\right], \end{aligned} $$  where $t \in[a, b], s \in I$, and $u(t) \rightarrow 0$ as $t \rightarrow x, v(s) \rightarrow 0$ as $s \rightarrow y$. Let $s=f(t)$.  Using first (5) and then (4), we obtain  or, if $t \neq x$,  $$ \begin{aligned} h(t)-h(x) & =g(f(t))-g(f(x)) \\ & =[f(t)-f(x)] \cdot\left[g^{\prime}(y)+v(s)\right] \\ & =(t-x) \cdot\left[f^{\prime}(x)+u(t)\right] \cdot\left[g^{\prime}(y)+v(s)\right] \end{aligned} $$  $$ \frac{h(t)-h(x)}{t-x}=\left[g^{\prime}(y)+v(s)\right] \cdot\left[f^{\prime}(x)+u(t)\right] $$  Letting $t \rightarrow x$, we see that $s \rightarrow y$, by the continuity of $f$, so that the right side of (6) tends to $g^{\prime}(y) f^{\prime}(x)$, which gives (3).",
5.9,"If $f$ and $g$ are continuous real functions on $[a, b]$ which are differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
[f(b)-f(a)] g^{\prime}(x)=[g(b)-g(a)] f^{\prime}(x)
$$

","Put  $$ h(t)=[f(b)-f(a)] g(t)-[g(b)-g(a)] f(t) \quad(a \leq t \leq b) $$  Then $h$ is continuous on $[a, b], h$ is differentiable in $(a, b)$, and  $$ h(a)=f(b) g(a)-f(a) g(b)=h(b) $$  To prove the theorem, we have to show that $h^{\prime}(x)=0$ for some $x \in(a, b)$.  If $h$ is constant, this holds for every $x \in(a, b)$. If $h(t)>h(a)$ for some $t \in(a, b)$, let $x$ be a point on $[a, b]$ at which $h$ attains its maximum  (Theorem 4.16). By (12), $x \in(a, b)$, and Theorem 5.8 shows that $h^{\prime}(x)=0$. If $h(t)<h(a)$ for some $t \in(a, b)$, the same argument applies if we choose for $x$ a point on $[a, b]$ where $h$ attains its minimum.",; ;  ;  ; 
6.10,"Suppose $f$ is bounded on $[a, b], f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$.","Let $\varepsilon>0$ be given. Put $M=\sup |f(x)|$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[u_{j}, v_{j}\right] \subset$ $[a, b]$ such that the sum of the corresponding differences $\alpha\left(v_{j}\right)-\alpha\left(u_{j}\right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap(a, b)$ lies in the interior of some $\left[u_{j}, v_{j}\right]$. Remove the segments $\left(u_{j}, v_{j}\right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta>0$ such that $|f(s)-f(t)|<\varepsilon$ if $s \in K, t \in K,|s-t|<\delta$. Now form a partition $P=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$ of $[a, b]$, as follows: Each $u_{j}$ occurs in $P$. Each $v_{j}$ occurs in $P$. No point of any segment $\left(u_{j}, v_{j}\right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_{j}$, then $\Delta x_{i}<\delta$. Note that $M_{i}-m_{i} \leq 2 M$ for every $i$, and that $M_{i}-m_{i} \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_{j}$. Hence, as in the proof of Theorem 6.8, $$ U(P, f, \alpha)-L(P, f, \alpha) \leq[\alpha(b)-\alpha(a)] \varepsilon+2 M \varepsilon $$ Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. Note: If $f$ and $\alpha$ have a common point of discontinuity, then $f$ need not be in $\mathscr{R}(\alpha)$. Exercise 3 shows this.","If $f$ is continuous on $[a, b]$ then $f \in \mathscr{R}(\alpha)$ on $[a, b]$; $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$;  ;  ; "
6.11,"Suppose $f \in \mathscr{R}(\alpha)$ on $[a, b], m \leq f \leq M, \phi$ is continuous on $[m, M]$, and $h(x)=\phi(f(x))$ on $[a, b]$. Then $h \in \mathscr{R}(\alpha)$ on $[a, b]$","Choose $\varepsilon>0$. Since $\phi$ is uniformly continuous on $[m, M]$, there exists $\delta>0$ such that $\delta<\varepsilon$ and $|\phi(s)-\phi(t)|<\varepsilon$ if $|s-t| \leq \delta$ and $s, t \in[m, M]$. Since $f \in \mathscr{R}(\alpha)$, there is a partition $P=\left\{x_{0}, x_{1}, \ldots, x_{n}\right\}$ of $[a, b]$ such that $U(P, f, \alpha)-L(P, f, \alpha)<\delta^{2}$. Let $M_{i}, m_{i}$ have the same meaning as in Definition 6.1, and let $M_{i}^{*}, m_{i}^{*}$ be the analogous numbers for $h$. Divide the numbers $1, \ldots, n$ into two classes: $i \in A$ if $M_{i}-m_{i}<\delta, i \in B$ if $M_{i}-m_{i} \geq \delta$. For $i \in A$, our choice of $\delta$ shows that $M_{i}^{*}-m_{i}^{*} \leq \varepsilon$. For $i \in B, $M_{i}^{*}-m_{i}^{*} \leq 2 K$, where $K=\sup |\phi(t)|, m \leq t \leq M$. By (18), we have $\delta \sum_{i \in B} \Delta \alpha_{i} \leq \sum_{i \in B}\left(M_{i}-m_{i}\right) \Delta \alpha_{i}<\delta^{2}$ so that $\sum_{i \in B} \Delta \alpha_{i}<\delta$. It follows that $U(P, h, \alpha)-L(P, h, \alpha) \leq \varepsilon[\alpha(b)-\alpha(a)]+2 K \delta<\varepsilon[\alpha(b)-\alpha(a)+2 K]$ Since $\varepsilon$ was arbitrary, Theorem 6.6 implies that $h \in \mathscr{R}(\alpha)$.","$f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$;  ; For a given interval $[a, b]$ and a bounded real function $f$ defined on $[a, b]$: By a partition $P$ of $[a, b]$ we mean a finite set of points $x_{0}, x_{1}, \ldots, x_{n}$, where

$$
a=x_{0} \leq x_{1} \leq \cdots \leq x_{n-1} \leq x_{n}=b .
$$

We write

$$
\Delta x_{i}=x_{i}-x_{i-1} \quad(i=1, \ldots, n)
$$

Now suppose $f$ is a bounded real function defined on $[a, b]$. Corresponding to each partition $P$ of $[a, b]$ we put

$$
\begin{array}{rlrl}
M_{i} & =\sup f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
m_{i} & =\inf f(x) & & \left(x_{i-1} \leq x \leq x_{i}\right), \\
U(P, f) & =\sum_{i=1}^{n} M_{i} \Delta x_{i}, & \\
L(P, f) & =\sum_{i=1}^{n} m_{i} \Delta x_{i}, &
\end{array}
$$

and finally

$$
\begin{aligned}
& \int_{a}^{b} f d x=\inf U(P, f), \\
& \int_{a}^{b} f d x=\sup L(P, f),
\end{aligned}
$$

where the inf and the sup are taken over all partitions $P$ of $[a, b] . The left members of (1) and (2) are called the upper and lower Riemann integrals of $f$ over $[a, b]$, respectively.

If the upper and lower integrals are equal, we say that $f$ is Riemannintegrable on $[a, b]$, we write $f \in \mathscr{R}$ (that is, $\mathscr{R}$ denotes the set of Riemannintegrable functions), and we denote the common value of (1) and (2) by

$$
\int_{a}^{b} f d x
$$

or by

$$
\int_{a}^{b} f(x) d x .
$$

This is the Riemann integral of $f$ over $[a, b]$. Since $f$ is bounded, there exist two numbers, $m$ and $M$, such that

$$
m \leq f(x) \leq M \quad(a \leq x \leq b)
$$

Hence, for every $P$,

$$
m(b-a) \leq L(P, f) \leq U(P, f) \leq M(b-a)
$$

so that the numbers $L(P, f)$ and $U(P, f)$ form a bounded set. This shows that the upper and lower integrals are defined for every bounded function $f$. The question of their equality, and hence the question of the integrability of $f$, is a more delicate one. Instead of investigating it separately for the Riemann integral, we shall immediately consider a more general situation.;  ; "
6.12 (a),"If $f_{1} \in \mathscr{R}(\alpha)$ and $f_{2} \in \mathscr{R}(\alpha)$ on $[a, b]$, then $f_{1}+f_{2} \in \mathscr{R}(\alpha)$, $c f \in \mathscr{R}(\alpha)$ for every constant $c$, and $\int_{a}^{b}\left(f_{1}+f_{2}\right) d \alpha=\int_{a}^{b} f_{1} d \alpha+\int_{a}^{b} f_{2} d \alpha$ and $\int_{a}^{b} c f d \alpha=c \int_{a}^{b} f d \alpha$","If $f=f_{1}+f_{2}$ and $P$ is any partition of $[a, b]$, we have $L\left(P, f_{1}, \alpha\right)+L\left(P, f_{2}, \alpha\right) \leq L(P, f, \alpha) \leq U(P, f, \alpha) \leq U\left(P, f_{1}, \alpha\right)+U\left(P, f_{2}, \alpha\right)$. If $f_{1} \in \mathscr{R}(\alpha)$ and $f_{2} \in \mathscr{R}(\alpha)$, let $\varepsilon>0$ be given. There are partitions $P_{j}$ $(j=1,2)$ such that $U\left(P_{j}, f_{j}, \alpha\right)-L\left(P_{j}, f_{j}, \alpha\right)<\varepsilon$. These inequalities persist if $P_{1}$ and $P_{2}$ are replaced by their common refinement $P$. Then (20) implies $U(P, f, \alpha)-L(P, f, \alpha)<2 \varepsilon$ which proves that $f \in \mathscr{R}(\alpha)$. With this same $P$ we have $U\left(P, f_{J}, \alpha\right)<\int f_{j} d \alpha+\varepsilon \quad(j=1,2)$ hence (20) implies $\int f d \alpha \leq U(P, f, \alpha)<\int f_{1} d \alpha+\int f_{2} d \alpha+2 \varepsilon \text {. }$ Since $\varepsilon$ was arbitrary, we conclude that $\int f d \alpha \leq \int f_{1} d \alpha+\int f_{2} d \alpha$ If we replace $f_{1}$ and $f_{2}$ in (21) by $-f_{1}$ and $-f_{2}$, the inequality is reversed, and the equality is proved.",
6.12 (c),"If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c<b$, then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$, and $\int_{a}^{c} f d \alpha+\int_{c}^{b} f d \alpha=\int_{a}^{b} f d \alpha","In part $(c)$ the point is that (by passing to refinements) we may restrict ourselves to partitions which contain the point $c$, in approximating $\int f d \alpha$.",
6.13 (a),"If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$, then $f g \in \mathscr{R}(\alpha)$","If we take $\phi(t)=t^{2}$, Theorem 6.11 shows that $f^{2} \in \mathscr{R}(\alpha)$ if $f \in \mathscr{R}(a)$. The identity  $$ 4 f g=(f+g)^{2}-(f-g)^{2} $$  completes the proof of $(a)$.","Suppose $f \in \mathscr{R}(\alpha)$ on $[a, b], m \leq f \leq M, \phi$ is continuous on $[m, M]$, and $h(x)=\phi(f(x))$ on $[a, b]$. Then $h \in \mathscr{R}(\alpha)$ on $[a, b]$;  ;  ; "
6.13 (b),"If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$, then $|f| \in \mathscr{R}(\alpha)$ and $\left|\int_{a}^{b} f d \alpha\right| \leq \int_{a}^{b}|f| d \alpha$","If we take $\phi(t)=|t|$, Theorem 6.11 shows similarly that $|f| \in \mathscr{R}(\alpha)$. Choose $c= \pm 1$, so that  Then  $$ c \int f d \alpha \geq 0 $$  $$ \left|\int f d \alpha\right|=c \int f d \alpha=\int c f d \alpha \leq \int|f| d \alpha $$  since $c f \leq|f|$.","Suppose $f \in \mathscr{R}(\alpha)$ on $[a, b], m \leq f \leq M, \phi$ is continuous on $[m, M]$, and $h(x)=\phi(f(x))$ on $[a, b]$. Then $h \in \mathscr{R}(\alpha)$ on $[a, b]$;  ;  ; "
6.15,"If $a<s<b, f$ is bounded on $[a, b], f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then

$$
\int_{a}^{b} f d \alpha=f(s)
$$
","Consider partitions $P=\left\{x_{0}, x_{1}, x_{2}, x_{3}\right\}$, where $x_{0}=a$, and $x_{1}=s<x_{2}<x_{3}=b$. Then  $$ U(P, f, \alpha)=M_{2}, \quad L(P, f, \alpha)=m_{2} $$  Since $f$ is continuous at $s$, we see that $M_{2}$ and $m_{2}$ converge to $f(s)$ as $x_{2} \rightarrow s$.",
6.16,"Suppose $c_{n} \geq 0$ for $1,2,3, \ldots, \Sigma c_{n}$ converges, $\left\{s_{n}\right\}$ is a sequence of distinct points in $(a, b)$, and

$$
\alpha(x)=\sum_{n=1}^{\infty} c_{n} I\left(x-s_{n}\right)
$$

Let $f$ be continuous on $[a, b]$. Then

$$
\int_{a}^{b} f d \alpha=\sum_{n=1}^{\infty} c_{n} f\left(s_{n}\right)
$$","The comparison test shows that the series (22) converges for every $x$. Its sum $\alpha(x)$ is evidently monotonic, and $\alpha(a)=0, \alpha(b)=\Sigma c_{n}$. (This is the type of function that occurred in Remark 4.31.)  Let $\varepsilon>0$ be given, and choose $N$ so that  $$ \sum_{N+1}^{\infty} c_{n}<\varepsilon $$  Put  $$ \alpha_{1}(x)=\sum_{n=1}^{N} c_{n} I\left(x-s_{n}\right), \quad \alpha_{2}(x)=\sum_{N+1}^{\infty} c_{n} I\left(x-s_{n}\right) $$  By Theorems 6.12 and 6.15,  $$ \int_{a}^{b} f d \alpha_{1}=\sum_{i=1}^{N} c_{n} f\left(s_{n}\right) $$  Since $\alpha_{2}(b)-\alpha_{2}(a)<\varepsilon$,  $$ \left|\int_{a}^{b} f d \alpha_{2}\right| \leq M \varepsilon $$  where $M=\sup |f(x)|$. Since $\alpha=\alpha_{1}+\alpha_{2}$, it follows from (24) and (25) that  (26)  $$ \left|\int_{a}^{b} f d \alpha-\sum_{i=1}^{N} c_{n} f\left(s_{n}\right)\right| \leq M \varepsilon $$  If we let $N \rightarrow \infty$, we obtain (23).",
6.17,"Assume $\alpha$ increases monotonically and $\alpha^{\prime} \in \mathscr{R}$ on $[a, b]$. Let $f$ be a bounded real function on $[a, b]$. Then $f \in \mathscr{R}(\alpha)$ if and only if $f \alpha^{\prime} \in \mathscr{R$. In that case

$$
\int_{a}^{b} f d \alpha=\int_{a}^{b} f(x) \alpha^{\prime}(x) d x
$$
","Let $\varepsilon>0$ be given and apply Theorem 6.6 to $\alpha^{\prime}$ : There is a partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ such that  $$ U\left(P, \alpha^{\prime}\right)-L\left(P, \alpha^{\prime}\right)<\varepsilon . $$  The mean value theorem furnishes points $t_{i} \in\left[x_{i-1}, x_{i}\right]$ such that  $$ \Delta \alpha_{i}=\alpha^{\prime}\left(t_{i}\right) \Delta x_{i} $$  for $i=1, \ldots, n$. If $s_{i} \in\left[x_{i-1}, x_{i}\right]$, then  $$ \sum_{i=1}^{n}\left|\alpha^{\prime}\left(s_{i}\right)-\alpha^{\prime}\left(t_{i}\right)\right| \Delta x_{i}<\varepsilon $$ by (28) and Theorem 6.7(b). Put $M=\sup |f(x)|$. Since  $$ \sum_{i=1}^{n} f\left(s_{i}\right) \Delta \alpha_{i}=\sum_{i=1}^{n} f\left(s_{i}\right) \alpha^{\prime}\left(t_{i}\right) \Delta x_{i} $$ it follows from (29) that  $$ \left|\sum_{i=1}^{n} f\left(s_{i}\right) \Delta \alpha_{i}-\sum_{i=1}^{n} f\left(s_{i}\right) \alpha^{\prime}\left(s_{i}\right) \Delta x_{i}\right| \leq M \varepsilon $$ In particular,  $$ \sum_{i=1}^{n} f\left(s_{i}\right) \Delta \alpha_{i} \leq U\left(P, f \alpha^{\prime}\right)+M \varepsilon $$ for all choices of $s_{i} \in\left[x_{i-1}, x_{i}\right]$, so that  $$ U(P, f, \alpha) \leq U\left(P, f \alpha^{\prime}\right)+M \varepsilon $$ The same argument leads from (30) to  $$ U\left(P, f \alpha^{\prime}\right) \leq U(P, f, \alpha)+M \varepsilon $$ $$ \left|U(P, f, \alpha)-U\left(P, f \alpha^{\prime}\right)\right| \leq M \varepsilon $$ Now note that (28) remains true if $P$ is replaced by any refinement. Hence (31) also remains true. We conclude that  $$ \left|\bar{\int}_{a}^{b} f d \alpha-\bar{\int}_{a}^{b} f(x) \alpha^{\prime}(x) d x\right| \leq M \varepsilon $$ But $\varepsilon$ is arbitrary. Hence  $$ \int_{a}^{b} f d \alpha=\bar{\int}_{a}^{b} f(x) \alpha^{\prime}(x) d x $$ for any bounded $f$. The equality of the lower integrals follows from (30) in exactly the same way. The theorem follows.","$f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$; If (13) holds for some $P$ and some $\varepsilon$, then (13) holds (with the same $\varepsilon$ ) for every refinement of $P$.;If (13) holds for $P=\left\{x_{0}, \ldots, x_{n}\right\}$ and if $s_{i}, t_{i}$ are arbitrary points in $\left[x_{i-1}, x_{i}\right]$, then

$$
\sum_{i=1}^{n}\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \Delta \alpha_{i}<\varepsilon
$$;If $f \in \mathscr{R}(\alpha)$ and the hypotheses of (b) hold, then

$$
\left|\sum_{i=1}^{n} f\left(t_{i}\right) \Delta \alpha_{i}-\int_{a}^{b} f d \alpha\right|<\varepsilon
$$;  ;  ; "
6.19,"(change of variable) Suppose $\varphi$ is a strictly increasing continuous function that maps an interval $[A, B]$ onto $[a, b]$. Suppose $\alpha$ is monotonically increasing on $[a, b]$ and $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Define $\beta$ and $g$ on $[A, B]$ by

$$
\beta(y)=\alpha(\varphi(y)), \quad g(y)=f(\varphi(y)) .
$$

Then $g \in \mathscr{R}(\beta)$ and

$$
\int_{A}^{B} g d \beta=\int_{a}^{b} f d \alpha
$$
","To each partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ corresponds a partition $Q=\left\{y_{0}, \ldots, y_{n}\right\}$ of $[A, B]$, so that $x_{i}=\varphi\left(y_{i}\right)$. All partitions of $[A, B]$ are obtained in this way. Since the values taken by $f$ on $\left[x_{i-1}, x_{i}\right]$ are exactly the same as those taken by $g$ on $\left[y_{i-1}, y_{i}\right]$, we see that  $$ U(Q, g, \beta)=U(P, f, \alpha), \quad L(Q, g, \beta)=L(P, f, \alpha) $$  Since $f \in \mathscr{R}(\alpha), P$ can be chosen so that both $U(P, f, \alpha)$ and $L(P, f, \alpha)$ are close to $\int f d \alpha$. Hence (38), combined with Theorem 6.6 , shows that $g \in \mathscr{R}(\beta)$ and that (37) holds. This completes the proof.","$f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$;  ;  ; "
6.20,"Let $f \in \mathscr{R}$ on $[a, b]$. For $a \leq x \leq b$, put

$$
F(x)=\int_{a}^{x} f(t) d t
$$

Then $F$ is continuous on $[a, b]$; furthermore, if $f$ is continuous at a point $x_{0}$ of $[a, b]$, then $F$ is differentiable at $x_{0}$, and

$$
F^{\prime}\left(x_{0}\right)=f\left(x_{0}\right) .
$$","Since $f \in \mathscr{R}, f$ is bounded. Suppose $|f(t)| \leq M$ for $a \leq t \leq b$. If $a \leq x<y \leq b$, then  $$ |F(y)-F(x)|=\left|\int_{x}^{y} f(t) d t\right| \leq M(y-x) $$  by Theorem $6.12(c)$ and $(d)$. Given $\varepsilon>0$, we see that  $$ |F(y)-F(x)|<\varepsilon, $$  provided that $|y-x|<\varepsilon / M$. This proves continuity (and, in fact, uniform continuity) of $F$.  Now suppose $f$ is continuous at $x_{0}$. Given $\varepsilon>0$, choose $\delta>0$ such that  $$ \left|f(t)-f\left(x_{0}\right)\right|<\varepsilon $$  if $\left|t-x_{0}\right|<\delta$, and $a \leq t \leq b$. Hence, if  $$ x_{0}-\delta<s \leq x_{0} \leq t<x_{0}+\delta \quad \text { and } \quad a \leq s<t \leq b $$  we have, by Theorem $6.12(d)$,  $$ \left|\frac{F(t)-F(s)}{t-s}-f\left(x_{0}\right)\right|=\left|\frac{1}{t-s} \int_{s}^{t}\left[f(u)-f\left(x_{0}\right)\right] d u\right|<\varepsilon . $$  It follows that $F^{\prime}\left(x_{0}\right)=f\left(x_{0}\right)$.",
6.21,"The fundamental theorem of calculus: If $f \in \mathscr{R}$ on $[a, b]$ and if there is a differentiable function $F$ on $[a, b]$ such that $F^{\prime}=f$, then

$$
\int_{a}^{b} f(x) d x=F(b)-F(a)
$$

","Let $\varepsilon>0$ be given. Choose a partition $P=\left\{x_{0}, \ldots, x_{n}\right\}$ of $[a, b]$ so that $U(P, f)-L(P, f)<\varepsilon$. The mean value theorem furnishes points $t_{i} \in\left[x_{i-1}, x_{i}\right]$ such that  $$ F\left(x_{i}\right)-F\left(x_{i-1}\right)=f\left(t_{i}\right) \Delta x_{i} $$  for $i=1, \ldots, n$. Thus  $$ \sum_{i=1}^{n} f\left(t_{i}\right) \Delta x_{i}=F(b)-F(a) $$  It now follows from Theorem 6.7(c) that  $$ \left|F(b)-F(a)-\int_{a}^{b} f(x) d x\right|<\varepsilon $$  Since this holds for every $\varepsilon>0$, the proof is complete.","If (13) holds for some $P$ and some $\varepsilon$, then (13) holds (with the same $\varepsilon$ ) for every refinement of $P$.;If (13) holds for $P=\left\{x_{0}, \ldots, x_{n}\right\}$ and if $s_{i}, t_{i}$ are arbitrary points in $\left[x_{i-1}, x_{i}\right]$, then

$$
\sum_{i=1}^{n}\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \Delta \alpha_{i}<\varepsilon
$$;If $f \in \mathscr{R}(\alpha)$ and the hypotheses of (b) hold, then

$$
\left|\sum_{i=1}^{n} f\left(t_{i}\right) \Delta \alpha_{i}-\int_{a}^{b} f d \alpha\right|<\varepsilon
$$;  ;  ; "
6.22,"Suppose $F$ and $G$ are differentiable functions on $[a, b], F^{\prime}=f \in \mathscr{R}$, and $G^{\prime}=g \in \mathscr{R}$. Then

$$
\int_{a}^{b} F(x) g(x) d x=F(b) G(b)-F(a) G(a)-\int_{a}^{b} f(x) G(x) d x .
$$

","Put $H(x)=F(x) G(x)$ and apply Theorem 6.21 to $H$ and its derivative. Note that $H^{\prime} \in \mathscr{R}$, by Theorem 6.13.","The fundamental theorem of calculus: If $f \in \mathscr{R}$ on $[a, b]$ and if there is a differentiable function $F$ on $[a, b]$ such that $F^{\prime}=f$, then

$$
\int_{a}^{b} f(x) d x=F(b)-F(a)
$$

; If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$, then $f g \in \mathscr{R}(\alpha)$;If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$, then $|f| \in \mathscr{R}(\alpha)$ and $\left|\int_{a}^{b} f d \alpha\right| \leq \int_{a}^{b}|f| d \alpha$;  ;  ; "
6.24,"If $\mathbf{f}$ and $\mathbf{F}$ map $[a, b]$ into $R^{k}$, if $\mathbf{f} \in \mathscr{R}$ on $[a, b]$, and if $\mathbf{F}^{\prime}=\mathbf{f}$, then

$$
\int_{a}^{b} \mathbf{f}(t) d t=\mathbf{F}(b)-\mathbf{F}(a) .
$$

","The analogue of Theorem $6.13(b)$ offers some new features, however, at least in its proof.",
6.25,"If $\mathbf{f}$ maps $[a, b]$ into $R^{k}$ and if $\mathbf{f} \in \mathscr{R}(\alpha)$ for some monotonically increasing function $\alpha$ on $[a, b]$, then $|\mathbf{f}| \in \mathscr{R}(\alpha)$, and

$$
\left|\int_{a}^{b} \mathbf{f} d \alpha\right| \leq \int_{a}^{b}|\mathbf{f}| d \alpha
$$
","If $f_{1}, \ldots, f_{k}$ are the components of $\mathbf{f}$, then  $$ |\mathbf{f}|=\left(f_{1}^{2}+\cdots+f_{k}^{2}\right)^{1 / 2} . $$  By Theorem 6.11, each of the functions $f_{i}^{2}$ belongs to $\mathscr{R}(\alpha)$; hence so does their sum. Since $x^{2}$ is a continuous function of $x$, Theorem 4.17 shows that the square-root function is continuous on $[0, M]$, for every real $M$. If we apply Theorem 6.11 once more, (41) shows that $|\mathbf{f}| \in \mathscr{R}(\alpha)$.  To prove (40), put $\mathbf{y}=\left(y_{1}, \ldots, y_{k}\right)$, where $y_{j}=\int f_{j} d \alpha$. Then we have $\mathbf{y}=\int \mathbf{f} d \alpha$, and  $$ |\mathbf{y}|^{2}=\sum y_{i}^{2}=\sum y_{j} \int f_{j} d \alpha=\int\left(\sum y_{j} f_{j}\right) d \alpha . $$  By the Schwarz inequality,  $$ \sum y_{J} f_{j}(t) \leq|\mathbf{y}||\mathbf{f}(t)| \quad(a \leq t \leq b) $$  hence Theorem 6.12(b) implies  $$ |\mathbf{y}|^{2} \leq|\mathbf{y}| \int|\mathbf{f}| d \alpha $$  If $\mathbf{y}=\mathbf{0}$, (40) is trivial. If $\mathbf{y} \neq \mathbf{0}$, division of (43) by $|\mathbf{y}|$ gives (40).","Suppose $f \in \mathscr{R}(\alpha)$ on $[a, b], m \leq f \leq M, \phi$ is continuous on $[m, M]$, and $h(x)=\phi(f(x))$ on $[a, b]$. Then $h \in \mathscr{R}(\alpha)$ on $[a, b]$; Suppose $f$ is a continuous 1-1 mapping of a compact metric space $X$ onto a metric space $Y$. Then the inverse mapping $f^{-1}$ defined on $Y$ by

$$
f^{-1}(f(x))=x \quad(x \in X)
$$
is a continuous mapping of $Y$ onto $X.; Suppose $f \in \mathscr{R}(\alpha)$ on $[a, b], m \leq f \leq M, \phi$ is continuous on $[m, M]$, and $h(x)=\phi(f(x))$ on $[a, b]$. Then $h \in \mathscr{R}(\alpha)$ on $[a, b]$; If $f_{1} \in \mathscr{R}(\alpha)$ and $f_{2} \in \mathscr{R}(\alpha)$ on $[a, b]$, then $f_{1}+f_{2} \in \mathscr{R}(\alpha)$, $c f \in \mathscr{R}(\alpha)$ for every constant $c$, and $\int_{a}^{b}\left(f_{1}+f_{2}\right) d \alpha=\int_{a}^{b} f_{1} d \alpha+\int_{a}^{b} f_{2} d \alpha$ and $\int_{a}^{b} c f d \alpha=c \int_{a}^{b} f d \alpha$;If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c<b$, then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$, and $\int_{a}^{c} f d \alpha+\int_{c}^{b} f d \alpha=\int_{a}^{b} f d \alpha;  ;  ; "
6.27,"If $\gamma^{\prime}$ is continuous on $[a, b]$, then $\gamma$ is rectifiable, and

$$
\Lambda(\gamma)=\int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t
$$

","If $a \leq x_{i-1}<x_{i} \leq b$, then  $$ \left|\gamma\left(x_{i}\right)-\gamma\left(x_{i-1}\right)\right|=\left|\int_{x_{i-1}}^{x_{i}} \gamma^{\prime}(t) d t\right| \leq \int_{x_{i-1}}^{x_{i}}\left|\gamma^{\prime}(t)\right| d t $$  $$ \Lambda(P, \gamma) \leq \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t $$  for every partition $P$ of $[a, b]$. Consequently,  $$ \Lambda(\gamma) \leq \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t $$  To prove the opposite inequality, let $\varepsilon>0$ be given. Since $\gamma^{\prime}$ is uniformly continuous on $[a, b]$, there exists $\delta>0$ such that  $$ \left|\gamma^{\prime}(s)-\gamma^{\prime}(t)\right|<\varepsilon \quad \text { if }|s-t|<\delta \text {. } $$  Let $P=\left\{x_{0}, \ldots, x_{n}\right\}$ be a partition of $[a, b]$, with $\Delta x_{i}<\delta$ for all $i$. If $x_{i-1} \leq t \leq x_{i}$, it follows that  $$ \left|\gamma^{\prime}(t)\right| \leq\left|\gamma^{\prime}\left(x_{i}\right)\right|+\varepsilon . $$  $$ \begin{aligned} \int_{x_{l-1}}^{x_{i}}\left|\gamma^{\prime}(t)\right| d t & \leq\left|\gamma^{\prime}\left(x_{i}\right)\right| \Delta x_{i}+\varepsilon \Delta x_{i} \\ & =\left|\int_{x_{t-1}}^{x_{i}}\left[\gamma^{\prime}(t)+\gamma^{\prime}\left(x_{i}\right)-\gamma^{\prime}(t)\right] d t\right|+\varepsilon \Delta x_{i} \\ & \leq\left|\int_{x_{l-1}}^{x_{i}} \gamma^{\prime}(t) d t\right|+\left|\int_{x_{i-1}}^{x_{i}}\left[\gamma^{\prime}\left(x_{i}\right)-\gamma^{\prime}(t)\right] d t\right|+\varepsilon \Delta x_{i} \\ & \leq\left|\gamma\left(x_{i}\right)-\gamma\left(x_{i-1}\right)\right|+2 \varepsilon \Delta x_{i} . \end{aligned} $$  If we add these inequalities, we obtain  Since $\varepsilon$ was arbitrary,  $$ \begin{aligned} \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t & \leq \Lambda(P, \gamma)+2 \varepsilon(b-a) \\ & \leq \Lambda(\gamma)+2 \varepsilon(b-a) . \end{aligned} $$  $$ \int_{a}^{b}\left|\gamma^{\prime}(t)\right| d t \leq \Lambda(\gamma) . $$  This completes the proof.",
6.4,"If $P^{*}$ is a refinement of $P$, then

and

$$
L(P, f, \alpha) \leq L\left(P^{*}, f, \alpha\right)
$$

$$
U\left(P^{*}, f, \alpha\right) \leq U(P, f, \alpha) .
$$
","To prove (9), suppose first that $P^{*}$ contains just one point more than $P$. Let this extra point be $x^{*}$, and suppose $x_{i-1}<x^{*}<x_{i}$, where $x_{i-1}$ and $x_{i}$ are two consecutive points of $P$. Put  $$ \begin{array}{ll} w_{1}=\inf f(x) & \left(x_{i-1} \leq x \leq x^{*}\right) \\ w_{2}=\inf f(x) & \left(x^{*} \leq x \leq x_{i}\right) \end{array} $$  Clearly $w_{1} \geq m_{i}$ and $w_{2} \geq m_{i}$, where, as before,  Hence  $$ m_{i}=\inf f(x) \quad\left(x_{i-1} \leq x \leq x_{i}\right) $$  $$ \begin{aligned} & L\left(P^{*}, f, \alpha\right)-L(P, f, \alpha) \\ & \quad=w_{1}\left[\alpha\left(x^{*}\right)-\alpha\left(x_{i-1}\right)\right]+w_{2}\left[\alpha\left(x_{i}\right)-\alpha\left(x^{*}\right)\right]-m_{i}\left[\alpha\left(x_{i}\right)-\alpha\left(x_{i-1}\right)\right] \\ & \quad=\left(w_{1}-m_{i}\right)\left[\alpha\left(x^{*}\right)-\alpha\left(x_{i-1}\right)\right]+\left(w_{2}-m_{i}\right)\left[\alpha\left(x_{i}\right)-\alpha\left(x^{*}\right)\right] \geq 0 . \end{aligned} $$  If $P^{*}$ contains $k$ points more than $P$, we repeat this reasoning $k$ times, and arrive at (9). The proof of (10) is analogous.",
6.6,"$f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$","For every $P$ we have  $$ L(P, f, \alpha) \leq \int_{-} f d \alpha \leq \bar{\int} f d \alpha \leq U(P, f, \alpha) $$  Thus (13) implies  $$ 0 \leq \bar{\int} f d \alpha-\int f d \alpha<\varepsilon . $$  Hence, if (13) can be satisfied for every $\varepsilon>0$, we have  $$ \bar{\int} f d \alpha=\int f d \alpha $$  that is, $f \in \mathscr{R}(\alpha)$.  Conversely, suppose $f \in \mathscr{R}(\alpha)$, and let $\varepsilon>0$ be given. Then there exist partitions $P_{1}$ and $P_{2}$ such that  $$ \begin{aligned} & U\left(P_{2}, f, \alpha\right)-\int f d \alpha<\frac{\varepsilon}{2} \\ & \int f d \alpha-L\left(P_{1}, f, \alpha\right)<\frac{\varepsilon}{2} \end{aligned} $$  We choose $P$ to be the common refinement of $P_{1}$ and $P_{2}$. Then Theorem 6.4 , together with (14) and (15), shows that  $$ U(P, f, \alpha) \leq U\left(P_{2}, f, \alpha\right)<\int f d \alpha+\frac{\varepsilon}{2}<L\left(P_{1}, f, \alpha\right)+\varepsilon \leq L(P, f, \alpha)+\varepsilon $$  so that (13) holds for this partition $P$.","If $P^{*}$ is a refinement of $P$, then

and

$$
L(P, f, \alpha) \leq L\left(P^{*}, f, \alpha\right)
$$

$$
U\left(P^{*}, f, \alpha\right) \leq U(P, f, \alpha) .
$$
;  ;  ; "
6.7 (a),"If (13) holds for some $P$ and some $\varepsilon$, then (13) holds (with the same $\varepsilon$ ) for every refinement of $P$.",Theorem 6.4 implies $(a)$.,"If $P^{*}$ is a refinement of $P$, then

and

$$
L(P, f, \alpha) \leq L\left(P^{*}, f, \alpha\right)
$$

$$
U\left(P^{*}, f, \alpha\right) \leq U(P, f, \alpha) .
$$
;  ;  ; "
6.7 (b),"If (13) holds for $P=\left\{x_{0}, \ldots, x_{n}\right\}$ and if $s_{i}, t_{i}$ are arbitrary points in $\left[x_{i-1}, x_{i}\right]$, then

$$
\sum_{i=1}^{n}\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \Delta \alpha_{i}<\varepsilon
$$","Under the assumptions made in $(b)$, both $f\left(s_{i}\right)$ and $f\left(t_{i}\right)$ lie in $\left[m_{i}, M_{i}\right]$, so that $\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \leq M_{i}-m_{i}$. Thus  $$ \sum_{i=1}^{n}\left|f\left(s_{i}\right)-f\left(t_{i}\right)\right| \Delta \alpha_{i} \leq U(P, f, \alpha)-L(P, f, \alpha) $$  which proves $(b)$.",
6.7 (c),"If $f \in \mathscr{R}(\alpha)$ and the hypotheses of (b) hold, then

$$
\left|\sum_{i=1}^{n} f\left(t_{i}\right) \Delta \alpha_{i}-\int_{a}^{b} f d \alpha\right|<\varepsilon
$$","The obvious inequalities  $$ L(P, f, \alpha) \leq \int f d \alpha \leq U(P, f, \alpha) $$  and  $$ L(P, f, \alpha) \leq \sum f\left(t_{i}\right) \Delta \alpha_{i} \leq U(P, f, \alpha) $$  prove $(c)$.",
6.8,"If $f$ is continuous on $[a, b]$ then $f \in \mathscr{R}(\alpha)$ on $[a, b]$","Let $\varepsilon>0$ be given. Choose $\eta>0$ so that  $$ [\alpha(b)-\alpha(a)] \eta<\varepsilon $$  Since $f$ is uniformly continuous on $[a, b]$ (Theorem 4.19), there exists a $\delta>0$ such that  $$ |f(x)-f(t)|<\eta $$  if $x \in[a, b], t \in[a, b]$, and $|x-t|<\delta$.  If $P$ is any partition of $[a, b]$ such that $\Delta x_{i}<\delta$ for all $i$, then (16) implies that  $$ M_{i}-m_{i} \leq \eta \quad(i-1, \ldots, n) $$  and therefore  $$ \begin{aligned} U(P, f, \alpha)-L(P, f, \alpha) & =\sum_{i=1}^{n}\left(M_{i}-m_{i}\right) \Delta \alpha_{i} \\ \leq \eta \sum_{i=1}^{n} \Delta \alpha_{i} & =\eta[\alpha(b)-\alpha(a)]<\varepsilon \end{aligned} $$  By Theorem 6.6, $f \in \mathscr{R}(\alpha)$.","Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$: Then $f$ is uniformly continuous on $X$.; $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$;  ;  ; "
6.9,"If $f$ is monotonic on $[a, b]$, and if $\alpha$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$. (We still assume, of course, that $\alpha$ is monotonic.)","Let $\varepsilon>0$ be given. For any positive integer $n$, choose a partition such that  $$ \Delta \alpha_{i}=\frac{\alpha(b)-\alpha(a)}{n} \quad(i=1, \ldots, n) $$  This is possible since $\alpha$ is continuous (Theorem 4.23).  We suppose that $f$ is monotonically increasing (the proof is analogous in the other case). Then  so that  $$ M_{i}=f\left(x_{i}\right), \quad m_{i}=f\left(x_{i-1}\right) \quad(i=1, \ldots, n) $$  $$ \begin{aligned} U(P, f, \alpha)-L(P, f, \alpha) & =\frac{\alpha(b)-\alpha(a)}{n} \sum_{i=1}^{n}\left[f\left(x_{i}\right)-f\left(x_{i-1}\right)\right] \\ & =\frac{\alpha(b)-\alpha(a)}{n} \cdot[f(b)-f(a)]<\varepsilon \end{aligned} $$  if $n$ is taken large enough. By Theorem 6.6, $f \in \mathscr{R}(\alpha)$.","Let $f$ be a continuous real function on the interval $[a, b]$. If $f(a)<f(b)$ and if $c$ is a number such that $f(a)<c<f(b)$, then there exists a point $x \in(a, b)$ such that $f(x)=c$.; $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon>0$ there exists a partition $P$ such that

$$
U(P, f, \alpha)-L(P, f, \alpha)<\varepsilon
$$;  ;  ; "
7.10,"Suppose $\left\{f_{n}\right\}$ is a sequence of functions defined on $E$, and suppose

$$
\left|f_{n}(x)\right| \leq M_{n} \quad(x \in E, n=1,2,3, \ldots)
$$

Then $\Sigma f_{n}$ converges uniformly on $E$ if $\Sigma M_{n}$ converges.","If $\Sigma M_{n}$ converges, then, for arbitrary $\varepsilon>0$,  $$ \left|\sum_{i=n}^{m} f_{i}(x)\right| \leq \sum_{i=n}^{m} M_{i} \leq \varepsilon \quad(x \in E) $$  provided $m$ and $n$ are large enough. Uniform convergence now follows from Theorem 7.8.","The sequence of functions $\left\{f_{n}\right\}$, defined on $E$, converges uniformly on $E$ if and only if for every $\varepsilon>0$ there exists an integer $N$ such that $m \geq N$, $n \geq N, x \in E$ implies

$$
\left|f_{n}(x)-f_{m}(x)\right| \leq \varepsilon
$$;  ;  ; "
7.11,"Suppose $f_{n} \rightarrow f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that

$$
\lim _{t \rightarrow x} f_{n}(t)=A_{n} \quad(n=1,2,3, \ldots)
$$

Then $\left\{A_{n}\right\}$ converges, and

$$
\lim _{t \rightarrow x} f(t)=\lim _{n \rightarrow \infty} A_{n} .
$$

In other words, the conclusion is that

$$
\lim _{t \rightarrow x} \lim _{n \rightarrow \infty} f_{n}(t)=\lim _{n \rightarrow \infty} \lim _{t \rightarrow x} f_{n}(t)
$$","Let $\varepsilon>0$ be given. By the uniform convergence of $\left\{f_{n}\right\}$, there exists $N$ such that $n \geq N, m \geq N, t \in E$ imply  $$ \left|f_{n}(t)-f_{m}(t)\right| \leq \varepsilon $$  Letting $t \rightarrow x$ in (18), we obtain  $$ \left|A_{n}-A_{m}\right| \leq \varepsilon $$  for $n \geq N, m \geq N$, so that $\left\{A_{n}\right\}$ is a Cauchy sequence and therefore converges, say to $A$.  Next,  $$ |f(t)-A| \leq\left|f(t)-f_{n}(t)\right|+\left|f_{n}(t)-A_{n}\right|+\left|A_{n}-A\right| . $$  We first choose $n$ such that  $$ \left|f(t)-f_{n}(t)\right| \leq \frac{\varepsilon}{3} $$  for all $t \in E$ (this is possible by the uniform convergence), and such that  $$ \left|A_{n}-A\right| \leq \frac{\varepsilon}{3} \text {. } $$  Then, for this $n$, we choose a neighborhood $V$ of $x$ such that  $$ \left|f_{n}(t)-A_{n}\right| \leq \frac{\varepsilon}{3} $$  if $t \in V \cap E, t \neq x$.  Substituting the inequalities (20) to (22) into (19), we see that  $$ |f(t)-A| \leq \varepsilon $$  provided $t \in V \cap E, t \neq x$. This is equivalent to (16).",
7.12,"If $\left\{f_{n}\right\}$ is a sequence of continuous functions on $E$, and if $f_{n} \rightarrow f$ uniformly on $E$, then $f$ is continuous on $E$.",This very important result is an immediate corollary of Theorem 7.11.,"Suppose $f_{n} \rightarrow f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that

$$
\lim _{t \rightarrow x} f_{n}(t)=A_{n} \quad(n=1,2,3, \ldots)
$$

Then $\left\{A_{n}\right\}$ converges, and

$$
\lim _{t \rightarrow x} f(t)=\lim _{n \rightarrow \infty} A_{n} .
$$

In other words, the conclusion is that

$$
\lim _{t \rightarrow x} \lim _{n \rightarrow \infty} f_{n}(t)=\lim _{n \rightarrow \infty} \lim _{t \rightarrow x} f_{n}(t)
$$;  ;  ; "
7.13,"Suppose $K$ is compact, and

(a) $\left\{f_{n}\right\}$ is a sequence of continuous functions on $K$,

(b) $\left\{f_{n}\right\}$ converges pointwise to a continuous function $f$ on $K$,

(c) $f_{n}(x) \geq f_{n+1}(x)$ for all $x \in K, n=1,2,3, \ldots$

Then $f_{n} \rightarrow f$ uniformly on $K$.","Put $g_{n}=f_{n}-f$. Then $g_{n}$ is continuous, $g_{n} \rightarrow 0$ pointwise, and $g_{n} \geq g_{n+1}$. We have to prove that $g_{n} \rightarrow 0$ uniformly on $K$.  Let $\varepsilon>0$ be given. Let $K_{n}$ be the set of all $x \in K$ with $g_{n}(x) \geq \varepsilon$.  Since $g_{n}$ is continuous, $K_{n}$ is closed (Theorem 4.8), hence compact (Theorem 2.35). Since $g_{n} \geq g_{n+1}$, we have $K_{n} \supset K_{n+1}$. Fix $x \in K$. Since $g_{n}(x) \rightarrow 0$, we see that $x \notin K_{n}$ if $n$ is sufficiently large. Thus $x \notin \bigcap K_{n}$. In other words, $\bigcap K_{n}$ is empty. Hence $K_{N}$ is empty for some $N$ (Theorem 2.36). It follows that $0 \leq g_{n}(x)<\varepsilon$ for all $x \in K$ and for all $n \geq N$. This proves the theorem.","A mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous on $X$ if and only if $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$; Closed subsets of compact sets are compact.; If $\left\{K_{\alpha}\right\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\left\{K_{\alpha}\right\}$ is nonempty, then $\cap K_{\alpha}$ is nonempty.;  ;  ; "
7.15,The above metric makes $\mathscr{C}(X)$ into a complete metric space.,"Let $\left\{f_{n}\right\}$ be a Cauchy sequence in $\mathscr{C}(X)$. This means that to each $\varepsilon>0$ corresponds an $N$ such that $\left\|f_{n}-f_{m}\right\|<\varepsilon$ if $n \geq N$ and $m \geq N$. It follows (by Theorem 7.8) that there is a function $f$ with domain $X$ to which $\left\{f_{n}\right\}$ converges uniformly. By Theorem 7.12, $f$ is continuous. Moreover, $f$ is bounded, since there is an $n$ such that $\left|f(x)-f_{n}(x)\right|<1$ for all $x \in X$, and $f_{n}$ is bounded.  Thus $f \in \mathscr{C}(X)$, and since $f_{n} \rightarrow f$ uniformly on $X$, we have $\left\|f-f_{n}\right\| \rightarrow 0$ as $n \rightarrow \infty$.","The sequence of functions $\left\{f_{n}\right\}$, defined on $E$, converges uniformly on $E$ if and only if for every $\varepsilon>0$ there exists an integer $N$ such that $m \geq N$, $n \geq N, x \in E$ implies

$$
\left|f_{n}(x)-f_{m}(x)\right| \leq \varepsilon
$$; If $\left\{f_{n}\right\}$ is a sequence of continuous functions on $E$, and if $f_{n} \rightarrow f$ uniformly on $E$, then $f$ is continuous on $E$.;  ;  ; "
7.16,"Let $\alpha$ be monotonically increasing on $[a, b]$. Suppose $f_{n} \in \mathscr{R}(\alpha)$ on $[a, b]$, for $n=1,2,3, \ldots$, and suppose $f_{n} \rightarrow f$ uniformly on $[a, b]$. Then $f \in \mathscr{R}(\alpha)$ on $[a, b]$, and

$$
\int_{a}^{b} f d \alpha=\lim _{n \rightarrow \infty} \int_{a}^{b} f_{n} d \alpha
$$

(The existence of the limit is part of the conclusion.)","It suffices to prove this for real $f_{n}$. Put  $$ \varepsilon_{n}=\sup \left|f_{n}(x)-f(x)\right|, $$  the supremum being taken over $a \leq x \leq b$. Then  $$ f_{n}-\varepsilon_{n} \leq f \leq f_{n}+\varepsilon_{n}, $$  so that the upper and lower integrals of $f$ (see Definition 6.2) satisfy  Hence  $$ \int_{a}^{b}\left(f_{n}-\varepsilon_{n}\right) d \alpha \leq \int_{-} f d \alpha \leq \bar{\int} f d \alpha \leq \int_{a}^{b}\left(f_{n}+\varepsilon_{n}\right) d \alpha $$  $$ 0 \leq \bar{\int} f d \alpha-\int_{-} f d \alpha \leq 2 \varepsilon_{n}[\alpha(b)-\alpha(a)] . $$  Since $\varepsilon_{n} \rightarrow 0$ as $n \rightarrow \infty$ (Theorem 7.9), the upper and lower integrals of $f$ are equal.  Thus $f \in \mathscr{R}(\alpha)$. Another application of (25) now yields  $$ \left|\int_{a}^{b} f d \alpha-\int_{a}^{b} f_{n} d \alpha\right| \leq \varepsilon_{n}[\alpha(b)-\alpha(a)] $$  This implies (23).",";  ; Let $\alpha$ be a monotonically increasing function on $[a, b]$. For any real function $f$ which is bounded on $[a, b]$ and each partition $P$ of $[a, b]$, define

$$
\Delta \alpha_{i}=\alpha\left(x_{i}\right)-\alpha\left(x_{i-1}\right)
$$

with $\Delta \alpha_{i} \geq 0$, and

$$
\begin{aligned}
& U(P, f, \alpha)=\sum_{i=1}^{n} M_{i} \Delta \alpha_{i} \\
& L(P, f, \alpha)=\sum_{i=1}^{n} m_{i} \Delta \alpha_{i}
\end{aligned}
$$

where $M_{i}, m_{i}$ are the supremum and infimum of $f$ on the $i$th subinterval, respectively. Then define

$$
\begin{aligned}
& \int_{a}^{b} f d \alpha=\inf U(P, f, \alpha) \\
& \int_{a}^{b} f d \alpha=\sup L(P, f, \alpha)
\end{aligned}
$$

If the left members of these definitions are equal, denote their common value by

$$
\int_{a}^{b} f d \alpha
$$

or sometimes by

$$
\int_{a}^{b} f(x) d \alpha(x)
$$

This is the Riemann-Stieltjes integral of $f$ with respect to $\alpha$, over $[a, b]$. If this integral exists, i.e., if the definitions are equal, say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$.;  ; "
7.17,"Suppose $\left\{f_{n}\right\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\left\{f_{n}\left(x_{0}\right)\right\}$ converges for some point $x_{0}$ on $[a, b]$. If $\left\{f_{n}^{\prime}\right\}$ converges uniformly on $[a, b]$, then $\left\{f_{n}\right\}$ converges uniformly on $[a, b]$, to a function $f$, and

$$
f^{
\prime}(x)=\lim _{n \rightarrow \infty} f_{n}^{
\prime}(x) \quad(a \leq x \leq b)
$$","Let $\varepsilon>0$ be given. Choose $N$ such that $n \geq N, m \geq N$, implies  $$ \left|f_{n}\left(x_{0}\right)-f_{m}\left(x_{0}\right)\right|<\frac{\varepsilon}{2} $$  and  $$ \left|f_{n}^{ \prime}(t)-f_{m}^{ \prime}(t)\right|<\frac{\varepsilon}{2(b-a)} \quad(a \leq t \leq b) $$  If we apply the mean value theorem 5.19 to the function $f_{n}-f_{m},(29)$ shows that  $$ \left|f_{n}(x)-f_{m}(x)-f_{n}(t)+f_{m}(t)\right| \leq \frac{|x-t| \varepsilon}{2(b-a)} \leq \frac{\varepsilon}{2} $$  for any $x$ and $t$ on $[a, b]$, if $n \geq N, m \geq N$. The inequality  $$ \left|f_{n}(x)-f_{m}(x)\right| \leq\left|f_{n}(x)-f_{m}(x)-f_{n}\left(x_{0}\right)+f_{m}\left(x_{0}\right)\right|+\left|f_{n}\left(x_{0}\right)-f_{m}\left(x_{0}\right)\right| $$  implies, by (28) and (30), that  $$ \left|f_{n}(x)-f_{m}(x)\right|<\varepsilon \quad(a \leq x \leq b, n \geq N, m \geq N) $$ so that $\left\{f_{n}\right\}$ converges uniformly on $[a, b]",
7.18,There exists a real continuous function on the real line which is nowhere differentiable.,"Define  $$ \varphi(x)=|x| \quad(-1 \leq x \leq 1) $$  and extend the definition of $\varphi(x)$ to all real $x$ by requiring that  $$ \varphi(x+2)=\varphi(x) $$  Then, for all $s$ and $t$,  $$ |\varphi(s)-\varphi(t)| \leq|s-t| $$  In particular, $\varphi$ is continuous on $R^{1}$. Define  $$ f(x)=\sum_{n=0}^{\infty}\left(\frac{3}{4}\right)^{n} \varphi\left(4^{n} x\right) $$  Since $0 \leq \varphi \leq 1$, Theorem 7.10 shows that the series (37) converges uniformly on $R^{1}$. By Theorem 7.12, $f$ is continuous on $R^{1}$.  Now fix a real number $x$ and a positive integer $m$. Put  $$ \delta_{m}= \pm \frac{1}{2} \cdot 4^{-m} $$  where the sign is so chosen that no integer lies between $4^{m} x$ and $4^{m}\left(x+\delta_{m}\right)$. This can be done, since $4^{m}\left|\delta_{m}\right|=\frac{1}{2}$. Define  $$ \gamma_{n}=\frac{\varphi\left(4^{n}\left(x+\delta_{m}\right)\right)-\varphi\left(4^{n} x\right)}{\delta_{m}} $$  When $n>m$, then $4^{n} \delta_{m}$ is an even integer, so that $\gamma_{n}=0$. When $0 \leq n \leq m$, (36) implies that $\left|\gamma_{n}\right| \leq 4^{n}$.  Since $\left|\gamma_{m}\right|=4^{m}$, we conclude that  $$ \begin{aligned} \left|\frac{f\left(x+\delta_{m}\right)-f(x)}{\delta_{m}}\right| & =\left|\sum_{n=0}^{m}\left(\frac{3}{4}\right)^{n} \gamma_{n}\right| \\ & \geq 3^{m}-\sum_{n=0}^{m-1} 3^{n} \\ & =\frac{1}{2}\left(3^{m}+1\right) . \end{aligned} $$  As $m \rightarrow \infty, \delta_{m} \rightarrow 0$. It follows that $f$ is not differentiable at $x$.","Suppose $\left\{f_{n}\right\}$ is a sequence of functions defined on $E$, and suppose

$$
\left|f_{n}(x)\right| \leq M_{n} \quad(x \in E, n=1,2,3, \ldots)
$$

Then $\Sigma f_{n}$ converges uniformly on $E$ if $\Sigma M_{n}$ converges.; If $\left\{f_{n}\right\}$ is a sequence of continuous functions on $E$, and if $f_{n} \rightarrow f$ uniformly on $E$, then $f$ is continuous on $E$.;  ;  ; "
7.23,"If $\left\{f_{n}\right\}$ is a pointwise bounded sequence of complex functions on a countable set $E$, then $\left\{f_{n}\right\}$ has a subsequence $\left\{f_{n_{k}}\right\}$ such that $\left\{f_{n_{k}}(x)\right\}$ converges for every $x \in E$.","Let $\left\{x_{i}\right\}, i=1,2,3, \ldots$, be the points of $E$, arranged in a sequence. Since $\left\{f_{n}\left(x_{1}\right)\right\}$ is bounded, there exists a subsequence, which we shall denote by $\left\{f_{1, k}\right\}$, such that $\left\{f_{1, k}\left(x_{1}\right)\right\}$ converges as $k \rightarrow \infty$.  Let us now consider sequences $S_{1}, S_{2}, S_{3}, \ldots$, which we represent by the array  $$ \begin{array}{cccccc} S_{1}: & f_{1,1} & f_{1,2} & f_{1,3} & f_{1,4} & \cdots \\ S_{2}: & f_{2,1} & f_{2,2} & f_{2,3} & f_{2,4} & \cdots \\ S_{3}: & f_{3,1} & f_{3,2} & f_{3,3} & f_{3,4} & \cdots \end{array} $$  and which have the following properties:  (a) $S_{n}$ is a subsequence of $S_{n-1}$, for $n=2,3,4, \ldots$  (b) $\left\{f_{n, k}\left(x_{n}\right)\right\}$ converges, as $k \rightarrow \infty$ (the boundedness of $\left\{f_{n}\left(x_{n}\right)\right\}$ makes it possible to choose $S_{n}$ in this way);  (c) The order in which the functions appear is the same in each sequence; i.e., if one function precedes another in $S_{1}$, they are in the same relation in every $S_{n}$, until one or the other is deleted. Hence, when going from one row in the above array to the next below, functions may move to the left but never to the right.  We now go down the diagonal of the array; i.e., we consider the sequence  $$ S: f_{1,1} \quad f_{2,2} \quad f_{3,3} \quad f_{4,4} \cdots \text {. } $$  By $(c)$, the sequence $S$ (except possibly its first $n-1$ terms) is a subsequence of $S_{n}$, for $n=1,2,3, \ldots$. Hence $(b)$ implies that $\left\{f_{n, n}\left(x_{i}\right)\right\}$ converges, as $n \rightarrow \infty$, for every $x_{i} \in E$.",
7.24,"If $K$ is a compact metric space, if $f_{n} \in \mathscr{C}(K)$ for $n=1,2,3, \ldots$, and if $\left\{f_{n}\right\}$ converges uniformly on $K$, then $\left\{f_{n}\right\}$ is equicontinuous on $K.","Let $\varepsilon>0$ be given. Since $\left\{f_{n}\right\}$ converges uniformly, there is an integer $N$ such that  $$ \left\|f_{n}-f_{N}\right\|<\varepsilon \quad(n>N) $$  (See Definition 7.14.) Since continuous functions are uniformly continuous on compact sets, there is a $\delta>0$ such that  $$ \left|f_{i}(x)-f_{i}(y)\right|<\varepsilon $$  if $1 \leq i \leq N$ and $d(x, y)<\delta$.  If $n>N$ and $d(x, y)<\delta$, it follows that  $$ \left|f_{n}(x)-f_{n}(y)\right| \leq\left|f_{n}(x)-f_{N}(x)\right|+\left|f_{N}(x)-f_{N}(y)\right|+\left|f_{N}(y)-f_{n}(y)\right|<3 \varepsilon $$  In conjunction with (43), this proves the theorem.",
7.25 (a),"If $K$ is compact, if $f_{n} \in \mathscr{C}(K)$ for $n=1,2,3, \ldots$, and if $\left\{f_{n}\right\}$ is pointwise bounded and equicontinuous on $K$, then $\left\{f_{n}\right\}$ is uniformly bounded on $K$","Let $\varepsilon>0$ be given and choose $\delta>0$, in accordance with Definition 7.22 , so that  $$ \left|f_{n}(x)-f_{n}(y)\right|<\varepsilon $$  for all $n$, provided that $d(x, y)<\delta$.  Since $K$ is compact, there are finitely many points $p_{1}, \ldots, p_{r}$ in $K$ such that to every $x \in K$ corresponds at least one $p_{i}$ with $d\left(x, p_{i}\right)<\delta$. Since $\left\{f_{n}\right\}$ is pointwise bounded, there exist $M_{i}<\infty$ such that $\left|f_{n}\left(p_{i}\right)\right|<M_{i}$ for all $n$. If $M=\max \left(M_{1}, \ldots, M_{r}\right)$, then $\left|f_{n}(x)\right|<M+\varepsilon$ for every $x \in K$. This proves $(a)$.",
7.25 (b),"If $K$ is compact, if $f_{n} \in \mathscr{C}(K)$ for $n=1,2,3, \ldots$, and if $\left\{f_{n}\right\}$ is pointwise bounded and equicontinuous on $K$, then $\left\{f_{n}\right\}$ contains a uniformly convergent subsequence.","Let $E$ be a countable dense subset of $K$. (For the existence of such a set $E$, see Exercise 25, Chap. 2.) Theorem 7.23 shows that $\left\{f_{n}\right\}$ has a subsequence $\left\{f_{n_{l}}\right\}$ such that $\left\{f_{n_{t}}(x)\right\}$ converges for every $x \in E$.  Put $f_{n_{i}}=g_{i}$, to simplify the notation. We shall prove that $\left\{g_{i}\right\}$ converges uniformly on $K$.  Let $\varepsilon>0$, and pick $\delta>0$ as in the beginning of this proof. Let $V(x, \delta)$ be the set of all $y \in K$ with $d(x, y)<\delta$. Since $E$ is dense in $K$, and $K$ is compact, there are finitely many points $x_{1}, \ldots, x_{m}$ in $E$ such that  $$ K \subset V\left(x_{1}, \delta\right) \cup \cdots \cup V\left(x_{m}, \delta\right) $$  Since $\left\{g_{i}(x)\right\}$ converges for every $x \in E$, there is an integer $N$ such that  $$ \left|g_{l}\left(x_{s}\right)-g_{j}\left(x_{s}\right)\right|<\varepsilon $$  whenever $i \geq N, j \geq N, 1 \leq s \leq m$.  If $x \in K$, (45) shows that $x \in V\left(x_{s}, \delta\right)$ for some $s$, so that  $$ \left|g_{l}(x)-g_{l}\left(x_{s}\right)\right|<\varepsilon $$  for every $i$. If $i \geq N$ and $j \geq N$, it follows from (46) that  $$ \begin{aligned} \left|g_{i}(x)-g_{j}(x)\right| & \leq\left|g_{i}(x)-g_{i}\left(x_{s}\right)\right|+\left|g_{i}\left(x_{s}\right)-g_{j}\left(x_{s}\right)\right|+\left|g_{j}\left(x_{s}\right)-g_{j}(x)\right| \\ & <3 \varepsilon . \end{aligned} $$  This completes the proof.","If $\left\{f_{n}\right\}$ is a pointwise bounded sequence of complex functions on a countable set $E$, then $\left\{f_{n}\right\}$ has a subsequence $\left\{f_{n_{k}}\right\}$ such that $\left\{f_{n_{k}}(x)\right\}$ converges for every $x \in E$.;  ;  ; "
7.26,"If $f$ is a continuous complex function on $[a, b]$, there exists a sequence of polynomials $P_{n}$ such that

$$
\lim _{n \rightarrow \infty} P_{n}(x)=f(x)
$$

uniformly on $[a, b]$. If $f$ is real, the $P_{n}$ may be taken real.","We may assume, without loss of generality, that $[a, b]=[0,1]$.  We may also assume that $f(0)=f(1)=0$. For if the theorem is proved for this case, consider  $$ g(x)=f(x)-f(0)-x[f(1)-f(0)] \quad(0 \leq x \leq 1) $$  Here $g(0)=g(1)=0$, and if $g$ can be obtained as the limit of a uniformly convergent sequence of polynomials, it is clear that the same is true for $f$, since $f-g$ is a polynomial.  Furthermore, we define $f(x)$ to be zero for $x$ outside $[0,1]$. Then $f$ is uniformly continuous on the whole line.  We put  $$ Q_{n}(x)=c_{n}\left(1-x^{2}\right)^{n} \quad(n=1,2,3, \ldots) $$  where $c_{n}$ is chosen so that  $$ \int_{-1}^{1} Q_{n}(x) d x=1 \quad(n=1,2,3, \ldots) $$  We need some information about the order of magnitude of $c_{n}$. Since  $$ \begin{aligned} \int_{-1}^{1}\left(1-x^{2}\right)^{n} d x=2 \int_{0}^{1}\left(1-x^{2}\right)^{n} d x & \geq 2 \int_{0}^{1 / \sqrt{n}}\left(1-x^{2}\right)^{n} d x \\ & \geq 2 \int_{0}^{1 / \sqrt{n}}\left(1-n x^{2}\right)^{n} d x \\ & =\frac{4}{3 \sqrt{n}} \\ & >\frac{1}{\sqrt{n}}, \end{aligned} $$  it follows from (48) that  $$ c_{n}<\sqrt{n} $$ The inequality $\left(1-x^{2}\right)^{n} \geq 1-n x^{2}$ which we used above is easily shown to be true by considering the function  $$ \left(1-x^{2}\right)^{n}-1+n x^{2} $$  which is zero at $x=0$ and whose derivative is positive in $(0,1)$.  For any $\delta>0$, (49) implies  $$ Q_{n}(x) \leq \sqrt{n}\left(1-\delta^{2}\right)^{n} \quad(\delta \leq|x| \leq 1) $$  so that $Q_{n} \rightarrow 0$ uniformly in $\delta \leq|x| \leq 1$.  Now set  $$ P_{n}(x)=\int_{-1}^{1} f(x+t) Q_{n}(t) d t \quad(0 \leq x \leq 1) $$  Our assumptions about $f$ show, by a simple change of variable, that  $$ P_{n}(x)=\int_{-x}^{1-x} f(x+t) Q_{n}(t) d t=\int_{0}^{1} f(t) Q_{n}(t-x) d t $$  and the last integral is clearly a polynomial in $x$. Thus $\left\{P_{n}\right\}$ is a sequence of polynomials, which are real if $f$ is real.  Given $\varepsilon>0$, we choose $\delta>0$ such that $|y-x|<\delta$ implies  $$ |f(y)-f(x)|<\frac{\varepsilon}{2} $$ Let $M=\sup |f(x)|$. Using (48), (50), and the fact that $Q_{n}(x) \geq 0$, we see that for $0 \leq x \leq 1$,  $$ \begin{aligned} \left|P_{n}(x)-f(x)\right| & =\left|\int_{-1}^{1}[f(x+t)-f(x)] Q_{n}(t) d t\right| \\ & \leq \int_{-1}^{1}|f(x+t)-f(x)| Q_{n}(t) d t \\ & \leq 2 M \int_{-1}^{-\delta} Q_{n}(t) d t+\frac{\varepsilon}{2} \int_{-\delta}^{\delta} Q_{n}(t) d t+2 M \int_{\delta}^{1} Q_{n}(t) d t \\ & \leq 4 M \sqrt{n}\left(1-\delta^{2}\right)^{n}+\frac{\varepsilon}{2} \\ & <\varepsilon \end{aligned} $$ for all large enough $n$, which proves the theorem.",
7.29,Let $\mathscr{B}$ be the uniform closure of an algebra $\mathscr{A}$ of bounded functions. Then $\mathscr{B}$ is a uniformly closed algebra.,"If $f \in \mathscr{B}$ and $g \in \mathscr{B}$, there exist uniformly convergent sequences $\left\{f_{n}\right\},\left\{g_{n}\right\}$ such that $f_{n} \rightarrow f, g_{n} \rightarrow g$ and $f_{n} \in \mathscr{A}, g_{n} \in \mathscr{A}$. Since we are dealing with bounded functions, it is easy to show that  $$ f_{n}+g_{n} \rightarrow f+g, \quad f_{n} g_{n} \rightarrow f g, \quad c f_{n} \rightarrow c f $$  where $c$ is any constant, the convergence being uniform in each case.  Hence $f+g \in \mathscr{B}, f g \in \mathscr{B}$, and $c f \in \mathscr{B}$, so that $\mathscr{B}$ is an algebra.  By Theorem 2.27, $\mathscr{B}$ is (uniformly) closed.",;  ;  ; 
7.31,"Suppose $\mathscr{A}$ is an algebra of functions on a set $E, \mathscr{A}$ separates points on $E$, and $\mathscr{A}$ vanishes at no point of $E$. Suppose $x_{1}, x_{2}$ are distinct points of $E$, and $c_{1}, c_{2}$ are constants (real if $\mathscr{A}$ is a real algebra). Then $\mathscr{A}$ contains a function $f$ such that

$$
f\left(x_{1}\right)=c_{1}, \quad f\left(x_{2}\right)=c_{2} .
$$","The assumptions show that $\mathscr{A}$ contains functions $g, h$, and $k$ such that  $$ g\left(x_{1}\right) \neq g\left(x_{2}\right), \quad h\left(x_{1}\right) \neq 0, \quad k\left(x_{2}\right) \neq 0 $$ Put  $$ u=g k-g\left(x_{1}\right) k, \quad v=g h-g\left(x_{2}\right) h . $$ Then $u \in \mathscr{A}, v \in \mathscr{A}, u\left(x_{1}\right)=v\left(x_{2}\right)=0, u\left(x_{2}\right) \neq 0$, and $v\left(x_{1}\right) \neq 0$. Therefore  $$ f=\frac{c_{1} v}{v\left(x_{1}\right)}+\frac{c_{2} u}{u\left(x_{2}\right)} $$ has the desired properties.",
7.33,"Suppose $\mathscr{A}$ is a self-adjoint algebra of complex continuous functions on a compact set $K, \mathscr{A}$ separates points on $K$, and $\mathscr{A}$ vanishes at no point of $K$. Then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all complex continuous functions on $K$. In other words, $\mathscr{A}$ is dense $\mathscr{C}(K)$.","Let $\mathscr{A}_{R}$ be the set of all real functions on $K$ which belong to $\mathscr{A}$. If $f \in \mathscr{A}$ and $f=u+i v$, with $u, v$ real, then $2 u=f+f$, and since $\mathscr{A}$ is self-adjoint, we see that $u \in \mathscr{A}_{R}$. If $x_{1} \neq x_{2}$, there exists $f \in \mathscr{A}$ such that $f\left(x_{1}\right)=1, f\left(x_{2}\right)=0$; hence $0=u\left(x_{2}\right) \neq u\left(x_{1}\right)=1$, which shows that $\mathscr{A}_{\mathrm{R}}$ separates points on $K$. If $x \in K$, then $g(x) \neq 0$ for some $g \in \mathscr{A}$, and there is a complex number $\lambda$ such that $\lambda g(x)>0$; if $f=\lambda g, f=u+i v$, it follows that $u(x)>0$; hence $\mathscr{A}_{R}$ vanishes at no point of $K$.  Thus $\mathscr{A}_{R}$ satisfies the hypotheses of Theorem 7.32. It follows that every real continuous function on $K$ lies in the uniform closure of $\mathscr{A}_{R}$, hence lies in $\mathscr{B}$. If $f$ is a complex continuous function on $K, f=u+i v$, then $u \in \mathscr{B}, v \in \mathscr{B}$, hence $f \in \mathscr{B}$. This completes the proof.",;  ;  ; 
7.8,"The sequence of functions $\left\{f_{n}\right\}$, defined on $E$, converges uniformly on $E$ if and only if for every $\varepsilon>0$ there exists an integer $N$ such that $m \geq N$, $n \geq N, x \in E$ implies

$$
\left|f_{n}(x)-f_{m}(x)\right| \leq \varepsilon
$$","Suppose $\left\{f_{n}\right\}$ converges uniformly on $E$, and let $f$ be the limit function. Then there is an integer $N$ such that $n \geq N, x \in E$ implies  so that  $$ \left|f_{n}(x)-f(x)\right| \leq \frac{\varepsilon}{2}, $$  $$ \left|f_{n}(x)-f_{m}(x)\right| \leq\left|f_{n}(x)-f(x)\right|+\left|f(x)-f_{m}(x)\right| \leq \varepsilon $$  if $n \geq N, m \geq N, x \in E$.  Conversely, suppose the Cauchy condition holds. By Theorem 3.11, the sequence $\left\{f_{n}(x)\right\}$ converges, for every $x$, to a limit which we may call $f(x)$. Thus the sequence $\left\{f_{n}\right\}$ converges on $E$, to $f$. We have to prove that the convergence is uniform.  Let $\varepsilon>0$ be given, and choose $N$ such that (13) holds. Fix $n$, and let $m \rightarrow \infty$ in (13). Since $f_{m}(x) \rightarrow f(x)$ as $m \rightarrow \infty$, this gives  $$ \left|f_{n}(x)-f(x)\right| \leq \varepsilon $$  for every $n \geq N$ and every $x \in E$, which completes the proof.","In any metric space $X$, every convergent sequence is a Cauchy sequence.;If $X$ is a compact metric space and if $\left\{p_{n}\right\}$ is a Cauchy sequence in $X$, then $\left\{p_{n}\right\}$ converges to some point of $X$.;In $R^{k}$, every Cauchy sequence converges.;  ;  ; "
8.11,"Let $\left\{\phi_{n}\right\}$ be orthonormal on $[a, b]$. Let

$$
s_{n}(x)=\sum_{m=1}^{n} c_{m} \phi_{m}(x)
$$

be the nth partial sum of the Fourier series of $f$, and suppose

$$
t_{n}(x)=\sum_{m=1}^{n} \gamma_{m} \phi_{m}(x)
$$

Then

$$
\int_{a}^{b}\left|f-s_{n}\right|^{2} d x \leq \int_{a}^{b}\left|f-t_{n}\right|^{2} d x
$$

and equality holds if and only if

$$
\gamma_{m}=c_{m} \quad(m=1, \ldots, n)
$$

That is to say, among all functions $t_{n}, s_{n}$ gives the best possible mean square approximation to $f$.","Let $\int$ denote the integral over $[a, b], \Sigma$ the sum from 1 to $n$. Then  $$ \int f \bar{t}_{n}=\int f \sum \bar{\gamma}_{m} \bar{\phi}_{m}=\sum c_{m} \bar{\gamma}_{m} $$  by the definition of $\left\{c_{m}\right\}$,  $$ \int\left|t_{n}\right|^{2}=\int t_{n} \bar{t}_{n}=\int \sum \gamma_{m} \phi_{m} \sum \bar{\gamma}_{k} \bar{\phi}_{k}=\sum\left|\gamma_{m}\right|^{2} $$ since $\left\{\phi_{m}\right\}$ is orthonormal, and so  $$ \begin{aligned} \int\left|f-t_{n}\right|^{2} & =\int|f|^{2}-\int f \bar{t}_{n}-\int f t_{n}+\int\left|t_{n}\right|^{2} \\ & =\int|f|^{2}-\sum c_{m} \bar{\gamma}_{m}-\sum \bar{c}_{m} \gamma_{m}+\sum \gamma_{m} \bar{\gamma}_{m} \\ & =\int|f|^{2}-\sum\left|c_{m}\right|^{2}+\sum\left|\gamma_{m}-c_{m}\right|^{2} \end{aligned} $$ which is evidently minimized if and only if $\gamma_{m}=c_{m}$.  Putting $\gamma_{m}=c_{m}$ in this calculation, we obtain  $$ \int_{a}^{b}\left|s_{n}(x)\right|^{2} d x=\sum_{1}^{n}\left|c_{m}\right|^{2} \leq \int_{a}^{b}|f(x)|^{2} d x $$ since $\int\left|f-t_{n}\right|^{2} \geq 0$.",
8.12,"If $\left\{\phi_{n}\right\}$ is orthonormal on $[a, b]$, and if

then

$$
f(x) \sim \sum_{n=1}^{\infty} c_{n} \phi_{n}(x)
$$

$$
\sum_{n=1}^{\infty}\left|c_{n}\right|^{2} \leq \int_{a}^{b}|f(x)|^{2} d x
$$

In particular,

$$
\lim _{n \rightarrow \infty} c_{n}=0
$$","Letting $n \rightarrow \infty$ in (72), we obtain (73), the so-called ""Bessel inequality.""",
8.14,"If, for some $x$, there are constants $\delta>0$ and $M<\infty$ such that

$$
|f(x+t)-f(x)| \leq M|t|
$$

for all $t \in(-\delta, \delta)$, then

$$
\lim _{N \rightarrow \infty} s_{N}(f ; x)=f(x)
$$","Define  $$ g(t)=\frac{f(x-t)-f(x)}{\sin (t / 2)} $$  for $0<|t| \leq \pi$, and put $g(0)=0$. By the definition (77),  $$ \frac{1}{2 \pi} \int_{-\pi}^{\pi} D_{N}(x) d x=1 $$  Hence (78) shows that  $$ \begin{aligned} s_{N}(f ; x) & -f(x)=\frac{1}{2 \pi} \int_{-\pi}^{\pi} g(t) \sin \left(N+\frac{1}{2}\right) t d t \\ & =\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left[g(t) \cos \frac{t}{2}\right] \sin N t d t+\frac{1}{2 \pi} \int_{-\pi}^{\pi}\left[g(t) \sin \frac{t}{2}\right] \cos N t d t . \end{aligned} $$ By (79) and (81), $g(t) \cos (t / 2)$ and $g(t) \sin (t / 2)$ are bounded. The last two integrals thus tend to 0 as $N \rightarrow \infty$, by (74). This proves (80).",
8.15,"If $f$ is continuous (with period $2 \pi$ ) and if $\varepsilon>0$, then there is a trigonometric polynomial $P$ such that

for all real $x$.

$$
|P(x)-f(x)|<\varepsilon
$$","If we identify $x$ and $x+2 \pi$, we may regard the $2 \pi$-periodic functions on $R^{1}$ as functions on the unit circle $T$, by means of the mapping $x \rightarrow e^{i x}$. The trigonometric polynomials, i.e., the functions of the form (60), form a self-adjoint algebra $\mathscr{A}$, which separates points on $T$, and which vanishes at no point of $T$. Since $T$ is compact, Theorem 7.33 tells us that $\mathscr{A}$ is dense in $\mathscr{C}(T)$. This is exactly what the theorem asserts.","Suppose $\mathscr{A}$ is a self-adjoint algebra of complex continuous functions on a compact set $K, \mathscr{A}$ separates points on $K$, and $\mathscr{A}$ vanishes at no point of $K$. Then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all complex continuous functions on $K$. In other words, $\mathscr{A}$ is dense $\mathscr{C}(K)$.;  ;  ; "
8.16,"Suppose $f$ and $g$ are Riemann-integrable functions with period $2 \pi$, and

$$
f(x) \sim \sum_{-\infty}^{\infty} c_{n} e^{i n x}, \quad g(x) \sim \sum_{-\infty}^{\infty} \gamma_{n} e^{i n x}
$$

Then

$$
\begin{aligned}
\lim _{N \rightarrow \infty} \frac{1}{2 \pi} \int_{-\pi}^{\pi}\left|f(x)-s_{N}(f ; x)\right|^{2} d x & =0, \\
\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) \overline{g(x)} d x & =\sum_{-\infty}^{\infty} c_{n} \bar{\gamma}_{n}, \\
\frac{1}{2 \pi} \int_{-\pi}^{\pi}|f(x)|^{2} d x & =\sum_{-\infty}^{\infty}\left|c_{n}\right|^{2} .
\end{aligned}
$$","Let us use the notation  $$ \|h\|_{2}=\left\{\frac{1}{2 \pi} \int_{-\pi}^{\pi}|h(x)|^{2} d x\right\}^{1 / 2} . $$  Let $\varepsilon>0$ be given. Since $f \in \mathscr{R}$ and $f(\pi)=f(-\pi)$, the construction described in Exercise 12 of Chap. 6 yields a continuous $2 \pi$-periodic function $h$ with  $$ \|f-h\|_{2}<\varepsilon \text {. } $$ By Theorem 8.15 , there is a trigonometric polynomial $P$ such that $|h(x)-P(x)|<\varepsilon$ for all $x$. Hence $\|h-P\|_{2}<\varepsilon$. If $P$ has degree $N_{0}$, Theorem 8.11 shows that  $$ \left\|h-s_{N}(h)\right\|_{2} \leq\|h-P\|_{2}<\varepsilon $$ for all $N \geq N_{0}$. By (72), with $h-f$ in place of $f$,  $$ \left\|s_{N}(h)-s_{N}(f)\right\|_{2}=\left\|s_{N}(h-f)\right\|_{2} \leq\|h-f\|_{2}<\varepsilon . $$ Now the triangle inequality (Exercise 11, Chap. 6), combined with (87), (88), and (89), shows that  $$ \left\|f-s_{N}(f)\right\|_{2}<3 \varepsilon \quad\left(N \geq N_{0}\right) . $$ This proves (83). Next,  $$ \frac{1}{2 \pi} \int_{-\pi}^{\pi} s_{N}(f) \bar{g} d x=\sum_{-N}^{N} c_{n} \frac{1}{2 \pi} \int_{-\pi}^{\pi} e^{i n x} \overline{g(x)} d x=\sum_{-N}^{N} c_{n} \bar{\gamma}_{n}, $$ and the Schwarz inequality shows that  $$ \left|\int f \bar{g}-\int s_{N}(f) \bar{g}\right| \leq \int\left|f-s_{N}(f) \| g\right| \leq\left\{\int\left|f-s_{N}\right|^{2} \int|g|^{2}\right\}^{1 / 2}, $$ which tends to 0 , as $N \rightarrow \infty$, by (83). Comparison of (91) and (92) gives (84). Finally, (85) is the special case $g=f$ of (84).","If $f$ is continuous (with period $2 \pi$ ) and if $\varepsilon>0$, then there is a trigonometric polynomial $P$ such that

for all real $x$.

$$
|P(x)-f(x)|<\varepsilon
$$; Let $\left\{\phi_{n}\right\}$ be orthonormal on $[a, b]$. Let

$$
s_{n}(x)=\sum_{m=1}^{n} c_{m} \phi_{m}(x)
$$

be the nth partial sum of the Fourier series of $f$, and suppose

$$
t_{n}(x)=\sum_{m=1}^{n} \gamma_{m} \phi_{m}(x)
$$

Then

$$
\int_{a}^{b}\left|f-s_{n}\right|^{2} d x \leq \int_{a}^{b}\left|f-t_{n}\right|^{2} d x
$$

and equality holds if and only if

$$
\gamma_{m}=c_{m} \quad(m=1, \ldots, n)
$$

That is to say, among all functions $t_{n}, s_{n}$ gives the best possible mean square approximation to $f$.;  ;  ; "
8.18 (a),"For $0<x<\infty$: The functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds.",An integration by parts proves $(a)$.,
8.18 (b),"For $0<x<\infty$ and the functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds: $\Gamma(n+1)=n !$ for $n=1,2,3, \ldots$","Since $\Gamma(1)=1,(a)$ implies (b), by induction.",
8.18 (c),"For $0<x<\infty$ and the functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds: $\log \Gamma$ is convex on $(0, \infty)$.","If $1<p<\infty$ and $(1 / p)+(1 / q)=1$, apply H\u00f6lder's inequality (Exercise 10, Chap. 6) to (93), and obtain  $$ \Gamma\left(\frac{x}{p}+\frac{y}{q}\right) \leq \Gamma(x)^{1 / p} \Gamma(y)^{1 / q} $$  This is equivalent to $(c)$.",
8.19,"If $f$ is a positive function on $(0, \infty)$ such that

(a) $f(x+1)=x f(x)$,

(b) $f(1)=1$,

(c) $\log f$ is convex,

then $f(x)=\Gamma(x)$.","Since $\Gamma$ satisfies $(a),(b)$, and $(c)$, it is enough to prove that $f(x)$ is uniquely determined by $(a),(b),(c)$, for all $x>0$. By $(a)$, it is enough to do this for $x \in(0,1)$.  Put $\varphi=\log f$. Then  $$ \varphi(x+1)=\varphi(x)+\log x \quad(0<x<\infty) $$  $\varphi(1)=0$, and $\varphi$ is convex. Suppose $0<x<1$, and $n$ is a positive integer. By (94), $\varphi(n+1)=\log (n !)$. Consider the difference quotients of $\varphi$ on the intervals $[n, n+1],[n+1, n+1+x],[n+1, n+2]$. Since $\varphi$ is convex  $$ \log n \leq \frac{\varphi(n+1+x)-\varphi(n+1)}{x} \leq \log (n+1) $$  Repeated application of (94) gives  $$ \varphi(n+1+x)=\varphi(x)+\log [x(x+1) \cdots(x+n)] $$  Thus  $$ 0 \leq \varphi(x)-\log \left[\frac{n ! n^{x}}{x(x+1) \cdots(x+n)}\right] \leq x \log \left(1+\frac{1}{n}\right) $$  The last expression tends to 0 as $n \rightarrow \infty$. Hence $\varphi(x)$ is determined, and the proof is complete.  As a by-product we obtain the relation  $$ \Gamma(x)=\lim _{n \rightarrow \infty} \frac{n ! n^{x}}{x(x+1) \cdots(x+n)} $$  at least when $0<x<1$; from this one can deduce that (95) holds for all $x>0$, since $\Gamma(x+1)=x \Gamma(x)$.",
8.2,"Suppose $\Sigma c_{n}$ converges. Put

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n} \quad(-1<x<1) .
$$

Then

$$
\lim _{x \rightarrow 1} f(x)=\sum_{n=0}^{\infty} c_{n}
$$","Let $s_{n}=c_{0}+\cdots+c_{n}, s_{-1}=0$. Then  $$ \sum_{n=0}^{m} c_{n} x^{n}=\sum_{n=0}^{m}\left(s_{n}-s_{n-1}\right) x^{n}=(1-x) \sum_{n=0}^{m-1} s_{n} x^{n}+s_{m} x^{m} $$  For $|x|<1$, we let $m \rightarrow \infty$ and obtain  $$ f(x)=(1-x) \sum_{n=0}^{\infty} s_{n} x^{n} $$ Suppose $s=\lim _{n \rightarrow \infty} s_{n}$. Let $\varepsilon>0$ be given. Choose $N$ so that $n>N$ implies  $$ \left|s-s_{n}\right|<\frac{\varepsilon}{2} $$ Then, since  $$ (1-x) \sum_{n=0}^{\infty} x^{n}=1 \quad(|x|<1) $$ we obtain from (9)  $$ |f(x)-s|=\left|(1-x) \sum_{n=0}^{\infty}\left(s_{n}-s\right) x^{n}\right| \leq(1-x) \sum_{n=0}^{N}\left|s_{n}-s\right||x|^{n}+\frac{\varepsilon}{2} \leq \varepsilon $$ if $x>1-\delta$, for some suitably chosen $\delta>0$. This implies (8).",
8.20,"If $x>0$ and $y>0$, then

$$
\int_{0}^{1} t^{x-1}(1-t)^{y-1} d t=\frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}
$$

This integral is the so-called beta function $B(x, y)$.","Note that $B(1, y)=1 / y$, that $\log B(x, y)$ is a convex function of $x$, for each fixed $y$, by H\""older's inequality, as in Theorem 8.18, and that  $$ B(x+1, y)=\frac{x}{x+y} B(x, y) $$  To prove (97), perform an integration by parts on  $$ B(x+1, y)=\int_{0}^{1}\left(\frac{t}{1-t}\right)^{x}(1-t)^{x+y-1} d t $$  These three properties of $B(x, y)$ show, for each $y$, that Theorem 8.19 applies to the function $f$ defined by  Hence $f(x)=\Gamma(x)$.  $$ f(x)=\frac{\Gamma(x+y)}{\Gamma(y)} B(x, y) $$","For $0<x<\infty$: The functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds.;For $0<x<\infty$ and the functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds: $\Gamma(n+1)=n !$ for $n=1,2,3, \ldots$;For $0<x<\infty$ and the functional equation

$$
\Gamma(x+1)=x \Gamma(x)
$$

holds: $\log \Gamma$ is convex on $(0, \infty)$.; If $f$ is a positive function on $(0, \infty)$ such that

(a) $f(x+1)=x f(x)$,

(b) $f(1)=1$,

(c) $\log f$ is convex,

then $f(x)=\Gamma(x)$.;  ;  ; "
8.3,"Given a double sequence $\left\{a_{i j}\right\}, i=1,2,3, \ldots, j=1,2,3, \ldots$, suppose that

$$
\sum_{j=1}^{\infty}\left|a_{i j}\right|=b_{i} \quad(i=1,2,3, \ldots)
$$

and $\Sigma b_{i}$ converges. Then

$$
\sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{i j}=\sum_{j=1}^{\infty} \sum_{i=1}^{\infty} a_{i j}
$$","We could establish (13) by a direct procedure similar to (although more involved than) the one used in Theorem 3.55. However, the following method seems more interesting.  Let $E$ be a countable set, consisting of the points $x_{0}, x_{1}, x_{2}, \ldots$, and suppose $x_{n} \rightarrow x_{0}$ as $n \rightarrow \infty$. Define  $$ \begin{aligned} f_{i}\left(x_{0}\right)=\sum_{j=1}^{\infty} a_{i j} & (i=1,2,3, \ldots), \\ f_{i}\left(x_{n}\right)=\sum_{j=1}^{n} a_{i j} & (i, n=1,2,3, \ldots), \\ g(x)=\sum_{i=1}^{\infty} f_{i}(x) & (x \in E) . \end{aligned} $$  Now, (14) and (15), together with (12), show that each $f_{i}$ is continuous at $x_{0}$. Since $\left|f_{i}(x)\right| \leq b_{i}$ for $x \in E$, (16) converges uniformly, so that $g$ is continuous at $x_{0}$ (Theorem 7.11). It follows that  $$ \begin{aligned} \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{i j} & =\sum_{i=1}^{\infty} f_{i}\left(x_{0}\right)=g\left(x_{0}\right)=\lim _{n \rightarrow \infty} g\left(x_{n}\right) \\ & =\lim _{n \rightarrow \infty} \sum_{i=1}^{\infty} f_{i}\left(x_{n}\right)=\lim _{n \rightarrow \infty} \sum_{i=1}^{\infty} \sum_{j=1}^{n} a_{i j} \\ & =\lim _{n \rightarrow \infty} \sum_{j=1}^{n} \sum_{i=1}^{\infty} a_{i j}=\sum_{j=1}^{\infty} \sum_{i=1}^{\infty} a_{i j} . \end{aligned} $$","If $\Sigma a_{n}$ is a series of complex numbers which converges absolutely, then every rearrangement of $\Sigma a_{n}$ converges, and they all converge to the same sum.; Suppose $f_{n} \rightarrow f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that

$$
\lim _{t \rightarrow x} f_{n}(t)=A_{n} \quad(n=1,2,3, \ldots)
$$

Then $\left\{A_{n}\right\}$ converges, and

$$
\lim _{t \rightarrow x} f(t)=\lim _{n \rightarrow \infty} A_{n} .
$$

In other words, the conclusion is that

$$
\lim _{t \rightarrow x} \lim _{n \rightarrow \infty} f_{n}(t)=\lim _{n \rightarrow \infty} \lim _{t \rightarrow x} f_{n}(t)
$$;  ;  ; "
8.4,"Suppose

$$
f(x)=\sum_{n=0}^{\infty} c_{n} x^{n}
$$

the series converging in $|x|<R$. If $-R<a<R$, then $f$ can be expanded in a power series about the point $x=a$ which converges in $|x-a|<R-|a|$, and

$$
f(x)=\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^{n} \quad(|x-a|<R-|a|) .
$$

This is an extension of Theorem 5.15 and is also known as Taylor's theorem.","We have  $$ \begin{aligned} f(x) & =\sum_{n=0}^{\infty} c_{n}[(x-a)+a]^{n} \\ & =\sum_{n=0}^{\infty} c_{n} \sum_{m=0}^{n}\left(\begin{array}{l} n \\ m \end{array}\right) a^{n-m}(x-a)^{m} \\ & =\sum_{m=0}^{\infty}\left[\sum_{n=m}^{\infty}\left(\begin{array}{l} n \\ m \end{array}\right) c_{n} a^{n-m}\right](x-a)^{m} . \end{aligned} $$  This is the desired expansion about the point $x=a$. To prove its validity, we have to justify the change which was made in the order of summation. Theorem 8.3 shows that this is permissible if  $$ \sum_{n=0}^{\infty} \sum_{m=0}^{n}\left|c_{n}\left(\begin{array}{l} n \\ m \end{array}\right) a^{n-m}(x-a)^{m}\right| $$  converges. But (18) is the same as  $$ \sum_{n=0}^{\infty}\left|c_{n}\right| \cdot(|x-a|+|a|)^{n} $$  and (19) converges if $|x-a|+|a|<R$.  Finally, the form of the coefficients in (17) follows from (7).  It should be noted that (17) may actually converge in a larger interval than the one given by $|x-a|<R-|a|$.","Given a double sequence $\left\{a_{i j}\right\}, i=1,2,3, \ldots, j=1,2,3, \ldots$, suppose that

$$
\sum_{j=1}^{\infty}\left|a_{i j}\right|=b_{i} \quad(i=1,2,3, \ldots)
$$

and $\Sigma b_{i}$ converges. Then

$$
\sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{i j}=\sum_{j=1}^{\infty} \sum_{i=1}^{\infty} a_{i j}
$$;  ;  ; "
8.5,"Suppose the series $\Sigma a_{n} x^{n}$ and $\Sigma b_{n} x^{n}$ converge in the segment $S=(-R, R)$. Let $E$ be the set of all $x \in S$ at which

$$
\sum_{n=0}^{\infty} a_{n} x^{n}=\sum_{n=0}^{\infty} b_{n} x^{n}
$$

If $E$ has a limit point in $S$, then $a_{n}=b_{n}$ for $n=0,1,2, \ldots$ Hence (20) holds for all $x \in S$.","Put $c_{n}=a_{n}-b_{n}$ and  $$ f(x)=\sum_{n=0}^{\infty} c_{n} x^{n} \quad(x \in S) $$  Then $f(x)=0$ on $E$.  Let $A$ be the set of all limit points of $E$ in $S$, and let $B$ consist of all other points of $S$. It is clear from the definition of ""limit point"" that $B$ is open. Suppose we can prove that $A$ is open. Then $A$ and $B$ are disjoint open sets. Hence they are separated (Definition 2.45). Since $S=A \cup B$, and $S$ is connected, one of $A$ and $B$ must be empty. By hypothesis, $A$ is not empty. Hence $B$ is empty, and $A=S$. Since $f$ is continuous in $S$, $A \subset E$. Thus $E=S$, and (7) shows that $c_{n}=0$ for $n=0,1,2, \ldots$, which is the desired conclusion.",
8.6 (a),Let $e^{x}$ be defined on $R^{1}$ by (35) and (25): $e^{x}$ is continuous and differentiable for all $x$,We have already proved $(a)$ to $(e)$,
8.6 (b),Let $e^{x}$ be defined on $R^{1}$ by (35) and (25): $\left(e^{x}\right)^{\prime}=e^{x}$,We have already proved $(a)$ to $(e)$,
8.6 (c),"Let $e^{x}$ be defined on $R^{1}$ by (35) and (25): $e^{x}$ is a strictly increasing function of $x$, and $e^{x}>0$",We have already proved $(a)$ to $(e)$,
8.6 (d),Let $e^{x}$ be defined on $R^{1}$ by (35) and (25): $e^{x+y}=e^{x} e^{y}$,We have already proved $(a)$ to $(e)$,
8.6 (e),"Let $e^{x}$ be defined on $R^{1}$ by (35) and (25): $e^{x} \rightarrow+\infty$ as $x \rightarrow+\infty, e^{x} \rightarrow 0$ as $x \rightarrow-\infty$",We have already proved $(a)$ to $(e)$,
8.6 (f),"Let $e^{x}$ be defined on $R^{1}$ by (35) and (25): $\lim _{x \rightarrow+\infty} x^{n} e^{-x}=0$, for every $n$","$(25)$ shows that  $$ e^{x}>\frac{x^{n+1}}{(n+1) !} $$  for $x>0$, so that  $$ x^{n} e^{-x}<\frac{(n+1) !}{x} $$  and $(f)$ follows. Part $(f)$ shows that $e^{x}$ tends to $+\infty$ ""faster"" than any power of $x$, as $x \rightarrow+\infty$.",
8.8,"Suppose $a_{0}, \ldots, a_{n}$ are complex numbers, $n \geq 1, a_{n} \neq 0$,

$$
P(z)=\sum_{0}^{n} a_{k} z^{k}
$$

Then $P(z)=0$ for some complex number $z$.","Without loss of generality, assume $a_{n}=1$. Put  If $|z|=R$, then  $$ \mu=\inf |P(z)| \quad(z \text { complex }) $$  $$ |P(z)| \geq R^{n}\left[1-\left|a_{n-1}\right| R^{-1}-\cdots-\left|a_{0}\right| R^{-n}\right] $$  The right side of (56) tends to $\infty$ as $R \rightarrow \infty$. Hence there exists $R_{0}$ such that $|P(z)|>\mu$ if $|z|>R_{0}$. Since $|P|$ is continuous on the closed disc with center at 0 and radius $R_{0}$, Theorem 4.16 shows that $\left|P\left(z_{0}\right)\right|=\mu$ for some $z_{0}$.  We claim that $\mu=0$.  If not, put $Q(z)=P\left(z+z_{0}\right) / P\left(z_{0}\right)$. Then $Q$ is a nonconstant polynomial, $Q(0)=1$, and $|Q(z)| \geq 1$ for all $z$. There is a smallest integer $k$, $1 \leq k \leq n$, such that  $$ Q(z)=1+b_{k} z^{k}+\cdots+b_{n} z^{n}, \quad b_{k} \neq 0 . $$  By Theorem 8.7(d) there is a real $\theta$ such that  $$ e^{i k \theta} b_{k}=-\left|b_{k}\right| $$  If $r>0$ and $r^{k}\left|b_{k}\right|<1$, (58) implies  $$ \left|1+b_{k} r^{k} e^{i k \theta}\right|=1-r^{k}\left|b_{k}\right| $$  so that  $$ \left|Q\left(r e^{i \theta}\right)\right| \leq 1-r^{k}\left\{\left|b_{k}\right|-r\left|b_{k+1}\right|- \cdots-r^{n-k}\left|b_{n}\right|\right\} $$  For sufficiently small $r$, the expression in braces is positive; hence $\left|Q\left(r e^{i \theta}\right)\right|<1$, a contradiction.  Thus $\mu=0$, that is, $P\left(z_{0}\right)=0$.  Exercise 27 contains a more general result.",; ;  ;  ; 
9.12,"Suppose $E$ and $\mathbf{f}$ are as in Definition 9.11, $\mathrm{x} \in E$, and (14) holds with $A=A_{1}$ and with $A=A_{2}$. Then $A_{1}=A_{2}$.","If $B=A_{1}-A_{2}$, the inequality  $$ |B \mathbf{h}| \leq\left|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A_{1} \mathbf{h}\right|+\left|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-A_{2} \mathbf{h}\right| $$  shows that $|B \mathbf{h}| /|\mathbf{h}| \rightarrow 0$ as $\mathbf{h} \rightarrow \mathbf{0}$. For fixed $\mathbf{h} \neq \mathbf{0}$, it follows that  $$ \frac{\mid B(t \mathbf{h})}{|t \mathbf{h}|} \rightarrow 0 \quad \text { as } \quad t \rightarrow 0 $$  The linearity of $B$ shows that the left side of (16) is independent of $t$. Thus $B \mathbf{h}=0$ for every $\mathbf{h} \in R^{n}$. Hence $B=0$.",
9.2,"Let $r$ be a positive integer. If a vector space $X$ is spanned by a set of $r$ vectors, then $\operatorname{dim} X \leq r$.","If this is false, there is a vector space $X$ which contains an independent set $Q=\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r+1}\right\}$ and which is spanned by a set $S_{0}$ consisting of $r$ vectors.",
9.21,"Suppose $\mathrm{f}$ maps an open set $E \subset R^{n}$ into $R^{m}$. Then $\mathrm{f} \in \mathscr{C}^{\prime}(E)$ if and only if the partial derivatives $D_{j} f_{i}$ exist and are continuous on $E$ for $1 \leq i \leq m$, $1 \leq j \leq n$.","Assume first that $\mathbf{f} \in \mathscr{C}^{\prime}(E)$. By (27),  $$ \left(D_{j} f_{i}\right)(\mathbf{x})=\left(\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{e}_{j}\right) \cdot \mathbf{u}_{i} $$  for all $i, j$, and for all $\mathrm{x} \in E$. Hence  $$ \left(D_{j} f_{i}\right)(\mathbf{y})-\left(D_{j} f_{i}\right)(\mathbf{x})=\left\{\left[\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right] \mathbf{e}_{j}\right\} \cdot \mathbf{u}_{i} $$  and since $\left|\mathbf{u}_{i}\right|=\left|\mathbf{e}_{j}\right|=1$, it follows that  $$ \begin{aligned} \left|\left(D_{j} f_{i}\right)(\mathbf{y})-\left(D_{j} f_{i}\right)(\mathbf{x})\right| & \leq\left|\left[\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right] \mathbf{e}_{j}\right| \\ & \leq\left\|\mathbf{f}^{\prime}(\mathbf{y})-\mathbf{f}^{\prime}(\mathbf{x})\right\| . \end{aligned} $$  Hence $D_{j} f_{i}$ is continuous.  For the converse, it suffices to consider the case $m=1$. (Why?) Fix $\mathbf{x} \in E$ and $\varepsilon>0$. Since $E$ is open, there is an open ball $S \subset E$, with center at $\mathbf{x}$ and radius $r$, and the continuity of the functions $D_{j} f$ shows that $r$ can be chosen so that  $$ \left|\left(D_{j} f\right)(\mathbf{y})-\left(D_{j} f\right)(\mathbf{x})\right|<\frac{\varepsilon}{n} \quad(\mathbf{y} \in S, 1 \leq j \leq n) . $$  Suppose $\mathbf{h}=\Sigma h_{j} \mathbf{e}_{j},|\mathbf{h}|<r$, put $\mathbf{v}_{0}=\mathbf{0}$, and $\mathbf{v}_{k}=h_{1} \mathbf{e}_{1}+\cdots+h_{k} \mathbf{e}_{k}$, for $1 \leq k \leq n$. Then  $$ f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})=\sum_{j=1}^{n}\left[f\left(\mathbf{x}+\mathbf{v}_{j}\right)-f\left(\mathbf{x}+\mathbf{v}_{j-1}\right)\right] $$  Since $\left|\mathbf{v}_{\boldsymbol{k}}\right|<r$ for $1 \leq k \leq n$ and since $S$ is convex, the segments with end points $\mathbf{x}+\mathbf{v}_{j-1}$ and $\mathbf{x}+\mathbf{v}_{j}$ lie in $S$. Since $\mathbf{v}_{j}=\mathbf{v}_{j-1}+h_{j} \mathbf{e}_{j}$, the mean value theorem (5.10) shows that the $j$ th summand in (42) is equal to  $$ h_{j}\left(D_{j} f\right)\left(\mathbf{x}+\mathbf{v}_{j-1}+\theta_{j} h_{j} \mathbf{e}_{j}\right) $$  for some $\theta_{j} \in(0,1)$, and this differs from $h_{j}\left(D_{j} f\right)(\mathbf{x})$ by less than $\left|h_{j}\right| \varepsilon / n$, using (41). By (42), it follows that  $$ \left|f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})-\sum_{j=1}^{n} h_{j}\left(D_{j} f\right)(\mathbf{x})\right| \leq \frac{1}{n} \sum_{j=1}^{n}\left|h_{j}\right| \varepsilon \leq|\mathbf{h}| \varepsilon $$  for all $\mathbf{h}$ such that $|\mathbf{h}|<r$.  This says that $f$ is differentiable at $\mathbf{x}$ and that $f^{\prime}(\mathbf{x})$ is the linear function which assigns the number $\Sigma h_{j}\left(D_{j} f\right)(\mathbf{x})$ to the vector $\mathbf{h}=\Sigma h_{j} \mathbf{e}_{j}$. The matrix $\left[f^{\prime}(\mathbf{x})\right]$ consists of the row $\left(D_{1} f\right)(\mathbf{x}), \ldots,\left(D_{n} f\right)(\mathbf{x})$; and since $D_{1} f, \ldots, D_{n} f$ are continuous functions on $E$, the concluding remarks of Sec. 9.9 show that $f \in \mathscr{C}^{\prime}(E)$.",
9.24 (a),"Suppose $\mathbf{f}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}, \mathbf{f}^{\prime}(\mathbf{a})$ is invertible for some $\mathbf{a} \in E$, and $\mathbf{b}=\mathbf{f}(\mathbf{a})$. Then there exist open sets $U$ and $V$ in $R^{n}$ such that $\mathbf{a} \in U, \mathbf{b} \in V, \mathbf{f}$ is one-toone on $U$, and $\mathbf{f}(U)=V$;","Put $\mathbf{f}^{\prime}(\mathbf{a})=A$, and choose $\lambda$ so that  $$ 2 \lambda\left\|A^{-1}\right\|=1 \text {. } $$  Since $\mathbf{f}^{\prime}$ is continuous at a, there is an open ball $U \subset E$, with center at a, such that  We associate to each $\mathrm{y} \in R^{n}$ a function $\varphi$, defined by  $$ \varphi(\mathbf{x})=\mathbf{x}+A^{-1}(\mathbf{y}-\mathbf{f}(\mathbf{x})) \quad(\mathbf{x} \in E) $$  Note that $\mathbf{f}(\mathbf{x})=\mathbf{y}$ if and only if $\mathbf{x}$ is a fixed point of $\varphi$.  Since $\varphi^{\prime}(\mathbf{x})=I-A^{-1} \mathbf{f}^{\prime}(\mathbf{x})=A^{-1}\left(A-\mathbf{f}^{\prime}(\mathbf{x})\right)$, (46) and (47) imply  that  $$ \left\|\varphi^{\prime}(\mathbf{x})\right\|<\frac{1}{2} \quad(\mathbf{x} \in U) . $$  Hence  $$ \left|\varphi\left(\mathbf{x}_{1}\right)-\varphi\left(\mathbf{x}_{2}\right)\right| \leq \frac{1}{2}\left|\mathbf{x}_{1}-\mathbf{x}_{2}\right| \quad\left(\mathbf{x}_{1}, \mathbf{x}_{2} \in U\right) $$  by Theorem 9.19. It follows that $\varphi$ has at most one fixed point in $U$, so that $\mathbf{f}(\mathbf{x})=\mathbf{y}$ for at most one $\mathbf{x} \in U$.  Thus $\mathbf{f}$ is $1-1$ in $U$.  Next, put $V=\mathbf{f}(U)$, and pick $\mathbf{y}_{0} \in V$. Then $\mathbf{y}_{0}=\mathbf{f}\left(\mathbf{x}_{0}\right)$ for some $\mathbf{x}_{0} \in U$. Let $B$ be an open ball with center at $\mathbf{x}_{0}$ and radius $r>0$, so small that its closure $\bar{B}$ lies in $U$. We will show that $\mathbf{y} \in V$ whenever $\left|\mathbf{y}-\mathbf{y}_{0}\right|<\lambda r$. This proves, of course, that $V$ is open.",;  ;  ; 
9.24 (b),"Suppose $\mathbf{f}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{n}, \mathbf{f}^{\prime}(\mathbf{a})$ is invertible for some $\mathbf{a} \in E$, and $\mathbf{b}=\mathbf{f}(\mathbf{a})$. Then if $\mathbf{g}$ is the inverse of $\mathbf{f}$ [which exists, by $(a)$ ], defined in $V$ by

$$
\mathbf{g}(\mathbf{f}(\mathbf{x}))=\mathbf{x} \quad(\mathbf{x} \in U)
$$

then $\mathbf{g} \in \mathscr{C}^{\prime}(V)$.","Pick $\mathbf{y} \in V, \mathbf{y}+\mathbf{k} \in V$. Then there exist $\mathbf{x} \in U, \mathbf{x}+\mathbf{h} \in U$, so that $\mathbf{y}=\mathbf{f}(\mathbf{x}), \mathbf{y}+\mathbf{k}=\mathbf{f}(\mathbf{x}+\mathbf{h})$. With $\varphi$ as in (48),  $$ \varphi(\mathbf{x}+\mathbf{h})-\varphi(\mathbf{x})=\mathbf{h}+A^{-1}[\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{x}+\mathbf{h})]=\mathbf{h}-A^{-1} \mathbf{k} $$  By (50), $\left|\mathbf{h}-A^{-1} \mathbf{k}\right| \leq \frac{1}{2}|\mathbf{h}|$. Hence $\left|A^{-1} \mathbf{k}\right| \geq \frac{1}{2}|\mathbf{h}|$, and  $$ |\mathbf{h}| \leq 2\left\|A^{-1}\right\||\mathbf{k}|=\lambda^{-1}|\mathbf{k}| $$  By (46), (47), and Theorem $9.8, \mathbf{f}^{\prime}(\mathbf{x})$ has an inverse, say $T$. Since  $$ \mathbf{g}(\mathbf{y}+\mathbf{k})-\mathbf{g}(\mathbf{y})-T \mathbf{k}=\mathbf{h}-T \mathbf{k}=-T\left[\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}\right] $$  (51) implies  $$ \frac{|\mathbf{g}(\mathbf{y}+\mathbf{k})-\mathbf{g}(\mathbf{y})-T \mathbf{k}|}{|\mathbf{k}|} \leq \frac{\|T\|}{\lambda} \cdot \frac{\left|\mathbf{f}(\mathbf{x}+\mathbf{h})-\mathbf{f}(\mathbf{x})-\mathbf{f}^{\prime}(\mathbf{x}) \mathbf{h}\right|}{|\mathbf{h}|} . $$  As $\mathbf{k} \rightarrow \mathbf{0}$, (51) shows that $\mathbf{h} \rightarrow \mathbf{0}$. The right side of the last inequality thus tends to 0 . Hence the same is true of the left. We have thus proved that $\mathbf{g}^{\prime}(\mathbf{y})=T$. But $T$ was chosen to be the inverse of $\mathbf{f}^{\prime}(\mathbf{x})=\mathbf{f}^{\prime}(\mathbf{g}(\mathbf{y}))$. Thus  $$ \mathbf{g}^{\prime}(\mathbf{y})=\left\{\mathbf{f}^{\prime}(\mathbf{g}(\mathbf{y}))\right\}^{-1} \quad(\mathbf{y} \in V) $$  Finally, note that $\mathbf{g}$ is a continuous mapping of $V$ onto $U$ (since $\mathbf{g}$ is differentiable), that $\mathbf{f}^{\prime}$ is a continuous mapping of $U$ into the set $\Omega$ of all invertible elements of $L\left(R^{n}\right)$, and that inversion is a continuous mapping of $\Omega$ onto $\Omega$, by Theorem 9.8. If we combine these facts with (52), we see that $\mathbf{g} \in \mathscr{C}^{\prime}(V)$.","Let $\Omega$ be the set of all invertible linear operators on $R^{n}$. If $A \in \Omega, B \in L\left(R^{n}\right)$, and

$$
\|B-A\| \cdot\left\|A^{-1}\right\|<1
$$

then $B \in \Omega$.;$\Omega$ is an open subset of $L\left(R^{n}\right)$, and the mapping $A \rightarrow A^{-1}$ is continuous on $\Omega$.;  ;  ; "
9.27,"If $A \in L\left(R^{n+m}, R^{n}\right)$ and if $A_{x}$ is invertible, then there corresponds to every $\mathbf{k} \in R^{m}$ a unique $\mathbf{h} \in R^{n}$ such that $A(\mathbf{h}, \mathbf{k})=\mathbf{0$}","By (54), $A(\mathbf{h}, \mathbf{k})=0$ if and only if  $$ A_{x} \mathbf{h}+A_{y} \mathbf{k}=\mathbf{0} $$  which is the same as (55) when $A_{x}$ is invertible.  The conclusion of Theorem 9.27 is, in other words, that the equation $A(\mathbf{h}, \mathbf{k})=\mathbf{0}$ can be solved (uniquely) for $\mathbf{h}$ if $\mathbf{k}$ is given, and that the solution $\mathbf{h}$ is a linear function of $\mathbf{k}$. Those who have some acquaintance with linear algebra will recognize this as a very familiar statement about systems of linear equations.","If $A \in L\left(R^{n+m}, R^{n}\right)$ and if $A_{x}$ is invertible, then there corresponds to every $\mathbf{k} \in R^{m}$ a unique $\mathbf{h} \in R^{n}$ such that $A(\mathbf{h}, \mathbf{k})=\mathbf{0$};  ;  ; "
9.28,"Let $\mathbf{f}$ be a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n+m}$ into $R^{n}$, such that $\mathbf{f}(\mathbf{a}, \mathbf{b})=\mathbf{0}$ for some point $(\mathbf{a}, \mathbf{b}) \in E$. Put $A=\mathbf{f}^{\prime}(\mathbf{a}, \mathbf{b})$ and assume that $A_{x}$ is invertible. Then there exist open sets $U \subset R^{n+m}$ and $W \subset R^{m}$, with $(\mathbf{a}, \mathbf{b}) \in U$ and $\mathbf{b} \in W$, having the following property: To every $\mathbf{y} \in W$ corresponds a unique $\mathbf{x}$ such that $(\mathbf{x}, \mathbf{y}) \in U$ and $\mathbf{f}(\mathbf{x}, \mathbf{y})=0$. If this $\mathbf{x}$ is defined to be $\mathbf{g}(\mathbf{y})$, then $\mathbf{g}$ is a $\mathscr{C}^{\prime}$-mapping of $W$ into $R^{n}, \mathbf{g}(\mathbf{b})=\mathbf{a}$, $\mathbf{f}(\mathbf{g}(\mathbf{y}), \mathbf{y})=\mathbf{0}$ $(\mathbf{y} \in W)$ and $\mathbf{g}^{\prime}(\mathbf{b})=-\left(A_{x}\right)^{-1} A_{y}$","Define $\mathbf{F}$ by $\mathbf{F}(\mathbf{x}, \mathbf{y})=(\mathbf{f}(\mathbf{x}, \mathbf{y}), \mathbf{y})$ $((\mathbf{x}, \mathbf{y}) \in E)$ Then $\mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of $E$ into $R^{n+m}$. We claim that $\mathbf{F}^{\prime}(\mathbf{a}, \mathbf{b})$ is an invertible element of $L\left(R^{n+m}\right)$ : Since $\mathbf{f}(\mathbf{a}, \mathbf{b})=\mathbf{0}$, we have $\mathbf{f}(\mathbf{a}+\mathbf{h}, \mathbf{b}+\mathbf{k})=A(\mathbf{h}, \mathbf{k})+\mathbf{r}(\mathbf{h}, \mathbf{k})$ where $\mathbf{r}$ is the remainder that occurs in the definition of $\mathbf{f}^{\prime}(\mathbf{a}, \mathbf{b})$. Since $\mathbf{F}(\mathbf{a}+\mathbf{h}, \mathbf{b}+\mathbf{k})-\mathbf{F}(\mathbf{a}, \mathbf{b})=(\mathbf{f}(\mathbf{a}+\mathbf{h}, \mathbf{b}+\mathbf{k}), \mathbf{k})=(\mathbf{A}(\mathbf{h}, \mathbf{k}), \mathbf{k})+(\mathbf{r}(\mathbf{h}, \mathbf{k}), \mathbf{0})$ it follows that $\mathbf{F}^{\prime}(\mathbf{a}, \mathbf{b})$ is the linear operator on $R^{n+m}$ that maps $(\mathbf{h}, \mathbf{k})$ to $(A(\mathbf{h}, \mathbf{k}), \mathbf{k})$. If this image vector is $\mathbf{0}$, then $A(\mathbf{h}, \mathbf{k})=\mathbf{0}$ and $\mathbf{k}=\mathbf{0}$, hence $A(\mathbf{h}, \mathbf{0})=\mathbf{0}$, and Theorem 9.27 implies that $\mathbf{h}=\mathbf{0}$. It follows that $\mathbf{F}^{\prime}(\mathbf{a}, \mathbf{b})$ is $1-1$; hence it is invertible (Theorem 9.5). The inverse function theorem can therefore be applied to $\mathbf{F}$. It shows that there exist open sets $U$ and $V$ in $R^{n+m}$, with $(\mathbf{a}, \mathbf{b}) \in U, (\mathbf{0}, \mathbf{b}) \in V$, such that $\mathbf{F}$ is a $1-1$ mapping of $U$ onto $V$. $\mathbf{b} \in W$. We let $W$ be the set of all $\mathbf{y} \in R^{m}$ such that $(0, \mathbf{y}) \in V$. Note that It is clear that $W$ is open since $V$ is open. If $\mathbf{y} \in W$, then $(\mathbf{0}, \mathbf{y})=\mathbf{F}(\mathbf{x}, \mathbf{y})$ for some $(\mathbf{x}, \mathbf{y}) \in U$. By $(60), \mathbf{f}(\mathbf{x}, \mathbf{y})=\mathbf{0}$ for this $\mathbf{x}$. Suppose, with the same $\mathbf{y}$, that $\left(\mathbf{x}^{\prime}, \mathbf{y}\right) \in U$ and $\mathbf{f}\left(\mathbf{x}^{\prime}, \mathbf{y}\right)=\mathbf{0}$. Then $\mathbf{F}\left(\mathbf{x}^{\prime}, \mathbf{y}\right)=\left(\mathbf{f}\left(\mathbf{x}^{\prime}, \mathbf{y}\right), \mathbf{y}\right)=(\mathbf{f}(\mathbf{x}, \mathbf{y}), \mathbf{y})=\mathbf{F}(\mathbf{x}, \mathbf{y})$ Since $\mathbf{F}$ is $1-1$ in $U$, it follows that $\mathbf{x}^{\prime}=\mathbf{x}$. This proves the first part of the theorem. For the second part, define $\mathbf{g}(\mathbf{y})$, for $\mathbf{y} \in W$, so that $(\mathbf{g}(\mathbf{y}), \mathbf{y}) \in U$ and (57) holds. Then $\mathbf{F}(\mathbf{g}(\mathbf{y}), \mathbf{y})=(\mathbf{0}, \mathbf{y})$ $\quad(\mathbf{y} \in W)$ If $\mathbf{G}$ is the mapping of $V$ onto $U$ that inverts $\mathbf{F}$, then $\mathbf{G} \in \mathscr{C}^{\prime}$, by the inverse function theorem, and (61) gives $(\mathbf{g}(\mathbf{y}), \mathbf{y})=\mathbf{G}(\mathbf{0}, \mathbf{y})$ $\quad(\mathbf{y} \in W)$ Since $\mathbf{G} \in \mathscr{C}^{\prime}$, (62) shows that $\mathbf{g} \in \mathscr{C}^{\prime}$. Finally, to compute $\mathbf{g}^{\prime}(\mathbf{b})$, put $(\mathbf{g}(\mathbf{y}), \mathbf{y})=\Phi(\mathbf{y})$. Then $\Phi^{\prime}(\mathbf{y}) \mathbf{k}=\left(\mathbf{g}^{\prime}(\mathbf{y}) \mathbf{k}, \mathbf{k}\right)$ $\quad\left(\mathbf{y} \in W, \mathbf{k} \in R^{m}\right) .$ By $(57), \mathbf{f}(\Phi(\mathbf{y}))=\mathbf{0}$ in $W$. The chain rule shows therefore that $\mathbf{f}^{\prime}(\Phi(\mathbf{y})) \Phi^{\prime}(\mathbf{y})=0$ When $\mathbf{y}=\mathbf{b}$, then $\Phi(\mathbf{y})=(\mathbf{a}, \mathbf{b})$, and $\mathbf{f}^{\prime}(\Phi(\mathbf{y}))=A$. Thus $A \Phi^{\prime}(\mathbf{b})=0 .$ It now follows from (64), (63), and (54), that $A_{x} \mathbf{g}^{\prime}(\mathbf{b}) \mathbf{k}+A_{y} \mathbf{k}=A\left(\mathbf{g}^{\prime}(\mathbf{b}) \mathbf{k}, \mathbf{k}\right)=A \Phi^{\prime}(\mathbf{b}) \mathbf{k}=\mathbf{0}$ for every $\mathbf{k} \in R^{m}$. Thus $A_{x} \mathbf{g}^{\prime}(\mathbf{b})+A_{y}=0$ This is equivalent to (58), and completes the proof.","If $A \in L\left(R^{n+m}, R^{n}\right)$ and if $A_{x}$ is invertible, then there corresponds to every $\mathbf{k} \in R^{m}$ a unique $\mathbf{h} \in R^{n}$ such that $A(\mathbf{h}, \mathbf{k})=\mathbf{0$}; A linear operator $A$ on a finite-dimensional vector space $X$ is one-to-one if and only if the range of $A$ is all of $X$.;  ;  ; "
9.3 (a),"Suppose $X$ is a vector space, and $\operatorname{dim} X=n$: A set $E$ of $n$ vectors in $X$ spans $X$ if and only if $E$ is independent.","Suppose $E=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$. Since $\operatorname{dim} X=n$, the set $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \mathbf{y}\right\}$ is dependent, for every $\mathbf{y} \in X$. If $E$ is independent, it follows that $\mathbf{y}$ is in the span of $E$; hence $E$ spans $X$. Conversely, if $E$ is dependent, one of its members can be removed without changing the span of $E$. Hence $E$ cannot span $X$, by Theorem 9.2. This proves $(a)$.","Let $r$ be a positive integer. If a vector space $X$ is spanned by a set of $r$ vectors, then $\operatorname{dim} X \leq r$.;  ;  ; "
9.3 (b),"Suppose $X$ is a vector space, and $\operatorname{dim} X=n$: $X$ has a basis, and every basis consists of $n$ vectors.","Since $\operatorname{dim} X=n, X$ contains an independent set of $n$ vectors, and (a) shows that every such set is a basis of $X ;(b)$ now follows from $9.1(d)$ and 9.2.",
9.3 (c),"Suppose $X$ is a vector space, and $\operatorname{dim} X=n$, and $1 \leq r \leq n$ and $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$ is an independent set in $X$: then $X$ has $a$ basis containing $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$.","let $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ be a basis of $X$. The set  $$ S=\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}, \mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\} $$  spans $X$ and is dependent, since it contains more than $n$ vectors. The argument used in the proof of Theorem 9.2 shows that one of the $\mathbf{x}_{i}{ }^{\prime}$ 's is a linear combination of the other members of $S$. If we remove this $\mathbf{x}_{i}$ from $S$, the remaining set still spans $X$. This process can be repeated $r$ times and leads to a basis of $X$ which contains $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$, by $(a)$.","Let $r$ be a positive integer. If a vector space $X$ is spanned by a set of $r$ vectors, then $\operatorname{dim} X \leq r$.;  ;  ; "
9.32,"Suppose $m, n, r$ are nonnegative integers, $m \geq r, n \geq r, \mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{m}$, and $\mathbf{F}^{\prime}(\mathbf{x})$ has rank $r$ for every $\mathbf{x} \in E$. Fix $\mathbf{a} \in E$, put $A=\mathbf{F}^{\prime}(\mathbf{a})$, let $Y_{1}$ be the range of $A$, and let $P$ be a projection in $R^{m}$ whose range is $Y_{1}$. Let $Y_{2}$ be the null space of $P$. Then there are open sets $U$ and $V$ in $R^{n}$, with $\mathbf{a} \in U, U \subset E$, and there is a 1-1 $\mathscr{C}^{\prime}$-mapping $\mathbf{H}$ of $V$ onto $U$ (whose inverse is also of class $\mathscr{C}^{\prime}$ ) such that

$$
\mathbf{F}(\mathbf{H}(\mathbf{x}))=A \mathbf{x}+\varphi(A \mathbf{x}) \quad(\mathbf{x} \in V)
$$

where $\varphi$ is a $\mathscr{C}^{\prime}$-mapping of the open set $A(V) \subset Y_{1}$ into $Y_{2}$.","If $r=0$, Theorem 9.19 shows that $\mathbf{F}(\mathbf{x})$ is constant in a neighborhood $U$ of a, and (66) holds trivially, with $V=U, \mathbf{H}(\mathbf{x})=\mathbf{x}, \varphi(\mathbf{0})=\mathbf{F}(\mathbf{a})$.  From now on we assume $r>0$. Since $\operatorname{dim} Y_{1}=r, Y_{1}$ has a basis $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$. Choose $\mathbf{z}_{i} \in R^{n}$ so that $A \mathbf{z}_{i}=\mathbf{y}_{i}(1 \leq i \leq r)$, and define a linear mapping $S$ of $Y_{1}$ into $R^{n}$ by setting  $$ S\left(c_{1} \mathbf{y}_{1}+\cdots+c_{r} \mathbf{y}_{r}\right)=c_{1} \mathbf{z}_{1}+\cdots+c_{r} \mathbf{z}_{r} $$  for all scalars $c_{1}, \ldots, c_{r}$.  Then $A S \mathbf{y}_{i}=A \mathbf{z}_{i}=\mathbf{y}_{i}$ for $1 \leq i \leq r$. Thus  $$ A S \mathbf{y}=\mathbf{y} \quad(\mathbf{y} \in Y_{1}) $$  Define a mapping $\mathbf{G}$ of $E$ into $R^{n}$ by setting  $$ \mathbf{G}(\mathbf{x})=\mathbf{x}+S P[\mathbf{F}(\mathbf{x})-A \mathbf{x}] \quad(\mathbf{x} \in E) . $$  Since $\mathbf{F}^{\prime}(\mathbf{a})=A$, differentiation of (69) shows that $\mathbf{G}^{\prime}(\mathbf{a})=I$, the identity operator on $R^{n}$. By the inverse function theorem, there are open sets $U$ and $V$ in $R^{n}$, with $\mathbf{a} \in U$, such that $\mathbf{G}$ is a 1-1 mapping of $U$ onto $V$ whose inverse $\mathbf{H}$ is also of class $\mathscr{C}^{\prime}$. Moreover, by shrinking $U$ and $V$, if necessary, we can arrange it so that $V$ is convex and $\mathbf{H}^{\prime}(\mathbf{x})$ is invertible for every $\mathbf{x} \in V$.  Note that $A S P A=A$, since $P A=A$ and (68) holds. Therefore (69) gives  $$ \psi(\mathbf{x})=\mathbf{F}(\mathbf{H}(\mathbf{x}))-A \mathbf{x} \quad(\mathbf{x} \in V) . $$  Since $P A=A,(71)$ implies that $P \psi(\mathbf{x})=\mathbf{0}$ for all $\mathbf{x} \in V$. Thus $\psi$ is a $\mathscr{C}^{\prime}$-mapping of $V$ into $Y_{2}$.  Since $V$ is open, it is clear that $A(V)$ is an open subset of its range $\mathscr{R}(A)=Y_{1}$.  To complete the proof, i.e., to go from (72) to (66), we have to show that there is a $\mathscr{C}^{\prime}$-mapping $\varphi$ of $A(V)$ into $Y_{2}$ which satisfies  $$ \varphi(A \mathbf{x})=\psi(\mathbf{x}) \quad(\mathbf{x} \in V) $$  As a step toward (73), we will first prove that  $$ \psi\left(\mathbf{x}_{1}\right)=\psi\left(\mathbf{x}_{2}\right) $$  if $\mathbf{x}_{1} \in V, \mathbf{x}_{2} \in V, A \mathbf{x}_{1}=A \mathbf{x}_{2}$.  Put $\Phi(\mathbf{x})=\mathbf{F}(\mathbf{H}(\mathbf{x}))$, for $\mathbf{x} \in V$. Since $\mathbf{H}^{\prime}(\mathbf{x})$ has rank $n$ for every $\mathbf{x} \in V$, and $\mathbf{F}^{\prime}(\mathbf{x})$ has rank $r$ for every $\mathbf{x} \in U$, it follows that By (71),  $$ \operatorname{rank} \Phi^{\prime}(\mathbf{x})=\operatorname{rank} \mathbf{F}^{\prime}(\mathbf{H}(\mathbf{x})) \mathbf{H}^{\prime}(\mathbf{x})=r \quad(\mathbf{x} \in V) $$  Fix $\mathbf{x} \in V$. Let $M$ be the range of $\Phi^{\prime}(\mathbf{x})$. Then $M \subset R^{m}, \operatorname{dim} M=r$.  $$ P \Phi^{\prime}(\mathbf{x})=A . $$  Thus $P$ maps $M$ onto $\mathscr{R}(A)=Y_{1}$. Since $M$ and $Y_{1}$ have the same dimension, it follows that $P$ (restricted to $M$ ) is 1-1.  Suppose now that $A \mathbf{h}=\mathbf{0}$. Then $P \Phi^{\prime}(\mathbf{x}) \mathbf{h}=\mathbf{0}$, by (76). But $\Phi^{\prime}(\mathbf{x}) \mathbf{h} \in M$, and $P$ is $1-1$ on $M$. Hence $\Phi^{\prime}(\mathbf{x}) \mathbf{h}=\mathbf{0}$. A look at (72) shows now that we have proved the following:  If $\mathbf{x} \in V$ and $A \mathbf{h}=\mathbf{0}$, then $\psi^{\prime}(\mathbf{x}) \mathbf{h}=\mathbf{0}$.  We can now prove (74). Suppose $\mathbf{x}_{1} \in V, \mathbf{x}_{2} \in V, A \mathbf{x}_{1}=A \mathbf{x}_{2}$. Put $\mathbf{h}=\mathbf{x}_{2}-\mathbf{x}_{1}$ and define  $$ \mathbf{g}(t)=\psi\left(\mathbf{x}_{1}+t \mathbf{h}\right) \quad(0 \leq t \leq 1) . $$  The convexity of $V$ shows that $\mathbf{x}_{1}+t \mathbf{h} \in V$ for these $t$. Hence  $$ \mathbf{g}^{\prime}(t)=\psi^{\prime}\left(\mathbf{x}_{1}+t \mathbf{h}\right) \mathbf{h}=\mathbf{0} \quad(0 \leq t \leq 1) $$  so that $\mathbf{g}(1)=\mathbf{g}(0)$. But $\mathbf{g}(1)=\psi\left(\mathbf{x}_{2}\right)$ and $\mathbf{g}(0)=\psi\left(\mathbf{x}_{1}\right)$. This proves (74).  By (74), $\psi(\mathbf{x})$ depends only on $A \mathbf{x}$, for $\mathbf{x} \in V$. Hence (73) defines $\varphi$ unambiguously in $A(V)$. It only remains to be proved that $\varphi \in \mathscr{C}^{\prime}$.  Fix $\mathbf{y}_{0} \in A(V)$, fix $\mathbf{x}_{0} \in V$ so that $A \mathbf{x}_{0}=\mathbf{y}_{0}$. Since $V$ is open, $\mathbf{y}_{0}$ has a neighborhood $W$ in $Y_{1}$ such that the vector  $$ \mathbf{x}=\mathbf{x}_{0}+S\left(\mathbf{y}-\mathbf{y}_{0}\right) $$  lies in $V$ for all $\mathbf{y} \in W$. By (68),  $$ A \mathbf{x}=A \mathbf{x}_{0}+\mathbf{y}-\mathbf{y}_{0}=\mathbf{y} $$  Thus (73) and (79) give  $$ \varphi(\mathbf{y})=\psi\left(\mathbf{x}_{0}-S \mathbf{y}_{0}+S \mathbf{y}\right) \quad(\mathbf{y} \in W) $$  This formula shows that $\varphi \in \mathscr{C}^{\prime}$ in $W$, hence in $A(V)$, since $\mathrm{y}_{0}$ was chosen arbitrarily in $A(V)$.  The proof is now complete.  Here is what the theorem tells us about the geometry of the mapping $\mathbf{F}$.  If $\mathbf{y} \in \mathbf{F}(U)$ then $\mathbf{y}=\mathbf{F}(\mathbf{H}(\mathbf{x})$ ) for some $\mathbf{x} \in V$, and (66) shows that $P \mathbf{y}=A \mathbf{x}$. Therefore  $$ \mathbf{y}=P \mathbf{y}+\varphi(P \mathbf{y}) \quad(\mathbf{y} \in \mathbf{F}(U)) $$  This shows that $\mathbf{y}$ is determined by its projection $P \mathbf{y}$, and that $P$, restricted to $\mathbf{F}(U)$, is a 1-1 mapping of $\mathbf{F}(U)$ onto $A(V)$. Thus $\mathbf{F}(U)$ is an "" $r$-dimensional surface"" with precisely one point ""over"" each point of $A(V)$. We may also regard $\mathbf{F}(U)$ as the graph of $\varphi$.  If $\Phi(\mathbf{x})=\mathbf{F}(\mathbf{H}(\mathbf{x}))$, as in the proof, then (66) shows that the level sets of $\Phi$ (these are the sets on which $\Phi$ attains a given value) are precisely the level sets of $A$ in $V$. These are ""flat"" since they are intersections with $V$ of translates of the vector space $\mathcal{N}(A)$. Note that $\operatorname{dim} \mathcal{N}(A)=n-r$ (Exercise 25).  The level sets of $\mathbf{F}$ in $U$ are the images under $\mathbf{H}$ of the flat level sets of $\Phi$ in $V$. They are thus "" $(n-r)$-dimensional surfaces"" in $U$.",;  ;  ; 
9.34 (a),"If I is the identity operator on $R^{n}$, then

$$
\operatorname{det}[I]=\operatorname{det}(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n})=1
$$","If $A=I$, then $a(i, i)=1$ and $a(i, j)=0$ for $i \neq j$. Hence  $$ \operatorname{det}[I]=s(1,2, \ldots, n)=1 \text {, } $$  which proves $(a)$. By $(82), s\left(j_{1}, \ldots, j_{n}\right)=0$ if any two of the $j$ 's are equal. Each of the remaining $n$ ! products in (83) contains exactly one factor from each column. This proves (b). Part $(c)$ is an immediate consequence of the fact that $s\left(j_{1}, \ldots, j_{n}\right)$ changes sign if any two of the $j$ 's are interchanged, and $(d)$ is a corollary of $(c)$.",
9.34 (b),"det is a linear function of each of the column vectors $\mathbf{x}_{j}$, if the others are held fixed.","By $(82), s\left(j_{1}, \ldots, j_{n}\right)=0$ if any two of the $j$ 's are equal. Each of the remaining $n$ ! products in (83) contains exactly one factor from each column. This proves (b). Part $(c)$ is an immediate consequence of the fact that $s\left(j_{1}, \ldots, j_{n}\right)$ changes sign if any two of the $j$ 's are interchanged, and $(d)$ is a corollary of $(c)$.",
9.34 (c),"If $[A]_{1}$ is obtained from $[A]$ by interchanging two columns, then $\operatorname{det}[A]_{1}=-\operatorname{det}[A]$.","Part $(c)$ is an immediate consequence of the fact that $s\left(j_{1}, \ldots, j_{n}\right)$ changes sign if any two of the $j$ 's are interchanged, and $(d)$ is a corollary of $(c)$.",
9.34 (d),"If $[A]$ has two equal columns, then $\operatorname{det}[A]=0$.",$(d)$ is a corollary of $(c)$.,
9.35,"If $[A]$ and $[B]$ are $n$ by $n$ matrices, then

$$
\operatorname{det}([B][A])=\operatorname{det}[B] \operatorname{det}[A] .
$$
","If $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$ are the columns of $[A]$, define  $$ \Delta_{B}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\Delta_{B}[A]=\operatorname{det}([B][A]) $$  The columns of $[B][A]$ are the vectors $B \mathbf{x}_{1}, \ldots, B \mathbf{x}_{n}$. Thus  $$ \Delta_{B}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\operatorname{det}\left(B \mathbf{x}_{1}, \ldots, B \mathbf{x}_{n}\right) $$  By (86) and Theorem 9.34, $\Delta_{B}$ also has properties $9.34(b)$ to $(d)$. By $(b)$ and (84),  $$ \Delta_{B}[A]=\Delta_{B}\left(\sum_{i} a(i, 1) \mathbf{e}_{i}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\right)=\sum_{i} a(i, 1) \Delta_{B}\left(\mathbf{e}_{i}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\right) $$  Repeating this process with $\mathbf{x}_{2}, \ldots, \mathbf{x}_{n}$, we obtain  $$ \Delta_{B}[A]=\sum a\left(i_{1}, 1\right) a\left(i_{2}, 2\right) \cdots a\left(i_{n}, n\right) \Delta_{B}\left(\mathbf{e}_{i_{1}}, \ldots, \mathbf{e}_{i_{n}}\right) $$  the sum being extended over all ordered $n$-tuples $\left(i_{1}, \ldots, i_{n}\right)$ with $1 \leq i_{r} \leq n$. By $(c)$ and $(d)$,  $$ \Delta_{B}\left(\mathbf{e}_{i_{1}}, \ldots, \mathbf{e}_{i_{n}}\right)=t\left(i_{1}, \ldots, i_{n}\right) \Delta_{B}\left(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right) $$  where $t=1,0$, or -1 , and since $[B][I]=[B],(85)$ shows that  $$ \Delta_{B}\left(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right)=\operatorname{det}[B] $$  Substituting (89) and (88) into (87), we obtain  $$ \operatorname{det}([B][A])=\left\{\sum a\left(i_{1}, 1\right) \cdots a\left(i_{n}, n\right) t\left(i_{1}, \ldots, i_{n}\right)\right\} \operatorname{det}[B] $$  for all $n$ by $n$ matrices $[A]$ and $[B]$. Taking $B=I$, we see that the above sum in braces is det $[A]$. This proves the theorem.","If I is the identity operator on $R^{n}$, then

$$
\operatorname{det}[I]=\operatorname{det}(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n})=1
$$;det is a linear function of each of the column vectors $\mathbf{x}_{j}$, if the others are held fixed.;If $[A]_{1}$ is obtained from $[A]$ by interchanging two columns, then $\operatorname{det}[A]_{1}=-\operatorname{det}[A]$.;If $[A]$ has two equal columns, then $\operatorname{det}[A]=0$.;  ;  ; "
9.36,A linear operator $A$ on $R^{n}$ is invertible if and only if $\operatorname{det}[A] \neq 0$.,"If $A$ is invertible, Theorem 9.35 shows that  $$ \operatorname{det}[A] \operatorname{det}\left[A^{-1}\right]=\operatorname{det}\left[A A^{-1}\right]=\operatorname{det}[I]=1, $$  so that $\operatorname{det}[A] \neq 0$.  If $A$ is not invertible, the columns $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$ of $[A]$ are dependent (Theorem 9.5); hence there is one, say, $\mathbf{x}_{k}$, such that  $$ \mathbf{x}_{k}+\sum_{j \neq k} c_{j} \mathbf{x}_{j}=0 $$  for certain scalars $c_{j}$. By $9.34(b)$ and $(d), \mathbf{x}_{k}$ can be replaced by $\mathbf{x}_{k}+c_{j} \mathbf{x}_{j}$ without altering the determinant, if $j \neq k$. Repeating, we see that $\mathbf{x}_{k}$ can be replaced by the left side of ( 90 ), i.e., by 0 , without altering the determinant. But a matrix which has $\mathbf{0}$ for one column has determinant 0 . Hence $\operatorname{det}[A]=0$.","If $[A]$ and $[B]$ are $n$ by $n$ matrices, then

$$
\operatorname{det}([B][A])=\operatorname{det}[B] \operatorname{det}[A] .
$$
; A linear operator $A$ on a finite-dimensional vector space $X$ is one-to-one if and only if the range of $A$ is all of $X$.;  ;  ; "
9.40,"Suppose $f$ is defined in an open set $E \subset R^{2}$, and $D_{1} f$ and $D_{21} f$ exist at every point of $E$. Suppose $Q \subset E$ is a closed rectangle with sides parallel to the coordinate axes, having $(a, b)$ and $(a+h, b+k)$ as opposite vertices $(h \neq 0, k \neq 0)$. Put

$$
\Delta(f, Q)=f(a+h, b+k)-f(a+h, b)-f(a, b+k)+f(a, b)
$$

Then there is a point $(x, y)$ in the interior of $Q$ such that

$$
\Delta(f, Q)=h k\left(D_{21} f\right)(x, y)
$$","Put $u(t)=f(t, b+k)-f(t, b)$. Two applications of Theorem 5.10 show that there is an $x$ between $a$ and $a+h$, and that there is a $y$ between $b$ and $b+k$, such that  $$ \begin{aligned} \Delta(f, Q) & =u(a+h)-u(a) \\ & =h u^{\prime}(x) \\ & =h\left[\left(D_{1} f\right)(x, b+k)-\left(D_{1} f\right)(x, b)\right] \\ & =h k\left(D_{21} f\right)(x, y) . \end{aligned} $$","If $f$ is a real continuous function on $[a, b]$ which is differentiable in $(a, b)$, then there is a point $x \in(a, b)$ at which

$$
f(b)-f(a)=(b-a) f^{\prime}(x) .
$$

;  ;  ; "
9.41,"Suppose $f$ is defined in an open set $E \subset R^{2}$, suppose that $D_{1} f$, $D_{21} f$, and $D_{2} f$ exist at every point of $E$, and $D_{21} f$ is continuous at some point $(a, b) \in E$.

Then $D_{12} f$ exists at $(a, b)$ and

$$
\left(D_{12} f\right)(a, b)=\left(D_{21} f\right)(a, b)
$$","Put $A=\left(D_{21} f\right)(a, b)$. Choose $\varepsilon>0$. If $Q$ is a rectangle as in Theorem 9.40, and if $h$ and $k$ are sufficiently small, we have  $$ \left|A-\left(D_{21} f\right)(x, y)\right|<\varepsilon $$  for all $(x, y) \in Q$. Thus  $$ \left|\frac{\Delta(f, Q)}{h k}-A\right|<\varepsilon $$  by (95). Fix $h$, and let $k \rightarrow 0$. Since $D_{2} f$ exists in $E$, the last inequality implies that  $$ \left|\frac{\left(D_{2} f\right)(a+h, b)-\left(D_{2} f\right)(a, b)}{h}-A\right| \leq \varepsilon \text {. } $$  Since $\varepsilon$ was arbitrary, and since (97) holds for all sufficiently small $h \neq 0$, it follows that $\left(D_{12} f\right)(a, b)=A$. This gives (96).","Suppose $f$ is defined in an open set $E \subset R^{2}$, and $D_{1} f$ and $D_{21} f$ exist at every point of $E$. Suppose $Q \subset E$ is a closed rectangle with sides parallel to the coordinate axes, having $(a, b)$ and $(a+h, b+k)$ as opposite vertices $(h \neq 0, k \neq 0)$. Put

$$
\Delta(f, Q)=f(a+h, b+k)-f(a+h, b)-f(a, b+k)+f(a, b)
$$

Then there is a point $(x, y)$ in the interior of $Q$ such that

$$
\Delta(f, Q)=h k\left(D_{21} f\right)(x, y)
$$;  ;  ; "
9.5,A linear operator $A$ on a finite-dimensional vector space $X$ is one-to-one if and only if the range of $A$ is all of $X$.,"Let $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}$ be a basis of $X$. The linearity of $A$ shows that its range $\mathscr{R}(A)$ is the span of the set $Q=\left\{A \mathbf{x}_{1}, \ldots, A \mathbf{x}_{n}\right\}$. We therefore infer from Theorem 9.3 $(a)$ that $\mathscr{R}(A)=X$ if and only if $Q$ is independent. We have to prove that this happens if and only if $A$ is one-to-one.  Suppose $A$ is one-to-one and $\Sigma c_{i} A \mathbf{x}_{i}=\mathbf{0}$. Then $A\left(\Sigma c_{i} \mathbf{x}_{i}\right)=\mathbf{0}$, hence $\Sigma c_{i} \mathbf{x}_{i}=\mathbf{0}$, hence $c_{1}=\cdots=c_{n}=0$, and we conclude that $Q$ is independent. Conversely, suppose $Q$ is independent and $A\left(\Sigma c_{i} \mathbf{x}_{i}\right)=\mathbf{0}$. Then $\Sigma c_{i} A \mathbf{x}_{i}=\mathbf{0}$, hence $c_{1}=\cdots=c_{n}=0$, and we conclude: $A \mathbf{x}=\mathbf{0}$ only if $\mathbf{x}=\mathbf{0}$. If now $A \mathbf{x}=A \mathbf{y}$, then $A(\mathbf{x}-\mathbf{y})=\boldsymbol{A} \mathbf{x}-\boldsymbol{A} \mathbf{y}=\mathbf{0}$, so that $\mathbf{x}-\mathbf{y}=\mathbf{0}$, and this says that $A$ is one-to-one.","Suppose $X$ is a vector space, and $\operatorname{dim} X=n$: A set $E$ of $n$ vectors in $X$ spans $X$ if and only if $E$ is independent.;Suppose $X$ is a vector space, and $\operatorname{dim} X=n$: $X$ has a basis, and every basis consists of $n$ vectors.;Suppose $X$ is a vector space, and $\operatorname{dim} X=n$, and $1 \leq r \leq n$ and $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$ is an independent set in $X$: then $X$ has $a$ basis containing $\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{r}\right\}$.;Suppose $m, n, r$ are nonnegative integers, $m \geq r, n \geq r, \mathbf{F}$ is a $\mathscr{C}^{\prime}$-mapping of an open set $E \subset R^{n}$ into $R^{m}$, and $\mathbf{F}^{\prime}(\mathbf{x})$ has rank $r$ for every $\mathbf{x} \in E$. Fix $\mathbf{a} \in E$, put $A=\mathbf{F}^{\prime}(\mathbf{a})$, let $Y_{1}$ be the range of $A$, and let $P$ be a projection in $R^{m}$ whose range is $Y_{1}$. Let $Y_{2}$ be the null space of $P$. Then there are open sets $U$ and $V$ in $R^{n}$, with $\mathbf{a} \in U, U \subset E$, and there is a 1-1 $\mathscr{C}^{\prime}$-mapping $\mathbf{H}$ of $V$ onto $U$ (whose inverse is also of class $\mathscr{C}^{\prime}$ ) such that

$$
\mathbf{F}(\mathbf{H}(\mathbf{x}))=A \mathbf{x}+\varphi(A \mathbf{x}) \quad(\mathbf{x} \in V)
$$

where $\varphi$ is a $\mathscr{C}^{\prime}$-mapping of the open set $A(V) \subset Y_{1}$ into $Y_{2}$.;If I is the identity operator on $R^{n}$, then

$$
\operatorname{det}[I]=\operatorname{det}(\mathbf{e}_{1}, \ldots, \mathbf{e}_{n})=1
$$;det is a linear function of each of the column vectors $\mathbf{x}_{j}$, if the others are held fixed.;If $[A]_{1}$ is obtained from $[A]$ by interchanging two columns, then $\operatorname{det}[A]_{1}=-\operatorname{det}[A]$.;If $[A]$ has two equal columns, then $\operatorname{det}[A]=0$.;If $[A]$ and $[B]$ are $n$ by $n$ matrices, then

$$
\operatorname{det}([B][A])=\operatorname{det}[B] \operatorname{det}[A] .
$$
;A linear operator $A$ on $R^{n}$ is invertible if and only if $\operatorname{det}[A] \neq 0$.;  ;  ; "
9.7 (a),"If $A \in L\left(R^{n}, R^{m}\right)$, then $\|A\|<\infty$ and $A$ is a uniformly continuous mapping of $R^{n}$ into $R^{m}$.","Let $\left\{\mathbf{e}_{1}, \ldots, \mathbf{e}_{n}\right\}$ be the standard basis in $R^{n}$ and suppose $\mathbf{x}=\Sigma c_{i} \mathbf{e}_{i}$, $|\mathbf{x}| \leq 1$, so that $\left|c_{i}\right| \leq 1$ for $i=1, \ldots, n$. Then  $$ |A \mathbf{x}|=\left|\sum c_{i} A \mathbf{e}_{i}\right| \leq \sum\left|c_{i}\right|\left|A \mathbf{e}_{i}\right| \leq \sum\left|A \mathbf{e}_{i}\right| $$  so that  $$ \|A\| \leq \sum_{i=1}^{n}\left|A \mathbf{e}_{i}\right|<\infty $$  Since $|A \mathbf{x}-A \mathbf{y}| \leq\|A\||\mathbf{x}-\mathbf{y}|$ if $\mathbf{x}, \mathbf{y} \in R^{n}$, we see that $A$ is uniformly continuous.",
9.7 (b),"If $A, B \in L\left(R^{n}, R^{m}\right)$ and c is a scalar, then

$$
\|A+B\| \leq\|A\|+\|B\|, \quad\|c A\|=|c|\|A\| .
$$

With the distance between $A$ and $B$ defined as $\|A-B\|, L\left(R^{n}, R^{m}\right)$ is a metric space.","The inequality in (b) follows from  $$ |(A+B) \mathbf{x}|=|A \mathbf{x}+B \mathbf{x}| \leq|A \mathbf{x}|+|B \mathbf{x}| \leq(\|A\|+\|B\|)|\mathbf{x}| $$  The second part of $(b)$ is proved in the same manner. If  $$ A, B, C \in L\left(R^{n}, R^{m}\right) $$  we have the triangle inequality  $$ \|A-C\|=\|(A-B)+(B-C)\| \leq\|A-B\|+\|B-C\|, $$  and it is easily verified that $\|A-B\|$ has the other properties of a metric (Definition 2.15).",
9.7 (c),"If $A \in L\left(R^{n}, R^{m}\right)$ and $B \in L\left(R^{m}, R^{k}\right)$, then

$$
\|B A\| \leq\|B\|\|A\| .
$$","Finally, (c) follows from  $$ |(B A) \mathbf{x}|=|B(A \mathbf{x})| \leq\|B\||A \mathbf{x}| \leq\|B\|\|A\||\mathbf{x}| . $$",
9.8 (a),"Let $\Omega$ be the set of all invertible linear operators on $R^{n}$. If $A \in \Omega, B \in L\left(R^{n}\right)$, and

$$
\|B-A\| \cdot\left\|A^{-1}\right\|<1
$$

then $B \in \Omega$.","Put $\left\|A^{-1}\right\|=1 / \alpha$, put $\|B-A\|=\beta$. Then $\beta<\alpha$. For every $\mathbf{x} \in R^{n}$,  $$ \begin{aligned} \alpha|\mathbf{x}| & =\alpha\left|A^{-1} A \mathbf{x}\right| \leq \alpha\left\|A^{-1}\right\| \cdot|A \mathbf{x}| \\ & =|A \mathbf{x}| \leq|(A-B) \mathbf{x}|+|B \mathbf{x}| \leq \beta|\mathbf{x}|+|B \mathbf{x}|, \end{aligned} $$  so that  $$ (\alpha-\beta)|\mathbf{x}| \leq|B \mathbf{x}| \quad(\mathbf{x} \in R^{n}) $$  Since $\alpha-\beta>0$, (1) shows that $B \mathbf{x} \neq 0$ if $\mathbf{x} \neq 0$. Hence $B$ is $1-1$. By Theorem 9.5, $B \in \Omega$. This holds for all $B$ with $\|B-A\|<\alpha$. Thus we have $(a)$ and the fact that $\Omega$ is open.",A linear operator $A$ on a finite-dimensional vector space $X$ is one-to-one if and only if the range of $A$ is all of $X$.;  ;  ; 
9.8 (b),"$\Omega$ is an open subset of $L\left(R^{n}\right)$, and the mapping $A \rightarrow A^{-1}$ is continuous on $\Omega$.","Next, replace $\mathbf{x}$ by $B^{-1} \mathbf{y}$ in (1). The resulting inequality  $$ (\alpha-\beta)\left|B^{-1} \mathbf{y}\right| \leq\left|B B^{-1} \mathbf{y}\right|=|\mathbf{y}| \quad(\mathbf{y} \in R^{n}) $$  shows that $\left\|B^{-1}\right\| \leq(\alpha-\beta)^{-1}$. The identity  $$ B^{-1}-A^{-1}=B^{-1}(A-B) A^{-1} $$  combined with Theorem 9.7(c), implies therefore that  $$ \left\|B^{-1}-A^{-1}\right\| \leq\left\|B^{-1}\right\|\|A-B\|\left\|A^{-1}\right\| \leq \frac{\beta}{\alpha(\alpha-\beta)} $$  This establishes the continuity assertion made in $(b)$, since $\beta \rightarrow 0$ as $B \rightarrow A$.","If $A \in L\left(R^{n}, R^{m}\right)$, then $\|A\|<\infty$ and $A$ is a uniformly continuous mapping of $R^{n}$ into $R^{m}$.;If $A, B \in L\left(R^{n}, R^{m}\right)$ and c is a scalar, then

$$
\|A+B\| \leq\|A\|+\|B\|, \quad\|c A\|=|c|\|A\| .
$$

With the distance between $A$ and $B$ defined as $\|A-B\|, L\left(R^{n}, R^{m}\right)$ is a metric space.;If $A \in L\left(R^{n}, R^{m}\right)$ and $B \in L\left(R^{m}, R^{k}\right)$, then

$$
\|B A\| \leq\|B\|\|A\| .
$$;  ;  ; "
